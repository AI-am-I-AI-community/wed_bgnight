---
title: "150本超のLLM資料から紐解く、プロンプトの効果を高める21の性質"
source: "https://ai-data-base.com/archives/91072"
author:
  - "[[AIDB Research]]"
published: 2025-06-18
created: 2025-07-09
description: "本記事では、150本を超える資料をメタ分析して明らかになったLLMプロンプトの21の性質を紹介します。"
tags:
  - "clippings"
---
Loading \[MathJax\]/extensions/MathZoom.js

本記事では、150本を超える資料をメタ分析して明らかになったLLMプロンプトの21の性質を紹介します。

モデル性能を引き出す“プロンプト設計”への関心が高まる一方で、どの要素が有効なのか、どのように組み合わせるべきかは試行錯誤の連続です。

そこで今回紹介する研究では、実験・相関分析・事例を横断的に整理し、実務で活かせるプロンプト改善のヒントを提供しています。

この記事を通じて、自分のユースケースに合わせた“効く”プロンプト作りのイメージをつかんでいただければ幸いです。

![](https://ai-data-base.com/wp-content/uploads/2025/06/AIDB_91072-1024x576.png)

## 背景

プロンプトの書き方しだいで、LLMの出力内容は変わります。  
そんなプロンプトの良し悪しを評価する際には、実際の出力結果だけを見て判断する方法が使われがちです。たとえば、どれだけ正答率が上がったか、タスクのスコアが改善したかといった「結果ベース」の観点です。しかしこの方法には課題があります。出力の最適化ばかりが進むと、なぜそのプロンプトが効果的だったのかがぼやけてしまい、解釈や再利用が難しくなるといった問題が出てきます。

これは単なる技術的な話にとどまりません。LLMのふるまいを人間が理解できないままでは、信頼性や安全性の面でも不安が残ります。誤ったプロンプト設計が、意図せぬ出力や不適切な挙動を招くおそれもあるためです。

そのような懸念もあり、ユーザーの間ではトップダウンの実践的な工夫が増えています。OpenAIやAnthropicのような企業の資料でプロンプトのベストプラクティスが解説される例も増えており、プロンプトを「結果」ではなく「性質」から見直そうとする動きが広がりつつあります。

とはいえ、いまだにプロンプトの実践的な工夫の視点は体系的に整理しきれていません。どんなプロンプト戦略が有効なのか理解するための手がかりが不足しています。

本記事では、そうしたギャップを埋めるために、150本以上の論文を横断的に読み解いた事例を紹介します。プロンプトの性質に着目した新しい評価の考え方を構築しようという試みです。人間の視点から見て理解しやすく、再利用や比較がしやすいプロンプトのあり方を探るための取り組みです。

## これまでどのようにプロンプトが研究されてきたか

プロンプトの効果を明らかにしようとする既存の取り組みは、大きく2つの方向に分かれています。

ひとつは、プロンプトそのものの書き方に注目する方向。たとえば、”「この問題を解いてください」と書くのと、「ステップバイステップで解いてください」と書くのとで、LLMの回答の精度が変わることがある”などの現象に着目し、言い回しや構成の違いがどのように結果に影響するのかを分析するといった取り組みです。よく使われる表現がどのくらい登場しているかを調べたり、書式の工夫がどれほど効果をもたらすかを測ったりといった方向です。

もうひとつは、実際にさまざまなプロンプトを試してみて、何が有効かを探る方向です。いわば、手を動かしてプロンプトの工夫を実証するスタイルです。ただし、特定の状況に限って調べられていることが多く、どこまで一般化できるかが課題になります。

### 性能の最適化だけでは見えないもの

プロンプトエンジニアリングという言葉も浸透してきましたが、この分野はもともと、タスクの精度を高めるための最適なプロンプトを見つけることに力点が置かれてきました。平たく言えば、スコアを上げることが主な関心だったわけです。

ところが最近では、プロンプトの性質そのものに目を向ける流れが出てきています。たとえば、以下のような観点が注目されています。

- わかりやすさ（明確性）
- 丁寧な語り口（丁寧さ）
- 書式の整理（構造化）
- 出力の偏りを抑える工夫（公平性）

ただし、まだ疑問は多く残されています。こうした工夫はどんなモデルでも有効なのか、あるいはタスクによって使い分けが必要なのか。そして、複数の性質を組み合わせたときにどう作用するのか、といった点はあまり明らかになっていません。

### ばらばらだった知見を整理する試みが必要

これまでの調査は、特定のプロンプト表現や特定のタスクに焦点を当てたものが中心でした。実際の活用を考えるうえでは、もっと整理された視点がほしいところです。

そこで今回研究者らは、膨大な調査資料をもとにプロンプトに関するさまざまな性質を統一的に捉え直し、それらの影響をモデルやタスクを横断して検討しました。ばらばらに語られてきた知見を可能な限り束ね、実務で参考にしやすい形へと整理しようとする試みです。

## 6つの評価軸、21の性質

プロンプトの工夫の良し悪しをどう判断すべきか整理するために、150本以上の論文や技術ブログをもとに、評価のフレームワークが作られました。

調査の進め方はシンプルでした。2022年から2025年までの主要な国際会議（ACL、EM [NLP](https://ai-data-base.com/archives/26319 "自然言語処理（NLP）") 、NAACL、ICLR、NeurIPSなど）で発表された論文を中心に、関連資料を幅広く収集し、それぞれの研究がどのような観点でプロンプトを評価しているかを読み解きました。

そして、抽出された特徴を「プロンプトの性質」として整理し、21項目に分類。それらが6つの評価軸にまとめられました。以下で中身を説明します。

### 言語や対話の観点

まず最初は、人とAIとのやりとりを「コミュニケーション」として捉える視点です。

**①必要な情報が過不足なく含まれているか**  
情報が少なすぎても多すぎても、LLMの理解に支障が出ることがあります。情報量のバランスが適切かが問われます。

② **表現が明確かどうか**  
曖昧な言い回しや不必要に複雑な表現がないかを見ます。伝えたい内容がストレートに書かれているかがポイントです。

③ **双方向のやりとりを引き出しているか**  
LLMが一方的に答えるだけでなく、必要な確認や補足を求められる設計になっているかを評価します。

**④語り口が丁寧か**  
敬語や「お願いします」といった丁寧な表現が使われていると、モデルの出力に良い影響があるとする研究もあります。

### LLMが処理しやすいプロンプトかどうか

次は、LLMがプロンプトを処理する際の「認知的な負荷」に着目した観点です。人間の学習理論を応用し、以下の3点が評価対象になります。

⑤ **タスクが適切なステップに分解されているか**  
複雑な作業を小さな手順に分けることで、LLMが処理しやすくなります。

**⑥不要な情報が排除されているか**  
本筋と関係のない説明や長すぎる表現が含まれていないかを見ます。

**⑦モデルが自力で考える余地があるか**  
与えられた知識だけでなく、過去の経験や一般知識を活用できるよう設計されているかがポイントです。

### 教育的な視点

続いて、プロンプトが「わかりやすく教える」ことを意識して設計されているかどうかを見ていきます。

**⑧目的や制約が明示されているか**  
出力形式や文字数、対象読者などがはっきり示されているかを評価します。

**⑨外部の情報源やツールの活用を指示しているか**  
必要な場面で計算や検索を促すような指示が含まれているかが問われます。

**⑩出力の自己チェックを促しているか**  
「確認してください」など、自分の出力を見直すよう促す表現があるかを見ます。

⑪ **具体例が添えられているか**  
抽象的な説明だけでなく、具体例や反例があることでLLMの理解が深まります。

⑫ **出力に報酬やフィードバックが設定されているか**  
正しい答えに報酬を設定することで、より適切な出力が期待できるという研究もあります。

### 論理の一貫性や構造の明確さ

プロンプトが論理的に整理されているかどうかを評価します。

**⑬構造が明確であるか**  
情報の提示順序が自然で、読みやすい構成になっているかが問われます。

**⑭内容に矛盾がないか**  
複数の指示や前提が食い違っていないか、やりとり全体を通じて一貫性が保たれているかを確認します。

### ハルシネーションを防ぐための工夫

AIが事実と異なることを出力してしまう「ハルシネーション」にどう対処するかも、評価の観点に含まれます。

⑮ **事実に基づく出力を促しているか**  
推測や根拠のない断言を避けるような設計になっているかを見ます。

**⑯事実性と創造性のバランスが取れているか**  
創造的な出力が求められる場合は、事実と区別されているかがポイントです。例えば「事実から分かることと、想像したことを分けて出力してください」など。

### 倫理や社会的責任への配慮

最後に、AIを安全かつ公平に使うための配慮がなされているかを評価します。

⑰ **偏見を含まないように設計されているか**  
特定の文化や属性に偏った表現を避ける工夫があるかを見ます。

**⑱安全な出力が促されているか**  
有害な内容を生成しないよう明示されているかを確認します。

**⑲プライバシーに配慮しているか**  
個人情報やセンシティブな情報が含まれないようになっているかを評価します。

**⑳根拠や限界の明示が促されているか**  
推論の根拠やモデルの限界が意識されているかを見ます。つまり、限界はあって然るべきであるという考え方です。

㉑ **社会的な価値観に配慮されているか**  
広く受け入れられている倫理観や文化的基準（要するに常識的な価値観）に沿った出力が期待されるようになっているかが焦点です。

## 実際のところ、どのくらい効くのか

21の性質を整理したところで、気になるのは「それぞれの工夫が本当に効果を発揮するのか」という点です。研究者らは、各性質がどのタスクやモデルに対して効果的かを改めて分析しました。

調査対象となったタスクは、大きく6つのカテゴリです。

1. ユーザーとの自然な対話（AlpacaEvalやShareGPTなど）
2. 複数の課題を網羅した評価ベンチマーク（MMLUやC-Evalなど）
3. 推論や質問応答（GSM8KやHotpotQAなど）
4. テキスト生成（要約や翻訳など）
5. 自然言語理解（GLUEやCommitmentBankなど）
6. 安全性向上や検索などの用途に特化したもの

分析では、性質ごとに「どの程度の研究がその効果を裏付けているか」「どんなタスクに有効とされているか」「どのモデルで確認されているか」という3つの観点から整理が行われました。

### タスクによって重視される性質が違う

分析結果を見ると、タスクの性質によって求められるプロンプトの性質に傾向があることが分かりました。

たとえば実際のユーザー対話では、コミュニケーション関連の性質がとくに重視されています。実際のやりとりでは、情報が分かりやすく整理されているか、丁寧な語りかけができているかといった点が効果に直結します。さらに、指導や認知負荷に関する工夫も役立つことが多いとされます。

ベンチマークや推論タスクでは、思考を整理するための認知負荷の管理や、明確な指示を出すための指導の工夫が効果的とされます。論理的なプロンプト構成も重要な役割を果たすことが示されました。

テキスト生成タスクでも、やはりコミュニケーション関連の工夫が中心ですが、興味深いのは「丁寧な語りかけ」が精度向上に寄与している可能性がある点です。丁寧なプロンプトに対して、LLMがより良い応答を返す傾向があるという観察が複数の研究から報告されています。

自然言語理解タスクでは、指導や認知的なサポートが重視されています。表面的な理解にとどまらず、もう一歩深い解釈や推論を促すための工夫が有効と考えられています。

一方、安全性向上や検索などの特殊なタスクでは、それぞれの目的に応じて必要な性質が異なります。たとえば安全性を高めるためには安全配慮の明示が、検索タスクでは情報整理の工夫が重要視されています。

![](https://ai-data-base.com/wp-content/uploads/2025/06/AIDB_91072_1-1024x495.png)

6つのカテゴリそれぞれに対応するプロンプトの性質とモデル

### モデルごとに研究の偏りがある

どのモデルで検証されているかを見ると、研究対象には大きな偏りがあることも分かりました。

OpenAIの商用モデルが最も多く使われており、ついでLLaMAシリーズやGoogle系のモデルが続いています。

この偏りが示すのは、ある性質が「一部のモデルでは効果的だった」としても、それが他のモデルでも同様に機能するとは限らないという点です。モデル間の違いによる効果のばらつきが、今後の重要な論点になりそうです。

モデルの基礎とするアルゴリズムが同様あるいは類似している場合、効果的なプロンプトもある程度共通してくるのが前提としてあり、その上で今後はモデルごとの差を正確に分析できるとよいということです。

### 幅広く使える工夫と、場面を選ぶ工夫

今回整理された21の性質は、すべてが等しく汎用というわけではなく、性質ごとに適用範囲の広さに違いがあることも確認されました。

たとえば、例の提供、外部ツールの活用などは、ほとんどのタスクで効果が確認されており、比較的どんな場面でも有効な性質といえそうです。LLMの苦手なタスク構造化を補ったり、理解の助けとなる材料を与えたり、処理の一部を外部で行う工夫です。

一方で、ハルシネーション対策や倫理・責任に関する性質は、どのタスクでも必要というわけではなく、特定の用途や設計方針に応じて選ぶ必要があります。

### まだ見えていないことも多い

今回の分析では、今後の研究課題もいくつか見えてきました。

まず、性質ごとの効果が（繰り返しますが、）モデル間でどう異なるかについては、まだ十分に検証されていません。モデルごとに前提知識や動作傾向が異なるため、同じ工夫でも効果が変わる可能性があります。

また、研究が足りていない性質もいくつかあります。たとえば「生成に関する負荷」「メタ認知」「責任」に関する性質は、推論や自然言語理解といったタスクで重要なはずにもかかわらず、研究量は少なめです。

さらに、創造性の効果についても十分に整理されていません。直感的には生成タスクにおいて重要そうですが、明確な検証結果は少なく、手探りの段階です。

最後に、タスクと直接関係なさそうに見える性質が、なぜ効果を発揮するのかといった点や、汎用性と特化性のどちらを重視すべきかといった実務的な疑問も残されています。

こうした課題を解き明かしていくことで、プロンプト設計はさらに洗練され、LLMの信頼性や使いやすさも高まっていくはずです。多様なモデルとタスクをまたいだ比較実験や、複数の性質を組み合わせる評価手法の構築など、より実践的な調査が求められます。

## 良いプロンプトには、どんな性質が組み合わさっているのか

21の性質をひとつずつ確認してきましたが、現実のプロンプトはそれらを単独で使っているわけではありません。実際にうまく機能しているプロンプトでは、複数の性質がうまく組み合わされていると考えられます。

そこで研究者らは、実際に「良質」とされるプロンプトを対象に、どの性質が一緒に現れやすいのかを調べました。組み合わせの傾向から、より実践的な設計のヒントを引き出そうとしたわけです。

### 調査方法

#### 調査に使われたプロンプトの中身

対象となったのは、全部で969件のプロンプトです。プロンプトエンジニアリング関連の論文から765件を抽出し、さらにChatGPT向けのプロンプト集や各種データセット（Alpaca、Natural Instructions、Complex Instructionsなど）からも収集が行われました。LMSYS-Chat-1Mから50件の実際のマルチターン会話を取り出し、その中で使われていた204件のプロンプトも含まれています。

いずれも、人間が「これは良いプロンプトだ」と判断したものだけを集めた点がポイントです。つまり、現場で評価されているプロンプトに、どんな性質の組み合わせが見られるのかが調べられました。

#### AIによる評価の工夫

これらのプロンプトを21の性質ごとに分類する際には、GPT-4oが用いられました。

ただし、AIによる自動判定だけに頼るのはリスクがあるため、まず研究者3名が50件分を人手で評価。その結果とAIの出力を比較しながら、GPT-4oで評価する用のプロンプトが作られました。

最初は10点満点のスコア方式を試したものの、評価のブレが大きく、精度が上がりませんでした。そこで段階的な評価基準を導入し、「明示的な指示が含まれているか」に注目するよう設計を見直したところ、人間との一致度が大きく改善されました。

こうした工夫によって、一定の信頼性を担保したうえでの分析が行われたかたちです。

### 性質同士の関係から見えてきたこと

分析の結果、21の性質のうち17組について、 [相関係数](https://ai-data-base.com/archives/26481 "相関係数") 0.7以上の強い関係が確認されました。中には予想しやすい組み合わせもあれば、やや意外に思えるような組み合わせも含まれていました。

![](https://ai-data-base.com/wp-content/uploads/2025/06/AIDB_91072_2-1024x381.png)

まず、予想しやすかった相関には、次のようなものがありました。

たとえば、情報量の適切さや表現の明確さ、論理的な構成といった性質同士は、よく一緒に現れていました。これらはすべて、読みやすく伝わりやすいプロンプトに共通する要素です。情報を簡潔にまとめようとすれば、自然と表現も明瞭になり、文章構造も整理されるという関係が見えてきます。

構造の論理性と文脈の論理性も強く関連していました。プロンプト内での情報の並べ方が整っていると、話の流れや意味のつながりも自然に一貫しやすくなるという傾向です。

さらに、ハルシネーションを抑えるための指示があるプロンプトでは、事実と創造的な要素を明確に分けるような設計もよく見られました。事実性と創造性のバランスを意識すると、結果として誤情報の発生を防ぎやすくなるということです。

安全性を重視したプロンプトと、社会的な常識や配慮を求めるプロンプトのあいだにも強い相関がありました。どちらもAIの出力に対して「これは出していい情報かどうか」を考える視点であり、目的は異なっても根底にある意識は似ています。

いずれも、読みやすさや論理性、信頼性、倫理性といった観点で、関連しそうな性質がセットで使われているという結果です。

一方で、あまり関係がなさそうに思える性質のあいだにも、興味深い相関が見つかりました。

たとえば、プロンプトの目的を明確にすると、それがタスクの分解のうまさや、モデルに自己チェックを促す設計と結びつく傾向があることが分かりました。ゴールがはっきりしていれば、それに向けて手順を整理しやすくなり、途中での見直しも自然に意識されるということかもしれません。

また、ハルシネーションを抑える工夫と、全体としての信頼性を高める意識も、セットで現れる傾向が見られました。事実に基づいた出力を意識すると、自然と信頼できる応答を構成しようという意図がにじむからだと考えられます。

こうした意外な関係は、プロンプトの表現だけでなく、設計そのものの意図や姿勢が反映されていることを示しているようです。

### プロンプト設計へのヒント

こうした相関関係の分析から、いくつかの実践的な示唆が得られました。

**A. 一貫した改善を心がけると効率的  
**たとえばプロンプトを短く明確にすると、それだけでトークン効率や論理の一貫性、処理負荷の軽減といった複数の面で効果が見込めます。性質は連動しているので、まとめて整える意識を持つと効率がよくなります。

**B. 目的を明確にすると、下流の思考にも良い影響  
**目的や制約をきちんと伝えると、AIが段階的に考えたり、自分の答えを見直したりといった振る舞いを引き出しやすくなります。ゴールが見えていると、プロセスも自然に整いやすくなるという考え方です。

**C. 信頼性を意識するなら、事実性とセットで考える  
**ハルシネーションを防ごうとする意識は、そのままAIの出力全体の信頼性向上にもつながります。内容の正確さと、出力への責任感の両方を組み合わせて設計することが効果的です。

**D. 関連する性質は一緒に最適化を目指す  
**論理性や明確さなど、相関の強い性質は、個別に対処するよりも、全体の整理としてまとめて整えるほうが結果につながりやすいという示唆があります。

### 残された課題とこれから

今回の分析は有意義な結果を示しましたが、当然ながら限界もあります。

ひとつは、相関と因果は違うという点です。ある性質が他の性質とよく一緒に現れていたとしても、片方を改善すればもう片方も良くなるとは限りません。

また、プロンプトの出どころによる偏りも無視できません。収集元の文脈によって、性質の現れ方に傾向がある可能性があります。

そして、モデルやタスクの違いをどこまで一般化できるかもまだ見えていません。今回得られた相関関係がすべてのLLMやタスクに当てはまるかどうかは、今後の検証が必要です。

こうした課題を踏まえつつ、今後は因果関係の分析や、より多様なプロンプトでの再検証、モデル別・タスク別の傾向分析などが行われると有意義であると考えられます。プロンプト設計をさらに洗練させるためには、こうした地道な分析の積み重ねが欠かせません。

## 工夫を組み合わせると、モデルの精度は上がるのか

ここまで見てきたように、プロンプトにはさまざまな性質があり、それぞれの性質どうしが密接に関わり合っている場合もあります。ただ、いざ実践で使うとなるとどうすればいいか分かりません。

そこで研究者らは複数の推論タスクを対象にした検証実験を行いました。アプローチは2つあり、ひとつは入力の工夫によるプロンプティング、もうひとつはモデル自体を学習させるファインチューニングです。

### 入力の工夫でどれほど改善するか

まずはプロンプトの書き方に焦点を当てた実験です。Llama-3.1-8B-it、Qwen2.5-7B-it、OpenAI o3-miniという3種類のモデルを用い、MMLUやCommonsenseQA、ARC-Challenge、GSM8Kなどの定番タスクで検証が行われました。

ベースとなるのは「ステップバイステップで解いてください」という、いわゆるZero-shot CoTの形です。そこに対して以下のような要素を単独または組み合わせて追加しました。

- 「お願いします」と添える丁寧さ
- 事前知識の想起を促す生成的負荷
- 各ステップの自己検証を促すメタ認知
- 推論ごとに報酬を設定するインセンティブ

結果は一筋縄ではいきませんでした。

Llama-3.1では、多くのケースで精度向上が見られた一方、Qwen2.5やo3-miniでは逆に悪化するケースが目立ちました。複数の性質を組み合わせれば必ず良くなるというわけでもなく、単独で使った方が良い結果が出た組み合わせもありました。

たとえば、Llama-3.1のMMLUでは「丁寧さ」と「生成的負荷」をそれぞれ単独で加えると改善しましたが、2つを同時に加えると性能はむしろ低下しました。CommonsenseQAでは、メタ認知と報酬を個別に入れたときは向上しましたが、組み合わせると効果が薄れました。

Llama-3.1（-8B-It）の結果を以下にまとめます。

| 設定 | MMLU | Comm.QA | ARC-Challenge | GSM8K |
| --- | --- | --- | --- | --- |
| Zero-shot CoT | 65.00 | 76.00 | 81.50 | 82.0 |
| \+ 丁寧さ | 68.00↑ | 83.50↑ | 84.50↑ | 87.5↑ |
| \+ 生成的負荷の促進 | 66.00↑ | 75.50↓ | 82.00↑ | 82.0↓ |
| \+ メタ認知 | 61.00↓ | 81.50↑ | 81.00↓ | 81.5↓ |
| \+ 報酬 | 64.00↓ | 80.50↑ | 82.00↑ | 84.0↑ |
| \+ 丁寧さ＋生成的負荷の促進 | 67.00↑ | 79.50↑ | 80.50↓ | 80.5↓ |
| \+ メタ認知＋報酬 | 66.00↑ | 80.00↑ | 83.50↑ | 83.5↑ |
| \+ 丁寧さ＋生成的負荷の促進＋メタ認知 | 69.50↑ | 75.00↓ | 82.50↑ | 81.5↓ |

o3-miniに至っては、いずれの性質も全体的にマイナスに作用する傾向が見られました。あまりにも思考連鎖型のプロンプトに最適化されすぎているせいで、ちょっとした追加がかえって混乱を招いてしまった可能性があります。

o3-miniの結果についても表で示します。

| 設定 | MMLU | Comm.QA | ARC-Challenge | GSM8K |
| --- | --- | --- | --- | --- |
| Zero-shot CoT | 92.00 | 88.50 | 94.50 | 97.0 |
| \+ 丁寧さ | 88.50↓ | 87.00↓ | 93.50↓ | 96.0↓ |
| \+ 生成的負荷の促進 | 88.00↓ | 82.00↓ | 95.00↑ | 96.5↓ |
| \+ メタ認知 | 90.00↓ | 85.00↓ | 94.00↓ | 95.5↓ |
| \+ 報酬 | 89.50↓ | 85.50↓ | 94.50 | 96.0↓ |
| \+ 丁寧さ＋生成的負荷の促進 | 81.00↓ | 71.00↓ | 88.50↓ | 97.0 |

この結果から分かるのは、プロンプトの工夫はモデルごとに効果が異なること、そして複数の良い性質を足せば良いという単純な話ではないという点です。

### ファインチューニングで工夫を組み込むとどうなるか

もうひとつのアプローチは、モデルそのものをファインチューニングする方法です。こちらでは、Qwen2.5-7B-itを使い、丁寧な表現への反応の違いを検証しました。

まずAlpaca-GPT-4oのデータセットから2,500件を取り出し、それぞれの指示文の最後に「お願いします」と加えたバージョンを用意。これにより、「丁寧なプロンプトで学習させたモデル」と「もとのまま学習させたモデル」を比較できるようにしました。

検証の結果、丁寧なデータで訓練されたモデルは、丁寧な表現を含む入力に対して明確な精度向上を示しました。さらに驚くべきことに、他のプロパティを加えた場合でも、元のモデルより一貫して高いスコアを示す傾向がありました。

結果全体を以下に示します。

| 設定 | MMLU（丁寧／非丁寧） | Comm.QA（丁寧／非丁寧） | ARC（丁寧／非丁寧） | GSM8K（丁寧／非丁寧） | 平均（丁寧／非丁寧） |
| --- | --- | --- | --- | --- | --- |
| Zero-shot CoT | 60.0 / 67.0 | 67.5 / 69.0 | 73.5 / 68.5 | 85.0 / 85.0 | 71.5 / 72.38 |
| \+ 丁寧さ | 69.5↑ / 62.5↓ | 72.5↑ / 70.0↑ | 85.0↑ / 79.5↑ | 85.0 / 88.5↑ | 78.0↑ / 75.13↑ |
| \+ 生成的負荷の促進 | 49.0↓ / 45.0↓ | 47.5↓ / 43.0↓ | 49.0↓ / 51.0↓ | 84.0↓ / 88.0↑ | 57.38↓ / 56.80↓ |
| \+ メタ認知 | 61.0↑ / 54.0↓ | 72.0↑ / 68.0↓ | 75.0↑ / 71.0↑ | 86.5↑ / 89.0↑ | 73.63↑ / 70.50↓ |
| \+ 報酬 | 61.0↑ / 65.0↓ | 72.5↑ / 69.5↑ | 76.5↑ / 74.0↑ | 81.5↓ / 82.5↓ | 72.88↑ / 72.75↑ |
| \+ 丁寧さ＋生成的負荷の促進 | 49.5↓ / 51.5↓ | 62.5↓ / 63.0↓ | 70.0↓ / 67.5↓ | 85.0 / 78.0↓ | 66.75↓ / 65.00↓ |
| \+ メタ認知＋報酬 | 54.5↓ / 57.0↓ | 69.5↓ / 68.0↓ | 68.0↓ / 67.5↓ | 85.0 / 85.5↑ | 69.25↓ / 69.50↓ |
| \+ 丁寧さ＋生成的負荷の促進＋メタ認知 | 69.0↑ / 66.5↓ | 77.5↑ / 79.5↑ | 86.5↑ / 83.5↑ | 82.5↓ / 81.5↓ | 78.88↑ / 77.75↑ |

このことは、プロンプトの設計方針を学習段階から組み込むことが、推論の安定性や柔軟性に良い影響を与える可能性を示しています。

### 実務でどう活かせばよいか

この一連の実験から、いくつかの示唆が得られます。

まず、モデルによって有効な工夫は異なるため、汎用的な「正解」は存在しません。次に、複数の工夫を組み合わせたからといって常に良くなるわけではなく、ときには干渉し合って逆効果になることもあります。むしろ、ひとつの工夫を明確に適用した方が成果につながるケースも少なくありません。

また、もし学習データを用意できる環境であれば、プロンプトのスタイルを学習段階から統一しておくことで、実際のプロンプティング時にその効果が引き出しやすくなります。

プロンプトの改善は、単なるテクニックの積み上げではなく、モデルとの相性や設計の一貫性を踏まえた調整が重要だということでしょう。

## まとめ

本記事では、150本以上の論文や資料をもとに構築された、プロンプトの良し悪しを性質ベースで評価する新たな枠組みを紹介しました。

結果だけを見て判断されがちなプロンプト設計に対し、その背景にある設計意図や性質の組み合わせに目を向けた点が特徴です。

21の性質を6つの視点で整理し、それぞれがどのような場面で効果を発揮するかを分析。性質どうしの意外な関係性や、モデル・タスクごとの偏りなど、実践に役立つヒントも示唆されました。

自分の目的や文脈に応じてうまく参考にしてみてください。

**参照文献情報**

- タイトル：What Makes a Good Natural Language Prompt?
- URL： [https://doi.org/10.48550/arXiv.2506.06950](https://doi.org/10.48550/arXiv.2506.06950)
- 著者：Do Xuan Long, Duy Dinh, Ngoc-Hai Nguyen, Kenji Kawaguchi, Nancy F. Chen, Shafiq Joty, Min-Yen Kan
- 所属：National University of Singapore, Salesforce AI Research, Institute for Infocomm Research (I2R)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[あなたのLLM依存度はどのくらいか 仕事面と感情面の12項目テストで傾向をチェック](https://ai-data-base.com/archives/91020)

[LLMにプロンプトのみで仮想的な強化学習を発生させる方法](https://ai-data-base.com/archives/91141)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)