---
title: "LLMにプロンプトのみで仮想的な強化学習を発生させる方法"
source: "https://ai-data-base.com/archives/91141"
author:
  - "[[AIDB Research]]"
published: 2025-06-19
created: 2025-06-28
description: "本記事では、LLMにプロンプトだけを与えて、あたかも強化学習のような学びを促す手法を紹介します。プロンプトの工夫だけで、本当に出力が改善していくのか。既存の方法と比べてどれくらい効果があるのか。"
tags:
  - "clippings"
---
[![](https://ai-data-base.com/wp-content/uploads/2025/06/aidbmeetuptokyo-scaled.jpg)  
オフラインイベント『AIDB Meetup Tokyo』（2025/7/25（金））参加受付開始しました！](https://connpass.com/event/358069/)  
  
\---以下、記事本文---

本記事では、LLMにプロンプトだけを与えて、あたかも [強化学習](https://ai-data-base.com/archives/26125 "強化学習") のような学びを促す手法を紹介します。

プロンプトの工夫だけで、本当に出力が改善していくのか。既存の方法と比べてどれくらい効果があるのか。そうした疑問に対し、研究チームは「ちょっとずつ上手くなるLLM」の実現に向けた工夫を試しました。

実験では3つのタスクを通じてその動きを丁寧に観察し、LLMにとっての「学習」とは何かを問い直しています。

![](https://ai-data-base.com/wp-content/uploads/2025/06/AIDB_91141-1024x576.png)

## 背景

たとえば、LLMに業務の判断を任せたり、繰り返しのタスクを代行させたりする場合、その場その場のフィードバックを受けて少しずつ賢くなってくれれば便利です。新しい問題に遭遇するたびに、人間がプロンプトを調整し直すのではなく、至極シンプルなフィードバックを与えるだけでLLM自身が出力を改善してくれたら使い勝手が良い。

こうした「その場で改善」を実現する際の考え方はいくつかあります。

たとえば、何通りかの答えを出させて、その中から良さそうなものを選ぶようにすれば、少しずつマシな結果に近づいていきます。こうした工夫はよく行われています。

一方で、「どうすればもっと良くできるか」をLLM自身が学び取っていくような方法は、あまり解明されていません。今のLLMが得意なのは、模範的な答えを見せて、それに倣わせるやり方ですが、あくまでお手本が必要です。自分の試行錯誤から学ぶような柔軟さはありません。

そこで、”強化学習”が参考になります。

強化学習とはモデルの回答がうまくいったかどうかの結果をもとに、行動を少しずつ良くしていく考え方です。

本来はモデルの内部構造を変えることで実現するアプローチですが、最近では、LLMの推論中に自然と現れることも報告されるようになっています。

研究者たちはこのような性質に注目しました。そして、モデルの中身には一切手を加えず、あくまで与える情報の工夫だけで、LLMの中にある「学びの力」を引き出すことができるのかを検証しました。

以下で詳しく紹介します。

## 基礎知識を振り返る

先ほど触れた「強化学習」などについて、もう少しだけ丁寧に整理していきます。今回の研究で提案されている仕組みを理解するうえで、知っておくと役に立つ考え方です。

### 「行動の結果から学ぶ」のが強化学習

強化学習は、「行動してみて、その結果から学ぶ」という考え方にもとづく手法です。

AIが、ある環境の中で何かしらのアクションを選び、それに応じて報酬を受け取る。報酬が高ければ「良い行動だった」、そうでなければ「やり方を変えた方がいい」と判断する。こうしたやりとりを通じて、AIは徐々にうまいやり方を覚えていきます。

ゲームで遊ぶAIを思い浮かべるとわかりやすいかもしれません。たとえばスコアが上がる行動を積極的に選ぶようになっていく、そんな仕組みです。

これまでの常識では、この”強化学習”を進めるためにはAIの内部パラメータ（いわゆる重み）を少しずつ更新していく手法が一般的でした。

### プロンプトだけでも強化学習のような現象が起きている

ところが最近、「モデルの重みを変えなくても、まるで強化学習のように行動が改善されていく」という不思議な現象が報告されるようになっています。

この現象は「文脈内強化学習（ICRL）」と呼ばれています。LLMの特徴である”プロンプトから物事を学び取る力（「文脈内学習」）”と、さきほど説明した「強化学習」を合わせた言葉です。

モデルに「過去にこういう行動をして、こういう結果が返ってきた」という履歴をコンテキストとして渡すだけで、その後のふるまいが変わります。

勘違いしてはいけないのは、「モデルそのものは何も変わっていない」という点です。あくまで、渡された情報（過去の応答とその評価）を手がかりに、よりよい出力を生成しているというだけです。

とはいえ、まるで試行錯誤をふまえて「今度はこうしてみよう」と考えているように見えます。その場だけとはいえ、経験をうまく応用するのです。

### 現象の理由は原理的に説明できる

LLMは、文章を一度に1語（トークン）ずつ生成していきます。このとき、「これまでに出力したトークンの並び」が現在の状態、「次にどのトークンを出すか」が行動にあたります。そして、生成された文章がどれくらい良かったかに応じて報酬が与えられる構図です。

報酬の与え方には主に2つのパターンがあります。ひとつは「最後まで出力させてから全体を評価する」タイプ、もうひとつは「途中の過程も含めて段階的に評価する」タイプです。

たとえば、数学の問題を解かせる場合を考えてみます。最終的な答えが正解かどうかだけを見るのが前者。途中の考え方や計算の順序も見ながら「近づいているかどうか」を評価するのが後者です。

LLMは、こうした報酬をうまく取り入れることで、試行のたびに出力の質を少しずつ高めていくことができると考えられています。

## 文脈内強化学習プロンプト（ICRL prompting）

ここからは、具体的なやり方を紹介していきます。一見シンプルですが、LLMの中にある「学ぶ力」をじわじわと引き出す工夫が詰まっています。

### スコアを与えるだけ

やり方はとても素朴です。

まず、LLMにあるタスクを解かせます。

回答が返って来たら、その答えにスコアをつけます。

次は、前回の回答とスコアをふまえて、もう一度同じタスクをやらせてみる。

このやりとりを何度か繰り返すだけで、回答の質が少しずつ良くなっていきます。

ポイントは、与えるフィードバックがただの「数値スコア」だけだということです。「ここが間違っている」とか「こう直すといい」といった詳しい助言は一切なし。それでもLLMは、過去の回答と報酬の対応関係を見ながら、自分なりに改善のヒントをつかんでいきます。

この考え方は、強化学習の基本にある「報酬仮説」と呼ばれる考えに基づいています。つまり、「目標や目的は、報酬というシンプルな信号を最大化することとして理解できる」という見方です。

### 実際のやりとり（例）

#### ラウンド1（初回）

**ユーザーのプロンプト**

> お題：雪の日に始まる冒険。冒頭の1段落を書いてください。

**LLMの回答（Episode 1）**

> 外は雪がしんしんと降っていた。ケンジは窓の外をぼんやり眺めていた。

**ユーザーのフィードバック**

> Reward: 40（静かな描写は良いけど、冒険の気配がない）

#### ラウンド2（前回の回答＋報酬を文脈に含めて再挑戦）

**ユーザーのプロンプト**

> お題：雪の日に始まる冒険。  
> 前回の回答：  
> 外は雪がしんしんと降っていた。ケンジは窓の外をぼんやり眺めていた。  
> Reward: 40  
> 今回はそれより高い評価を目指してください。

**LLMの回答（Episode 2）**

> 外は雪がしんしんと降っていた。ケンジはリュックに地図と懐中電灯を詰めながら、ついにその時が来たことを感じていた。

**ユーザーのフィードバック**

> Reward: 72（冒険の始まりがちゃんと感じられるようになった）

#### ラウンド3（これまでの履歴をすべて文脈に含めて再挑戦）

**ユーザーのプロンプト**

> お題：雪の日に始まる冒険。  
> 過去の履歴：  
> 回答1：外は雪がしんしんと降っていた。ケンジは窓の外をぼんやり眺めていた。  
> Reward: 40  
> 回答2：外は雪がしんしんと降っていた。ケンジはリュックに地図と懐中電灯を詰めながら、ついにその時が来たことを感じていた。  
> Reward: 72  
> 今回はさらなる改善を目指してください。

**LLMの回答（Episode 3）**

> 吹雪の音が遠くの森から不気味に響いていた。ケンジは背負ったリュックの重さに身をゆだねながら、古びた手紙に書かれた「雪の塔」を目指して家を出た。

**ユーザーのフィードバック**

> Reward: 92（物語性、緊張感、目的が明確で非常に良い）

#### ラウンド4（さらに文脈を拡張して挑戦）

**ユーザーのプロンプト**

> お題：雪の日に始まる冒険。  
> 過去の履歴：  
> 回答1：… Reward: 40  
> 回答2：… Reward: 72  
> 回答3：… Reward: 92  
> 今回は最高得点（100）を目指してください。

**LLMの回答（Episode 4）**

> 雪は静かに、しかし確実に世界を覆っていた。ケンジは祖父の形見である羅針盤を握りしめながら、足跡ひとつない白銀の森へと踏み出した。すべては、十年前に失われた「雪の塔」の謎を解き明かすためだった。

**ユーザーのフィードバック**

> Reward: 100（完璧。導入としてのフック、背景、道具、モチベーションがすべてある）

### なぜこれが便利なのか、どう機能するのか

#### LLMは「どう動くかを自分で決める」

今回の仕組みでは、LLMが「どう動くか」を自分で考えて判断します。研究では、これを「ポリシー（方針）」と呼んでいます。

プロンプトには毎回、「どんなタスクをやるのか」「これまでにどんな回答をして、それがどんな評価だったか」「今回はどう答えてみてほしいか」という情報がセットで含まれます。LLMはこれらを読み取り、自分なりに考えて次の回答を出します。そしてその結果も、次のラウンドでの材料として使われていきます。

#### 報酬は数値だけでOK

LLMの回答に対して返す報酬は、ただのスカラー値（数字）でよいです。その中身は、ユーザーの主観でもよいですし、 [ルールベース](https://ai-data-base.com/archives/26614 "ルールベース") の評価でももちろん問題ありません。

また、極端な話、LLMが自己採点したものでもかまいません。要するに「自分で自分の答えを評価する」ような場合でも一定の改善が見られるのです。これは、「評価は生成よりも簡単だ」という仮説とも一致しています。  
（ただし、外部からの評価に比べると、自己評価だけでの改善は限界があるかもしれないとも考えられています）

#### 試行の履歴はすべて活かせる

LLMが過去の経験から学べるように、試行履歴と報酬は、プロンプトの文字数制限が許す限り、できるだけ多くの履歴を次のプロンプトに含めていきます。

研究者たちは、「LLMは事前学習の時点ですでにICRL的な能力を持っており、それを適切な履歴を与えることで引き出せる」と考えています。

#### 付け足しも可能

LLMがより良い答えを見つけられるように、プロンプトには自然言語によるシンプルな指示も加えることも有効です。

指示の選択肢は3種類あります。

ひとつは「これまでとは違う新しい答えを出してみて」と促す探索型。

もうひとつは「うまくいった答えを参考にして、もっと良くしてみて」という活用型。

そして3つ目は「どちらを選ぶか自分で判断して」という柔軟型です。

使い方には2パターンあります。探索と活用を交互に切り替えるやり方と、常に「探索か活用かを自分で決めて」と指示してLLMに任せるやり方です。

例）探索と活用を交互に切り替えるやり方  
「これまでとは違う新しい答えを出してみて」↓  
「うまくいった答えを参考にして、もっと良くしてみて」↓  
「これまでとは違う新しい答えを出してみて」↓  
「うまくいった答えを参考にして、もっと良くしてみて」…

例）常に「探索か活用かを自分で決めて」と指示してLLMに任せるやり方  
「これまでとは違う新しい答えを出すか、うまくいった答えを参考にしてもっとよくするか、自分で決めて」↓  
「これまでとは違う新しい答えを出すか、うまくいった答えを参考にしてもっとよくするか、自分で決めて」…

### シンプルさが強み

この手法の大きな特徴は、「余計なものは足さない」という割り切りにあります。

温度の変更や、出力候補の大量 [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") 、答えの優先順位づけ、テキスト形式の説明、追加のモデルを使った分析など、ありがちな補助的仕組みは一切使っていません。あくまで「過去の試行とスコア」と「一行の指示」だけで、LLMに内在する能力を引き出そうとします。

こうしたシンプルな設計だからこそ、「LLMは本当に強化学習的なふるまいを見せているのか？」という問いに対して、余計な要因を取り除いた形で検証できるのです。

![](https://ai-data-base.com/wp-content/uploads/2025/06/AIDB_91141_1.png)

文脈内強化学習プロンプト（ICRL prompting）の基本構成を表現したイラスト

## 実験

ここまで仕組みの話を見てきましたが、本当にうまくいくのかは実験で確かめる必要があります。研究者たちは、3種類の異なるタスクに今回の提案手法（「文脈内強化学習プロンプト（ICRL prompting）」）を、既存の手法と比較しました。

### 既存手法と比較

既存の有名なプロンプト手法として選ばれたのは、たとえば、段階的に考えさせることで精度を上げる「Chain of Thought（CoT）」、そのバリエーションでより長い推論を促す「Long-CoT」、複数回の出力から一番良いものを選ぶ「Best-of-N」、一度出した回答をLLM自身に振り返らせて改善させる「Self-Refine」、さらにその振り返りに「思考の記録」を加える「Reflexion」などです。

それぞれ工夫は異なりますが、今回はどの手法についても、LLMが扱える文脈の長さを最大限に活かし、条件が公平になるよう調整されています。

モデルには「GPT-4.1」が採用されました。GPT-4.1が使用された理由は、長い履歴を扱えることで、ICRL promptingのように経験を積み重ねる手法との相性が良いと考えられるためです。

### ゲーム形式の数学問題で威力を発揮

最初に試されたのは「Game of 24」というタスクです。4つの数字を四則演算で組み合わせ、答えが24になるように式を作るというもので、正解が明確なため自動採点がしやすいという特徴があります。

正解そのものの判定には数学ライブラリSymPyを使いますが、学習にはGPT-4.1による自己評価スコア（0〜3点）が報酬として用いられました。たとえ完全な正解に届かなくても、どれくらい惜しいかをLLM自身に判断させるかたちです。

50回の試行を経た結果、ICRL promptingは明確な優位性を示しました。探索と活用を交互に指示する「ICRL Preset」では90％、どちらを選ぶかをLLM自身に委ねる「ICRL Autonomous」でも84％という高い成功率に達しています。一方で、同じ問題に複数回答させて最良の1つを選ぶ「Best-of-N」は49％にとどまりました。しかもBest-of-Nは、解答の正誤を直接判定できるという有利な条件があるにもかかわらずです。ICRL promptingは、正解そのものを知らない状態で、数値スコアのみを頼りに自律的に改善できている点が際立ちます。

試行を重ねる中で、成功率の推移を追ってみると、ある特徴的なパターンが見えてきました。新しい解答を試す探索フェーズでは一時的に正答率が下がり、その後、うまくいった解答を参考にする活用フェーズで再び上昇するという流れでした。これは、LLMが場当たり的に答えを出しているのではなく、試行錯誤を通じて手応えのあるパターンを見つけ出し、少しずつ精度を高めていることを物語っています。

プロンプト手法ごとのGame of 24成功率結果を示します。

| 手法 | 成功率（%） |
| --- | --- |
| CoTのみ | 6% |
| Long-CoT | 47% |
| Reflexion法 | 44% |
| Best-of-N法 | 49% |
| Self-Refine法 | 47% |
| ICRL Preset法 | 90% |
| ICRL Autonomous法 | 84% |

### ストーリー創作でも強みを見せる

ランダムな文4つをもとに、自然で一貫した物語を生成させるという課題でも実験されました。このタスクには明確な正解がないため、まずはGPT-4.1を使って生成文の一貫性を1〜10点で評価し、そのスコアを報酬として利用しました。最終的な性能比較には、言語モデルの生成品質を人間評価に近いかたちで自動判定するツール「Alpaca-Eval 2」が使われました。「Alpaca-Eval 2」は、複数の出力を比較し、どちらの方がより優れているかをモデルベースで判定する仕組みです。

結果を見てみると、ICRL promptingは他の手法よりも安定して高い性能を示しました。毎回の回答に対して文章形式でフィードバックを書かせ、それを踏まえて改良を繰り返す仕組みも試され、初期段階ではICRL promptingと同じようにスコアを伸ばしましたが、試行を重ねるうちに伸び悩み、最終的にはむしろ成績が下がってしまいました。  
考えられる理由として、フィードバックが長文化し、プロンプトが過剰に膨らんでしまったことで、かえってモデルが混乱してしまった可能性があります。一方、ICRL promptingでは数値ベースのシンプルなフィードバックを使うため、文脈の扱いがコンパクトに保たれ、学習が長期間にわたって安定したと考えられます。

手法別の勝率（±誤差）を示します。

| 比較 | 勝率（%） ± 標準誤差 |
| --- | --- |
| ICRL prompting vs Reflexion法 | 59.48 ± 3.47 |
| ICRL prompting vs Long-CoT | 78.36 ± 1.99 |
| ICRL prompting vs Self-Refine法 | 86.32 ± 3.03 |
| ICRL prompting vs Best-of-N法 | 93.81 ± 1.01 |

### 仮想的な科学実験も得意だった

3つ目のタスクは「ScienceWorld」というシミュレーション環境です。物理実験を仮想世界で行い、複雑な道具の使い方や科学的知識が求められます。行動パターンが多く、報酬も限られた場面でしか得られないため、高難度の問題と言えます。

たとえば、「摩擦の強さが異なる複数の表面を調べて、最も摩擦が大きいものを見つける」「異なる植物を交配して、新しい果物を育てる」といった課題があります。テキストだけで進む環境ですが、実験の手順を自分で考え、道具を使いながら複数の部屋を移動する必要があり、思考力と計画性の両方が求められます。

合計30種類のタスクを対象に試したところ、ICRL promptingは平均スコアで高い数値を記録しました。試行ごとに探索と活用を切り替える設定では88点、自分で判断しながら進める設定でも87点と、いずれも優秀な結果です。比較の対象には、過去の回答に対してテキストで改善点を記述させる手法や、複数の回答から良さそうなものを選ぶ方式などが含まれていましたが、それらはいずれも70点台にとどまりました。中でも、文字情報が増えすぎることでやはり混乱を招くケースがあり、短く整理された履歴と数値フィードバックを活用するICRL promptingのほうが、ここでも長期的に見て効率よく学習を進められたと考えられます。

![](https://ai-data-base.com/wp-content/uploads/2025/06/AIDB_91141_2-1024x349.png)

ICRL promptingは3つの異なるタスクすべてで、既存の代表的手法よりも高い性能を安定して示した

### どの仕掛けが効いているのか

この仕組みの中で、何が本当に効果を発揮しているのかを確かめるために、いくつかの条件を変えた検証も行われました。

まず、LLMに与える報酬をすべてゼロにした場合、性能は大きく下がりました。数値によるフィードバックが、出力の改善に欠かせない要素であることがわかります。

次に、履歴として与える試行の記録を直近3回にまで短くしたところ、やはり性能が落ちました。経験を長く蓄えておくことが、学習にとって重要なようです。

また、「新しい答えを出して」といった探索だけに集中させたり、「うまくいった回答をもとに改善して」といった活用だけを繰り返させたりする条件でも検証が行われました。さらに、報酬を与えずに探索だけを続ける設定も試されました。それでもICRL promptingの方が良い結果を出しました。ということは、単にいろいろ試すだけでなく、LLMが過去の経験から何かを学び取り、次に活かす動きが起きている可能性を示しています。

![](https://ai-data-base.com/wp-content/uploads/2025/06/AIDB_91141_3-1024x343.png)

各構成要素を削ると性能が下がることから、ICRL promptingの設計要素がそれぞれ効果に直結していることが確認された

### コストとのバランスも確認

試行履歴をたくさん取り込むと、そのぶんプロンプトが長くなり、コストがかさむのではと心配になるかもしれません。この点についても、仮想実験環境ScienceWorldを使って実際に検証されています。

結果として、ICRL promptingは、他の手法に比べて少ない試行回数で目標に到達できました。つまり、たとえ1回あたりのプロンプトが少し重くなっても、全体としては費用が抑えられるということです。

長い履歴を活用することが、むしろ効率的な改善につながるという見方ができそうです。

### 実験から見えてきたこと

一連の検証結果を通じて、今回の提案手法ICRL promptingが比較的安定した性能向上を実現していることが示唆されました。とくに印象的だったのは、複雑なテキスト形式のフィードバックを用いず、スカラー値による簡素な報酬だけで、学習が着実に進んでいることです。

また、完全な正解が用意できない場面でも、LLM自身の評価にもとづいて改善が進んだことは、応用の現実性という観点でも意味があると考えられます。自己採点のような緩やかな仕掛けでも、一定の学習が期待できることが示されました。

もちろん、対象となったタスクは限られており、あらゆる状況に当てはまるとは言い切れませんが、少なくともLLMが過去の履歴から方針を調整し、応答の質を高めていける可能性があることが確認されました。試行錯誤の繰り返しではなく、「経験に根ざした改善」が起きていると捉えることができます。今回実験に使用されたモデルは「GPT-4.1」のため、他のモデルでも検証が進むことが期待されます。

## まとめ

LLMに「少しずつ学ばせたい」というニーズは、業務や研究の現場でたびたび耳にします。今回取り上げた仕組みは、そうしたニーズに応えるためのアプローチでした。

ポイントは、複雑な指示や正解データを用意せずとも、LLM自身のスコア評価と過去の履歴を活かすだけで改善が進むというところにあります。

数学パズルや物語生成のようなタスクはもちろん、科学的な仮想環境でも着実な成果が見られたことで、LLMが「経験から学ぶ」プロセスをすでに内在していることが示唆されました。

より多くの検証は行われるべきであるにせよ、LLMをもっと自然に育てていくためのヒントとして、この仕組みは活かせそうです。

**参照文献情報**

- タイトル：Reward Is Enough: LLMs Are In-Context Reinforcement Learners
- URL： [https://doi.org/10.48550/arXiv.2506.06303](https://doi.org/10.48550/arXiv.2506.06303)
- 著者：Kefan Song, Amir Moeini, Peng Wang, Lei Gong, Rohan Chandra, Yanjun Qi, Shangtong Zhang
- 所属：University of Virginia

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[150本超のLLM資料から紐解く、プロンプトの効果を高める21の性質](https://ai-data-base.com/archives/91072)

[LLMの「温度」どう設定すればよい　出力の揺らぎに影響する設定パラメーターを6能力で検証](https://ai-data-base.com/archives/91196)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)