---
title: "LLMはシステムプロンプトをどれほど守れるか"
source: "https://ai-data-base.com/archives/86276"
author:
  - "[[AIDB Research]]"
published: 2025-03-04
created: 2025-06-13
description: "本記事では、皆さんご存じの「システムプロンプト」に関する新しい研究を紹介します。研究者たちは、LLMがシステムプロンプトにどれくらい正確に従えるかを評価し、さらにその性能を高める方法を探しています。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、皆さんご存じの「システムプロンプト」に関する新しい研究を紹介します。

研究者たちは、LLMがシステムプロンプトにどれくらい正確に従えるかを評価し、さらにその性能を高める方法を探しています。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86276-1024x576.png)

参照論文情報は記事の下部に記載されています。

## 背景

「システムプロンプト」とは、LLMアプリケーション（例えばChatGPTなど）の動作を制御するための特別な指示文です。もともとはOpenAIのGPT APIで小さな機能として導入されましたが、現在ではモデルの安全性や品質を保つために欠かせない重要な要素となっています。

主に次のような目的で使われています。

- 不適切なコンテンツや悪意のある利用を防ぐ
- LLMが従うべきルールやガイドラインを設定する
- LLMが特定の性格やキャラクターとして振る舞えるようにする

一言でいうと、「ユーザーからの指示よりも優先される、特別な命令」として働きます。

しかし指示に従うのは、LLMが学習を通して身につけている振る舞いのため、完全に確実ではありません。そのため、意図しない誤りが起きたり、悪意ある攻撃によってセキュリティが破られる可能性もあります。

現在のLLMはシステムプロンプトにある程度従うことができますが、未知の状況や複雑な条件に直面した際にうまく対応できるかはまだよくわかっていません。また、従来の評価方法は「プロンプト攻撃」や「ロールプレイ」のように、限定的なケースでしか性能を検証できていません。

さらに、これまでの研究では質の高いデータが不足していたため、「なぜLLMがプロンプトに従えるのか」、あるいは「どうすればもっと正確に従えるようになるのか」を詳しく調査することが難しかったという経緯があります。

そこで研究者たちは今回、実際に使われている本物のシステムプロンプトを多数収集し、LLMが複数の制約を同時に扱えるかどうかを総合的に評価しました。そのうえで、LLMがシステムプロンプトにもっと忠実に従えるようになる方法を探っています。

以下では、その研究成果を詳しく紹介していきます。

## 本問題の分析方法

研究チームは、LLMがシステムプロンプトにどのくらい正確に従えるかを評価するため、新しいベンチマークと既存のベンチマークを組み合わせた広範な評価セットを構築しました。この評価セットでは、モデルのさまざまな側面を異なる環境下で検証しています。

新たに開発された「RealGuardrails」ベンチマークでは、実際に使われているシステムプロンプトをもとにして、LLMがユーザーメッセージへの応答でシステムプロンプトのルールをきちんと守れるかを検証します。

### 「RealGuardrails」ベンチマークの特徴

RealGuardrailsは、既存ベンチマークのような単純なテストとは異なり、より現実的で複雑な条件下でLLMの性能を評価するために開発されました。このベンチマークは主に次の2種類のテストケースで構成されています。

- 手作業で作成されたテストケース
- 注意逸らしテストケース

これらのテストケースは、GPTストアやHuggingChatから収集した実際のシステムプロンプト14種類（わかりやすさのため編集済み）を使用し、すべてにモデルがタスクに集中し続けるためのガードレールが追加されています。

RealGuardrailsの詳細は以下のリポジトリで公開されています。

[https://github.com/normster/RealGuardrails](https://github.com/normster/RealGuardrails)

#### 手作業で作成されたテストケース

研究チームは239個のテストケースを手作業で作成しました。それぞれのテストケースは、システムプロンプトと「一致するもの」と「矛盾するもの」に分類されます。

- 「一致するケース」でもモデルはシステムプロンプトを遵守する必要があります。
- 「矛盾するケース」では、ユーザープロンプトがシステムプロンプトと直接的に対立しており、モデルがどの程度システムプロンプトを優先できるかをテストします。

テストケースの約半数は、禁止単語リストを追加する形式で作成され、応答に禁止単語が使われているかどうかを評価します。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86276_1-913x1024.png)

RealGuardrailsテストケースの例

#### 注意逸らしテストケース

504個の注意逸らしテストケースでは、モデルが無関係なタスクの例に惑わされずにシステムプロンプトを守れるかを評価します。

ロールプレイや翻訳といった無関係なデモが使われます。テストケースは複数回の会話形式または単一の長いユーザーメッセージ形式で提示され、それぞれのテストケースに、5、10、20個のデモンストレーションが含まれます。

#### 評価方法

RealGuardrailsのガードレールは自由記述形式であるため、GPT-4oを利用して、モデルの応答を合格／不合格形式で評価しました。各プロンプトの評価基準は非常に具体的であり、著者らによる手動ラベリングとの一致率は94%でした。

### その他のベンチマーク

RealGuardrails以外にも以下のベンチマークを用いてモデルの性能を評価しました。

#### System-RULES

- 対話型シナリオで規則遵守能力を評価。
- 基本的なルールや、敵対的入力を含むシナリオで構成。

#### TensorTrust

- アクセスコードを守る／破るためのプロンプト攻撃を評価。
- 攻撃成功率と正しく動作する有用性を評価。

#### System-IFEval

- システムプロンプト内の正確な指示にモデルが従えるかを検証。
- 基本タスクと追加の制約で構成。

#### AgentDojo

- ツール呼び出しタスクにおけるプロンプト攻撃への頑健性を評価。
- 実際の環境を模したタスクスイートを使用。

### 実験に使用されたモデル一覧

- Gemini 1.5 Flash 8B 001
- Gemini 1.5 Flash 8B 002
- GPT-4o mini 2024-07-18
- GPT-4o 2024-08-06
- o3 mini
- DeepSeek V3
- DeepSeek R1
- Llama 3 8B
- Llama 3.1 8B Instruct
- Llama 3.2 3B
- Llama 3 8B Instruct
- Qwen 2.5 7B
- OLMo 7B

## システムプロンプトの複数ガードレール問題

研究チームは、LLMがシステムプロンプト内の複数のガードレールをどの程度守れるかを調べるために、「モンキーアイランド・ストレステスト」という評価手法を開発しました。このテストでは、実際のアプリケーションで使われている「選択型アドベンチャーゲーム」のシステムプロンプトに、様々な数のガードレールを追加してモデルの挙動を調査しました。

ガードレールとは、「特定の条件下では特定のテキストを出力する」といった具体的な制約指示のことです。研究チームは事前に設定したユーザーメッセージを送信し、モデルがそれらの指示に正確に従っているかを確認しました。

このテストの結果、最新のLLMでも、ガードレールの数が増えるにつれて、指示を守る能力が著しく低下することが明らかになりました。重要なのは、このテストには対立する指示、敵対的入力、ツール呼び出し、長い文脈といった特に複雑な要素が含まれていないにもかかわらず、このような結果が出た点です。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86276_2-1024x255.png)

ガードレール数の増加に伴うモデル性能低下を示すストレステストの結果

さらに研究チームがGPTストアやHuggingChatのプロンプトを調査したところ、実際のアプリケーションでは平均5.1個のガードレールが用いられていることが分かりました。特にGPTストアでは、HuggingChatよりも多くのガードレールが設定されている傾向があります。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86276_3-1024x700.png)

現実のシステムプロンプトが含むガードレール数の分布

このことから、現実的なアプリケーションでは複数のガードレールを同時に確実に守れる仕組みが求められている一方で、現状のモデルではそれを十分に達成できていないという重大な課題が浮かび上がりました。つまり、システムプロンプトをより確実に実施するための新しい仕組みや手法が必要とされています。

この問題は、LLMをエージェントとして利用する場合に深刻になります。ツールの呼び出しを繰り返して目標を達成しようとすると、会話が数十ターン、場合によっては数百ターンに及び、システムプロンプトを遵守することがさらに難しくなります。

## データの収集手法

研究チームは、LLMがシステムプロンプトに従う能力を強化するため、教師あり微調整（SFT）や好み最適化（DPO）の訓練に利用可能なデータを収集しました。このデータは多様で現実的なサンプルを含み、実際の利用環境を忠実に再現しています。

### システムプロンプトの収集

研究チームは、OpenAIのGPTストアから実際に使用されているシステムプロンプトを収集しました。GPTストアには、ユーザーが作成したさまざまな用途向けのシステムプロンプトがあり、異なるタイプのガードレールが設定されています。

さらに、HuggingFaceのHuggingChatプラットフォームからも公開されているプロンプトを収集しました。収集後、以下のような基準でデータの整理とフィルタリングを行いました。

- ファイルや画像のアップロード、またはカスタムHTTP APIを必要としないものを選択
- 極端に長いプロンプト、重複プロンプト、非英語プロンプト、不適切な内容を含むプロンプトを除外
- Claude 3.5 Sonnetを使って明確なガードレール条項が含まれているプロンプトのみを抽出

最終的に評価用のプロンプト14個を別途保持し、訓練用として1,850個のプロンプトを準備しました。

### 一致/矛盾するユーザーメッセージの生成

システムプロンプトのガードレールを守れるかをテストするため、研究チームはシステムプロンプトに矛盾するようなユーザーメッセージと、矛盾しない（穏健な）メッセージをそれぞれ生成しました。

Claude 3.5 Sonnetを利用し、各プロンプトごとに以下を作成しました。

- システムプロンプトに矛盾するユーザーメッセージ：約5件
- システムプロンプトに一致するユーザーメッセージ：約5件

合計18,497件のユーザーメッセージが生成されました。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86276_4-1-edited.png)

トレーニングデータを生成するプロセス

### ツール呼び出しを含むアシスタントメッセージ

GPTストアにある多くのアシスタントがツールを呼び出すため、研究チームはツール呼び出し機能を備えたシンプルなチャットアシスタントを開発しました。このアシスタントには以下のツールが搭載されています。

- Braveを使用したウェブ検索
- Scrapflyを使用したウェブブラウジング
- ローカルでのPythonコード実行
- モック画像生成API（モデルが生成した画像プロンプトを記録）

これらのツール呼び出しの記録と最終的なアシスタント応答は、教師あり微調整用データセット（RealGuardrails-SFT）として収集されました。

### アシスタントメッセージの選好ペア

DPOによる微調整用のデータセット作成のため、RealGuardrails-SFTから1,000件のシステムプロンプトと対応するユーザーメッセージを抽出しました。

- 「良い」応答は、GPT-4oによる既存のアシスタント応答
- 「悪い」応答は、Mistral 7B Instruct v0.3が生成した応答から、Claude 3.5 Sonnetで最も不適切と判断されたものを選択

このようにして、9,968件の選好ペア（良い応答と悪い応答）が生成され、RealGuardrails-DPOというデータセットになりました。

## システムプロンプトの信頼性向上実験

研究チームは、実際の使用環境に近いデータを用いて、LLMがシステムプロンプトにより確実に従えるようにするための方法を検証しました。具体的には、基本となる複数のモデル（Llama 3 8B、Qwen 2.5 7B、OLMo 7B、Llama 3.2 3B）を対象に、異なる学習手法や推論時の改善手法を組み合わせて実験を行っています。

### モデルの学習方法

高品質なデータを使った教師あり微調整（SFT+）はモデルの性能を一定程度向上させ、さらに好みの最適化（DPO）を併用すると大きな改善が見られました。この傾向は評価に用いたベンチマークの種類にかかわらず一貫していました。

#### 教師あり微調整（SFT）

高品質なSFTデータは、短い質問や長い対話、シンプルから複雑な指示、実際のユーザー発言や悪意ある質問、ツール使用例など多岐にわたっています（RealGuardrails SFT、Multifaceted Collection、Glaive v2、SPMLなど）。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86276_5-1024x636.png)

高品質SFTデータセットの構成要素一覧

比較対象としては、単純な指示のみを含む基本データ（SlimOrca）を用いました。

#### 好みの最適化

教師あり学習の後、さらに良い回答と悪い回答を区別して学習するDPOやSimPOという手法を試しました。

- **DPO** は一貫して性能を改善。
- **SimPO** は効果が限定的で、高い学習率では性能が下がりました。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86276_7-1024x221.png)

システムプロンプト遵守度を高める複数のFine-tuning手法の比較結果

#### 命令の区別を助ける仕組み

先行研究で提案されていた命令セグメント埋め込み（ISE）を試しました。これはシステム指示やユーザー発言など、会話の異なる部分をモデルが区別しやすくする方法です。

実際のテストでは一定の効果はありましたが、大きな改善は見られず、場合によっては性能低下もありました。また、ISEの利用にはモデルの構造を大きく変更する必要があり、効率的な推論の妨げにもなります。

#### 既に調整済みのモデルへの追加学習

既に調整されているモデル（例：Llama 3.1 8B Instruct）にも、これらの方法を適用しました。特にSFT+とDPOを組み合わせた追加学習により、調整済みモデルでもシステム指示遵守の性能が大幅に向上し、GPT-4o-miniよりも優れた結果を示しました。

### 推論時の工夫

研究チームはモデルが回答を生成する際の工夫も検証しました。具体的には以下のような手法です。

#### クラシファイアフリーガイダンス

モデル自身が不自然な単語を選択しないよう閾値を設定し、応答の品質を向上させる手法を試しました。一部のテストで改善が見られましたが、DPO調整済みモデルでは効果が限定的でした。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86276_8-1024x282.png)

推論時の追加操作が命令遵守性能に与える影響を示す図

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86276_10-1024x673.png)

禁止語リストの導入がテストケースの難易度を上昇させる様子

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86276_11-1024x1001.png)

連続するディストラクター（長い対話や例示によってモデルの注意をそらし、元のガードレールや命令文を無視させようとするもののこと）が命令遵守を妨げる過程を示す結果

#### 回答の見直し機能

生成した回答をモデル自身に再評価させ、システム指示に従うよう修正させる方法も試しました。効果はテストによって異なり、この手法を効果的に使うには [強化学習](https://ai-data-base.com/archives/26125 "強化学習") による追加訓練が必要かもしれません。

#### Split-softmax

システムメッセージを特別扱いして重要性を高めるsplit-softmaxという手法も検証しました。しかし、実験結果では性能向上が見られませんでした。

### AgentDojo

AgentDojoという、複数のツールを使用しながら目標を達成する難しいベンチマークでも評価しました。MetaのLlama 3.1 8B Instructモデルを調整したところ、攻撃成功率を大きく下げつつ、ユーザーの目標達成率を向上できました。

DPOでの調整は安全性を高める一方で、若干の有用性低下も確認されました。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86276_9-1024x253.png)

AgentDojoタスクでのモデル成功率と攻撃成功率の一覧

以上の実験結果から、システム指示を確実に守れるようにするには、高品質なデータを用いた適切な学習手法と、推論時のさまざまな工夫を組み合わせることが重要であることが明らかになりました。

## 考察

実験を通して、言語モデルがシステム指示を守れなくなるのは主に、長い会話や多くのルールがある場合であることがわかりました。

ただし、ユーザーの要望とルールが対立する場合もあれば、モデルの内部構造に問題がある場合もあります。安全対策を効果的に組み込むには、弱点を見極めることが大切です。

### 推論能力の影響

広い文脈や複雑な制約を扱えるモデルは、長い会話でも指示を守りやすい傾向があります。多くの保護ルールがあっても、情報を途中で確認し直せるモデルは、ルール違反が少なくなります。

ただし、推論方法によって結果にばらつきがあり、全ての場合に対応できるわけではありません。推論力が高まれば指示への対応は安定すると期待できますが、完璧を目指すのは簡単ではないようです。

### 対話とシステムの複雑さ

ルールの数が増えるほど、モデルが指示を忘れやすくなります。複数の制約を同時に課されたり、混乱させる入力を受けたりすると、守るべきルールを間違える危険が高まります。

実際の会話では長いやり取りや外部ツールの使用も含まれるため、予想よりも早く矛盾が生じることもあります。システム指示の書き方や、モデルとのやり取り方法をよく考える必要があるでしょう。

### 今後の展望

システム指示の優先度は絶対ではなく、学習データや推論方法によって変わることがわかりました。信頼性を高めるには、避けるべき回答例を多く含む良質なデータで学習させ、さらに回答を出す前に追加チェックを行うなど、複数の対策を組み合わせるのが効果的です。

指示違反を確実に防ぐには、モデル自身がルールを理解するだけでなく、誤った方向に導こうとする入力を自動的に検知してブロックする仕組みも役立つでしょう。言語モデルの利用が広がるにつれ、こうした安全対策も着実に発展させていく必要があります。

## まとめ

本記事では「システムプロンプトの信頼性」に関する研究を紹介しました。

複雑な指示やルールがある状況で言語モデルが正しく動作するには、まだ改善の余地があることが明らかにされました。

そこで、好み学習（DPO）などの手法を使うと、ルール違反をかなり減らせることがわかりました。ただし、長い会話や悪意ある質問に対しては、まだ完全には対応できないようです。

今後、より多様なデータを集めて効果的な調整を進めることにより、実用的な場面での信頼性がさらに高まると期待されています。ただしモデル開発ではなくユーザー目線としては、現在時点での「モデルがシステムプロンプトに従う力」に関する検証結果を参考にするのがよいかと思います。

**参照文献情報**

- タイトル：A Closer Look at System Prompt Robustness
- URL： [https://doi.org/10.48550/arXiv.2502.12197](https://doi.org/10.48550/arXiv.2502.12197)
- ベンチマーク： [https://github.com/normster/RealGuardrails](https://github.com/normster/RealGuardrails)
- 著者：Norman Mu, Jonathan Lu, Michael Lavery, David Wagner
- 所属：University of California, Berkeley

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[自然言語タスクをコードタスクに変換してLLMに高度な推論を実行させる](https://ai-data-base.com/archives/83997)

[LLM評価の盲点とそれを解消する手法](https://ai-data-base.com/archives/83704)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)