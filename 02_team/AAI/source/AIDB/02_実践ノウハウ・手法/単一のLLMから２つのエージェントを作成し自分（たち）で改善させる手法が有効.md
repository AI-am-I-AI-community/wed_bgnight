---
title: "単一のLLMから２つのエージェントを作成し自分（たち）で改善させる手法が有効"
source: "https://ai-data-base.com/archives/82124"
author:
  - "[[AIDB Research]]"
published: 2025-01-15
created: 2025-06-13
description: "本記事では、言語モデルの\"自己改善\"に新しいアプローチを提案する研究を紹介します。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、言語モデルの”自己改善”に新しいアプローチを提案する研究を紹介します。

従来の単一モデルによる自己改善では数回の学習で頭打ちになる問題がありましたが、今回複数のモデルを協調させることで継続的な性能向上を実現する手法が開発されました。

その背景には、LLMは既存のインターネット上のデータを使い尽くしていると言われている状況があり、そのため性能向上の新たな方法が必要とされています。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82124-1-1024x576.jpg)

**発表者情報**

- 研究者：Vighnesh Subramaniam et al.
- 研究機関：MIT CSAIL, ハーバード大学, スタンフォード大学, Google DeepMind

## 背景

LLMの開発はどんどんと進展していますが、実は根本的な課題を抱えているとされています。学習に使用できるデータ量が限られているのです。現在のLLMはインターネット上の質の高いデータのほとんどを使い尽くしていると考えられており、さらなる性能向上には新たなアプローチが必要とされています。

これまでは最先端のLLMを教師として追加の学習データを生成する手法が試みられてきました。しかし教師となるモデル以上の性能は得られず、計算コストも膨大になります。また商用モデルを使用する場合、法的な制約も立ちはだかります。

また、LLMが自分で生成したデータを使って学習を重ねる「自己改善」という手法も提案されてきました。しかし数回の反復学習で性能向上が頭打ちになり、出力される回答の多様性も失われていく傾向が確認されています。

そこで今回、MIT、ハーバード大学、スタンフォード大学、Google DeepMindの研究チームは、複数のLLMを協調させるアプローチを開発しました。単一のモデルを改善する代わりに、複数のモデルをそれぞれ異なる得意分野に特化させます。モデル間で多様な推論能力を維持しながら、継続的な性能向上を実現することを目指しています。

複数のモデルを「専門家チーム」のように機能させることで、単一モデルでは克服できなかった限界を乗り越えようとする意欲的な試みです。

以下で詳しく紹介します。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82124_1-1024x345.png)

MATHデータセットにおける複数回のファインチューニング結果。マルチエージェントファインチューニングは、単一モデルのファインチューニングと比較して継続的な性能向上を示す。

## マルチエージェント方式による言語モデルの改善手法

今回の提案手法のポイントは大きく2つです。第一に、モデル間の議論を通じて学習データを生成するマルチエージェントディベートです。第二に、生成されたデータを用いて各モデルを特化させる学習方法です。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82124_2-1024x492.png)

マルチエージェントファインチューニングの概要図。まずマルチエージェントディベートと多数決で学習データを作成し、それを生成モデルと評価モデルのファインチューニングに使用する流れを示す。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82124_3.png)

上記は今回提案されたマルチエージェントファインチューニングのアルゴリズムです。LLMを複数の専門家に育てていく過程を表現したものです。

1. プロセスはまず必要な要素を整えることから始まります。事前学習済みのLLMモデル、入力データのセット、エージェントの数、ディベートのラウンド数、そしてファインチューニングの反復回数が必要とされます。
2. 最初のステップでは、同じLLMをN個コピーして「生成エージェント」のチームを作り、さらに同じ数だけコピーして「評価エージェント」のチームを編成します。
3. 次に反復学習のプロセスに入ります。このプロセスはL回繰り返されます。まずディベートプロセスでは、生成エージェントたちが最初の回答を出し、それに対して評価エージェントたちが改善案を提示します。その後、多数決によって「正解」が決定されます。
4. 続いて学習データの収集が行われます。生成モデルのためには、正解と一致した回答が収集されます。評価モデルのためには、ディベート過程での改善事例が集められます。
5. 最後に専門性の確立段階に入ります。各生成エージェントと評価エージェントは、それぞれ収集されたデータを用いて個別に学習を行います。
6. 各エージェントは自身が正しく回答できた事例から学習を進めていきます。異なるデータで学習することで、チーム全体としての多様性を維持しながら、総合的な性能向上が図られます。生成エージェントは質の高い初期回答を、評価エージェントは他者の回答の改善を得意とする専門家として成長していきます。

以下で手法の全体を段階的に紹介します。

### モデル間の議論による回答生成

マルチエージェントディベートでは、N個の言語モデルが議論を通じて回答を洗練させていきます。議論は以下のように進行します。

1. 各モデルが問題に対する回答を生成します
2. 他のモデルの回答を要約した情報が共有されます
3. 各モデルは自身の回答と他モデルの回答を踏まえて、より良い回答を生成します
4. 最終ラウンドで多数決を取り、最終的な回答が決定されます

### 生成データによる学習

マルチエージェントディベートで生成されたデータを用いた学習方法について説明します。

まず、入力文のセット{xi}に対して、N個のモデルによるM回のディベートが実施されます。

多数決で採用された回答ŷiが、その問題の「正解」として扱われます。

単一モデルの場合、入力xiに対する回答yiがŷiと一致する場合のみ、そのデータが学習に使用されます。しかし、最終的な回答ŷiは似通った形式や解法に偏りやすく、複数回の学習を重ねると性能向上が頭打ちになることが確認されています。

### 生成モデルと評価モデルの役割分担

提案手法では、複数のモデルを連携させながらも、それぞれのモデルが異なるデータセットで学習を行い、特定の役割に特化していきます。単一のデータセットで全てのモデルを学習させるのではなく、各モデルに適したデータセットを構築することで、精度と多様性の両立を目指しています。

#### （１）生成モデルの学習

生成モデルの役割は、問題に対する最初の回答を作成することです。学習データとして、最終的な多数決の結果と一致した回答のみが使用されます。N個の生成モデルそれぞれに対して、以下の手順でデータセットが構築されます。

1. 入力xに対する各モデルの回答yを収集
2. 多数決で採用された回答ŷと一致するyのみを抽出
3. 入力xと回答yのペアを学習データとして使用

各モデルで異なる学習データセットが作られることで、モデル間の多様性が維持されます。

#### （２）評価モデルの学習

評価モデルは、他のモデルの回答を吟味し、改善案を提案する役割を担います。単純な正誤判定だけでなく、より良い回答を導き出すためのフィードバックが求められます。特に難しい問題では、評価モデルによる改善プロセスが重要となります。

評価モデルの学習データには、ディベートの最終ラウンドで正解となった回答と、そこに至るまでの議論の過程が含まれます。学習を通じて、有効な改善提案ができるようになることが期待されます。

これらの生成モデルと評価モデルは、互いに補完し合いながら全体の性能向上に貢献します。単一のモデルでは実現が難しい、継続的な改善が可能となる仕組みが構築されています。

### 反復学習による改善

学習済みの生成モデルと評価モデルは、次の学習サイクルでマルチエージェントディベートを実施するために活用されます。学習済みモデルを用いてデータセットを収集し、さらなる学習を進めることで、段階的な性能向上が目指されます。

学習サイクルのプロセスは以下のようになります。

1. 前回の学習で得られた生成モデルと評価モデルでディベートを実施
2. 新たに生成されたデータを用いて、次回の学習用データセットを構築
3. モデルの役割に応じて異なるデータセットで学習を実施
4. 学習済みモデルを用いて次のサイクルへ

なお実験では、このプロセスを5回繰り返すことで、継続的な性能向上が確認されています。

### 推論時の動作

実際の運用時には、学習済みの生成モデル群と評価モデル群が連携して動作します。

#### （１）回答生成の流れ

1. 生成モデルが初期回答を提案
2. 各評価モデルが他のモデルの回答を参照
3. 回答の要約が共有され、改善案が提示
4. 複数ラウンドの議論を経て最終回答を決定

#### （２）最終判断

最後のラウンドでは、各モデルの回答に対して多数決が実施されます。投票で最も支持された回答が、システムの最終出力として採用されます。

実験結果からは、複数のモデルによる段階的な改善プロセスが、単一モデルよりも高い精度をもたらすことが示されています。また、異なる役割に特化したモデルを組み合わせることで、回答の多様性が維持されることも確認されています。実験全体の概要は下記で詳しく紹介します。

## 評価実験

マルチエージェント学習の効果を検証するため、3つの数学的推論タスクを用いた評価が実施されました。

### 算術計算タスク

算術計算タスクでは、以下のような形式の1000問の問題が使用されました。

a + b · c + d – e · f

各変数には30以下のランダムな整数が割り当てられます。基本的な四則演算の処理能力を評価することを目的としています。

### 学校数学（GSM）タスク

GSMタスクは、複数のステップを必要とする文章題で構成されています。単純な計算だけでなく、問題文の理解と解法の組み立てが求められます。各問題には、問題文、数値による解答、解答に至る説明が含まれています。

### 競技数学（MATH）タスク

MATHタスクは、5段階の難易度に分類された競技レベルの数学問題から構成されています。評価実験では、最初の3段階の難易度から問題が選択されました。

### 評価方法

各タスクについて、500問がモデルの学習用に、別の500問が評価用に使用されました。評価は、モデルが出力した解答と正解の一致率で測定されています。また、性能向上の有意性を確認するため、各精度値の標準誤差も計算されました。

なお、GSMタスクについてのGPT-3.5の評価のみ、より正確な信頼区間を得るため、1000問の評価用問題が使用されました。

### 比較対象の手法

提案手法の有効性を検証するため、以下の5つの手法との比較が行われました。

#### （１）基本モデル（Base）

単一の言語モデルが入力を処理し、回答を生成する最もシンプルな手法です。

#### （２）多数決方式（Majority）

複数のモデルからの回答について多数決を取る手法です。過半数の合意が得られない場合は、候補となる回答からランダムに1つが選択されます。

#### （３）ディベート方式（Debate）

先行研究で提案された、モデル間の議論を通じて回答を導き出す手法です。本研究の手法のベースとなっていますが、学習による改善は含まれていません。

#### （４）STaR方式

既存の自己改善手法の一つです。まずモデルが問題に対する回答を生成し、正解と照合して正しい回答が学習データに追加されます。不正解だった問題については、正解を含むヒントを与えて再度回答を生成し、正解を含む回答が得られた場合に学習データに追加されます。学習損失が収束するまで、データ収集と学習のプロセスが繰り返されます。

#### （５）多数決と学習の組み合わせ（Majority FT）

多数決方式に学習を組み合わせた手法です。複数のモデルに問題を解かせ、多数決で採用された回答と一致する回答を学習データとして収集します。収集したデータで学習したモデルの出力に対して、再度多数決を適用して最終的な回答を決定します。

公平な比較のため、マルチエージェントを使用する手法では全て3つのエージェントと2ラウンドのディベートが採用されました。また、5つのエージェントを使用した追加実験の結果は付録に記載されています。

### 定量評価の結果

評価は1回のみの学習を適用した状態で実施され、4種類の言語モデルで検証されました。

#### （１）オープンソースモデルでの結果

- Phi-3 (4B)
- Mistral (7B)
- LLaMA-3 (8B)

これらのモデルを用いた実験では、提案手法が全てのタスクで最高精度を達成しました。たとえば、Phi-3では算術タスクで99.4%、学校数学タスクで88.6%、MATHタスクで58.8%の正答率を記録しています。

#### （２）商用モデルでの結果

GPT-3.5を用いた実験でも同様の傾向が見られ、全てのタスクで既存手法を上回る結果が得られました。

#### （３）STaR方式との比較

注目すべき点として、STaR方式は正解データを使用して学習データを選別し、複数回の学習を実施しているにもかかわらず、提案手法（正解を使用しない1回の学習）の方が高い性能を示しています。

#### 性能向上の要因

提案手法の優位性は、以下の要素によってもたらされていると考えられます。

1. 多数決による投票、マルチエージェントディベート、学習による改善が相互に補完し合っている点
2. 生成モデルと評価モデルという異なる役割に特化させることで、より洗練された回答生成が可能になっている点

また、提案手法は500問という比較的少ない学習データでも有意な改善を示しており、特に学校数学やMATHといった複雑なタスクでその効果が顕著に表れています。

### 複数回の学習による効果

#### ①長期的な性能向上の検証

MATHデータセットを用いて、Phi-3とMistralの2つのモデルで5回の繰り返し学習の効果が検証されました。

#### ②提案手法の結果

提案手法では、学習を重ねるごとに継続的な性能向上が確認されました。

- Phi-3：58.8%から66.0%へ向上
- Mistral：22.5%から28.2%へ向上

最も性能の高いベースラインと比較しても、5回の学習後には、

- Phi-3で12.6%
- Mistralで9.31%

の性能向上が達成されています。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82124_4-1024x794.jpg)

提案手法と既存手法の定量的な比較結果。すべてのデータセットで提案手法が最高性能を達成したことを示す。

#### 従来手法との比較

一方、単一モデルによる従来の学習手法（Single-agent FT）では、1回目の学習後に性能が頭打ちとなり、その後は低下する傾向が観察されました。性能低下の原因は、モデルが限られた種類の回答パターンに固定化（オーバーフィット）してしまうためと分析されています。

#### 多様性の維持

提案手法では、複数の生成モデルと評価モデルをそれぞれ異なるデータで学習させることで、回答の多様性が維持されています。多様性の維持が継続的な性能向上を可能にする要因となっていると考えられます。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82124_5-1024x228.png)

ファインチューニングを重ねても多様性が維持されることを示す。マルチエージェント方式は回答の多様性を保ちながら性能を向上させる一方、単一エージェント方式では多様性が急激に低下する。

## 詳細分析

マルチエージェント学習の効果をより深く理解するため、いくつかの観点から分析が行われました。

### 観点①各要素の重要性

提案手法の構成要素を1つずつ取り除いた実験が実施されました。要約部分を省いた場合、評価モデルを省いた場合、単一モデルのみを使用した場合など、様々な条件での性能が比較されました。実験結果から、要約による情報の整理、生成・評価モデルの役割分担、複数モデルの協調が、それぞれ性能向上に貢献していることが確認されました。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82124_6-1024x590.jpg)

アブレーション実験の結果。提案手法の各コンポーネント（要約、評価モデル、マルチエージェント学習、マルチエージェントディベート）がそれぞれ性能向上に寄与することを示す。

### 観点②回答の多様性評価

回答の多様性は複数の指標を用いて測定されました。

#### 確率的な評価

あるモデルから見て、他のモデルの回答がどの程度予測しにくいものかを確率で評価しています。予測しにくい回答が多いほど、多様性が高いと判断されます。

#### 埋め込み表現の違い

回答文を数値ベクトルに変換し、ベクトル間の距離で多様性を測定しています。距離が大きいほど、回答内容が異なることを示します。

実験結果からは、提案手法では学習を重ねても回答の多様性が維持される一方、単一モデルでの学習では多様性が急速に失われていく傾向が観察されました。

### 観点③性能と多様性の関係

多様性の指標と正答率の相関が分析されました。分析結果から、回答の多様性が高いほど正答率も高くなる傾向が見出されています。単一のアプローチに固執せず、複数の視点から問題を検討できることが、正確な回答の導出につながっていると解釈されます。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82124_7.png)

多様性と精度の関係を示すグラフ。マルチエージェントファインチューニングは多様性を維持しながら精度を向上させることを示す。

### 観点④汎用性の検証

MATHデータセットで学習したモデルを、学校数学（GSM）タスクで評価する実験が実施されました。提案手法で学習したモデルは、GSMデータで直接学習した既存手法よりも高い性能を示しました。学習していない問題でも高い性能を発揮できることから、提案手法による学習が特定のタスクに過度に特化することなく、汎用的な推論能力の向上に寄与していることが示唆されます。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82124_8.png)

モデルのゼロショット汎化能力を示す結果。MATHデータセットで学習したモデルがGSMデータセットでも高い性能を発揮することを示す。

## まとめ

本記事では、複数の言語モデルを協調させることで継続的な性能向上を実現する研究を紹介しました。

実験では、オープンソースモデルと商用モデルの両方で提案手法の有効性が確認されましたが、複数のモデルを訓練・実行する必要があるため、計算コストの増加が課題として指摘されています。

今後は、人間からのフィードバックを取り入れた学習手法との組み合わせなど、さらなる発展が期待されます。

**参照文献情報**

- タイトル：Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains
- URL： [https://arxiv.org/abs/2501.05707](https://arxiv.org/abs/2501.05707)
- プロジェクトページ： [https://llm-multiagent-ft.github.io/](https://llm-multiagent-ft.github.io/)
- 著者：Vighnesh Subramaniam, Yilun Du, Joshua B. Tenenbaum, Antonio Torralba, Shuang Li, Igor Mordatch
- 所属：MIT CSAIL, Harvard University, Stanford University, Google DeepMind

## 理解度クイズ（β版）

1\. 提案された手法において、複数のモデルを連携させる主な目的は何ですか？

各モデルが異なるデータセットで学習し、特定の役割に特化することで多様な推論能力を維持。単一モデルでは達成できない継続的な性能向上を実現している。

解説を見る

2\. 評価モデルの主な役割は何ですか？

評価モデルは他のモデルの回答を検討し、改善のためのフィードバックを提供する役割を担う。特に難しい問題では、評価モデルによる改善プロセスが重要となる。

解説を見る

3\. 実験結果から、単一モデルによる従来の学習手法の主な問題点として明らかになったことは？

単一モデルでは限られた回答パターンに固定化（オーバーフィット）する傾向がある。複数回の学習を重ねても性能向上が見られず、むしろ低下する問題が確認された。

解説を見る

4\. マルチエージェントディベートのプロセスで、最終的な回答はどのように決定されますか？

マルチエージェントディベートの最終ラウンドでは多数決が実施される。投票で最も支持された回答がシステムの最終出力として採用される。

解説を見る

5\. 提案手法の汎用性を示すために行われた実験は何ですか？

MATHデータセットで学習したモデルをGSMタスクで評価し、直接学習していない問題でも高い性能を示すことを確認。学習が特定のタスクに過度に特化せず、汎用的な推論能力の向上に寄与することが示された。

解説を見る

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[マルチモーダルLLMによる表やグラフの理解力を向上させる方法](https://ai-data-base.com/archives/82014)

[生成AIシステムのセキュリティ評価 マイクロソフトが100事例から得た教訓](https://ai-data-base.com/archives/82195)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)