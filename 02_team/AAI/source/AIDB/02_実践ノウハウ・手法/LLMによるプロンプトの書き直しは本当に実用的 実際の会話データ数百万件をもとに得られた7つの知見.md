---
title: "LLMによるプロンプトの書き直しは本当に実用的 実際の会話データ数百万件をもとに得られた7つの知見"
source: "https://ai-data-base.com/archives/87309"
author:
  - "[[AIDB Research]]"
published: 2025-04-01
created: 2025-06-13
description: "結論から言うと、”LLMによるプロンプトの書き直し”は本当に実用的だと報告されています。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

結論から言うと、”LLMによるプロンプトの書き直し”は本当に実用的だと報告されています。

Microsoftなどの研究者らは、数百万件におよぶ実際の会話データをもとに、LLMによるプロンプトの書き直しによってLLMから「より優れた回答」が得られるのかを確かめました。

その結果、

1. 普遍的に明確な効果がある
2. 会話が長く続くほどプロンプト書き直しの効果が大きい　など

全部で7つの実用的知見が明らかになりました。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87309-1024x576.png)

参照論文情報は記事の下部に記載されています。

## 背景

人間がLLMとやり取りする機会は、仕事や日常生活の中でますます増えています。しかし、自分の意図をLLMに明確に伝えるプロンプトを作ることは難しく、望んだ回答が得られない状況は頻繁に生じます。

例えばLLMで資料作成や情報収集といったことをやるにしても、LLMが理解しやすいプロンプトを書ける人とそうでない人がいます。プロンプトが曖昧だと、LLMが適宜内容を推測して回答を生成するため、ユーザーの本来の意図とは異なる回答を提示する場合があります。

プロンプト改善を支援する既存ツールもありますが、専門知識を持つ一部のユーザー向けのものが多く、一般的なユーザーが手軽に利用できる状況にはありません。その結果、一部のユーザーにとっては、LLMの活用において十分な利益を得られているとは言えない状況があります。

こうした背景の中、今回マイクロソフトなどの研究チームは、”LLM自身にプロンプトの書き直しを行わせる”ことでユーザーが望む回答は得やすくなるのかを検証しました。このテクニックはユーザーコミュニティでは親しまれてきましたが、実際の膨大な会話データで効果を検証されたのは今回が初です。

研究者らは、さまざまな種類や規模のLLMを調査し、プロンプト書き直しの効果を幅広く確認しました。 その結果、単に有効性を確認するだけではなく、プロンプト書き直しのプロセスで役立つ実用的なノウハウや知見も合計で7つ得ることができました。

以下で詳しく紹介します。

## 研究のアプローチ

研究で具体的にどのような問題設定やデータが使われたかを説明します。

今回の研究の目的は、LLMを使って人間が入力したプロンプトを適切に書き直すことで、LLMから得られる回答の質が向上するかどうかを確かめることです。

研究者らはこの検証をリアルタイムの会話ではなく、過去に実際に行われたLLMとのやり取りを使って、後からシミュレーション的に行いました。

まず、「LLMとのやりとり」（対話）とは、ユーザーが何か質問をしたり依頼をしたり（これをユーザーターンと呼びます）、それに対してLLMが回答したり（これをモデルターンと呼びます）というやりとりが繰り返されるものです。

研究チームは過去の対話の記録から、ユーザーがLLMの回答に対して明確に不満足を示した箇所を特定します。こうした不満足な回答が起きた原因の一つは、ユーザーが書いたプロンプト（質問や指示）が不明確だったり不適切だったりする場合です。

ここで、研究チームが行うのは以下の流れです。

1. まず、LLMにユーザーの元々のプロンプトを与え、これを「より良い形」に書き直させます。この書き直しを行うLLMを「rewriter（書き直しモデル）」と呼びます。
2. 次に、書き直したプロンプトを別のLLM（「chatbot（対話モデル）」と呼ばれる、ユーザーが元々やりとりしていたもの）に与え、元の回答より改善された回答が得られるかどうかを確認します。

具体的な例としては、あるユーザーが「ルーレットゲーム」とだけ書いてLLMに質問した場合、元々の回答はユーザーが満足するものではなかったかもしれません。そこで、rewriterが「Pythonで複数人プレイ可能なカジノのルーレットゲームを作る方法を教えてください」というようにプロンプトを書き直すことで、より良い回答が引き出せる可能性がある、という流れになります。

つまり、この研究ではユーザーの意図が曖昧なプロンプトをLLM自身に明確化させることで、その回答の質を向上させる可能性を探ります。ユーザーが満足しなかった回答に遡って検証するという点が、この研究の大きな特徴です。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87309_1.png)

ユーザーが不満を示した回答から逆方向に遡り、どのターンのプロンプトを書き直すべきかを特定する方法を示した概念図

### 使用したプロンプト

今回「LLMによってプロンプトを書き直す」際に使用されたプロンプトは以下の通りです。

原文

```js
Goal: Given a user’s query and their conversational history with an AI Chatbot, your task is to identify the aspects in which the query can be improved or if it’s already optimal, identify the aspects in which it is already effective. To do so, first analyze the query for aspects of improvement or describe aspects that are already effective. Then, propose a list of one or more possible rewrites that communicates the user’s needs and goals more effectively as an input to an AI Chatbot while keeping the user intent intact. Be careful not to change the goal or the intent of the user when you propose a rewrite keeping in mind the Conversational History. For each rewrite, if you have to add any new information that is not present in the Conversational History to make the query better, list the assumptions you need to make.

Task: Given a user Query, your task is to output the following:  
First, output whether or not the Query needs modification for eliciting an effective response from an AI Chatbot. If it’s a good query and doesn’t need any modification at all, output NO MOD. If it requires some modification, output SOME MOD. If the Query requires to be heavily rewritten, output HEAVY MOD.

If you chose NO MOD, output the aspects of the Query that makes it an effective query in a markdown table in the following format:  
<table format>

If the query needs any rewrite (that is, if you answered SOME MOD or HEAVY MOD in the previous question), output the aspects of improvement in a markdown table in the format below:  
<table format>

DO NOT answer the input Query, your job is only to evaluate how well it expresses the user’s information need from a Chatbot.

Conversational History: query_context  
Query: target_query
```

日本語訳

```js
目的：ユーザーがAIチャットボットに対して行ったプロンプト（質問や指示）とその会話履歴をもとに、そのプロンプトを改善できる要素を特定するか、または既に十分なプロンプトであればその有効な要素を特定してください。そのために、まずプロンプトの改善点、あるいは既に良い点を分析します。その上で、ユーザーの意図を変えずに、AIチャットボットがユーザーのニーズや目的をよりよく理解できるように、プロンプトを書き直してください。書き直しの際、会話履歴に含まれていない新たな情報を追加する必要がある場合には、どのような仮定や推測を行ったかを明記してください。

タスク：与えられたユーザーのプロンプトに対して、以下の内容を出力してください：

まず、そのプロンプトがAIチャットボットから効果的な回答を引き出すために修正が必要かどうかを判断してください。修正が不要な良いプロンプトなら「NO MOD」と出力してください。多少の修正が必要なら「SOME MOD」と出力してください。大幅な修正が必要なら「HEAVY MOD」と出力してください。

「NO MOD」を選んだ場合、そのプロンプトを良いプロンプトにしている要素を以下のマークダウン形式の表で出力してください：  
<table format>

プロンプトに何らかの書き直しが必要である場合（「SOME MOD」または「HEAVY MOD」と判断した場合）は、改善すべき要素を以下のマークダウン形式の表で出力してください：  
<table format>

※入力されたプロンプトそのものに回答してはいけません。あなたの役割は、プロンプトがAIチャットボットにユーザーの情報ニーズをどれだけ適切に伝えているかを評価することだけです。

会話履歴： query_context  
対象プロンプト： target_query
```

### 使用したデータセット

本研究を行うためには、実際のユーザーとLLMの対話データが必要です。また、ユーザーがLLMの回答に対して不満足を示した場面を特定する必要もあります。

そのため、この研究では「WildChat」というデータセットの一部を使いました。WildChatは、実際にユーザーとLLMが行った多様なテーマの対話が記録されている大規模なデータセットです。

参考： [ChatGPTと実際に交わされた会話の世界最大規模データセット「WildChat」](https://ai-data-base.com/archives/67317)

研究では、このWildChatデータの中から、次のような特徴を持つデータを抽出しています。

- 英語での対話であること
- 有害（トキシック）な内容が含まれないこと
- 少なくとも3ターン以上のやりとりが存在していること

（※本記事の読者は日本語ユーザーがほとんどであるため、本来は日本語をメインで扱っている研究が望ましいですが、今回の研究は英語で調査されています）

さらに、このデータからユーザーが明確に不満足を示した箇所を抽出するために、先行研究で提案された「ユーザー満足度評価」の基準を利用しました。対話データに対してユーザーの各ターンを「満足（SAT）」「不満足（DSAT）」「どちらでもない（NONE）」のいずれかに分類するという方法です。この中で、少なくとも1つの「不満足（DSAT）」のラベルが付いた対話のみを分析対象として選びました。

WildChatはもともと規模が非常に大きく、さまざまなテーマ（ドメイン）やユーザーの目的（インテント）が含まれています。そのため、分析をより意味のあるものにするために、研究チームは対話データを「ドメイン（話題）」と「インテント（ユーザーが達成したいこと）」の組み合わせで分類しました。

より詳細には、

- ソフトウェア開発（情報を求める）
- ソフトウェア開発（何かを作成する）
- 文章作成・ジャーナリズム（何かを作成する）
- 技術関連（情報を求める）
- 数学・論理（情報を求める）

という5つのカテゴリに対話データを分類し、それぞれにおいてプロンプト書き直しの効果を調査しました。

このように、実際の対話データをユーザーの不満足度、会話のテーマ、ユーザーの目的といった観点から分類することで、プロンプト書き直しがどのような条件や状況で有効であるかを詳しく分析する準備を整えました。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87309_2.png)

会話データの基本情報をまとめた表。会話のテーマ（ドメイン）とユーザーの目的（インテント）ごとに、会話数やターン数を記載

## LLMがどうプロンプトを書き直すのか、何を考えているのか検証する

繰り返しになりますが、ユーザーが満足できない回答を得てしまう原因の一つとして、そもそもユーザーのプロンプト（質問や指示）が曖昧だったり、情報が足りなかったりする場合があります。先述の通り本研究では、そのような場面において、LLM自身にユーザーのプロンプトを書き直させることで、会話への「介入」を行い、回答の質を改善するという手法を用いています。

研究チームは、LLMに対して特別な追加トレーニング（ファインチューニング）を行うのではなく、「プロンプティング」を採用しています。特定の指示やガイドラインをLLMに与えることで、望ましい作業を行わせる手法です。ユーザーの元のプロンプトを与えた上で、それをどのように書き直すべきかについてLLMに指示を与えています。

### プロンプトを書き直す方法

このプロンプト書き直しの手法は、大きく次の2つの段階に分けて実施されました。

まず研究チームは、書き直しが必要かどうかを判断するプロセスを導入しました。LLMは元のプロンプトを見て、それが書き直し不要（NO MOD）、少しの修正が必要（SOME MOD）、大きな修正が必要（HEAVY MOD）のいずれであるかを評価します。段階的な評価を行うことで、本当に書き直しが有効なケースのみを選び出すことができると考えられたのです。

次に、少しの修正が必要（SOME MOD）または大きな修正が必要（HEAVY MOD）と判断されたプロンプトについては、ユーザーの本来の意図を保ちながら、LLM自身がプロンプトを書き直します。つまり、書き直しは元のプロンプトにあった情報や文脈を損なわず、むしろそれを明確化・具体化する方向で行われます。

こうして書き直されたプロンプトを、再度、元の会話に登場した別のLLM（chatbot）に与え、新たな回答を生成します。そして、元の回答と書き直し後の回答を比較し、改善があったかどうかを確認します。

### 書き直しプロセスの中でLLMがどのように考えているか

本研究では単にプロンプトを書き直すだけでなく、書き直しの過程でLLMがどのように推測や判断を行っているかについても詳しく調べました。LLMに対して「なぜその書き直しをしたのか」を説明させることで、その判断の根拠を明確にしようとしたのです。

LLMには次の2つの点を説明するように求めました。

**①プロンプトの改善点について**

LLMに元のプロンプトを見せ、そのプロンプトの何が不十分なのか（例えば、「明確さ」や「具体性」が不足しているなど）を指摘させます。こうすることで、どのような要素を改善することでユーザーの質問や依頼が明確に伝わるのかを具体的に把握できます。 例えば、ユーザーが「圧縮アルゴリズムを作って」とだけ書いた場合、LLMはそれに対して「テキストデータ用の可逆圧縮アルゴリズムをPythonで書くコードを教えてください」というように具体化した書き直しを行うかもしれません。この場合、改善点として「具体性」や「明確さ」が挙げられます。

**②ユーザーの意図に関する推測について**

また、プロンプトが曖昧な場合、LLMは書き直しをする際にユーザーが本当に求めている内容を推測する必要があります。この推測についても明示させ、どのような前提や仮定を設けたのかを詳しく分析します。 例えば上記の圧縮アルゴリズムの例であれば、「ユーザーはテキストデータを可逆的に圧縮する方法を知りたいのだろう」という仮定をLLMが行っている可能性があります。このような仮定が明確に示されることで、LLMがどれほどユーザーの意図を適切に推測できるかを確認することができます。

以上のプロセスにより、プロンプトを改善する際にLLMがどのような判断をしているかを詳細に理解し、またノウハウを得られるようにしました。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87309_3.png)

ユーザーのプロンプトを書き直し、新たに生成された回答と元の回答を比較評価するプロセスを示した全体的な流れ図

## 実験の評価方法と結果

ここまで説明したように、この研究では実際のユーザーとLLMとの会話データを用いて、LLM自身がプロンプトを書き直すことで回答の質がどのように改善されるかを調査しました。

元々のプロンプトに対するLLMの回答（オリジナル回答）と、書き直したプロンプトに対するLLMの回答（書き直し後の回答）を比較して評価を行ったのです。

### 書き直し後の回答が本当に良くなったのかを評価する方法

実際のユーザーがどのような回答を「良い」と感じるかは主観的であり、明確な正解があるわけではありません。そのため、この研究では、書き直し後の回答が本当に改善されているかどうかを慎重に評価する必要があります。

そこで研究チームは、以下の2つの方法で評価を行いました。

**①LLMを評価者として使う方法（自動評価）**

性能が高く評価タスクに適した「gpt-4o」というLLMを用い、元々のプロンプトへの回答と書き直し後のプロンプトへの回答を比較させました。その際、元の会話の流れや文脈を考慮した上で、どちらの回答がユーザーにとってより役立つかを5段階評価で判定させています。

**②人間による評価（人手評価）**

しかし、LLMによる自動評価だけでは評価結果が本当に妥当であるかが保証できないため、100件の会話を対象に、人間による追加評価も行い、自動評価の妥当性を確認しました。人間が評価する際も、元の回答と書き直し後の回答のどちらがより良いかを同様の5段階評価で判定しています。

### この研究で調べられたLLMのモデル

- gpt-4o
- gpt-4o-mini
- llama-3-70B-Instruct
- llama-3-8B-Instruct
- Ministral-3B

### 調査から得られた主な知見

研究の結果、以下のような重要な知見が得られました。

① **プロンプトの書き直しは回答を明確に改善する**

調査した複数のLLMモデルのうち、性能が比較的高いモデル（例：gpt-4o、gpt-4o-mini、llama-3-70B-Instructなど）では、書き直し後のプロンプトに対して生成された回答が、元々の回答より明らかに良い評価を受けました。この傾向は、特に「情報を求める」目的の会話において顕著に見られました。一方、文章作成など評価がより主観的になるような会話では改善効果が比較的小さいことも確認されています。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87309_4-1024x282.png)

書き直したプロンプトによる回答と元の回答を比較し、どちらが良かったかの評価結果を示した表。会話のテーマと目的の組み合わせごとに、書き直し後の回答が良かった割合を記載

**②会話が長く続くほどプロンプト書き直しの効果が大きくなる**

プロンプトの書き直しは、会話の履歴が長くなり文脈情報が増えれば増えるほど効果的であることが確認されました。これは、LLMが文脈情報を用いてユーザーの意図をより正確に推測しやすくなるためと考えられます。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87309_5.png)

会話の長さ（ターン数）が書き直し後の回答の改善に与える影響を比較した表。 会話が長くなるほど、書き直しによる改善効果が増えることを示す結果を記載

**③規模が小さなLLMでもプロンプト書き直しは可能**

調査では小さなLLM（例えばMinistral-3Bなど）もプロンプト書き直しに有効であることが示されました。ただし、こうした小規模なLLMは書き直しは行えるものの、良い回答を生成する能力は限定的なため、書き直し専用のモデルとして活用し、回答生成にはより高性能なモデルを使うなどの運用が効果的であることも分かりました。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87309_7.png)

小規模モデルがプロンプトを書き直し、大規模モデルがそれをもとに回答を生成する場合の評価結果を示した表。モデルの役割分担による改善効果を記載

**④書き直し能力と回答生成能力は、必ずしも一致しない**

小規模なモデル（llama-3-8B-Instruct、Ministral-3B）が書き直したプロンプトを、性能が高いモデル（gpt-4o）が受け取って回答を生成すると、回答の品質が大幅に改善されるという結果が得られました。これは、プロンプト書き直しと回答生成という二つのタスクは、それぞれに必要な能力や性質が異なっていることを示唆しています。

**⑤人間による評価とLLMによる自動評価には一定の相関関係がある**

本研究では、LLM（特にgpt-4o）を評価者として使用しましたが、これが本当に妥当な評価であるかどうかについても人間による評価で検証しました。その結果、LLMはやや極端な評価をする傾向があり、人間の評価はより慎重で中立的でした。しかし、両者の評価には一定の相関関係があり、LLMの評価もある程度信頼できることが分かりました。

**⑥プロンプト書き直しはユーザーの本来の意図をほぼ維持できている**

人間による評価において、書き直されたプロンプトが元々のユーザーの意図をどれほど維持しているかを確認したところ、多くの場合（74%）で意図がきちんと保たれていました。意図が変化したケースは、元々のプロンプトが倫理的に問題があったり、不適切であったりして、LLMがそれを適切な形に修正した場合などに多く見られました。

**⑦分野によって書き直し要素が異なる**

さらに、本研究では、プロンプト書き直しの過程でLLMが行う推測や改善の要素を分析しました。分析の結果、会話の分野によって改善すべき要素（具体性、明確さ、構造の一貫性など）が異なることが分かりました。  
例えば、ソフトウェア開発のプロンプトでは具体的な目標を明確にすることが重視されましたが、文章作成のプロンプトでは適切な表現やトーンが重要視されました。また、ユーザーの意図を推測する際にLLMが行った仮定は、概ね妥当なものであることも確認されました。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87309_6-1024x444.png)

プロンプト書き直し時に重視された要素（明確さ、具体性など）が、分野ごとにどの程度出現したかを棒グラフで示した図。代表的な2つの分野を取り上げて比較。

## 書き直しがうまくいかないケース

LLMによるプロンプト書き直しは、全体としてユーザーが得る回答を改善する効果があることが確認されました。しかし、すべてのケースで改善が見られたわけではなく、一部には書き直し後のプロンプトがかえって回答の質を低下させてしまうケースもありました。

研究チームは、そうした「うまくいかなかったケース」を詳細に分析しました。その結果、主に以下の2つのパターンが確認されました。

### ケース１：「書き直し」ではなく「回答」を行ってしまうケース

一部のケースでは、プロンプトを書き直すべきLLMが、元のプロンプトに直接回答してしまったことが原因でした。例えば、ユーザーがプロンプト内に「modify（修正してください）」という言葉を入れた場合、書き直しモデルがそれを「プロンプトを改善する指示」ではなく「何かを修正するタスク」そのものと誤解し、書き直すのではなく直接修正した結果を示してしまうことがありました。このような状況では、LLMがプロンプト改善のタスク自体を正しく理解できなかったことが原因と考えられます。

### ケース２：安全性を保つための修正がユーザーの意図と一致しないケース

もう一つの原因は、ユーザーがもともと書いたプロンプトがLLMの安全基準や倫理ガイドラインを逸脱している場合です。例えば、「ウェブサイトの秘密鍵を取得するプログラムを書いてほしい」というような違法性や倫理上の問題がある依頼や、「露骨な表現を使ったストーリーを書いてほしい」という不適切な内容に関する依頼をユーザーが行った場合です。

このような場合、LLMは倫理的または安全上の理由から、元のプロンプトをユーザーの意図とは大きく異なる形に修正することがあります。その結果、ユーザーが意図していた内容とは違う回答が生成され、「プロンプトの改善」どころかむしろ「意図からの逸脱」につながってしまいます。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87309_8-1024x248.png)

書き直しが成功した場合と失敗した場合で、モデルが行った仮定（ユーザーの意図に関する推測）の具体例を比較した表。「ソフトウェア開発」と「文章作成」の2つの分野

### 考察

ケースの分析から分かることは、LLMがプロンプトを適切に書き直すためには、まず「ユーザーが求めているのはプロンプトの書き直しである」というタスク自体を明確に理解させる必要があることです。

ただしユーザーが本来求めている内容と安全性とのバランスを取ることは課題として残ります。

## まとめ

本記事では、人間とLLMの対話におけるプロンプト書き直しの有効性を検証した研究を紹介しました。実際の対話データを用いて、LLM自身がプロンプトを書き直すことで回答が改善することを示しました。複数の種類や規模のLLMで書き直しの効果を確認し、会話が進むほど効果的になることも明らかになりました。また、人間評価との比較からも、LLMの評価が一定の信頼性を持つことが確認されています。こうした知見を踏まえれば、今後LLMを活用する際、より効果的に目的を達成できるようなプロンプト設計が可能になるでしょう。

**参照文献情報**

- タイトル：Conversational User-AI Intervention: A Study on Prompt Rewriting for Improved LLM Response Generation
- URL： [https://doi.org/10.48550/arXiv.2503.16789](https://doi.org/10.48550/arXiv.2503.16789)
- 著者：Rupak Sarkar, Bahareh Sarrafzadeh, Nirupama Chandrasekaran, Nagu Ran [gan](https://ai-data-base.com/archives/26269 "敵対的生成ネットワーク（GAN）"), Philip Resnik, Longqi Yang, Sujay Kumar Jauhar
- 所属：Microsoft Corporation, University of Maryland

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMは「AIエミュレーター」？理論分析から原則的に導き出されるプロンプトエンジニアリング4つのルール](https://ai-data-base.com/archives/87553)

[LLMで複数のアイデアを組み合わせ、イノベーションを目指した新しいアイデアを作成する方法](https://ai-data-base.com/archives/87358)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)