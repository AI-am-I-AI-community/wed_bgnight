---
title: "推論時のトークン数を80%以上削減しながら出力精度を保つプロンプト手法の新提案"
source: "https://ai-data-base.com/archives/86361"
author:
  - "[[AIDB Research]]"
published: 2025-03-17
created: 2025-06-13
description: "本記事では、LLMが行う推論プロセスの効率性を向上させる新たな研究を紹介します。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、LLMが行う推論プロセスの効率性を向上させる新たな研究を紹介します。

プロンプト手法としても推論モデルの内部動作としてもよく使用されているCoTは、推論過程を詳しく書き出すため、正確性は高まるものの処理時間が長くなりがちです。そこで本研究は、人間が自然に行うような簡潔なメモを取る形で、効率的な推論を実現する手法を提案しています。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86361-1024x576.png)

参照論文情報は記事の下部に記載されています。

**本記事の関連研究**

- [三段論法でLLMの推論能力を高める　プロンプト手法の新提案](https://ai-data-base.com/archives/82746)
- [LLMには正解例だけでなく、「よくある間違い例」と理由も一緒に教えるのが有効](https://ai-data-base.com/archives/77507)
- [LLMの「自己対話」により複雑な問題の解決能力を飛躍的に向上させる手法『Iteration of Thought』](https://ai-data-base.com/archives/76134)

## 背景

LLMは、複雑な問題を段階的に詳しく解きほぐすことで優れた結果を示しています。これは「Chain-of-Thought（CoT）」という手法で、問題を細分化し、一つひとつ丁寧に説明するように答えを導く方法です。多くの推論モデル（o1やDeepSeek-R1など）の内部ではCoTのような段階的な推論プロセスが展開する傾向があります。

しかし、CoTには問題点もあります。CoTを用いた推論は、途中の説明が長く、使われるトークン数（文字数）が非常に多いため、計算時間が長くなりコストも高くなります。特にリアルタイム性が求められる場面では、このような冗長性が大きな障壁となっていました。

人間が実際に問題を解く時を考えると、私たちは全ての細部を詳細に書き出すことはほとんどありません。むしろ、重要な要素だけを短くメモしながら、最小限の情報で効率よく考えを進めています。つまり、長く詳細な説明はせず、核心部分だけを短く書き留めるというやり方です。

そこで今回Zoomの研究者らは、LLMにも人間が自然に行っているような「簡潔で核心的な推論方法」を取り入れようと考えました。この考えに基づき、従来の詳細な説明をする代わりに、必要最小限の短い推論メモを作成しながら回答を導く新たな方法が提案されました。

考案された手法は、推論プロセスの中で余分な説明を省き、本当に必要な情報だけを短くまとめます。実験の結果、CoTと同程度かそれ以上の正確さを維持したまま、トークン数を大幅に削減し、計算時間やコストを著しく抑えることが可能になりました。

プロンプトテンプレートを参照して試せるため、ぜひ確認してみてください。以下で詳しく紹介します。

## Chain-of-Draft

人間が数学やパズルなどの複雑な問題を解くとき、すべての考え方を丁寧に書き出すことはほとんどありません。むしろ、途中の過程では簡単なメモや短い式だけを残し、最終的な答えに必要な要点のみを書き留める場合が多いです。

一方、LLMが従来採用している方法（Chain-of-Thought）は、あらゆる細かな情報まで言葉にして表現する傾向があります。そのため、非常に長く詳細な説明を生成し、処理に多くの時間とコストを要してしまいます。

そこで人間のような効率的な方法に学び、「Chain-of-Draft」という新しい方法が提案されています。Chain-of-Draftでは、問題解決の際に途中の考え方を可能な限り簡潔に示すことを目指します。

言葉をできる限り削り、要点となる数式や重要なポイントのみを書き出すようにします。こうした方法を使えば、無駄な情報が省かれ、短時間で効率よく正解に辿り着けることが期待されます。

以下に示す例を通じて、Chain-of-Draftが従来の方法とどのように違うかを直感的に理解しましょう。

### 具体例

例として次のような簡単な算数問題を考えます。

> 「ジェイソンは20本の棒付きキャンディを持っていました。そのうち何本かをデニーにあげました。するとジェイソンの手元には12本残りました。ジェイソンは何本のキャンディをデニーに渡したでしょうか？」

従来の「Chain-of-Thought」という手法では、このような問題に対して非常に丁寧な説明が書き出されます。例えば以下のように、初めにジェイソンが何本持っていたかを説明し、次にデニーに渡したことを改めて説明し、その後に計算の式を立てて答えを導きます。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86361_3.png)

この方法は正確ですが、キャンディや人の名前といった計算に直接関係ない情報まで何度も繰り返すため、必要以上に長い文章になりがちです。

一方、Chain-of-Draftでは、同じ問題を解く際に冗長な部分を省略し、必要な数式だけを簡潔に書き出します。例えば、次のような非常にシンプルな形になります。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86361_4.png)

登場人物やキャンディの細かな説明は書かれません。最も大事な計算だけを短くまとめるため、解答に至るまでに使用される言葉（トークン）が大幅に減ります。なお、算数や数学以外の分野でも適用可能と考えられています。

ちなみに標準的なプロンプト（推論を行わせない場合）では以下のようになります。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86361_2.png)

### なぜ簡潔にすると良いのか？

Chain-of-Draftを使った場合、生成される文章が短くなるため、LLMが問題を処理する時間が劇的に短縮されます。しかも、問題を解く上で本質的な部分だけが明確に表現されるため、むしろ考え方が整理され、正確性が高まる可能性もあります。

後述する評価実験では、この方法を用いた場合でも従来の詳しい説明を伴う方法とほぼ同じ精度が維持されることが示されました。

## 実験

今回考案されたChain-of-Draft（以下CoD）が従来の方法（Chain-of-Thought、以下CoT）と比べてどのような利点を持つのかを明らかにするため、研究者らは具体的な検証実験を行いました。実験の手順と結果を見ていきましょう。

### 実験の基本的な設定

実験では、次の3つの方法を比較しています。

1つ目は標準的な方法です。問題をモデルに与える際に途中の説明を一切させず、いきなり最終的な答えだけを出させる方法です。

2つ目はCoTで、これは途中の推論過程を詳しく書き出してから答えを出す方法です。

3つ目が、新しく提案されたCoDで、推論過程をできるだけ簡潔に書き出させる方法です。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86361_5.png)

これら3つの方法を比較するために、3種類の典型的な課題が選ばれました。算数問題、常識問題、そして記号を用いた論理問題です。

実際の評価には、現在広く利用されている言語モデルのうち、GPT-4o（OpenAI）とClaude 3.5 Sonnet（Anthropic）という2つの代表的なモデルが使われました。

### 算数問題

算数問題の検証には、「GSM8k」という有名なデータセットが使われました。小学校レベルの算数問題が約8500問収録されており、それぞれの問題には詳しい解説付きの正解が用意されています。

評価の結果、標準的な方法（推論過程を書き出さない方法）では、GPT-4oの [正解率](https://ai-data-base.com/archives/25930 "正解率") は53.3%、Claude 3.5 Sonnetでは64.6%でした。

これに対して、詳しい説明を行うCoTを使った場合は、両モデルとも95%以上という非常に高い正解率を記録しました。ただし、詳しい説明を書くために、回答の文章が非常に長くなり、トークン（文章に含まれる文字数）の数が多くなり、応答までに時間がかかることが確認されました。

一方、短い説明を行うCoDでは、正解率が約91%と、CoTとほぼ変わらない精度を保ちながら、トークン数が約80%少なくなり、応答までの待ち時間が最大76%短縮されました。つまり、CoDは、詳しい説明を省いても高い正解率を維持し、より早く回答が得られる方法だと分かりました。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86361_6.png)

### 常識問題

常識問題の評価では、日付に関する問題とスポーツに関する問題が使われました。人間が日常生活で自然に使っている知識を利用して答えることが求められます。

ここでも同じ3つの方法が比較されました。その結果、標準的な方法ではある程度の正解率はありましたが、途中の考え方を書き出すことで精度が向上しました。

しかし、詳しい説明を書くCoTでは、文章が非常に長くなり、特にClaude 3.5 Sonnetは1つの質問に対して約170～190個ものトークンを使用しました。

対してCoDでは、回答を非常に簡潔にしたことで、トークン数が約90%も削減されました。精度はCoTとほぼ同じか、課題によってはむしろ少し高くなる場合もありました。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86361_7.png)

日付に関する問題の回答精度

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86361_8.png)

スポーツに関する問題の回答精度

この結果は、実際には細かな説明がなくても、モデルが問題の本質を十分理解できていることを示しています。

### 記号論理問題

記号を使った論理問題では、コインを複数回ひっくり返して、最終的な表・裏の状態を当てる問題が用意されました。例えば、「コインは最初表向きで、ロビンがコインをひっくり返し、ペギーがまたひっくり返し…、最終的に表向きでしょうか？」といった問題です。

評価の結果、標準的な方法でもある程度の精度がありましたが、CoTとCoDでは両方とも100%の精度に達しました。ただし、ここでもCoTは説明が冗長になる傾向があり、CoDはトークン数を68〜86%減らすことに成功しました。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86361_9.png)

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86361_10.png)

CoDでは、本当に必要な情報のみが簡潔に示されるため、冗長性が避けられ、精度を高く保ったまま効率的に回答が得られました。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86361_1.png)

全体の結果

### 実験結果の解釈

これらの実験から明らかになったことは、従来の詳しい説明を伴うCoTの方法は、確かに高い精度を実現しますが、必要以上の詳しい説明によって多くのトークン数を消費し、回答までの待ち時間が長くなるという課題を抱えているということです。

一方、CoDは、人間が普段の問題解決で行っているように、余計な説明を排除して重要な要素だけを簡潔にまとめることで、非常に短い時間で高精度な回答を出せることが実証されました。リアルタイム性が求められる場面や、使用コストをできるだけ抑えたい場面で非常に効果的であることを示しています。

つまり、問題解決のプロセスを丁寧に書き出すことが常に最善ではなく、状況に応じて本質的なポイントだけを短く示すほうが合理的である場合も多い、という知見が得られました。

## プロンプト手法の整理

論文で報告されている内容をもとに、プロンプト手法を整理してテンプレートにします。

```js
指示:
以下の通り常識推論の問題に答えてください：
- 推論ステップはとても短く (1ステップ5単語以下など)
- 不要な背景説明や長文を避ける
- 「####」の直後に最終回答を記述する

回答フォーマット:
Q: [質問文]
A: [ドラフトステップ1]; [ドラフトステップ2]; ... #### [最終答]

例1:
Q: 猫は鳥より大きい？
A: 猫 > 鳥; #### はい

例2:
Q: フライパンで氷を長時間加熱したらどうなる？
A: 温度上昇; 氷は溶解; #### 水になる

以下の問題に取り組んでください。
[問題文を提示]
```

## まとめ

本記事では、LLMが推論を行う際に、簡潔な途中経過のみを書き出す「Chain-of-Draft（CoD）」という研究を紹介しました。従来の詳しい推論（CoT）と比べて、説明を大幅に短縮しつつ精度を維持することを目指しています。

実施された実験では、CoDが推論プロセスのトークン数を大きく減らし、応答の待ち時間を短縮できることが明らかになりました。

こうした手法は、コスト削減やリアルタイム性が重視される場面で特に役立つ可能性があります。今後、推論の精度と効率を両立させるための重要な手がかりになるかもしれません。

**参照文献情報**

- タイトル：Chain of Draft: Thinking Faster by Writing Less
- URL： [https://doi.org/10.48550/arXiv.2502.18600](https://doi.org/10.48550/arXiv.2502.18600)
- 著者：Silei Xu, Wenhao Xie, Lingxiao Zhao, Pengcheng He
- 所属：Zoom Communications

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[株式会社電算の公共×AIで社会を変える“安定基盤と最先端技術”](https://ai-data-base.com/archives/86862)

[APIベース vs GUIベース　LLMエージェントの使い分け](https://ai-data-base.com/archives/86923)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)