---
title: "エージェントなしで行うLLMによるソフトウェアのバグ修正手法"
source: "https://ai-data-base.com/archives/73060"
author:
  - "[[AIDB Research]]"
published: 2024-07-18
created: 2025-06-13
description: "本記事では、ソフトウェア開発におけるバグ修正の新アプローチ「AGENTLESS」を紹介します。これまで多くのエージェントベースアプローチが開発されましたが、複雑さが課題となっています。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、ソフトウェア開発におけるバグ修正の新アプローチ「AGENTLESS」を紹介します。

これまで多くのエージェントベースアプローチが開発されましたが、複雑さが課題となっています。AGENTLESSは、シンプルなプロセスで問題解決を試みます。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73060-1024x576.jpg)

**参照論文情報**

- タイトル：Agentless: Demystifying LLM-based Software Engineering Agents
- 著者：Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, Lingming Zhang
- 所属：University of Illinois Urbana-Champaign

## 背景

LLMが様々なコード関連タスクで驚異的な成果を上げ、ソフトウェア開発の自動化に大きな期待が寄せられています。中でも、コード生成やバグ修正、テスト生成といった分野で活用が進んでいます。

実際のソフトウェア開発プロジェクトにおいては、単純なコード生成だけでなく、リポジトリ全体を理解し、複数のファイルにまたがる依存関係を考慮しながら問題を解決する能力が求められます。

そこで、多くの研究者や企業がエージェントベースのアプローチを使用しています。エージェントベースのアプローチとはすなわち、LLMにツールの使用や環境からのフィードバック観察、将来の行動計画といった能力を持たせるものです。例えば、ファイルの操作やテストの実行、シェルコマンドの実行などのツールが与えられます。

しかし、エージェントベースのアプローチには以下のような課題があります。

1. 複雑なツールの使用と設計が必要
2. 意思決定プロセスの制御が難しい
3. 自己反省能力が限られている

そのため今回研究者らは、エージェントなしのアプローチ『AGENTLESS』を考案しました。エージェントを使用せずにソフトウェア開発の問題を自動的に解決することを目指した方法論です。

## AGENTLESSの概要

AGENTLESSは、コードの問題を自動的に解決するシステムです。その処理は大きく分けて「バグ特定」と「バグ修正」の2段階で行われます。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73060_1-1024x580.jpg)

AGENTLESSの概要を示す図。バグ特定とバグ修正の2つのフェーズを図示

まず、バグ特定の段階では次のような手順で問題のある場所を絞り込んでいきます。

1. プロジェクトの全コードを見やすい形に整理する
2. 問題の説明を基に、怪しいファイルを何個か選ぶ
3. 選んだファイルの中から、関係ありそうな部分（クラスや関数）を探す
4. 最終的に、具体的にどの行を直せばよいかを決める

次に、バグ修正の段階では、以下のプロセスを踏みます。

1. 見つけた問題箇所と問題の説明を基に、複数の修正案を考える
2. それぞれの修正案をテストし、明らかにおかしいものを除外する
3. 残った修正案の中から、最も良さそうなものを選ぶ

### バグ特定プロセス

AGENTLESSは、コードの中からバグがありそうな場所を効率よく見つけ出すために、3段階の方法を使います。大きな範囲から小さな範囲へと段階的に絞り込んでいくのが特徴です。

（下記の番号は、上述のプロセスの番号とは一致していません）

#### 1\. 怪しいファイルの特定

まず、プロジェクト全体の中から問題がありそうなファイルを探します。ただし、すべてのコードを細かく調べるのではなく、フォルダやファイルの構造を整理した「地図」のようなものを作ります。この「地図」を使って、効率よく怪しいファイルを見つけます。

#### 2\. ファイルの中の関連部分を特定する

次に、見つけた怪しいファイルの中身を調べます。ただし、ファイルの内容すべてを見るのではなく、クラスや関数の名前だけをリストアップした「骨組み」を作ります。この「骨組み」を使って、問題に関係しそうな部分を絞り込みます。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73060_2.png)

骨組みの例

#### 3\. 具体的な編集場所を決める

最後に、絞り込んだ部分の具体的な内容をLLMに見せます。LLMはこの情報を基に、実際にどの行、どの関数、またはどのクラスを修正すべきかを決めます。

### バグ修正フェーズ

AGENTLESSがバグを直す方法は、以下の3つのステップで行われます。この過程が、AGENTLESSの中心的な部分です。

#### 1\. 修正する場所の周辺情報を集める

まず、バグがありそうな場所の前後の部分も含めてコードを取り出します。これを「コンテキストウィンドウ」と呼びます。例えば、問題のある部分が40行目から78行目だとわかっていれば、その前後数行も含めて情報を集めます。複数の場所を修正する必要がある場合は、それぞれの場所のコンテキストウィンドウを”…”で区切ってつなげます。これでLLMが問題の状況をより良く理解できるようになります。

#### 2\. 修正案（パッチ）を作る

次に、LLMに修正案を作ってもらいます。AGENTLESSでは「Search/Replace編集」という方法を使います。これは、「変更前のコード」と「変更後のコード」の2つの部分からなります。全てのコードを書き直すのではなく、変更が必要な部分だけを指定するので、小さな変更に集中でき、ミスも減らせます。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73060_3-1024x584.png)

Search/Replace編集フォーマットの例

#### 3\. 最良の修正案を選ぶ

LLMは複数の修正案を作ります。まず「貪欲法」という、その時点で最も良さそうな選択をする方法で案を作り、次に「高温 [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") 」という、より多様な案を生み出す方法も使います。

※貪欲法とは、LLMがテキストを生成する際に使用される最も単純な方法の一つです。各ステップで、モデルは次の単語として最も確率の高い選択肢を常に選びます。これは「貪欲」な選択であり、その時点で最も良さそうな選択を常に行います。また「温度」はLLMの出力の多様性を制御するパラメータです。温度が高いほど、モデルはより多様な（そして時にはより予測不可能な）出力を生成します。

これらの修正案に対して、以下の手順で最良のものを選びます。

a. 全ての修正案をテストし、既存のテスト（回帰テスト）に失敗するものを除外します。新しい問題を引き起こす修正案を取り除くためです。

b. 残った修正案を「 [正規化](https://ai-data-base.com/archives/26401 "正規化") 」します。空白やコメントなどの表面的な違いを無視して、本質的な違いだけを比較するためです。

c. [正規化](https://ai-data-base.com/archives/26401 "正規化") された修正案の中から、最も多く出てきたパターンを選びます。最終的な修正案となります。

d. 選ばれた修正案を整える作業を行います。具体的には、コードを抽象構文木に変換し、それを標準的な形式に戻す過程で、ドキュメント文字列（docstring）を削除したり、一貫した形式に整えたりします。

このように、段階的に修正案を作り、選び、整えることで、効率的にバグを修正します。複雑な手順を使わずに、シンプルでありながら効果的な方法で問題を解決するのが特徴です。

## 実験のセットアップ

### データセット

AGENTLESSの評価には、実世界のソフトウェアエンジニアリング問題を解決する能力をテストするために作られた [SWE-bench](https://www.swebench.com/) データセットが使用されました。SWE-benchにおける各問題では、入力された問題説明に基づいて基礎となる問題を解決するためのパッチを提出することが求められます。

今回は、フィルタリングされたサブセットである [SWE-bench Lite](https://www.swebench.com/lite.html) に焦点が当てられました。提出されたパッチの機能的正確性を評価するためのテストを含む300の問題が含まれています。

さらに、SWE-bench Liteベンチマークに対する詳細な研究が実施され、潜在的な問題やバイアスが示されただけでなく、より厳密な評価のためのフィルタリングされた問題セットが作成されました（後述します）。

### 実装方法

AGENTLESSの実装において、バックボーンにはGPT-4o (gpt-4o-2024-05-13)が使用されました。

動作は以下のような流れです。

1. バグのありそうな場所を探す
	- まず、問題のありそうな上位3つのファイルを特定する
	- 次に、それらのファイル内で怪しいクラスや関数を可能な限り多く見つける  
		※このプロセスでは、LLMに「最も確実そうな選択をする」よう指示する
2. 修正箇所を決める
	- 各問題に対して、4つの修正候補場所を提案させる
	- これを2回行い、合計8つの候補を得る
3. 修正案（パッチ）を作る
	- 各修正候補場所の前後10行を含めて、LLMに渡す
	- LLMは各候補に対して21の修正案を作る（1個は確実な方法で、それ以外の20個は多様性を持たせた方法で）。
	- 結果として、1つの問題に対して合計42の修正案が生まれる
4. 修正案を整理する
	- 修正案は「ここをこう変える」という形式で表現される
	- 修正案を比較しやすくするため、Pythonの標準ライブラリを使って整形する
5. 評価する
	- 作られた修正案の性能を測るため、SWE-bench-dockerという既存の評価システムを使う

このように、段階的にバグを見つけ、多数の修正案を生成し、それらを整理して評価するという複雑なプロセスを（自動的に）行います。

### ベースライン

AGENTLESSは13のエージェントベースアプローチと比較されました。SWE-benchにおける最先端の性能を示しているアプローチです。オープンソースの最先端ツールだけでなく、商用またはクローズドソースのベースライン（記号で示される）も含まれています。

また、SWE-benchの一部として提案された、RAGを使用したシンプルなエージェントレスベースラインも比較対象として含まれています。BM25を使用して取得された最も関連性の高いファイルの内容をLLMに提供し、直接パッチファイルを生成する手法です。

さらに、可能な限り各ツールが使用する基礎となるLLMもリストアップされています。

ベースラインを一覧にすると以下の通りです。

1. Alibaba Lingma Agent (クローズドソース)
2. Factory Code Droid (クローズドソース)
3. AutoCodeRover-v2
4. CodeR (クローズドソース)
5. IBM Research Agent-101 (クローズドソース)
6. OpenCSG StarShip (クローズドソース)
7. Bytedance MarsCode (クローズドソース)
8. Amazon Q Developer (クローズドソース)
9. RepoUnderstander
10. Aider (クローズドソース)
11. AutoCodeRover
12. SWE-agent
13. OpenDevin (クローズドソース)
14. RAG（検索拡張生成を使用したエージェントレスベースライン）

### 評価指標

先行研究に従い、以下の指標で評価されます。

1. % Resolved：ベンチマークで解決された問題の割合
2. Avg. $ Cost：ツールを実行するための平均推論コスト
3. Avg. # Tokens：LLMに問い合わせるために使用された入力および出力トークンの平均数

さらに、% Correct Locationも報告されます。ツールが生成したパッチが正解の開発者パッチの編集位置と一致する問題の割合です。ファイル、関数、行の3つの粒度で計算されます。

パッチが正解の位置にあると報告されるのは、正解パッチのすべての位置のスーパーセットを編集している場合です。ベースラインツールについては、公式リーダーボードまたはツールの公式論文/リポジトリから報告された結果が直接使用されます。

## 評価結果

### バグ修正性能

AGENTLESSと先行のエージェントベースアプローチのSWE-bench Liteにおける主な評価結果が下記の表に示されています。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73060_4-1024x634.jpg)

SWE-bench Liteでの結果。各ツールの解決率、平均コスト、平均トークン数、正しい位置の割合

AGENTLESSは300問中82問（27.33%）を解決することができました。最高のスコアではありませんが、非常にシンプルな設計と全体的な技術を用いながら、先行のエージェントベースアプローチと比較して競争力を示しています。

ここで、多くのトップパフォーマンスの技術がクローズドソース/商用であり、実験を再現するためのソースコードや詳細な軌跡さえも公開されていないことに気をつけなければいけません。オープンソースのアプローチだけで比較すると、AGENTLESSはSWE-bench Liteで27.33%（82 / 300）という最高のパフォーマンスを達成しています。

さらに、AGENTLESSの平均コストはわずか0.34ドルであり、先行のエージェントベースアプローチと比較すると大幅に低い数字です。（RAGと比較するとAGENTLESSのコストはわずかに高いもののスコアは遥かに高いです）

### 解決できる問題の種類

下の図は、AGENTLESSが解決した問題を、トップパフォーマンスのクローズドソース/商用アプローチおよびオープンソースアプローチと比較して示しています。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73060_5-1024x564.jpg)

問題修正のベン図。AGENTLESSと他のアプローチがバグ修正できる問題の重複を示す

まず、オープンソースのエージェントベースの技術と比較すると、AGENTLESSは15の問題を多くバグ修正できることがわかります。

さらに、高性能な商用アプローチと比較しても、AGENTLESSだけが解決する問題があることは確認できます。トップの商用ソリューションであるAlibaba Lingma Agentよりも多くのユニークなパッチを生成しています。つまり、商用ベースのエージェントを補完する存在になれるということです。

### バグ特定性能

実世界のソフトウェア開発では、問題を直接修正するだけでなく、人間の開発者にとって正しい編集位置を提供することが求められます。

そこで、各手法によって生成されたパッチの位置が正解の開発者パッチと比較されました。

下記の表（再掲）には、各ツールの正しい位置を含むパッチの割合が、行、関数、ファイルのレベルで示されています。まず、正しい位置を含むパッチの割合が解決率と強く相関していることが観察されました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73060_4-1024x634.jpg)

SWE-bench Liteでの結果。各ツールの解決率、平均コスト、平均トークン数、正しい位置の割合

興味深いことに、ファイルレベルの位置において最高の結果を示したのはOpenCSG StarShipで、90.0%という高い割合を達成しています。総合的に最高のパフォーマンスを示すアプローチよりもはるかに高い割合であり、同時に比較的低い解決率（23.67%）を示しています。OpenCSG StarShipは商用製品であり中身が開示されていないので、このギャップがなぜ起こるのはわかりません。

AGENTLESSは、すべてのオープンソースアプローチの中で、関数レベルで最高、ファイルレベルと行レベルで2番目に高いバグ特定性能を示しています。

### AGENTLESSのコンポーネントに関する分析

次に、バグ特定フェーズとバグ修正フェーズの各コンポーネントがAGENTLESSの最終的なパフォーマンスにどのように貢献したか、という細かい検証が行われました。

下記の表は、AGENTLESSのバグ特定フェーズにおける3つのステップの性能を示しています（ステップ3については、2セットの位置の平均が示され、コストは合計コストです）。各ステップ後に、正解の編集位置が残っている割合、各ステップにおけるコード行数、各ステップの平均ドルコストが示されています。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73060_6-1024x314.png)

AGENTLESSの異なるバグ特定ステップの性能を示す表。

AGENTLESSは77.7%のケースで正解のファイルをバグ特定できることが観察されました。しかし、バグ特定されたすべてのファイルを使用すると、コンテキストの一部として膨大な数のコード行が生成されます。そのため、2番目のステップでは関連するクラスと関数におけるバグが特定され、コンテキストウィンドウが大幅に縮小されます。

下記の表は、AGENTLESSの異なるバグ修正セットアップが最終的なパフォーマンスに与える影響を示しています。単一のサンプル（つまり、貪欲デコーディングを使用）を生成するだけで、AGENTLESSはバグあたり平均0.11ドルのコスト（バグ特定を含む総コスト）で70の正しいバグ修正を達成できます。このシンプルなパッチ生成ステップでも、AGENTLESSは先行のオープンソースエージェントベースアプローチの大部分を上回り、コストは4倍以上削減できています。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73060_7.png)

AGENTLESSの異なるバグ修正セットアップの性能を示す表

LLMを複数回 [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") し、多数決を使用してパッチを選択することで、パフォーマンスをさらに78のバグ修正まで向上させることができます。AGENTLESSの完全なパフォーマンスは、既存の回帰テストに合格するパッチのみを選択するフィルタリングを適用することで達成されます。

各問題に対して複数のパッチが [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") されるため、すべてのサンプルを使用した場合のAGENTLESSが解決できる可能性のある問題の総数は123（41.0%）であることも観察されました。

## SWE-bench Lite-Sでの評価

### 新しいベンチマークの作成

研究チームは、より正確な評価を行うために、既存のSWE-bench Liteから一部の問題を取り除いた新しいベンチマーク「SWE-bench Lite-S」を作成しました。この新しいベンチマークには252の問題が含まれています。

除外された問題は主に3種類あります。

1. 答えとなるパッチが問題文に書かれているもの
2. 間違った解決策が示されているもの
3. 問題を解くのに必要な情報が足りないもの

除外によってベンチマークの質が向上し、各問題の難しさがより均等になりました。

理想的には、問題のある部分を修正したり、必要な情報を追加したりして、元のベンチマークを改善することが望ましいです。しかし、そうすると既存の商用ツールでの評価結果が使えなくなってしまいます。そのため、今回は問題のある問題を単純に除外する方法が選ばれました。

将来的には、SWE-bench Liteの管理者と協力して、ベンチマーク全体の質を向上させることが期待されています。

### SWE-bench Lite-Sでの評価結果

下記の表には、SWE-bench Lite-Sベンチマークでの結果と、各アプローチのランキングが示されています。比較のために、元のSWE-bench Liteの300問での結果も含まれています。

全体的なアプローチのランキングはほぼ同じままですが、いくつかの小さなランキングの変更が観察されました。元のSWE-bench Liteと比較して、フィルタリングされたベンチマークであるSWE-bench Lite-Sは、自律型ソフトウェア開発ツールの真の能力をより正確に反映していると言えます。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73060_9-1024x740.jpg)

SWE-bench Lite-Sでの性能とランキングを示す表。SWE-bench Liteとの比較も含まれる

### 問題カテゴリ別の性能分析

研究チームは、AGENTLESSと他のアプローチがSWE-bench Lite-Sの異なる種類の問題をどれだけ解決できるか調査しました。その結果を3つのグラフにまとめています。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73060_10-1024x343.png)

SWE-bench Lite-Sの異なる問題カテゴリにおける選択されたアプローチの解決率を示す

最初のグラフ（a）は、問題文にエラーを再現するコード例があるかどうかで解決率が変わるかを示しています。驚いたことに、コード例がある問題の方が、どのアプローチでも解決率が低くなりました。これはエラーの再現方法をさらに改善する必要があることを示唆しています。

2つ目のグラフ（b）は、問題文に解決のヒントがある場合の影響を示しています。予想通り、ヒントがある問題の方が、すべてのアプローチで解決率が高くなりました。

3つ目のグラフ（c）は、問題がコードのどの部分にあるかという情報の影響を示しています。この情報が自然言語で書かれている場合に最も解決率が高く、次いでスタックトレース（エラーの発生場所を示す情報）がある場合が続きます。位置の手がかりが全くない問題が最も難しいことがわかりました。

AGENTLESSは、問題の位置情報が提供されている場合、他のアプローチと同等の性能を示しました。しかし、位置情報がない場合は、クローズドソースのツールの方が優れていました。これは、それらのツールが複雑なコード検索機能を持っているためと考えられます。

この結果は、AGENTLESSが今後改善すべき点を示しています。中でも、コードの中から問題箇所を見つける能力を向上させることが重要だと言えるでしょう。

## まとめ

本記事では、ソフトウェア開発問題を自動解決するAGENTLESSアプローチを紹介しました。

研究者らは複雑なエージェントは必ずしも常に必要ではないと考え、シンプルなバグ特定とバグ修正の2段階プロセスを考案しました。手法はSWE-bench Liteで評価され、オープンソース技術中最高のパフォーマンスを低コストで達成しました。また、研究者らはより厳密なSWE-bench Lite-Sも構築しました。

AGENTLESSのようなアプローチは、ソフトウェア開発の自動化と生産性向上の新しい可能性を生むものとして期待されます。

- 参照論文URL： [https://arxiv.org/abs/2407.01489](https://arxiv.org/abs/2407.01489)
- コード： [https://github.com/OpenAutoCoder/Agentless](https://github.com/OpenAutoCoder/Agentless)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[心の理論をLLMエージェントに実装することの効果](https://ai-data-base.com/archives/72954)

[100個の事例を分析して明らかになったLLM-RAGアプリケーション「19の欠陥パターン」](https://ai-data-base.com/archives/73120)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)