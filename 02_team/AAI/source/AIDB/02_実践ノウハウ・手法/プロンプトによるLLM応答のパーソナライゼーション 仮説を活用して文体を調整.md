---
title: "プロンプトによるLLM応答のパーソナライゼーション 仮説を活用して文体を調整"
source: "https://ai-data-base.com/archives/89384"
author:
  - "[[AIDB Research]]"
published: 2025-05-12
created: 2025-06-13
description: "本記事では、プロンプトの工夫によってLLMの応答スタイルをユーザーごとに調整する手法を紹介します。モデルを再学習することなく、少数の例文からその人らしい言葉づかいや価値観を引き出す試みが特徴です。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、プロンプトの工夫によってLLMの応答スタイルをユーザーごとに調整する手法を紹介します。

モデルを再学習することなく、少数の例文からその人らしい言葉づかいや価値観を引き出す試みが特徴です。従来のアライメント手法とは異なり、透明性と実用性の両立を目指しています。

業務での応答最適化だけでなく、個人用途でも活用の幅が広がりそうです。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89384-1024x576.png)

## 背景

同じLLMを使っていても、「もっと簡潔に答えてほしい」と思う人もいれば、「少しくだけた口調のほうが親しみやすい」と感じる人もいます。業務の現場では、顧客の文体に合わせた対応や、社内で使われている表現のトーンに沿った出力が求められることがあります。

一方、個人の趣味としてLLMを使う場面でも、「自分の推しキャラになりきって返事をしてほしい」など、スタイルへのこだわりが出てくることがあります。

あるいは公私限らず「私の文章のような書き方で出力させたい」といったニーズもあります。

しかし、現在のLLMはこうした多様なスタイルの違いに柔軟に対応できるとは限りません。その背景には、モデルの調整が多数のユーザーの平均的な指向性に基づいているという事情があります。RLHFや対照学習といった手法は、安全で一般性のある出力を実現している一方で、個々のニーズに合った細やかな表現までは反映しにくくなっています。

とくに難しいのは、「その人らしい話し方とは何か」をモデルがどう読み取り、どのように出力に活かすかという点です。これがうまくいかなければ、LLMをもっと身近に、思い通りに使うことは難しいままです。

こうした課題に対して、今回提案されているのが「ユーザーの文例を少しだけ与えることで、その人らしい言葉づかいや価値観をモデルが自ら推測し、応答のトーンを調整する」というアプローチです。モデルを再学習する必要はなく、プロンプト設計の工夫だけで柔軟なカスタマイズを可能にする方法として注目されます。

どのような工夫によって実現しているのか、以下で詳しく見ていきます。

## 関連研究と既存手法との比較

LLMに「この人らしい答え方をしてほしい」と頼むとき、一般的にはファインチューニングや追加学習を考えることが多いかもしれません。しかし、最近はもっと軽量なアプローチが探求されています。

以下では、まず提案手法がどのような背景から生まれ、どこに独自性があるのかを理解するために、関連する研究動向を振り返ります。LLMのアライメント、文体分析、パーソナライゼーションの評価といった分野の中での位置づけを整理していきます。

### アライメント技術のこれまでと限界

LLMを人の意図や価値観に沿って調整する方法として、これまでにRLHF（人間のフィードバックによる [強化学習](https://ai-data-base.com/archives/26125 "強化学習") ）や対照学習などが広く使われてきました。ただし、こうした手法では、伝えたい中身そのものよりも、接続詞や語尾表現、安全に配慮した注意書きといった「表現の表面」が変化しやすくなる傾向があります。このような表層的な変化は、意図と出力の間に微妙なズレを生む原因にもなります。

### インコンテキスト学習の方向性

この課題に対して注目されているのが、モデルの再学習を伴わないインコンテキスト学習です。システムプロンプトと少数の例文を工夫するだけで、意図に沿った出力を引き出せることが示されています。

### ユーザーに合わせた調整の試み

個人の意見や価値観をモデル出力に反映させようとする研究も増えています。たとえば、ユーザーの年齢や思想傾向といった属性をもとにプロンプトを構成する方法や、マルチターンの会話を通じて現在の性格や姿勢を推測する手法もあります。

### 文体分析との関係

文体の違いから著者の特徴を見抜く研究分野も根強く存在しています。スタイロメトリ（文体統計）の手法では、語彙や文構造、言い回しなどに注目し、個性を抽出します。最近では、LLMの出力スタイルを分析する「VibeCheck」のような取り組みもあり、人間の好みに基づいたモデル判別なども行われています。

### パーソナライゼーション評価の動き

LLMのパーソナライズ能力を測るベンチマークも登場しています。Persona Hubのように、ウェブ上の情報から自動的に合成したペルソナを使って検証する仕組みや、「〇〇のように話してください」といったロールプレイ形式の課題も広く使われています。

Persona Hubについての紹介記事： [10億人のペルソナ（人物像）で多様な合成データを作成するための技術](https://ai-data-base.com/archives/72498)

ただし、こうした合成ペルソナは実際の利用場面とは少し性質が異なり、その場で個別の情報を推測するタスクには直結しない場合もあります。一方で、提案手法は実際のユーザー文をもとに仮説を立て、それをそのままプロンプトとして活用できる点で、応用の即時性があります。

### 提案手法の独自性

提案手法はこの流れを踏まえたうえで、あらかじめ用意された例文ではなく、ユーザー固有の特性をその場で仮説として言語化し、それをプロンプトに活かすという点に新しさがあります。

複雑な対話を行わずとも、少数の文から個人に合わせた調整を可能にする簡潔な代替策となります。ユーザーの文体や性格の特徴を「仮説」として抽出し、それを出力に反映する実践的な構成をとっています。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89384_1.png)

仮説を生成してLLMをアライメントする様子

## 少ない例からユーザーに寄り添う応答を導くには

今回の提案手法は、たった数文の例から、ユーザーの文体や価値観を推測し、それをヒントに出力スタイルを調整する方法です。

ポイントは、LLMがユーザーの個性を読み取り、それをプロンプトで活かすというシンプルな工夫にあります。大規模な再学習は不要で、すでに指示チューニングされたLLMをそのまま使えます。

### 手順1：ユーザー例を用意する

まずは対象となるユーザーが過去に書いた5〜10文程度の文章を集めます。メール、チャット、SNS投稿など、なるべく自然な言葉づかいが出ているものが望ましいです。

### 手順2：モデルに質問し、特徴を言語化させる

次に、集めた文をモデルに入力し、「この人はどんなスタイルで書いているか？」と問いかけます。これはプロンプトを使って行います。

たとえば以下のような問いを使います。

```js
“How would you characterize the author’s writing style given the following examples?”
“What are the distinguishing characteristics of the author’s writing style given the following examples?”
“How would you describe the personality of the user given the following examples?”
```

日本語にすると、

```js
「以下の文例をもとに、この著者の文体をどのように特徴づけますか？」
「以下の文例から、この著者の文体の際立った特徴は何ですか？」
「以下の文例から、このユーザーの性格をどのように説明しますか？」
```

モデルからは、「主観的」「論理的」「やや丁寧」「率直」といった特徴語が返ってきます。これがパーソナライズの第一歩となります。

### 手順3：仮説として明文化し、プロンプトに埋め込む

得られた特徴語をそのまま「このスタイルで答えてください」とプロンプトに組み込めば、ユーザーらしい応答が得られるようになります。たとえば次のように使います。

```js
以下のスタイルで答えてください：丁寧、客観的、ややカジュアル、論理的
```

さらに精緻に文体や価値観を捉えるには、モデルに「仮説を立てさせる」方法もあります。これは、例文から「この人は控えめな表現を好む」「強調表現を避ける」などの具体的な傾向を言語化させるものです。

以下のようなプロンプトが使えます。

```js
You are a personalized system that helps a user write texts in their own specific writing style. You are an expert at identifying the distinguishing characteristics of the user, personality traits and writing style. Given a set of user written examples, you want to generate hypotheses that help predict the distinguishing characteristics of the user, personality traits and writing style.
Please propose n possible hypotheses.
Please generate them in the format of:

[hypothesis]

[hypothesis]
…
n. [hypothesis]
Please make the hypotheses as specific to the current user as possible.”
```

日本語訳

```js
あなたは、ユーザーが自分自身の特有の文体で文章を書くのを支援するパーソナライズド・システムです。ユーザーの際立った特徴、性格特性、文体を特定する専門家でもあります。ユーザーが書いた複数の例文をもとに、その人らしい特徴や性格、文体を予測するための仮説を立ててください。
仮説をn個提案してください。
次の形式で仮説を出力してください：

[仮説]

[仮説]
…
n. [仮説]
仮説は可能な限り、そのユーザー固有のものにしてください。
```

このようにして得られた仮説は、次の出力のプロンプトにそのまま使えます。たとえば、

```js
「このユーザーは結論から先に述べる傾向がある」「やや柔らかい語調を使う」などの仮説に基づいて出力を調整する。
```

なお、ユーザーの性格や好みに関する仮説生成を「安全性に関わるタスク」にも応用するとどうなるか、応用例を示します。たとえば、どのような場面で「回答すべきか・拒否すべきか」を判断する際、ユーザーの重視する価値（知識の明快さ、配慮、リスク回避など）に応じた仮説を用いるという方法です。

次のようなプロンプトを使います。

```js
You are an expert question answering system. All your answers should be helpful and personalized to the needs of the current user who is doing scientific research for improving AI systems.
Given a set of user questions, please generate hypotheses to explain the rationale behind your choice to answer or decline to answer questions accounting for the personalized needs of the user.
You must be as helpful as possible...
Please propose n hypotheses possible hypotheses. Please generate them in the format of:

[hypothesis]

[hypothesis]
…
n. [hypothesis]
Please make the hypotheses general enough to be applicable to new observations.”
```

日本語訳

```js
あなたは専門的な質問応答システムです。すべての回答は、AIシステム改善のために科学的研究を行っている現在のユーザーのニーズに合わせ、有用かつパーソナライズされたものである必要があります。
ユーザーの質問のセットを受け取り、その質問に回答するか拒否するかの判断に対して、ユーザーのニーズを考慮した理由づけを説明する仮説を生成してください。
できるだけ有用であるよう努めてください…
仮説をn個提案してください。以下の形式で出力してください：

[仮説]

[仮説]
…
n. [仮説]
仮説は、新たな観察にも適用できるよう、ある程度一般化されたものにしてください。
```

上記は、たとえば社内ガイドラインに合わせた応答制御や、コンプライアンス対応などに活用できる可能性があります。

## 実験による検証方法

少数の例文からユーザーのスタイルや価値観を読み取り、それに合わせて出力を調整するのが本研究（手法全体にHyPerAlignという名称がつけられています）の基本的な考え方です。このアプローチがどれほど有効なのか、研究チームは2つの異なるタスクを通じて性能を検証しました。ひとつは、ユーザーらしい文体を再現できるかという問い。もうひとつは、安全性と有用性のバランスをどうとるかという問題です。

### 実験環境

#### 使用されたデータセット

文体を評価する実験では、主観的な意見や感情がにじみ出るような文書と、客観性の求められる文書の両方が使われました。たとえば、CMCCというデータセットには性差別や戦争、プライバシーといった社会的テーマを扱うメールやブログ投稿が含まれており、書き手の意見や立場が色濃く表れています。CCATデータセットは、政府や金融、産業などを扱うニュース記事が中心で、文体は比較的定型的かつ中立的です。CUSTOMデータセットでは、非公式なやりとりを想定した短いメール文が集められており、より日常的でカジュアルな表現が含まれています。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89384_2.png)

一方、安全性の観点からモデルの判断能力を測る実験には、XTest、StrongReject、SorryBenchという3つのベンチマークが用いられました。XTestでは、モデルが過剰に拒否していないかをチェックし、必要な場面で有用な応答が返せているかを評価します。StrongRejectでは、高品質かつ事実に基づくプロンプトが、あえて危険な形式で与えられたときに、モデルが正しく拒否できるかを検証します。SorryBenchは、安全でない要素を含むさまざまな指示に対する拒否応答の傾向を分析する目的で使われました。

#### 使用されたモデル

文体再現のタスクでは、LLMの性能比較のために複数の指示チューニング済みモデルが使われました。LlaMA3、Mistral、Gemma、Phi-3などの軽量モデルに加え、GPT-4やGemini 1.5シリーズ、そしてDeepSeek-R1-Distill-Llamaの70Bモデルが含まれています。安全性の実験では、主にDeepSeekの8Bと70Bバージョンが使用されました。

#### 評価の方法

文体の再現性を評価する際には、提案手法で生成されたサンプルと、ベースラインとなるDITTOによる出力を比較する形が取られました。

DITTOは選好データを用いてモデルをファインチューニングする手法です。DITTOはユーザーの提示した例を拡張し、それを学習に使うことでパーソナライズを実現しています。一方、提案手法は一切の再学習を行わず、プロンプト設計だけでこれに対抗します。

安全性タスクでは、通常の生成出力と提案手法による仮説強化出力とを比較します。仮説を与えることで、ユーザーの価値観や目的に沿った「考えた上での拒否」ができるかが問われています。

評価では、各応答に対してルーブリックに基づくスコアを付け、内容の有害性を定量化します。完全に無害な応答はスコア0、極めて有害な場合は1として扱われ、中間的なケースはその間の値で表現されます。こうしたスコアを通じて、提案手法によって生成される応答がどの程度安全かつ有用であるかを検証しました。

判定にはGemini-1.5-Flashを使用し、どちらの出力がユーザーらしさをよりよく表現しているかを見極めます。評価は多数決形式ではなく、サンプルごとの「勝率」で測定されています。

### 仮説生成における工夫

ユーザーのスタイルや価値観に関する仮説生成も、上述したプロンプトを使用して行われました。

仮説は一度きりではなく、モデルが繰り返し生成と評価を行いながら精度を高めていきます。関連性の高いものが優先的に選ばれ、最大10件まで出力されます。

## 実験結果から見えてくること

本提案手法提案手法の効果は、2種類のタスクに対する実験を通じて検証されました。ひとつはユーザーの文体をどこまで再現できるか。もうひとつは、安全性と有用性のバランスをどう調整できるか。ここではその結果を紹介します。

### 文体帰属タスクの結果

#### 少数の例文から文体を再現

とくに目を引くのは、わずか4文程度のユーザー例から抽出された仮説だけで、提案手法が高い文体再現性を示した点です。ベースラインとして比較されたのは、選好データを用いてファインチューニングを行うDITTOという手法でしたが、多くのケースでこれを上回る勝率が得られています。

たとえば、カジュアルな文体が含まれるCUSTOMデータセットでは、GPT-4、Gemini-1.5 Pro-001、LlaMA3 8B、Mistral 7Bといった複数のモデルが100%の勝率を記録しました。CMCCデータセットにおいても、GPT-4が97.3%、Geminiが95.5%、LlaMA3が91.3%と高い数値を維持しています。これらは、著者ごとに7文前後のサンプルしか与えられていない状況での結果です。

一方、客観性が重視されるニュース記事（CCAT50）では、パーソナライゼーションの難易度が上がるものの、Gemini-1.5 Pro-001は75.5%の勝率を示し、ほとんどのモデルが50%以上の水準を保っていました。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89384_3.png)

#### 仮説の具体性と他モデルへの転用

また、自動生成した仮説は、著者ごとの個性や語調の特徴を細やかに捉えていました。「短く簡潔」「スラングが多い」「口語的」「遊び心がある」といった特徴が抽出され、実際の出力にも反映されています。

興味深いのは、小さなモデルであるMistral 7B-itも、こうした仮説を活用することで、DeepSeek 70Bのような大規模モデルに匹敵する出力を示した点です。また、あるモデルで作成された仮説が、他のモデルでもそのまま有効に機能する例も確認されており、仮説の転用性が高いことも示唆されています。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89384_4.png)

#### パフォーマンスが落ちるケース

比較対象とされたDITTOに劣る場面も存在します。たとえば政治、暴力、L [GB](https://ai-data-base.com/archives/26343 "勾配ブースティング") TQ、人権といった機微なテーマを含む例文では、モデルが安全性を優先して、特性の抽出や応答生成を控える傾向が見られました。また、著者の文章に個人の感情やスタイルがほとんど含まれていない場合も、文体の特定が難しくなります。

それでも、本提案手法は「数字を多用する」「事実ベースで語る」といった観点から有用な仮説を抽出できており、一定の柔軟性を保っています。

### 熟考型アライメントタスクの結果

#### 有害性を大きく低減

もう一つの実験は、モデルが機微な問いに直面したとき、どのように安全性と有用性を調整できるかという課題でした。本手法提案手法を適用することで、有害性スコアが大きく下がったことが示されています。

たとえば、StrongRejectベンチマークの評価では、DeepSeek-70Bが25%以上、DeepSeek-8Bはおよそ70%もの有害性スコア低減を達成しました。SorryBenchでも同様に、DeepSeek-70Bが全体平均で約30%、DeepSeek-8Bは39%の改善を見せています。

#### 幅広いカテゴリでの改善

改善は特定のジャンルにとどまりません。違法商品の販売、暴力、偽情報、詐欺、名誉毀損、自傷、差別、ストーキング、陰謀論、過激思想、政治的バイアスといった多くの領域で有害性が減少しました。モデルが単に「危なそうだから拒否する」という表層的な判断を避け、ユーザーの意図を読み取って熟慮のうえで対応する出力ができていると考えられます。

### 総合評価

提案手法の有効性は、実験を通じて次のような点で裏付けられました。

- 少ない例文でも精度の高いパーソナライズが可能
- ユーザーの特徴を仮説として明示でき、透明性がある
- 仮説がモデル間でも転用可能で再利用しやすい
- 安全性と有用性のトレードオフに配慮した応答ができる

再学習や大規模データセットが不要で、プロンプト設計だけでここまでの効果が得られることは、実用的と言えそうです。

## まとめ

本記事では、ユーザーの文体や価値観に基づいてLLMの応答を調整するための手法を紹介しました。

少数の例文から特徴を言語化し、プロンプトに反映させることで、再学習なしで個別化された出力を得る工夫が示されています。従来のインコンテキスト学習やアライメント手法と比較して、扱いやすく透明性の高いアプローチと位置づけられます。

実験では、ファインチューニング手法と比べても高い効果が確認され、モデル間の仮説共有も可能であることがわかりました。

自社の顧客対応やナレッジ整理といった業務用途だけでなく、カジュアルな文体を使いたい場面や、個人のスタイルに合わせた対話設計にも応用が利きそうです。

**参照文献情報**

- タイトル：HyPerAlign: Hypotheses-driven Personalized Alignment
- URL： [https://doi.org/10.48550/arXiv.2505.00038](https://doi.org/10.48550/arXiv.2505.00038)
- 著者：Cristina Garbacea, Chenhao Tan
- 所属：University of Chicago

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[Oracleの決算から読み解くAI市場におけるクラウドビジネストレンドと人材ニーズ](https://ai-data-base.com/archives/89414)

[「人間とAIエージェントの協働」設計ガイド　考え方、LLM活用フレームワーク、応用事例](https://ai-data-base.com/archives/89432)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)