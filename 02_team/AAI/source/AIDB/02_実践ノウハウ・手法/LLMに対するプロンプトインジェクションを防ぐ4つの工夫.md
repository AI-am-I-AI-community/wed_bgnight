---
title: "LLMに対するプロンプトインジェクションを防ぐ4つの工夫"
source: "https://ai-data-base.com/archives/87403"
author:
  - "[[AIDB Research]]"
published: 2025-04-03
created: 2025-06-13
description: "LLMの普及が急速に進む中、プロンプトと応答を通して情報漏洩や不正操作が行われるプロンプトインジェクションが発生するリスクが指摘されています。そこで今回研究者らは具体的な防御手法を検討し、実践的な知見をまとめました。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

LLMの普及が急速に進む中、プロンプトと応答を通して情報漏洩や不正操作が行われるプロンプトインジェクションが発生するリスクが指摘されています。

そこで今回研究者らは具体的な防御手法を検討し、実践的な知見をまとめました。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87403-1024x576.png)

**本記事の関連研究**

- [LLMアプリケーション（LLMを利用したシステム）の安全評価方法・レッドチーミングの進め方](https://ai-data-base.com/archives/86443)
- [LLMに対するオープンソース安全性評価ツールの比較](https://ai-data-base.com/archives/77301)
- [生成AIシステムのセキュリティ評価 マイクロソフトが100事例から得た教訓](https://ai-data-base.com/archives/82195)

## 背景

ChatGPTをはじめとするLLMを活用したチャットボットが急速に広がっています。企業のホームページに設置されるものから、個人が社内や友人間で気軽に使うものまで、その用途は多岐にわたっています。しかし、こうしたチャットボットにはセキュリティリスクも存在します。

例えば、悪意のある指示（プロンプト）をボットに入力すると、本来は許されないはずの操作が実行されてしまうケースがあります。この「プロンプトインジェクション攻撃」により、企業の機密データが外部に漏洩したり、不特定多数にスパムメールが大量送信されたりする可能性があります。実際、すでに多くの場面でこうした攻撃が報告されているのが現状です。

また、一般の人々がチャットボットを手軽に設置し、広く公開する機会が増えている中で、これらのリスクへの認識は十分に広まっていません。多くの利用者は利便性だけに目を向け、安全性への配慮が後回しになっています。

このような背景の中、研究者らはLLMを安全に活用しながら、プロンプトインジェクション攻撃を防ぐ仕組みを構築する研究に取り組みました。その成果として4つの方法が整理されました。

モデルを改良するのではなく、その周辺の設計でどうにかするといった話です。

以下で詳しく紹介します。

## どれだけプロンプトインジェクションを防げるかは設計次第

プロンプトインジェクション攻撃とは、ユーザーがLLMに与える指示（プロンプト）を悪用して、モデルが本来意図していない操作を実行するよう誘導する攻撃です。通常、こうした攻撃は、モデルが「安全だ」と判断した入力の中に紛れ込んでおり、モデルがそのまま指示通りに動くことで成功してしまいます。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87419_1.png)

エージェントが動作するときの「指示の流れ」と「データの流れ」を示した例

### なぜプロンプトインジェクションが起きるのか

LLMは、ユーザーの指示に従ってさまざまな処理を行いますが、その際「ユーザーが悪意ある指示を出すはずがない」という前提で動いているケースがほとんどです。そのため、ユーザーの指示に従う仕組みそのものが、攻撃の入り口になることがあります。

これに対応するためには、本来であれば「どの指示に従うべきで、どの指示を拒否すべきか」をあらかじめモデル自身が区別できるように設計する必要があります。しかし、現実的にはあらゆる悪意ある指示を事前に想定して対策を立てることは難しく、多くのシステムが攻撃に対して脆弱になっています。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87419_2-579x1024.png)

制御の流れを乗っ取らなくても起きるプロンプトインジェクションの具体例

### 新しい提案の概要

モデルを直接改変して防御するのではなく、「モデルがどのようにデータや指示を処理するか」という仕組み自体を安全にする設計が今回の提案です。ここでは概要を手短にまとめ、後述のセクションで詳細をお伝えします。

#### 制御フローとデータフローを分離する

指示を処理するために二つの異なる役割を持つLLMを利用するのが第一のポイントです。

**役割１：特権LLM**

ユーザーの指示を基に安全な行動計画を立てますが、危険なデータには触れません。あくまで「何を行うべきか」という安全な判断だけを担当します。

**役割２：プロンプトインジェクション隔離LLM**

潜在的に危険なデータを扱いますが、自ら自由に行動を決定することは許されません。もし悪意のある指示が与えられたとしても、その影響を外部に広げることは困難です。

こうすることで、攻撃者が与えた指示が直接危険な操作に結びつくことを防ぎます。

#### 明示的なセキュリティポリシーの設定

「何が許可され、何が許可されないか」を明確なルール（セキュリティポリシー）としてあらかじめ定義します。例えば、特定の人以外へのメール送信は禁止する、ファイルは指定した相手以外には共有しない、などです。このポリシーがあると、モデルが不適切な指示を実行しようとした場合に、自動的に拒否できます。

#### 権限（Capability）ベースの制御

データや操作ごとに「誰が何を許可されるか」という細かな権限を設定します。この権限設定があることで、たとえモデルが攻撃指示を受け取っても、不適切な操作を実行することが難しくなります。

#### 専用インタプリタによる安全性の確保

さらに、カスタマイズされた専用Pythonインタプリタが各指示の流れを追跡し、「指示がどこから来て、どこへ行くのか」を把握します。不正な操作やデータ漏洩を検知・遮断することを可能にするためです。

## そもそもどんなプロンプトインジェクションリスクがあるのか

どのような攻撃が起こり得るかを事前に想定しておくことが必要です。前提を整理していきます。

### 攻撃者の能力と動機

攻撃者はユーザーになりすましてLLMに対して自由に指示を与えることが可能です。攻撃の目的は通常、許可されていないデータへのアクセスや、不正な行動をLLMに実行させることです。例えば、機密情報の漏洩、システムの誤動作、サービスの妨害などが考えられます。

**攻撃例①データの不正アクセスや漏洩**

攻撃者が本来許可されていない情報に対し、モデルを通じてアクセスを試みることがあります。

****攻撃例②** システムへの影響を目的とした操作**

攻撃者がLLMを悪用して、メール大量送信やスパムの生成、Webサイトの改ざんなどを試みる可能性があります。

### 攻撃者が制御できる要素

攻撃者は基本的にユーザーと同様にテキスト入力を通じてモデルを操作します。そのため、入力する指示を自由に設計できますが、直接的にモデル自体やシステム内部のコードを改変することはできません。ただし、攻撃者の指示が巧妙な場合、モデルが内部システムの脆弱性を意図せず利用する可能性があります。

### 防御側が想定する状況

今回は、攻撃者がシステムのコードそのものや、LLMモデル自体を直接改変することはできないという状況を想定しています。モデルは安全な範囲内でのみ行動できるように設計されているため、攻撃者が自由に操作可能な範囲は極めて限定されています（実際、このようなケースが多いです）。

また、攻撃者が指示を送り込む手段が、テキストベースのプロンプトに限定されていると仮定しています。画像や音声などの他の形式を用いた攻撃については、本研究の範囲外となっています。

### 除外する攻撃シナリオ

今回考案された防御手法の場合、対応範囲を超えている課題があります。

例えば、物理的な攻撃やネットワークレベルの攻撃（サーバーへの侵入や物理的破壊など）、LLMモデル自体に対する直接的な改ざん、モデルの性能や精度向上、誤情報や偏見の生成防止、人間の判断ミスや内部犯行といった課題については、今回の防御方法の対象とはなりません。

また、ユーザーがモデルを適切に利用し、安全な運用ポリシーを設定する責任についても、完全には防ぐことが難しいため注意が必要です。そのため、防御策を導入する際にはその限界を理解したうえで、他のセキュリティ手法と組み合わせて包括的な対策を行うことが重要となります。

## 防御設計の詳細

以下のように、モデルの役割分担、明確なセキュリティポリシー、細かな権限管理、専用のインタプリタによる実行環境という4つの要素を組み合わせることで、プロンプトインジェクション攻撃に対する高い防御力を目指します。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87419_3.png)

今回の手法がユーザーからの問い合わせをどのように処理するかの流れを示した図

### 特権を持つLLMと隔離されたLLMの役割分担

二つの異なる役割を持つLLMを用いて、攻撃に対して強い防御力を実現します。それぞれのLLMには明確に異なる役割が割り当てられています。

特権を持つLLMは、ユーザーの指示を受けて、安全で適切な行動を決定します。ただし、このモデルは攻撃者が悪用しやすいデータ（例えば、ユーザーがアップロードしたファイルなど）には一切触れません。特権LLMの役割はあくまでも安全な行動計画を策定することに限定されています。

一方で隔離されたLLMは、攻撃の可能性があるデータを扱いますが、自由に行動を決定したり、ツールを呼び出したりすることは許可されていません。隔離LLMは、決められた範囲内でのみデータを処理し、悪意ある入力があっても、被害を局所的に抑え、システム全体への影響を最小限にとどめる役割を果たします。

これら二つのモデルを明確に分離することで、攻撃の被害を最小化しつつ、利用者にとっての使いやすさや柔軟性を維持します。

### セキュリティポリシーによる明確なルール設定

プロンプトインジェクション攻撃を防ぐためには、何が許可され、何が許可されないのかを具体的にルール化する必要があります。

まずは、各操作について明確で詳細なセキュリティポリシーをあらかじめ定義します。例えば、メール送信の場合は「あらかじめユーザーが明示的に指定した相手以外への送信は禁止」といったルールを設けます。

また、ファイル共有やイベントの作成といった操作に対しても、細かな条件や制限を明記します。こうすることで、モデルが受け取った指示がこのルールに照らして許可されるかどうかを自動で判断し、不適切な操作を自動的に拒否できる仕組みになっています。

要するに、ルールが具体的かつ明確に設定されていることで、曖昧な判断や人為的ミスのリスクを大きく低減できます。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87419_4-1024x535.png)

Pythonでセキュリティポリシーを具体的に記述したコードの例

### 権限（Capability）を使った細かな制御

データや操作には、それぞれ詳細な権限（Capability）が設定されています。この権限は、特定のデータや操作にアクセスできる対象を細かく規定したものです。例えば、あるデータは特定のユーザーだけが閲覧可能であるとか、特定の操作は管理者権限を持つユーザーのみ許可されるといった細かなルールが設定できます。

権限の設定により、モデルが悪意あるプロンプトを受け取ったとしても、権限のない操作を行うことは許可されず、攻撃の影響を局所的に抑えることが必要です。

さらに、各操作の実行履歴やデータの利用状況を明確に追跡することで、権限違反や不正操作の兆候を迅速に把握することも推奨されます。

### 専用インタプリタによる安全な実行

専用のPythonインタプリタを導入することも重要なポイントです。

各指示のデータ経路を常に監視し、データがどこから来て、どのような処理を経て、どこへ向かうかという流れを詳細に記録します。そして、許可されていない動作や疑わしい操作を検知した場合には、その操作を即座に停止してシステムを保護します。

また、操作を実行する前にあらかじめ定義されたセキュリティポリシーや権限を参照し、その指示が安全であるかを確認します。こうした仕組みにより、攻撃者が巧妙に仕込んだ悪意あるプロンプトがあったとしても、システムがそれを早期に検知し、安全な範囲内でのみ動作を許可することができます。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87419_5.png)

データの依存関係を視覚化するグラフを作るためのコード例

## 評価

プロンプトインジェクション防御のために設計された手法が実際に役立つかどうかを判断する際は、利便性（使いやすさ）とセキュリティの両方を考える必要があります。今回の研究では、セキュリティ評価ベンチマークであるAgentDojoを使い、さまざまな領域（ワークスペース、銀行取引、旅行予約、Slackコミュニケーション）の現実的なタスクを対象に評価しています。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87419_6-1-1024x427.png)

さまざまなモデルがタスクをどれくらい成功できたかの比較結果

### 利便性への影響はどれくらいか？

評価ではまず、導入した防御策がシステムの利便性をどれほど低下させるかを調査しています。論文で使われたモデルには、Gemini 2.0シリーズやClaude 3.5シリーズ、GPT-4oシリーズなど複数の最新モデルが含まれています。

実験結果によると、ほとんどのモデルで防御策導入後も大きな利便性低下はありませんでした。ただし、「旅行予約（Travel）」タスクだけは例外で、このタスクに関しては成功率が著しく下がりました。例えば、全体としてのタスク成功率が63.92%だったのに対し、「旅行予約」では成功率がわずか25%にとどまりました。

また興味深いことに、銀行取引（Banking）のタスクでは、防御策を導入したGemini Pro 2.0モデルの成功率が公式ツールAPI利用時よりも向上するというケースもありました。これは防御策が単に利便性を犠牲にするのではなく、状況次第ではプラスに働く可能性もあることを示しています。

### 誤った拒否（False Positive）の問題

利便性評価では、「誤った拒否（False Positive）」がどの程度発生するかも重要な視点です。評価の中で防御策が過度に厳格な場合、実行すべきタスクを不適切に拒否してしまうことがあります。実験によると、STRICTモードを使用した場合に約10%程度の誤った拒否が発生しました。これは防御策の厳格さを調整する際に参考にすべき結果です。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87419_7.png)

Claudeの失敗要因を8種類に分類し、各タスクスイートごとに集計した表

### セキュリティ評価の結果は？

次に、プロンプトインジェクション攻撃に対する防御性能を検証しています。評価では、防御策がない場合と防御策を導入した場合の両方で攻撃が行われました。

防御策がない状況では、モデルは攻撃者によるプロンプトに簡単に騙され、意図しない操作を行ってしまいました。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87419_8-1024x542.png)

攻撃を受けた状況での防御策の脆弱性を検証した結果

しかし、防御策を導入すると、大部分の攻撃を未然に防ぐことができました。セキュリティポリシーを設定した場合、実験で行われた攻撃の90%以上が成功しなくなりました。

ただし一部例外もありました。特に旅行タスクでのある攻撃シナリオでは、防御策が機能せず攻撃が成功しました。この攻撃はプロンプトインジェクションに分類されない特殊なもので、防御策の適用範囲外だったことが原因です。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87419_9-1024x554.png)

実際の統合認証システムでセキュリティポリシーがどのくらい発動したかの推移

### 他の防御手法との比較結果

既存の防御手法（Prompt Sandwiching、Spotlighting、Tool Filterなど）との比較も行われました。その結果、本研究で提案された防御手法は、特に旅行予約のケースを除いて、他の防御手法と比較するとやや成功率が低いことが分かりました。例えば、Prompt Sandwichingの成功率が89.69%だったのに対し、本手法は63.92%でした。しかし、データフローを使った特殊な攻撃に対しては他の手法よりも高い防御性能を示しました。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87419_10-1024x432.png)

今回の方法と既存の防御策の比較（防御の成功率とタスクの成功率）

### データが指示に変わる問題

プロンプトインジェクションの難しい問題は、普通のデータが知らぬ間に「指示」として受け取られてしまうことにあります。評価ではこうした状況が実際に再現され、モデルが単なる情報を誤って操作の指示と認識してしまうケースが確認されました。こうした問題に対処するには、「データ」と「指示」を明確に区別する仕組みを導入する必要があることがわかりました。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87419_11.png)

データの流れを利用して意図しないコードを実行させようとする攻撃の具体例

### システムの負担（オーバーヘッド）はどの程度増えたか

防御策導入によってシステムの負担がどのくらい増加するかについても評価されています。実験結果によれば、防御策導入後にタスクあたりの入力トークン数は平均して約7.2倍増加しました。また出力トークン数も約6.2倍に増えました。これにより、システム負荷の増加は比較的顕著であり、防御策をどの程度厳格に設定するかについては慎重に検討する必要があることが分かります。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87419_12-1024x474.png)

防御策を導入したとき、トークンの使用量がどれくらい増えるかの傾向を示した図

## 思わぬところから情報が漏れる可能性にも注意する

プロンプトインジェクション攻撃への防御策を考えるとき、見落とされがちなポイントが一つあります。それは、直接的な攻撃経路ではなく、意図しない「抜け道（サイドチャネル）」から情報が漏れるリスクです。

たとえば、システムが処理した結果の内容自体は適切に保護されていても、「処理にかかった時間」や「処理の際に生じるエラーの回数」といった、一見無害な情報を通じて攻撃者が機密情報を推測できる場合があります。こうした「間接的な情報流出」は、多くの防御策で盲点になりやすく、知らず知らずのうちに漏洩を招く原因になる可能性があります。

実際に、外部の画像を参照させるプロンプトを送信し、その画像が何回取得されたかを数えることで、システムが非公開の情報をどのように処理しているのかを推測する攻撃事例があります。また、特定のエラーを意図的に発生させ、そのエラーの内容や頻度から非公開データの存在を推測するケースや、処理時間のわずかな差から機密データを推測する攻撃事例も報告されています。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87419_13-1024x333.png)

STRICTモードを使用しない場合に発生し得るデータ漏洩の具体的な経路

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87419_14.png)

例外処理が原因で起こる個人情報漏洩の具体例

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87419_15-1024x338.png)

タイミングの微妙な差を使って情報を抜き取る攻撃（サイドチャネル攻撃）の可能性について示した図

そのため、防御策を検討する際には、目に見える範囲の直接的な攻撃だけでなく、こうした「隠れた情報漏洩」の可能性についても考えておく必要があります。データの処理方法を工夫したり、処理にかかる時間やエラーの発生状況を一定に保つなど、攻撃者に情報を与えないよう注意することが大切になります。

## 他の攻撃パターンへの応用可能性について

プロンプトインジェクション攻撃への防御を考える際、他にもさまざまなタイプの攻撃が存在することを意識しておく必要があります。今回の研究で取り上げた防御方法は、もともとプロンプトインジェクション攻撃を念頭に置いていますが、実際にはそれ以外の攻撃パターンへの対策としても参考になる部分があります。

例えば、第三者が悪意を持って開発した不正なプラグインやツールを知らずにシステムに取り入れてしまった場合、そのツールが許可されていない動作を試みる可能性があります。ここで研究が示しているような「権限を細かく管理する」「どのツールがどのデータにアクセスできるかを明確に制限する」といったノウハウを適用すると、被害を最小限に抑えることができます。

実際に、外部の画像を参照させるプロンプトを送信し、その画像が何回取得されたかを数えることで、システムが非公開の情報をどのように処理しているのかを推測する攻撃事例があります。また、特定のエラーを意図的に発生させ、そのエラーの内容や頻度から非公開データの存在を推測するケースや、処理時間のわずかな差から機密データを推測する攻撃事例も報告されています。

また、社内のユーザーや管理者が意図的に、あるいは意図せずに不正なデータの持ち出しやアクセス権限を濫用してしまうケースでも、同様の対策が有効です。システムのセキュリティを細分化して設定することで、内部の不正行為や過失による情報漏洩のリスクも抑えることができます。

つまり、今回の研究の考え方は、プロンプトインジェクションに限らず、さまざまなシナリオに広く応用できる知見やノウハウを含んでいます。これを参考にしてセキュリティの設計を進めれば、幅広いリスクに対して柔軟な対応が可能になります。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87419_16.png)

プロンプトインジェクション以外の攻撃にも今回の防御策を応用した具体的な例

## 考察

### 権限管理の導入が難しかった過去の経緯から学べること

過去に、システムの安全性を高めるために「細かな権限管理」が提案されたことがありますが、実際にはほとんど普及しませんでした。これは技術的な困難さよりも、使いやすさや手軽さが損なわれることが主な理由でした。権限設定が細かすぎると管理者やユーザーが混乱してしまい、結局「面倒だから」という理由で設定を甘くしてしまうことが多かったためです。

セキュリティを高めるために重要なのは、使いやすさと安全性のバランスです。サービスに権限管理を導入するときは、細かくしすぎることの弊害を意識して、ユーザーに無理なく受け入れられる範囲で設計する必要があります。

### 権限を緩めるときの注意点とユーザーの負担軽減

セキュリティの世界には、「最初は厳しく設定して、必要に応じて徐々に緩める」という考え方があります。しかし、この方法を取ると、最終的に多くの権限がユーザー任せになり、ユーザー自身が安全性について判断しなくてはいけない場面が増えます。その結果、ユーザーに過剰な負担がかかり、実際には十分な注意が払われない可能性も高まります。

そのため、サービスを提供する側は「ユーザーに判断を丸投げしない」ことを基本とし、権限やポリシーの緩和は慎重に行い、ユーザーが自然に安全な選択をできるような仕組みを提供することが重要になります。

### 結局のところ、プロンプトインジェクション攻撃は解決されるのか？

プロンプトインジェクション攻撃への防御手法が進んだことで、確かに多くの攻撃を防げるようになってきました。しかし、これで問題が完全に解決されたと考えるのは早計です。実際にはまだまだ未知の攻撃方法が出てくる可能性がありますし、現時点での対策が未来永劫通用するとは限りません。

防御方法を常に改善していくためには、攻撃と防御が常に「いたちごっこ」の関係にあることを理解し、新しい攻撃手法にいち早く気づいて対処する姿勢を持ち続けることが欠かせません。ユーザーや開発者が常に最新の攻撃事例に注意を払い、実際に起きた事例から学び続けることが大切になります。

## 今後の展望

今後の課題として、今回の防御手法をさらに実践的にするための改善が挙げられます。例えば、ユーザーが細かな設定を意識せずに自然に安全な行動が取れるようにする工夫や、防御策を入れたときにシステムの動作が重くなりすぎないような工夫が求められています。また、新しく登場する攻撃パターンにも対応できるよう、攻撃手法の研究や防御技術の柔軟性をさらに高めていく必要もあります。

## まとめ

本記事では、プロンプトインジェクション攻撃への防御手法を提案した研究を紹介しました。この研究は、二つのLLMを分離運用し、権限や明確なポリシーを活用することで、安全性を高める方法を示しています。一方で、導入の難しさやシステムの負荷といった課題も指摘されています。また、実践的な視点から、見落とされがちな間接的な情報漏洩についての注意点も明示されています。読者の皆さんも、これらの知見を参考に、自身の状況に応じて活用できるでしょう。

**参照文献情報**

- タイトル：Defeating Prompt Injections by Design
- URL： [https://doi.org/10.48550/arXiv.2503.18813](https://doi.org/10.48550/arXiv.2503.18813)
- 著者：Edoardo Debenedetti, Ilia Shumailov, Tianqi Fan, Jamie Hayes, Nicholas Carlini, Daniel Fabian, Christoph Kern, Chongyang Shi, Andreas Terzis, Florian Tramèr
- 所属：Google, Google DeepMind, ETH Zurich

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMで複数のアイデアを組み合わせ、イノベーションを目指した新しいアイデアを作成する方法](https://ai-data-base.com/archives/87358)

[LLMに「意図」を含んだ回答をさせる方法の効果](https://ai-data-base.com/archives/87486)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)