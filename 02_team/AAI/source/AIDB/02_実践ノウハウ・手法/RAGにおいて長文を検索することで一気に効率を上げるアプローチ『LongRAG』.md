---
title: "RAGにおいて長文を検索することで一気に効率を上げるアプローチ『LongRAG』"
source: "https://ai-data-base.com/archives/71774"
author:
  - "[[AIDB Research]]"
published: 2024-06-27
created: 2025-06-13
description: "RAGには、膨大なデータの中から関連性のある短い文章を探し出すのが難しいという課題があります。そんな中、研究者らは、より大きな「塊」で情報を探すことで、検索の効率を劇的に向上させる手法を考案しました。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

RAGには、膨大なデータの中から関連性のある短い文章を探し出すのが難しいという課題があります。そんな中、研究者らは、より大きな「塊」で情報を探すことで、検索の効率を劇的に向上させる手法を考案しました。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71774-1024x576.jpg)

**参照論文情報**

- タイトル：LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs
- 著者：Ziyan Jiang, Xueguang Ma, Wenhu Chen
- 所属：University of Waterloo

**本記事の関連研究** ：

- [ロングコンテキストはRAGもText to SQLも解決するか　Googleがケーススタディを実施](https://ai-data-base.com/archives/71486)
- [RAGの失敗パターン7選と教訓9箇条](https://ai-data-base.com/archives/69154)
- [RAGにおいて取得された情報と事前知識が矛盾しても、情報に説得力があるときLLMは受け入れる](https://ai-data-base.com/archives/64979)

## 背景

外部知識を活用してLLMの能力を拡張する「検索拡張生成（RAG）」が注目を集めています。質問に関連する情報を外部データベースから検索し、情報をLLMに与えることで、より正確で適切な回答を生成する手法です。

RAGシステムでは、短い文章（例えば通常100語程度）を検索単位として使用することが多いです。しかしそれでは検索システムが多くの候補を検索する必要があり過度の負担がかかります。また、短い文章を検索単位とすることで、文脈の一貫性が失われる恐れもあります。

今回研究者らはそのようなRAGにおける現状の課題に対応するため、「LongRAG」というフレームワークを提案しています。長い文章を検索単位として採用することで、検索の負担を軽減する手法です。

以下では、そんな「LongRAG」についてより詳しく紹介していきます。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71774_1-683x1024.jpg)

従来のRAGとLongRAGの比較。LongRAGは長い検索単位を使用し、検索器の負担を軽減しながら長文脈LLMの能力を活用している。

## LongRAGの考え方

LongRAGでは、「長文検索器」と「長文読解器」という2つの要素が重要になります。一つずつ見ていきます。

### 長文検索器の仕組み

1. 長文検索単位の形成

従来とは打って変わって約4,000トークン（単語や記号の単位）からなる長い文章を検索単位として採用します。ここで、検索単位とする長い文章は複数の関連文書をグループ化することで作成します。

グループ化のプロセスは以下のように行われます。

まず、Wikipedia等の文書に存在する、関連する他の記事へのハイパーリンク情報を活用して、互いに関連する文書を特定します。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71774_2-1024x523.jpg)

LongRAGの例。Wikipedia文書のグループ化と長文検索・読解の流れを示している。

次に以下のような手順でグループ化を行います。  
a. 文書をリンク数（関連文書の数）の少ない順にソートする  
b. 各文書について、関連する文書を探し、既存のグループに属しているかチェックする  
c. 新しいグループを作成し、関連する文書を可能な限り統合する  
d. グループのサイズが一定の上限（例：4,000トークン）を超えないように調整する

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71774_3-584x1024.jpg)

上記手順を表現しているアルゴリズム

このプロセスによって、単に長さだけでなく、意味的にも関連性の高い文書群が1つの検索単位として形成されます。また、関連する情報が1つの検索単位内に含まれることで、文脈や複雑な関係性が保持されやすくなります。

1. 類似度検索

検索プロセスでは、入力された質問と検索単位の類似度が計算されます。この際、質問と検索単位それぞれにエンコーダー（文章を数値ベクトルに変換するモデル）が適用されます。しかし、長文をそのままエンコードすることは技術的に困難なため、検索単位内の複数の短い文章セグメントの中で最も類似度の高いものが代表値として使用されます。

1. 検索結果の集約

類似度に基づいて上位の検索単位が選ばれ、それらが連結されて長文コンテキスト（文脈）として扱われます。通常、4〜8個の検索単位が使用され、合計で約30,000トークンの長さになります。

### 長文読解器

長文読解器は要するにジェネレーターです。検索された長文コンテキストを入力として受け取り、質問に対する回答を生成します。そのため、長いコンテキストを理解できるLLMを使用します。例えば、Gemini-1.5-ProやGPT-4oといったモデルです。

質問、指示、そして検索された長文コンテキストがLLMに入力され、モデルはこの情報を基に推論を行い、最終的な回答を生成します。

### 回答生成のプロセスについて

短い文脈（1,000トークン未満）の場合は、LLMに直接回答を抽出するよう指示が出されます。一方、長い文脈（4,000トークン以上）の場合は、まず詳細な回答を生成し、その後でその回答から簡潔な短い回答を抽出するという2段階のプロセスを踏みます。そうすることで情報抽出の精度が向上することが経験的に確認されています。

### LongRAGのメリット

1. 検索対象のデータ量が大幅に削減され（例：Wikipediaの場合、2,200万件から60万件に）、検索システムの負担が軽減される
2. 長い検索単位を使用することで、文脈の一貫性が保たれ、より包括的な情報が提供される
3. 複数の文書からの情報を必要とする複雑な質問に対しても、一度の検索で必要な情報を含む長い文脈を取得できる可能性が高まる

## 実験

LongRAGの性能を評価する実験が行われました。主に2つのデータセットが使用されました。

1. Natural Questions (NQ)：  
	Googleの実際の検索クエリから作成された3,610の質問からなるデータセットです。回答はWikipedia記事内の特定の文章範囲として提供されています。
2. HotpotQA：  
	7,405の質問を含む開発セットが評価に使用されました。回答を得るために2つのWikipedia文書の情報を組み合わせる必要があるのが特徴です。質問は主に比較型と連鎖型の2種類に分類されます。

知識ソースとして、それぞれのデータセットに対応するバージョンの英語版Wikipediaが採用されました。NQには2018年12月20日版（約300万文書、2,200万段落）、HotpotQAには2017年10月1日版（約520万文書）が使用されました。

### 検索性能の評価

まず、主に「検索」の性能に焦点を当てた実験結果を説明します。つまり、質問に対して関連する文書や情報をどれだけ正確に取得できるかを評価した結果です。

検索性能の評価には、主に2つの指標が用いられました。

1. 回答 [再現率](https://ai-data-base.com/archives/26095 "再現率") （Answer Recall, AR）  
	検索された文書群の中に正解の回答文字列が含まれている割合を示します。
2. 再現率（Recall）  
	HotpotQAで使用され、検索結果に2つのゴールド文書（正解に必要な2つの文書）が含まれている割合を表します。

実験では、オープンソースの密度検索ツールキットTevatronが活用され、ベースとなる埋め込みモデルにはbge-large-en-v1.5が採用されました。

#### 検索性能の結果

NQデータセットでの実験結果は以下の通りでした。

- 長文検索単位（文書レベルやグループ化された文書レベル）を使用することで、 [コーパス](https://ai-data-base.com/archives/26324 "コーパス") サイズが約30分の1（2,200万から60万）に削減されました。
- トップ1の回答再現率が約20ポイント向上し、52.24%から71.69%に改善されました。
- 従来の手法と同等の性能を得るために必要な検索単位の数が大幅に減少しました（例：パッセージレベルの100単位に対し、グループ化された文書レベルでは8単位で同等の性能）。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71774_4-1024x373.png)

NQデータセットにおける検索性能の比較。長文検索単位の使用による コーパス サイズの削減と回答再現率の向上を示している。

そしてHotpotQAデータセットでの実験結果は以下の通りでした。

- NQデータセットと同様に、長文検索単位の使用により検索器の負担が大幅に軽減されました。
- 文書レベルでの検索単位2つを使用した場合、再現率が30.01%から56.30%に、回答再現率が47.75%から72.49%に向上しました。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71774_5-1024x250.png)

HotpotQAデータセットにおける検索性能の比較。長文検索単位の使用による再現率と回答再現率の向上を示している。

上記の結果は、LongRAGの有効性を明確に示しています。

まずデータ量を大幅に削減しながらも、検索精度を向上させることに成功しています。複雑な推論を要するHotpotQAでも顕著な性能向上を達成しているため、LongRAGが文脈を保持しつつ関連情報を効果的に統合できることを示唆しています。

つまり、より少ない計算リソースでより高品質な回答を生成できる可能性があるということです。

#### 長文検索単位のエンコーディング手法の比較

また、長文検索単位全体をエンコードする代わりに、近似手法（検索単位内の各チャンクと質問との類似度の最大値を取る方法）の有効性が検証されました。結果は以下の通りでした。

- 一般的な埋め込みモデル（BGE-Large）を使用し、512トークンごとのチャンクで近似する方法が最も高い性能（回答再現率@1で71.7%）を示しました。
- 長文埋め込みモデル（E5-Mistral-7B）を使用して文書全体（平均4,000トークン）をエンコードする方法は54.2%の性能を示しました。
- 同じく長文埋め込みモデルを使用してグループ化された文書（平均6,000トークン）全体をエンコードする方法は19.6%の性能に留まりました。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71774_6-1024x272.png)

長文検索単位のエンコーディング手法の比較。近似手法の有効性を示している。

この結果は、長文を直接エンコードするよりも、短いチャンクに分割して最大類似度を取る近似手法の方が効果的であることを示しています。また一般的な埋め込みモデルを使用した近似手法が最も高い性能を発揮したことは注目に値します。

現状の長文埋め込みモデルにはまだ改善の余地があり、長文理解技術の更なる発展が、LongRAGの性能向上に直結するのかもしれません。

### 質問応答タスクの全体的な性能

次に「質問応答タスク全体」の性能に焦点を当てた実験内容を説明します。検索結果を利用して最終的に正確な回答をどれだけ生成できるかの評価結果です。

LongRAGの性能は、以下3つのベースラインと比較されました。

1. クローズドブック方式：外部知識を使用せず、最新のLLMに直接質問を投げかける方法。
2. 完全教師ありRAG：訓練データを用いて微調整されたRAGモデル。
3. 微調整なしRAG：訓練データを使用せずにRAGフレームワークを適用する方法。

結果は以下の通りです。

#### Natural Questions (NQ) での性能

NQデータセットでは、LongRAGは62.7%の完全一致率（Exact Match, EM）を達成しました。訓練済みのRAGモデルの最高性能に匹敵する性能です。

なお完全一致率とは、システムの回答が人間が用意した正解と完全に一致する割合を指します。

より詳しくは以下のような結果が得られています。

- 検索単位の種類に関わらず、読解器に入力する検索単位の数には最適値が存在することが分かりました。
- 最適な入力トークン数は約30,000トークンであることが示唆されました。
- 意味的な一貫性を保つため、より長い検索単位（文書レベルやグループ化された文書レベル）の方が短い検索単位（パッセージレベル）よりも優れた性能を示しました。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71774_7.png)

NQデータセットにおける質問応答タスクの性能比較。LongRAGと他の手法の完全一致率を示している。

#### HotpotQAでの性能

HotpotQAデータセットにおいては、LongRAGは64.3%の完全一致率を記録しました。HotpotQAは複数の文書から情報を組み合わせる必要がある複雑な質問を含むデータセットであり、高い性能を達成するということは、複雑な推論タスクにも対応できることを示唆しています。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71774_8.png)

HotpotQAデータセットにおける質問応答タスクの性能比較。LongRAGと他の手法の完全一致率を示している。

#### 検索単位の選択と最適な数の検討

異なる検索単位の種類と数による性能の変化も整理して示されています。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71774_9-1024x594.png)

NQデータセットにおけるLongRAGの異なる設定の比較。検索単位の種類と数による性能の変化を示している。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71774_10-1024x594.png)

HotpotQAデータセットにおけるLongRAGの異なる設定の比較。検索単位の種類と数による性能の変化を示している。

両データセットにおいて、以下のことがわかりました。

- まず、検索単位の種類に関わらず、読解器に入力する検索単位の数には最適値が存在することが分かりました。
- また最適な入力トークン数は約30,000トークンであることが示唆されました。
- さらに、意味的な一貫性を保つため、より長い検索単位（文書レベルやグループ化された文書レベル）の方が短い検索単位（パッセージレベル）よりも優れた性能を示しました。

#### 読解器モデルの比較

最後に、異なる読解器モデルの性能比較が行われました。Gemini-1.5-pro、GPT-4-Turbo、GPT-4oの3つのモデルが比較され、GPT-4oが最も高い完全一致スコアを記録しました。今回の比較では、GPT-4oが長文脈の処理と理解に最も適していることを示唆しています。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71774_11.png)

NQデータセットにおける異なる読解器モデルの性能比較。GPT-4oが最高の完全一致スコアを示している。

実験結果全体から、LongRAGが従来のRAGシステムと比較して、効率的かつ高性能な質問応答システムであることが実証されました。また、複雑な質問や多段階の推論を必要とする問題に対しても優れた性能を発揮することが確認されました。

## まとめ

本記事では、LongRAGという新しいRAGフレームワークの研究を紹介しました。

従来のRAGシステムの課題に対応するため、長い検索単位（約4,000トークン）を採用し、検索対象となるデータ量を大幅に削減しています。同時に、ロングコンテキストを理解できる最新のLLMを活用することで、より正確な回答生成を実現するというのもポイントです。

実験結果から、LongRAGは特に複雑な質問や多段階の推論を必要とする問題に対して優れた性能を発揮することが確認されました。Natural QuestionsやHotpotQAといったデータセットにおいて、微調整を行わずに訓練済みモデルに匹敵する性能を達成しています。

ただし、長文を効率的にエンコードできる埋め込みモデルの改善や、より一般的な文書グループ化手法の開発など、いくつかの課題も残されています。

今後、ロングコンテキスト処理の技術がさらに発展することで、より高度で効率的な質問応答システムの実現が期待されます。

- 参照論文URL： [https://arxiv.org/abs/2406.15319](https://arxiv.org/abs/2406.15319)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMにハイレベルな問題の解決アプローチを自分で考えさせるエージェント化手法「SelfGoal」](https://ai-data-base.com/archives/71720)

[LLMはRAGコンテキストと事前知識のどちらに依存する？](https://ai-data-base.com/archives/71857)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)