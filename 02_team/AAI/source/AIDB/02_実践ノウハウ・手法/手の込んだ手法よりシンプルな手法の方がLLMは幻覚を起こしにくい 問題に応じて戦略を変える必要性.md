---
title: "手の込んだ手法よりシンプルな手法の方がLLMは幻覚を起こしにくい 問題に応じて戦略を変える必要性"
source: "https://ai-data-base.com/archives/77700"
author:
  - "[[AIDB Research]]"
published: 2024-10-30
created: 2025-06-13
description: "本記事では、LLMにおける「ハルシネーション」（幻覚）の課題とその対策に関する最新の研究を紹介します。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、LLMにおける「ハルシネーション」（幻覚）の課題とその対策に関する最新の研究を紹介します。

ChatGPTの利用者が1.8億人を超え、WhatsAppでもLLMベースのチャットボットの導入が進むなか、重要分野での誤情報の影響が特に懸念されています。そこで研究者らは、プロンプトエンジニアリングやLLMエージェントといった「外側からのアプローチ」で、どこまでハルシネーションを抑制できるのかを明らかにしようとしています。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_77700-1024x576.jpg)

**参照論文情報**

- タイトル：Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models
- 著者：Liam Barkley, Brink van der Merwe
- 所属：Stellenbosch University

## 背景

LLMは産業界でも学術界でも幅広く大きな注目を集めています。しかし、LLMには「ハルシネーション（幻覚）」と呼ばれる重要な課題があります。実際の根拠がないにもかかわらず、もっともらしい誤った情報を生成してしまう現象です。

ChatGPTは約1億8550万人のユーザーを抱え、WhatsAppでもLLMを活用したチャットボットが2024年4月から導入されるなど、一般の人々の間でLLMの利用が急速に広がっている状況で、ハルシネーションの問題もより重要になりつつあります。さらに政治や医療などの重要分野でも導入が進むにつれ、LLMが生成する誤った情報が重大な結果を招くリスクがますます無視できなくなっています。

ハルシネーションを減らすための手法としては、例えばプロンプトエンジニアリングが期待されています。LLMに対する指示の出し方を工夫することで、より正確な出力を得ようとする取り組みです。また、外部ツールと組み合わせてLLMの機能を拡張する「LLMエージェント」アプローチも登場しています。

多くの最新のLLMは企業の独自技術であり、その内部の仕組みにアクセスすることはできません。そのために、モデルの中身に関係なく、プロンプトの設計や外部ツールの活用といった「外側からのアプローチ」で、どこまでハルシネーションを抑制できるのかが注目されているのです。

なお、ハルシネーションが必ずしも悪いものではないケースもあることに留意は必要です。例えば創作活動では、事実に基づかない新しいアイデアを生み出す能力が求められます。したがって、用途に応じてハルシネーションをコントロールする手法の開発が重要です。

このような背景から、今回研究者らはプロンプトエンジニアリングやLLMエージェントの活用が、様々な場面でハルシネーションの発生率にどのような影響を与えるのかを網羅的に調査しました。

以下でその内容を紹介します。まずは、LLM、幻覚、プロンプト技術、そしてLLMエージェントについての基礎をおさらいします。

## 前提知識

### LLMの基本構造

#### トランスフォーマーアーキテクチャ

現在最も高性能なLLMは、2017年にGoogleが発表したトランスフォーマーと呼ばれる特殊な [ニューラルネットワーク](https://ai-data-base.com/archives/26117 "ニューラルネットワーク") を基盤としています。単語や文字などのテキストを処理する人工ニューロン層で構成されています。

注目すべき特徴として「アテンション機構」が採用されています。入力データの異なる部分に選択的に注意を向けることができ、言語データにおける長距離の関係性や複雑なパターンを効果的に捉えることが可能となりました。

#### 計算リソースの要件

LLMをローカル環境で動作させるには、大量のメモリ（RAM）やグラフィックスメモリ（VRAM）が必要とされます。例えば、80億個のパラメータを持つモデルを実行するには、最低でも8 [GB](https://ai-data-base.com/archives/26343 "勾配ブースティング") のVRAMを搭載した [GPU](https://ai-data-base.com/archives/26570 "GPU") が求められます。

#### パラメータ量と精度のトレードオフ

モデルのパラメータ数を増やすことで、より複雑な言語パターンを学習できるようになります。しかしデータの質などの要因も影響してきます。

#### 量子化による最適化

LLMの量子化とは、モデルの重みを低精度に変換することで、サイズを縮小する技術です。例えば32ビット浮動小数点数を8ビット整数に変換することで、計算リソースの節約とモデルの軽量化を実現しています。

#### 温度パラメータの役割

LLMの「温度」は、出力の多様性を制御するハイパーパラメータです。低温設定では学習データのパターンに忠実な、予測可能な出力が得られます。一方で高温設定では、より創造的で予測不可能な出力が生成されます。

### 幻覚の分類

幻覚は主に以下の2つに分類されます。

（１）事実性の幻覚

現実世界の知識と直接矛盾する情報や、現実世界の知識では検証できない情報などが該当します。

（２）忠実性の幻覚

プロンプトの指示に従っていない応答や提供された文脈と矛盾する応答、応答内で自己矛盾する内容などが該当します。

### プロンプトエンジニアリング技術

#### 思考連鎖（Chain-of-Thought）

数学的問題解決などの推論タスクに特に有効な戦略です。タスクを小さなステップに分解し、各ステップでの推論過程を明示的に示すことで、より正確な解答を導き出します。

#### 自己一貫性（Self-Consistency）

複数回のLLM呼び出しによる多数決を行う手法です。温度設定を調整することで、創造性と正確性のバランスを取ることができます。

#### その他のアプローチ

問題を段階的に分解し、各段階で最適な推論経路を選択する「思考の木（Tree-of-Thoughts）」という手法や、LLMに自己批評と改善を行わせる反復的なアプローチ「リフレクション」なども有名です。

### LLMエージェント

最もシンプルなエージェント [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") は、事前に定められた順序でタスクを実行する「チェーン [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") 」です。より高度なものとして、LLMが外部ツールを活用して多段階の意思決定を行う一般的なツール呼び出しエージェントがあります。

また、ReActフレームワークと呼ばれる人気の高い [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") も存在します。以下の3つのモジュールで構成されます。

（１）ツール  
外部システムへのアクセス機能

（２）メモリ  
過去のステップの情報保持

（３）プランニング  
タスク達成のための段階的な計画立案

## 実験手法と設計

本研究で使用されたLLMシステム、モデルやライブラリ、様々なプロンプト戦略とエージェントの実装方法、そしてそれらを比較評価するプロセスについて記載します。

### 実装環境

実験システムは、以下のツールを用いて構築されました。

- Python（プログラミング言語）
- LangChain（LLMを活用したアプリケーション開発のためのオープンソースフレームワーク）
- Ollama（ローカル環境でLLMを実行するためのプラットフォーム）

なおハードウェアとしては、8 [GB](https://ai-data-base.com/archives/26343 "勾配ブースティング") のVRAMを搭載したNvidia GeForce RTX 2080 [GPU](https://ai-data-base.com/archives/26570 "GPU") が使用されました。

### モデルの選択と設定

実験では、主に2つのモデルが使用されました。

（１）Meta-Llama-3-8B-Instruct-Q6\_Kモデル

- 80億のパラメータを持つ
- 6ビットの量子化が適用されている
- 温度設定は0.2、0.5、0.8の3段階で検証が行われた

（２）Meta-Llama-3.1-8Bモデル

- エージェント [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") のテスト用に採用
- 量子化は適用されていない
- 温度設定は0.5で固定された

### プロンプト技術の実装

#### 思考連鎖（CoT）の実装

物事を順序立てて考えるタスクのために、思考連鎖という方法が実装されました。

LLMが出す最終的な答えに加えて、どのように考えて答えにたどり着いたのかという手順も一緒に保存されます。また、LLMが指示された通りの形式で回答を出さなかった場合には、正しい形式の回答が得られるまで、何度かやり直すことができるように設定されました。

#### 自己一貫性（SC）の実装

自己一貫性という方法では、まず同じ質問に対して5回の回答を集めました。その後、最も多く出てきた回答を最終的な答えとして採用しました。

人間が重要な決定をする時に、何度か考えて最も納得できる答えを選ぶのに似た仕組みです。

#### マルチエージェントディベート（MAD）の実装

マルチエージェントディベートは、LangChainというツールの中にある「会話を記録する機能」が使われて実装されました。2つのLLMが意見を出し合い、話し合いを行う形で進められます。両方のLLMが同じ意見で一致した場合、あるいは10回話し合いを重ねた時点で終了するように設定されました。1つの問題に対して異なる視点から検討を行うためです。

#### Chat Protect（CP）戦略の実装

LLMから複数の回答を集めて矛盾がないかをチェックする方法「Chat Protect」も採用されました。まず同じ質問に対して5つの回答を集め、それらの回答の間に矛盾が見つかった場合は、その質問への回答を控えます。LLMが自信を持てない質問に対して、誤った情報を出すリスクを減らすためのアプローチです。

#### 知識グラフ（KGR）戦略の実装

知識グラフを活用する方法では、Wikidataという大規模なデータベースが使用されました。以下の手順で実装されました。

1. まずLLMが質問に対して仮の回答を出す
2. 次に、その質問と回答に関連する重要な項目（エンティティ）を特定する
3. Wikidataから、その項目に関連する情報が検索される
4. 最後に、見つかった情報を参考にして、LLMが最終的な回答を作る

LLMが実際の事実に基づいた回答を行うようにするための手法です。

#### Chain-of-Verification (CoVe-2)戦略の実装

多肢選択問題に特化した検証の仕組みとして、CoVe-2戦略が実装されました。この方法は以下の手順で行われます。

1. まず、LLMが選択肢の中から1つの回答を選ぶ
2. 次に、同じ質問を選択肢を見せずにLLMに投げかけ、自由回答を得る
3. 最後に、1で選んだ選択肢と2で得た自由回答が一致するかどうかが確認される
4. もし一致しない場合は、その質問への回答は控えられる

例えば、「太陽系で最も大きい惑星は？」という質問で、最初に選択肢から「木星」を選んだ場合、次に選択肢なしで「木星」と答えれば回答が採用され、異なる惑星を答えた場合は回答が控えられるという具合です。

### 評価用ベンチマーク

3種類のベンチマークデータセットが使用されました。

（１）GSM8K（Grade School Math 8K）

1,319問の数学的な文章題で構成されるベンチマークです。論理的な推論能力の評価に使用されました。

（２）TriviaQA

読解力とクイズ形式の問題を含みます。事実に基づく正確性の評価に使用されました。検証セットから1,000問が選択されました。

（３）MMLU（Massive Multitask Language Understanding）

57の異なる分野にわたる多肢選択問題です。各分野から約17問が選択され、合計1,000問が使用されました。

### 評価方法

以下の評価指標を基準に各手法が測定されました。

- 正解数
- 幻覚（誤った回答）の数
- 正確性（accuracy）
- Top-N正確性（N個のサンプル中に正解が含まれる割合）

なお計算リソースの制約により、各戦略について3回の独立した実行が行われました。また、プロンプトあたりの平均計算コストを示すため、各戦略の平均プロンプト数も記録されました。

### エージェントアーキテクチャの実装

この実験では、3つの異なるタイプのエージェントが作られました。

1つ目は「チェーン [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") 」と呼ばれるもので、LLMへの異なる問い合わせを続けて行う仕組みになっています。1回目の問い合わせでは使用するツールのリストを作り、2回目の問い合わせではそのツールから得られた結果を使って回答を作ります。

2つ目は「ReAct [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") 」で、LangChainが用意している特別な機能を使って作られました。このエージェントは、複数の段階を踏んで判断を行うことができます。

3つ目は「ReAct-DDG [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") 」という特別なバージョンで、DuckDuckGo（検索ツール）だけを使うように制限されています。

そしてエージェントには、3種類の便利なツールが用意されました。1つ目は「Wikipedia」で、人物や場所、物事について調べることができます。2つ目は「DuckDuckGo」で、インターネット全体から情報を検索することができます。3つ目は「Riza」というツールで、Pythonというプログラミング言語のコードを安全に実行することができます。

## 実験結果

各実験の結果を紹介します。まずプロンプト技術の性能が評価され、最後のパートではエージェントの結果が分析されました。

### GSM8K（数学問題）の結果

温度設定を変えながら、さまざまな手法が試されました。下の表では異なるプロンプト戦略とその温度設定がGSM8K数学問題ベンチマークでどのような結果を出したかがまとめられています。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_77700_1.png)

GSM8K数学ベンチマークにおける各戦略の性能比較（温度設定別）

下の図はSCとSC-CoT戦略において、正解が何回出現したかの頻度を分析しているものです。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_77700_2.png)

GSM8KベンチマークにおけるSCとSC-CoT戦略の正解出現頻度の比較

また下の図はSCとSC-CoTの上位N個の回答における精度（Top-N accuracy）を比較するものです。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_77700_3.png)

GSM8KベンチマークにおけるSCとSC-CoTのTop-N精度の比較

主な発見として、まず自己一貫性（SC）戦略が、温度0.8の設定で最も良い成績を収めました。また高い温度設定では、より多様な回答が生成されましたが、それと同時に誤りのリスクも高まることが確認されました。SCアプローチでは、複数の回答を集めて多数決を取ることで、創造性と正確性のバランスが取られました。

数学的な問題解決には、ある程度の創造性と正確な推論の両方が必要とされます。実験結果から、SC戦略がこの両方をうまく実現できたことが示されました。

### TriviaQA（クイズ問題）の結果

クイズ形式の問題では、以下の興味深い結果が得られました。まず、下の表ではTriviaQAベンチマークにおける各戦略の性能が比較されています。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_77700_4.png)

TriviaQAベンチマークにおける各戦略の性能比較（温度設定別）

Chat Protect（CP）戦略が最も高い正確性を示しました。ただし、CPは多くの質問に対して「回答を控える」という選択を取りました。下のグラフはTriviaQAデータセットにおけるSC戦略の正解サンプル出現頻度を示しています。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_77700_5.png)

TriviaQAベンチマークにおけるSC戦略の正解出現頻度

また、DuckDuckGo検索を活用した方法（DDGA）では、正しい回答の数が大きく増加しました。知識グラフを使用した方法（KGR）は、必要な情報を見つけることに苦労し、期待したほどの効果は得られませんでした。

なお温度設定が高くなるにつれて、CP戦略はより慎重に回答するようになり、回答数は減少しましたが正確性は向上しました。  
下のグラフはTriviaQAデータセットにおけるSC戦略のTop-N精度を温度別で示しています。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_77700_6.png)

TriviaQAベンチマークにおけるSC戦略のTop-N精度

### MMLU（総合問題）の結果

様々な分野の問題に対する結果として、Chain-of-Verification (CoVe-2)戦略が最も高い正確性を示しました。下の表ではMMLUベンチマークにおける各戦略の性能が示されています。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_77700_7.png)

MMLUベンチマークにおける各戦略の性能比較（温度設定別）

また、マルチエージェントディベート（MAD）戦略では、最も多くの正解回答が得られました。また幅広い分野の問題に対して安定した性能を示しました。複数のLLMが協力して回答を導き出す方式が有効であることを示しています。なお、MMLUベンチマークの科目別性能は以下の図で確認できます。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_77700_8-1-623x1024.png)

MMLUデータセットの科目別におけるSCとMAD戦略の平均精度比較

一方で、リフレクション（自己批評）戦略は、期待したほどの効果が得られませんでした。

### エージェントの結果

エージェントを使用した実験では、予想外の結果が得られました。下の表は各エージェント [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") の性能を示しています。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_77700_9.png)

MMLUベンチマークにおける各エージェント アーキテクチャ の性能比較

まず、基本的な制御戦略が、すべてのベンチマークで最も良い性能を示しました。

しかし外部ツールを使用するエージェントでは、かえって誤った回答（幻覚）が増加する傾向が見られました。

またReActエージェントは、与えられたツールを効果的に使用することに苦労しました。ReActエージェントがTriviaQA問題を解く過程は以下のように図示できますが、途中でエラーが発生していることも例として表現されています。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_77700_10-687x1024.png)

TriviaQA問題に対するReActエージェントの制御フロー図

なお、チェーン [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") における各ベンチマークでのツール使用状況は下のグラフで確認できます。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_77700_11.png)

チェーン アーキテクチャ におけるベンチマーク別のツール呼び出し回数の平均

この結果からは、小規模なLLMに外部ツールを追加する際には注意が必要であることが示唆されました。ツールの追加は機能を拡張できる一方で、新たな種類の誤りを引き起こす可能性があります。

### 全体に関する分析

異なるタイプの問題に対して、それぞれ最適な戦略が異なることが分かりました。単純な戦略が、より複雑な方法よりも良い結果を示すことがあります。また外部ツールの使用は、必ずしも性能向上につながるわけではないことが明らかになりました。

## まとめ

本記事では、LLMのハルシネーション（幻覚）を抑制するための様々な手法を検証した研究を紹介しました。研究チームは、GSM8K、TriviaQA、MMLUという3つの異なるベンチマークテストを用いて、各種手法の効果を綿密に検証しました。

その結果、タスクの特性によって最適な手法が異なることが明らかになりました。数学的な問題に対しては、複数の回答から多数決を取る「Self-Consistency（SC）」戦略が最も効果的でした。一方、一般知識を問う質問では、矛盾する回答が出た場合は回答を控える「Chat Protect」戦略が、精度と回答数のバランスを取る上で優れた性能を示しました。

また、外部ツールをLLMと組み合わせることについても興味深い発見がありました。小規模なモデルの場合、外部ツールの追加はかえってハルシネーションを増加させる可能性があることが判明しました。

研究チームは、今後より強力なモデルでの追加検証が必要であるとしています。

- 参照論文URL： [https://arxiv.org/abs/2410.19385](https://arxiv.org/abs/2410.19385)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[開発企業や言語ごとに異なるLLMのイデオロギー、価値観や態度](https://ai-data-base.com/archives/77645)

[プレイヤーの行動に応じてゲームを自動生成する技術　Googleなどが開発](https://ai-data-base.com/archives/77790)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)