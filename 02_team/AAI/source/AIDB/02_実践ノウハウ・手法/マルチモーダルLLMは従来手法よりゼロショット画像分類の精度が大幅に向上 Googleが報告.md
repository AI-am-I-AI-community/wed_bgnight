---
title: "マルチモーダルLLMは従来手法よりゼロショット画像分類の精度が大幅に向上 Googleが報告"
source: "https://ai-data-base.com/archives/70709"
author:
  - "[[AIDB Research]]"
published: 2024-06-11
created: 2025-06-13
description: "Googleの研究チームは、マルチモーダルLLMを用いて画像から豊かなテキスト（説明文）を作り出すことで、視覚的特徴とテキストの特徴を組み合わせ、従来手法よりもゼロショット画像分類の精度を向上させています。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

Googleの研究チームは、マルチモーダルLLMを用いて画像から豊かなテキスト（説明文）を作り出すことで、視覚的特徴とテキストの特徴を組み合わせ、従来手法よりもゼロショット画像分類の精度を向上させています。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70709-1024x576.jpg)

**参照論文情報**

- タイトル：What Do You See? Enhancing Zero-Shot Image Classification with Multimodal Large Language Models
- 著者：Abdelrahman Abdelhamed, Mahmoud Afifi, Alec Go
- 所属：Google Research, Google

## 背景

近年、大規模言語モデル（LLM）が画像分類を含む様々な画像系タスクに利用されるようになってきました。

特にゼロショット画像分類の分野では、以下の手順で画像分類を行う手法が提案されていました。

1. LLMを用いてクラスラベルに関する説明文を生成
2. CLIPなどの対照学習モデルを用いて、上記の説明文と入力画像の類似度を計算
3. 入力画像と最も類似度の高い説明文のクラスを分類結果に決定

なお本分野におけるゼロショットとは、例を受け取らずに事前学習の知識のみでタスクを実行することを指します。反対に例をX個与える場合はX-shotと表現します。少しだけ与える際はFew-shotとなります。

従来手法では、推論時に入力画像のエンベディング（視覚的特徴）のみを [特徴量](https://ai-data-base.com/archives/26406 "特徴量") として扱っているため、クラスラベルの説明文に含まれる詳細な情報を十分に活用できていないという課題がありました。  
視覚的特徴だけに頼ると、精度が落ちることがあります。なぜなら、「クラスラベルを説明する文章に含まれる細かい情報」を、画像の特徴だけでは捉えきれないからです。

そこで本研究では、GeminiなどのマルチモーダルLLMを用いて精度向上を図っています。

以下の3つのデータを生成し、これらを [特徴量](https://ai-data-base.com/archives/26406 "特徴量") として画像分類に利用するのが主なアイデアです。

- クラスラベルの説明文
- 入力画像の内容を説明する文
- 単純な分類結果（初期分類を表すテキスト）

概要は以下の図の通りです。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70709_1-1024x558.jpg)

このように生成した各テキストを、CLIPなどの対照学習モデルのテキストエンコーダを用いてベクトル化します。そして、 [特徴量](https://ai-data-base.com/archives/26406 "特徴量") ベクトルを融合し、線形 [分類器](https://ai-data-base.com/archives/26489 "分類器") でゼロショット分類を実行します。

その結果、ゼロショット画像分類の性能を大きく改善できることを示しました。また、データセット毎のプロンプトエンジニアリングは不要で、単純なプロンプトセットのみでOKとのことです。

次のセクションでは、マルチモーダルLLMを用いた具体的な画像分類の方法について詳しく解説します。

## 手法

以下の図が概要です。緑の点線が従来手法の流れで、実線が本手法の流れです。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70709_9-1024x576.png)

具体的な流れとしては、以下の通りです。

1. クラスラベル [特徴量](https://ai-data-base.com/archives/26406 "特徴量") （検索結果）の生成
2. クエリ [特徴量](https://ai-data-base.com/archives/26406 "特徴量") （説明変数）の生成
3. 分類

「クエリの [特徴量](https://ai-data-base.com/archives/26406 "特徴量") を用いて、最も類似度の高い画像のクラスラベルの [特徴量](https://ai-data-base.com/archives/26406 "特徴量") 」を探し出すことを行っているため、画像検索と似ています。

### クラスラベル特徴量（検索結果）の生成

まず、あらかじめデータセットにおけるクラスラベルの [特徴量](https://ai-data-base.com/archives/26406 "特徴量") を生成します。具体的には、以下のようにクラスラベルをエンコードします。

- クラスラベルのテキストをそのまま数値ベクトルに変換
- 「A photo of {class\_label}」のようなテンプレートを使って数値ベクトルに変換（{class\_label}はクラスラベル）
- 大規模言語モデル（LLM）を使って生成された複数のクラスの説明文を数値ベクトルに変換し、平均化して単一のエンコード特徴を作成

例えば、クラスラベル「猫」をエンコードする方法は以下の通りです。

- 直接エンコード： [特徴量](https://ai-data-base.com/archives/26406 "特徴量") = エンコーダ(“猫”)
- テンプレート使用： [特徴量](https://ai-data-base.com/archives/26406 "特徴量") = エンコーダ(“A photo of a cat”)
- LLM生成説明文： [特徴量](https://ai-data-base.com/archives/26406 "特徴量") = エンコーダ(LLMが生成した「猫」の説明文)

これらの特徴を平均化などにより組み合わせて、1つのベクトルにします。そして、分類の際にこの [特徴量](https://ai-data-base.com/archives/26406 "特徴量") を、いわゆる検索における「検索結果」として扱います。

### クエリ特徴量（説明変数）の生成

次に、分類対照となる入力画像から、 [特徴量](https://ai-data-base.com/archives/26406 "特徴量") を生成する方法です。具体的には、以下のように、入力画像から [特徴量](https://ai-data-base.com/archives/26406 "特徴量") を生成します。

- 事前学習済みのCLIPなどのクロスモーダルモデルの画像エンコーダを用いて、入力画像をエンコードし画像 [特徴量](https://ai-data-base.com/archives/26406 "特徴量") を生成
- GeminiなどのマルチモーダルLLMを用いて入力画像の説明文を生成し、説明文をクロスモーダルモデルのテキストエンコーダを用いてテキスト [特徴量](https://ai-data-base.com/archives/26406 "特徴量") に変換
- マルチモーダルLLMに入力画像とクラスラベル一覧を与え、初期のクラス予測を生成し、結果を上記のテキストエンコーダでエンコードし [特徴量](https://ai-data-base.com/archives/26406 "特徴量") に変換

そして、このようにして得た画像・テキスト・初期予測の各 [特徴量](https://ai-data-base.com/archives/26406 "特徴量") ベクトルを単純に加算して平均化することで融合し、最終的なクエリ [特徴量](https://ai-data-base.com/archives/26406 "特徴量") （説明変数）を生成します。

### 分類方法

最後に、先ほど生成したクエリ [特徴量](https://ai-data-base.com/archives/26406 "特徴量") とクラスラベル [特徴量](https://ai-data-base.com/archives/26406 "特徴量") の類似度を計算し、最終的なクラスを予測します。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70709_2-1024x283.jpg)

具体的には、クエリ [特徴量](https://ai-data-base.com/archives/26406 "特徴量") とすべてのクラスラベル [特徴量](https://ai-data-base.com/archives/26406 "特徴量") のコサイン類似度を計算し、最も類似度の高いクラスラベルを最終予測とします。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70709_3.png)

上記の数式のWが予測結果にあたり、argmax(W)を計算することで、最大の類似度スコアを持つ画像のインデックスを返します。

ちなみに、マルチモーダルLLMを使った [特徴量](https://ai-data-base.com/archives/26406 "特徴量") 生成において、使用されたプロンプトは、以下の通りです。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70709_4-1024x577.png)

**画像分類タスクのプロンプト**

```js
画像とそのクラスラベルのリストが与えられます。クラスラベルに基づいて画像を分類してください。可能な限り単語一語で答えてください。クラスラベル:
{classes}
```

**画像の説明タスクのプロンプト**

```js
何が見えますか? オブジェクトの種類やクラスを含め、正確に説明してください。
```

**クラスの説明タスクのプロンプト**

```js
1. {class_label}がどのように見えるか、1～2文で説明してください。
2. {class_label}をどのように識別できるか、1～2文で説明してください。
3. {class_label}はどのように見えますか? 1～2文で答えてください。
4. インターネットの{class_label}の画像を説明してください。1～2文で答えてください。
5. {class_label}の画像の短いキャプション:
```

## 実験

本研究では、提案手法が従来のゼロショット画像分類手法よりも高い精度を達成できるかを検証するために、ImageNet、CIFAR-10/100、Food-101、SUN397、Stanford Cars、DTD、Caltech-101、Pets、Places365の10のデータセットで評価しています。

また、比較対照となる既存モデルには、CLIP、CuPL、CALIPなどの手法が採用されています。

さらに、実験ではマルチモーダルLLMとしてGemini Proを、クロスモーダルモデルの画像エンコーダとしてCLIP (ViT-L/14)を使用しています。

結果は以下の通りです。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70709_8-1024x388.jpg)

すべてのデータセットで平均4.1%、ImageNetでは6.8%の精度向上を達成したとのことです。また、上図でも示されている通り、クラス [特徴量](https://ai-data-base.com/archives/26406 "特徴量") をすべて組み合わせた場合が、ほとんどのデータセットで最高精度に達しています。

## アブレーション実験

著者らは提案手法の各コンポーネントの影響を調べるため、いくつかのアブレーション実験を行っています。

1つ目の実験では、入力のクエリ [特徴量](https://ai-data-base.com/archives/26406 "特徴量") の影響を調べています。具体的には、以下のようなクエリ [特徴量](https://ai-data-base.com/archives/26406 "特徴量") の組み合わせの違いによる、分類精度への影響を調べています。

- 入力画像の説明文の [特徴量](https://ai-data-base.com/archives/26406 "特徴量") (DF)のみを使用
- 初期予測の [特徴量](https://ai-data-base.com/archives/26406 "特徴量") (PF)のみを使用
- DFとPFを組み合わせて使用
- DFと入力画像の [特徴量](https://ai-data-base.com/archives/26406 "特徴量") (IF)を組み合わせて使用
- PFとIFを組み合わせて使用
- DF、PF、IFの3つを組み合わせて使用

その結果、多くのデータセットではDF、PF、IFの3つを組み合わせるのが最良であることが分かりました。

ただし、低解像度のCIFARデータセットでは、IFを使わない方が良い場合もあるようです。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70709_6-1024x252.png)

2つ目の実験では、ImageNetデータセットから無作為に選んだ5,000枚の画像を使い、異なるクロスモーダル埋め込みモデルと、 [特徴量](https://ai-data-base.com/archives/26406 "特徴量") の融合方法を比較しています。

クロスモーダルモデルとしてはCLIPの複数のバリエーション(ViT-L/14, ViT-B/32, ViT-B/16)を比較しています。融合方法としては、各入力 [特徴量](https://ai-data-base.com/archives/26406 "特徴量") (DF, PF, IF)とクラスラベル [特徴量](https://ai-data-base.com/archives/26406 "特徴量") の類似度を別々に計算して最大値を取る方法と、平均を取る方法を比較しています。

その結果、3つの [特徴量](https://ai-data-base.com/archives/26406 "特徴量") (IF, DF, PF)の平均を取り、ViT-L/14を用いるのが最良であることが分かりました。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70709_5.png)

## 入力データが最終予測に与える影響の可視化

以下の図は、提案手法の入力データが最終予測に与える影響を可視化した図です。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70709_10-1024x812.jpg)

Places、ImageNet\[4\]、Food-101、CIFAR-100の各データセットから、例を示しています。各行が1つの入力画像に対する結果を表しており、以下の情報が示されています。

1\. 入力画像

2\. LLMが生成した画像の説明文

3\. LLMが生成した初期クラス予測

4\. 各入力の最終予測への寄与度を色の濃淡で示した図(赤が濃いほど寄与度が高い)

5\. 提案手法による最終予測クラス

6\. 正解クラス

7\. 正解クラスのインターネット上の画像例(参考)

寄与度の可視化は、画像に対してはパッチを順にマスクアウトし、テキストに対しては単語を順にマスクアウトして予測の変化を調べることで行っています。

これらの例から、画像、説明文、初期予測の3つの入力が相互に補完し合って最終予測に寄与していることが分かります。場合によっては、特定の1つまたは2つの入力の影響が大きくなることもあります。

1行目と2行目の例では、LLMの初期予測は正解クラスとは異なりますが、画像の内容とは整合したものになっています。

## 注意点

LLMの利用にはある程度の計算リソースが必要で、以下のような要件が挙げられています。

- クロスモーダルエンコーディングには、NVIDIA V100 [GPU](https://ai-data-base.com/archives/26570 "GPU") で画像またはテキスト1件あたり約15ミリ秒
- LLMをローカルで動かすには、16基の [GPU](https://ai-data-base.com/archives/26570 "GPU") /TPUと約256 [GB](https://ai-data-base.com/archives/26343 "勾配ブースティング") のメモリが必要
- LLMへの1クエリあたり約700ミリ秒の処理時間
- マルチスレッド化により高速化可能

また、提案手法には以下のような制約もあります。

- LLMへの複数クエリが必要なため、計算リソースが限られるデバイスでは実行が難しい
- 他の手法と比べて処理に時間がかかる可能性がある

ただし、今後LLMの性能向上により、より少ない計算リソースでの実行が可能になれば、これらの課題は解消に向かうと期待されています。

さらに、提案手法の分類の失敗例として、画像説明文や初期予測が誤っている場合に誤分類につながることがあるとされています。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70709_7-1024x370.jpg)

上図の上側では、初期予測と画像説明が「ロブスター」として画像を分類したが、実際には「ザリガニ」であった例が示されています。また、下側の結果では、画像説明に猫の特徴が不足しており、初期予測が「ベンガル猫」としたが、実際のクラスは「エジプシャンマウ」であった例も示されています。

## まとめ

本記事では、マルチモーダルLLMを用いたゼロショット画像分類に関するGoogleの研究をご紹介しました。

事前学習済みのマルチモーダル言語モデルを活用し、画像とテキストの双方向からの特徴抽出を行うことで、学習時に使われなかったカテゴリの画像を高精度で分類することを可能にする手法です。  
入力画像から豊富なテキスト表現を生成し、それを画像特徴と組み合わせることで、ゼロショット分類の精度を大きく向上させています。

この手法の特筆すべき点は、データセット固有のプロンプトエンジニアリングを必要としないことです。シンプルかつ汎用的なプロンプトセットを用いることで、容易に実装可能であり、幅広いデータセットで効果を発揮します。

- 参照論文URL： [https://arxiv.org/abs/2405.15668](https://arxiv.org/abs/2405.15668)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[難しいベンチマークで高性能なLLMでも単純な問題で間違えてしまう現象について「不思議の国のアリス問題」とGPT-4o、Claude-3、Llama 3などで分析](https://ai-data-base.com/archives/70613)

[LLMは与えられたペルソナ（役割）に応じてバイアスが変化することが明らかに](https://ai-data-base.com/archives/70696)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)