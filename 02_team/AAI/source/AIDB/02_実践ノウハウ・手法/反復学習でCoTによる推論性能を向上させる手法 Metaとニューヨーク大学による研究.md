---
title: "反復学習でCoTによる推論性能を向上させる手法 Metaとニューヨーク大学による研究"
source: "https://ai-data-base.com/archives/69296"
author:
  - "[[AIDB Research]]"
published: 2024-05-20
created: 2025-06-13
description: "LLMは論理的な推論をする能力が限られており、特に数学や科学の問題では精度が低いという課題があります。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

LLMは論理的な推論をする能力が限られており、特に数学や科学の問題では精度が低いという課題があります。そこでMetaとニューヨーク大学の研究チームは、複数の推論ステップを繰り返し行うことでモデルの答えをより正確にする新しい方法を考案しました。結果として、他の従来モデルを上回る高い精度を達成しました。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_69296-1024x576.jpg)

**参照論文情報**

- タイトル：Iterative Reasoning Preference Optimization
- 著者：Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, Jason Weston
- 所属：FAIR at Meta, New York University

## 背景

これまで、LLMの推論能力を向上させる手法の研究は、数多く行われてきました。その中でも、特に注目されてきたプロンプト・学習手法には、以下のようなものがあります。

- Chain-of-Thought (CoT): LLMに一連の推論ステップを生成させることで、推論能力を向上させる手法
- STaR (Self-Taught Reasoning): CoTを生成し、正解を導く推論のみを用いて反復的にSFT (Supervised Fine-Tuning、教師付きファインチューニング)を行う手法
- V-STaR: DPO（Direct Preference Optimization）で学習した検証モデルを用いてSFTの生成サンプルをフィルタリングする手法
- Expert Iteration: 報酬モデルを仮定し、生成サンプルをフィルタリングしてSFTを反復的に行う手法

また、反復的なPreference Optimization（モデルの出力が人間の選好や価値観に合うように最適化するための手法）に関する研究についても盛んに行われ、以下のような手法が提案されてきました。

- Iterative DPO: DPOを用いて選好ペアを最適化し、更新されたモデルで新たな選好ペアを生成する反復的な手法。
- Self-Rewarding LLMs: LLM自身を報酬モデルとして用いたIterative DPO。
- SPIN: 人間のラベルをWinner、前の反復の生成サンプルをLoserとするIterative DPOに類似した手法。

これらの既存の反復的な学習手法は、一般的な指示に対するチューニングでは良い性能を発揮します。しかし、この学習方法においては、LLMの推論LLMを向上できないというのが現状です。

そこで今回、LLMの推論性能を向上させるために、新たな学習方法「Iterative Reasoning Preference Optimization」が開発されました。この手法によって、LLMの推論タスクにおける性能を引き伸ばせるだけでなく、Human-in-the-Loop（機械学習モデルの学習プロセスに、人間を関与させる手法）や追加データを不要とするため、効率的に学習を進められます。

なおHuman-in-the-Loopでは、例として以下のようなプロセスを繰り返すことで、モデルの性能を向上させていきます。

1. モデルが予測を行う
2. 人間がその予測結果をレビューし、フィードバックを与える
3. そのフィードバックをもとに、モデルを再学習する

他にも、一連の機械学習プロセスの中で、意思決定や判断、制御などの「AIが苦手とするタスク」を人間が行う場合も、Human-in-the-Loopと呼びます。

また、完全自動運転など「まったく人間を介さないプロセス」は、Human-out-of-the-Loopと呼ばれることもあります。

## 手法

Iterative RPOの大まかな手順は、以下の通りです。

1. 学習用の入力プロンプトからLLMを用いて複数のCoTを生成
2. 各CoTによる回答を生成
3. 最終答えの正誤に基づいて報酬を計算
4. 回答から、正解を導く推論ステップを持つ回答をWinner、不正解の回答をLoserとしてペアを作成
5. 修正版のDPO (Direct Preference Optimization) [損失関数](https://ai-data-base.com/archives/27017 "損失関数") と負の対数尤度項を用いてLLMを学習

上記のステップ1〜5を複数回繰り返します。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_69296_2-1024x438.png)

具体的には、まず初期モデルM0を用意します。このモデルには、Llama-2-70B-Chatが用いられています。また、質問xiと正解yiのペアからなる学習データセットDを用意します。

そして、反復tにおける現在のモデルMtを用いて、各入力xiに対してN個の回答を生成します。各回答は、推論ステップ(Chain-of-Thought) cinと、最終答えyinで構成されます。

さらに、最終答えの正誤に基づいて、報酬rinを計算し、報酬付きの生成回答のセットGiを構築します。

そして、生成回答のセットGiから、WinnerとLoserのペアを選択し、ペアデータセットDtpairsを構築します。

このペアデータを使って、修正版のDPO [損失関数](https://ai-data-base.com/archives/27017 "損失関数") と負の対数尤度項からなる目的関数で新しいモデルMを学習し、これを次の反復のモデルMt+1とします。

ここで、モデルMの学習に用いる「修正版のDPO [損失関数](https://ai-data-base.com/archives/27017 "損失関数") と負の対数尤度項からなる目的関数」は、以下の数式で表現できます。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_69296_4-1-1024x760.jpg)

[損失関数](https://ai-data-base.com/archives/27017 "損失関数") の設計により、正解の推論を導く回答を優先的に学習し、不正解の回答の尤度を下げるように最適化が行われます。

これらの処理を繰り返し行い、モデルを反復的に更新していき、反復を重ねるごとにモデルの推論能力が向上します。そして、所定の反復回数に達したら、または性能が収束したら、学習を終了し、最終的に得られたモデルが、推論能力が向上したLLMとなるのです。

## 実験内容

本論文では、Iterative RPOの有効性を検証するために、以下の3つのデータセットを用いた実験を行っています。

- GSM8K (Grade School Math 8K)
- ARC-Challenge (AI2 Reasoning Challenge)
- MATH (Mathematics)

各データセットにおける実験の設定条件や結果について、順に解説します。

### GSM8K (Grade School Math 8K)

GSM8Kは、小学校レベルの算数の文章題を解くタスクです。本研究では、訓練セットの約7,500問のみを使用しており、外部データは一切使用していません。各問題には、質問xi、正解のCoT (Chain-of-Thought)解法ci、最終的な数値答えyiが含まれています。

実験の設定は以下の通りです。

- 初期モデルM0として、Instruction用に調整されたLlama-2-70B-Chatモデルを使用
- 各反復で、問題ごとにN=30の解を生成
- 反復1-2では温度0.8、反復3-4では温度1.3で [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング")
- 問題ごとにK=10のペアを選択し、長すぎる生成や不正解の生成がない例をフィルタリング
- 各反復で約55-60kのペアを学習に使用
- 最大5000ステップ学習し、訓練セットから抜き出した1kサンプルで最良のチェックポイントを選択
- その後、1kサンプルを含めて再学習
- ハイパーパラメータは、α=1、β=0.1、 [バッチサイズ](https://ai-data-base.com/archives/26582 "バッチサイズ") 16、学習率7e-7に設定
- 4回の反復で、モデルM1、M2、M3、M4を学習

実験の結果、Iterative RPOは、以下のようなベースラインを大幅に上回りました。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_69296_3-1024x530.png)

さらに、反復を重ねるごとに性能が向上し(73.1% → 78.0% → 81.1% → 81.6%)、32サンプルの多数決を取ることでさらに88.7%まで改善しました。

また、アブレーション実験により、以下のような知見が得られました。

- NLL損失項がDPOの学習に重要
- NLLなしでは性能が大幅に低下
- 反復ごとのモデル更新(前のモデルから初期化)が性能向上に寄与している
- 単に学習データを増やすだけでは同等の改善は得られない

以下の図(a)(b)は、SFTによる学習の効果を示しています。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_69296_8.png)

(a) では、反復1で生成されたWinnerの回答のみでSFTを行った場合、Winnerの回答の対数尤度は上昇しますが、Loserの回答の対数尤度も同様に上昇し、両者の差はあまりありません。

一方、DPO+NLL損失を用いた学習では、Winnerの対数尤度を上げつつ、Loserの対数尤度を下げることができています。このことから、SFTのみでは不十分で、DPO+NLLによる学習が重要であることがわかります。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_69296_9.png)

(b)では、データセットに含まれるCoTでSFTを行った場合も、(a)と同様の傾向が見られます。WinnerとLoserの対数尤度の差はやや大きいものの、Winnerの対数尤度はほとんど上昇していません。

また、以下の図(a)(b)は、DPO損失にNLL項を加えることの効果を示しています。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_69296_6-1.png)

(a)では、初期モデルからDPOのみで学習を行うと、Winnerの回答の対数尤度が学習ステップとともに低下してしまいます。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_69296_10-1024x520.png)

(b)では、SFTで学習済みのモデルから始めた場合は、その傾向がさらに顕著になります。

一方、NLL項を加えたDPO+NLLで学習を行うと、Winnerの対数尤度は上昇し続けます。また、WinnerとLoserの対数尤度の差も開いていきます。

これらの結果から、DPO損失にNLL項を加えることが、Iterative RPOの学習において重要な役割を果たしていることがわかります。Winnerの回答の尤度を上げつつ、Loserdの回答の尤度を下げるように学習することで、推論性能の向上が達成されているのです。

### ARC-Challenge (AI2 Reasoning Challenge)

ARCは、複数の科学の科目をカバーする選択式問題のデータセットです。訓練セットには約7,700問の問題が含まれ、easyセットとchallengeセットに分かれています。ここでは、1,172問からなるARC-Challengeテストセットでの結果を報告しています。

ARCの訓練データには、問題ごとの正解のCoT (Chain-of-Thought)推論が提供されていません。しかし、Iterative RPOでは最終的な答えの正誤のみを必要とするため、これは問題にはならないです。ただし、モデルが生成した解答の中に正解がない問題は、学習に使用されないという点には注意が必要です。

実験の設定は、GSM8Kの場合とほぼ同様です。

- 各反復で、問題ごとにN=30の解答を生成
- 反復1-2では温度0.8、反復3では温度1.3で [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング")
- 問題ごとにK=20ペアの解答を選択

その結果、反復1では約20,000ペア、反復2では約11,000ペア、反復3では約5,000ペアの学習データが得られました。反復を重ねるごとにペア数が減少しているのは、後の反復では不正解のサンプルがない問題が増えるためです。各反復は最大4,000ステップ学習し、ハイパーパラメータのチューニングには提供された開発セットを使用しています。

実験の結果、Iterative RPOは反復を重ねるごとに性能が向上し(84.8% → 86.2% → 86.7%)、3反復目のモデルで32サンプルの多数決を取ることで87.9%まで達しました。これらの結果は、zero-shot CoT (77.8%)、正解を導く推論のみでのSFT (79.8%)、標準的なDPO (83.5%)を上回るものでした。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_69296_1-1024x439.png)

ARCが多肢選択（4つ以上）の形式であることを考えると、これらの結果は特筆すべきでしょう。

選択肢が通常4つしかないため、ランダムに推論しても25%の確率で正解できるノイズが含まれます。つまり、Iterative RPOのステップ(i)で生成されたデータには、偶然正解のCoTと最終答えが含まれている可能性があり、ステップ(ii)の選好最適化で使用されるCoTにノイズが入る恐れがあります。

それにも関わらず、提案手法はこの多肢選択の問題に [ロバスト](https://ai-data-base.com/archives/26590 "ロバスト") であり、性能向上が見られたことは素晴らしい点だと言えます。

### MATH (Mathematics)

MATHは、12,500問の競技数学の問題からなるデータセットです。テストセットには5,000問の問題が含まれています。GSM8Kと同様に、各問題には正解のCoT (Chain-of-Thought)解法が提供されており、 [正規化](https://ai-data-base.com/archives/26401 "正規化") 後の予測答えと正解を照合することで報酬を計算できます。ただし、付属の事前学習用 [コーパス](https://ai-data-base.com/archives/26324 "コーパス") は使用していません。

また、各問題に対して、下記に示されている4つの例を含むfew-shotプロンプトを、LLMへの入力として使用します。これらのデモンストレーションにより、最終答えを適切にLaTeXでフォーマットできるようになります。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_69296_5-1024x359.png)

実験の設定は、以下の通りです。

- 各反復において、反復1-2では温度0.8、反復3では温度1.0で [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング")
- 問題ごとにN=20の解を生成
- 問題ごとにK=15ペアの解を選択

長すぎる生成をフィルタリングした後、各反復で約75kのペアが得られました。最大5000ステップの学習を行い、その他の詳細はGSM8Kの設定と同様です。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_69296_7-1-1024x439.png)

実験の結果、Iterative RPOは反復を重ねるごとに性能が向上し、3回の反復で12.5%から20.8%まで改善しました。32サンプルの多数決を取ることで、さらに29.1%まで大幅に向上しました。これらの結果は、few-shot CoT (12.5%)、正解を導く推論のみでのSFT (16.8%)、標準的なDPO (12.4%)を上回るものでした。特にDPOでは、初期化した性能から低下する結果となりました。

とはいえ、やはりまだAccuracy自体は低いようなので、LLMには高度な数学タスクを解く性能に乏しいと言えるかもしれません。

## まとめ

Iterative Reasoning Preference Optimization (Iterative RPO)は、追加データや人間の介入を必要とせずに、LLMの推論能力を向上させる新しい手法です。この研究では、反復的な学習を通じて、モデルが自身の生成した回答から学習を進め、正解を導く推論の尤度を高め、誤りを減らすことに成功しています。

特に数学や科学の問題を解くタスクでその効果が明確に見られ、複数回の反復学習により、モデルの性能が顕著に向上しました。

- 参照論文URL： [https://arxiv.org/abs/2404.19733](https://arxiv.org/abs/2404.19733)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[スタンフォード大学の研究者ら、GPT-4oとGemini1.5 Proで「マルチモーダルモデルにおける『Many-Shot』の効果」を検証](https://ai-data-base.com/archives/69211)

[GPT-4o、Gemini、Claude 3などにおける「長いプロンプトのマルチモーダルタスク」性能を測定した結果](https://ai-data-base.com/archives/69354)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)