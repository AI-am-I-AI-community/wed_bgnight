---
title: "いまだ対策が求められる幻覚（ハルシネーション） プロンプト手法とRAGの組み合わせでLLMの事実性を守る"
source: "https://ai-data-base.com/archives/88124"
author:
  - "[[AIDB Research]]"
published: 2025-05-23
created: 2025-06-13
description: "本記事では、LLMの出力に含まれがちな幻覚（ハルシネーション）を抑えるために、プロンプト設計と検索拡張生成（RAG）を組み合わせた検証研究を紹介します。段階的に手法を追加しながら、その効果を実験的に比較している点が特徴です。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、LLMの出力に含まれがちな幻覚（ハルシネーション）を抑えるために、プロンプト設計と検索拡張生成（RAG）を組み合わせた検証研究を紹介します。

段階的に手法を追加しながら、その効果を実験的に比較している点が特徴です。よく知られた手法が実際にどれほど機能するのかを、具体的な評価を通じて確認できます。

信頼性を求める場面で、どの手法をどう取り入れるかを考えるヒントになるはずです。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_88124-1024x576.png)

## 背景

LLMは、文章の生成や質問応答など、多くの [自然言語処理](https://ai-data-base.com/archives/26319 "自然言語処理（NLP）") タスクで高い性能を示すようになってきました。一方で、もっともらしく見えても事実に基づかない「でまかせ」を含む出力が生じることがあり、この現象は「幻覚（ハルシネーション）」と呼ばれています。こうした出力は、実用上の信頼性に影響を与えるため、重要な課題とされています。

この課題に対して、思考の過程を明示させるプロンプト技法が注目されてきました。モデルに段階的な推論を促すと、答えの妥当性が見えやすくなるためです。しかし、それだけでは知識の裏付けが弱く、根拠のないでまかせを完全には防げません。

そこで、外部の知識を検索して参照する仕組みを取り入れることで、推論の過程に確かな情報を組み込もうとする動きが広がっています。必要な情報をその都度取り込みながら考えることで、事実に基づいた出力が期待できます。

加えて、モデル自身が出力の整合性を確認したり、複数回の生成結果から一貫性のある答えを選び取る工夫も有効とされています。

今回の記事では、こうした複数の考え方を組み合わせて、より安定して正確な出力を実現する方法を紹介します。LLMの活用を考える方にとって、応答の信頼性をどう高めていくかを考えるうえで参考になる内容といえます。

## LLMの信頼性を高めるためにできること

LLMにおける「正しそうに見えて根拠がない」出力を返す現象（ハルシネーション）を減らし、安定して便利に使うためには、いくつかの手法を組み合わせて対策を講じる必要があります。

以下では、過去の代表的な研究で提案されてきた4つの方向性を取り上げ、その理論的な背景と、実務に取り入れる際のポイントをまとめます。

### 推論を段階的に進めさせる

まず注目されてきたのが、モデルに「一歩ずつ考えさせる」工夫です。これはChain-of-Thought（CoT） promptingと呼ばれる手法で、最終的な答えだけでなく、途中の思考過程を言語化させることで、論理的な誤りやでまかせの抑制につながります。

この考え方は、論文「 [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903) 」をきっかけに広まりました。数学や戦略的な質問応答など、複数のステップを経て解答に至るタスクでの有効性が報告されています。

最近のLLM（たとえばOpenAIのGPT-4oなど）では、明示的な指示がなくても段階的に推論する傾向が見られますが、「一歩ずつ考えてみましょう」といった簡単なプロンプトを添えることで、出力の安定性が高まるケースもあります。

### 外部知識を検索して活用する

モデルの内部知識は事前学習に基づいており、時に古かったり不完全だったりすることがあります。そうした限界を補うために、外部の情報を検索して補完するRetrieval-Augmented Generation（いわゆるRAG）という手法が用いられます。

この方法では、ユーザーの質問に対して関連する文書を検索し、モデルに一緒に渡すことで、出力の根拠を外部ソースに求めることができます。たとえば、自分の知識だけで考えるのではなく、辞書や資料を参照しながら回答を組み立てる感覚です。

実際に、RAGとCoTを組み合わせることで、でまかせの頻度を大きく減らす効果が報告されています。

検索対象の品質も重要で、信頼できる情報源に絞ったり、取得件数を調整することで、ノイズの混入を抑えることができます。

### 出力を揃えて選ぶ

モデルの出力にばらつきがあるときは、同じ質問に対して複数回出力を生成し、その中から一貫性のあるものを選ぶという考え方があります。これはSelf-Consistencyと呼ばれる手法です。

この手法が有名になったきっかけとなった論文「 [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171) 」では、温度パラメータ（出力のランダム性）を変えながら9回生成し、最も多く出現した答えを最終的な出力として採用する方法が紹介されています。

こうした多数決的な集約を行うと、偶然生じたでまかせが選ばれるリスクを減らすことができます。判断を自動化したい場合は、生成された出力の類似度を計算して自動的に集約する仕組みを取り入れることも検討されます。

### 出力をモデル自身に再評価させる

もう一つのアプローチは、モデルに出力の妥当性を自分で見直させるSelf-Verificationです。これは、モデルが最初に出力した答えを、あらかじめ与えた正解や検索で取得した情報と照らし合わせながら再評価させる方法です。

これを提唱した論文「 [Large Language Models are Better Reasoners with Self-Verification](https://arxiv.org/abs/2212.09561) 」では、クエリ・初回出力・参考知識の3つを提示したうえで、「この答えは正しいですか？」とモデルに再考を促す手法が検討されています。

これは人間で言えば、自分の書いた答えを模範解答と見比べて「間違っていたかも」と気づくようなプロセスに近いもので、精度だけでなく出力の納得感にもつながります。

### 実装する際の注意点

これらの手法は出力の信頼性を高めるうえで有効ですが、現実の利用ではいくつかのコストも発生します。

まず、処理時間の増加が挙げられます。Self-Consistencyでは複数回の推論が必要になり、Self-Verificationでは検証のためのプロンプトを追加で投げる必要があります。RAGでも検索処理が入るため、応答時間が長くなりやすいです。

評価方法の検討も必要です。出力の正確さを人が目で確認するか、自動指標に任せるかも考える必要があります。人間の確認は信頼できますが手間がかかり、自動評価は手軽な一方で細かい誤りを見逃すことがあります。用途に応じて併用するのが現実的です。

さらに、外部情報のノイズ対策も重要になります。検索で取得した情報が不適切だった場合、かえってモデルの出力がぶれてしまう可能性もあります。検索対象のドメインやスコアリング条件をあらかじめ調整しておくことが重要です。

## 検証の手順

LLMの出力を安定させるうえで、評価用データと検証フローの設計は欠かせません。以下では研究で採用された三つのデータセットと、それぞれに合わせた検証の流れを整理します。皆さんが独自に再現・応用しやすいよう、工程ごとにポイントを示します。

### 評価用データセット例

#### HaluEval

ハルシネーション検出用に設計されたベンチマークで、質問・正答・でまかせ例・根拠知識をセットで収録します。約1万件のオープンドメインQ&Aが含まれ、幅広い分野で出力の正確さを測れます。

#### FEVER

Wikipediaを根拠とした事実検証データセットです。「支持される／否定される／情報不足」の三分類ラベルで約14万5千件を収録し、エビデンス付きの推論性能を確認できます。

#### TruthfulQA

誤解や俗説が紛れやすい場面での正確さを測る目的で構築された817件のセットです。正答リストと、もっともらしい誤答リストが対になっており、モデルの真実性を評価できます。

### 前処理で整えるポイント

三つのデータセットはいずれも句読点揺れや余計な空白を含んでいるため、まずは

- 小文字化
- 特殊文字と余分な空白の除去
- 先頭・末尾スペースの削除

を行い、HuggingFaceのトークナイザーでトークン化します。ハードウェア負荷を抑えるため、各データセットから500件を抽出して評価対象とする構成も現実的です。

### まずはステップバイステップ

段階的な推論を促すチェーン・オブ・ソート（CoT）は、次のような簡潔な一文で十分機能します。

> *“Let’s think step-by-step”* （論文より引用）

日本語で試す場合は「一歩ずつ考えてください。」と置き換えても効果が保たれます。モデルに中間思考を書き出させるだけで、論理の飛躍やでまかせが減りやすくなります。

### 検索拡張生成（RAG）

モデル内部の知識だけに頼らず、外部ドキュメントを取り込むことで回答を裏付けます。実装時は

1. クエリに合わせて上位5件の関連文書を取得
2. 文書を小さなチャンクへ分割し、PineconeなどのベクトルDBへ格納
3. クエリのベクトルを投げ、類似度上位チャンクを再取得
4. クエリとチャンク群をまとめてモデルに渡して回答生成

という流れが扱いやすいです。資料取得が難しいTruthfulQAでは、まず質問トピックを言語モデルに判定させ、Wikipediaで該当記事を検索する二段構えにすると精度が安定します。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_88124_1.png)

RAGの設定イメージ

### CoTとRAGを組み合わせる

外部情報で根拠を添えつつ、段階的な思考で推論を整理する構成です。具体的には

1. RAGで関連文書を添付
2. システムメッセージにCoT一文を加える
3. モデルが文書を参照しながら思考を展開し、最終回答をまとめる

という順序でプロンプトを組み立てるだけで、一貫性と事実性の両立を狙えます。

プロンプトの例は以下の通りです（理論をもとに本記事用に組み立てました）。

```js
システム: あなたは信頼性を重視するリサーチアシスタントです。  
ユーザー: {質問文}  
アシスタント（ステップ思考）: 
1. 質問を分解して理解します。  
2. 必要な外部情報を要約します。  
3. 外部情報を根拠に段階的に推論します。  
4. 最終的な答えをまとめます。
```

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_88124_2.png)

異なるプロンプトによるCoTの出力

### 出力を揃える自己一貫性

温度を変えて同じ質問を9回生成し、回答同士の類似度を計算して最頻値の答えを採用します。自動集約スクリプトを用意しておくと、手間をかけずに「ぶれにくい回答」を得られます。

プロンプトの例は以下の通りです（こちらも、理論をもとに本記事用に組み立てました）。

```js
システム: あなたは複数回推論を行い、一貫した答えを選択します。  
ユーザー: {質問文}  
アシスタント: 思考過程を示しながら回答を生成します。（温度を変えて n 回実行）
```

### 出力を点検する自己検証

初回回答をモデル自身に再提示し、外部エビデンスと突き合わせて再評価させます。

プロンプトの例は以下の通りです（こちらも、理論をもとに本記事用に組み立てました）。

```js
システム: あなたは検証モードのリサーチアシスタントです。  
ユーザー:  
- 質問: {質問文}  
- 初回回答: {モデルが出した答え}  
- 参考情報: {外部知識の抜粋}  
タスク: 初回回答が事実と整合するかを判定し、必要なら修正案を提示します。
```

こうするとモデルが自分の誤りに気づきやすくなり、最終出力の安心感が高まります。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_88124_3.png)

自己検証時の出力と通常の出力を比較

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_88124_4.png)

自己検証の構図

## 実験と評価の進め方

ハルシネーションを減らすための工夫が、実際にどれだけ効果を発揮するのか。その検証には、段階的に戦略を導入しながら比較するアプローチが取られました。

まずはベースラインとなるLLMの出力から始め、そこにCoT（段階的推論）、RAG（検索による知識補完）、Self-Consistency（複数回答の集約）、Self-Verification（出力の自己点検）を順に加えていくことで、それぞれの手法がどのように性能に寄与するかを確認しています。

### 実験の進め方

検証にあたっては、各手法の特性に合わせていくつかのパラメータ調整が行われました。

まず、CoTについては複数のプロンプト表現を試し、それぞれの効果を比較しています。各データセットから20〜30件のサンプルを選び、「一歩ずつ考えてみましょう」など3〜4種類のプロンプトを使ってモデルに推論させたところ、この古典的な表現が最も安定しており、解釈しやすい出力が得られたため、以降のすべての実験で標準プロンプトとして採用されました。

RAGの設定では、取得する外部文書の数（2件、5件、10件）を変えてテストが行われました。文書数が少なすぎると文脈が不十分になり、多すぎるとノイズが混入する傾向が確認されました。スコアベースでのフィルタリングも検討されましたが、類似度スコアが低い場合は検索そのものが失敗することもありました。最終的に、関連性と情報量のバランスが良かった「上位5件」の取得が標準設定として選ばれています。

また、生成時の温度（出力のばらつき）や最大トークン数も調整されました。温度は0.3〜0.7の範囲で検討され、0.4が最も安定した結果を示しました。特にTruthfulQAのような自由記述形式のタスクでは、モデルが十分に表現できるよう最大トークン数150を設定しています。

### ベースラインモデルとの比較

まずは比較対象として、CoTやRAGといった手法を使わない「素の状態」のLLMを評価しました。使用されたのはGPT-3.5-Turbo、LLaMA-2-7b、DeepSeek-R1といったモデルです。

それぞれのモデルがハルシネーションをどれだけ含みやすいか、またその傾向に [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") の違いがどう関係するかを確認しています。ここで得られたスコアが、後述する各手法の効果を見極めるための基準となります。

なお、GPT-3.5-TurboやLLaMA-2-7bといったモデルは最新とは言えませんが、これまで非常に多くの実装で使われてきた背景があります。そのため、提案手法の効果を過去の標準的な環境で確かめておく意味は十分にあります。  
そしてDeepSeek-R1のような比較的新しいモデルも合わせて検証することで、提案されたアプローチが新旧さまざまなモデルに対して一貫して有効かどうかを確認する意図が読み取れます。つまり、単に「性能が良いか」ではなく、「いろいろな実装環境でちゃんと機能するか」を見ているということです。

### 評価指標の設計

評価には、使われたデータセットごとに適した指標が用いられました。

#### ハルシネーション率

このデータセットでは、出力がハルシネーションか否かという2値のラベルが用意されています。モデルが生成した回答のうち、でまかせと判定された割合を「ハルシネーション率」として計算します。

ハルシネーション率 = ハルシネーション件数 / 総サンプル数

数値が低いほど、事実に基づいた出力が多いことを示します。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_88124_5.png)

HaluEvalデータセットにおけるSelf-Consistency（複数回答の集約）のサンプル

#### 分類精度

FEVERでは、出力が「支持される」「否定される」「情報不足」のいずれかに正しく分類されるかを測ります。これは、証拠に基づいて主張の正しさを判定できているかどうかを反映するものです。

精度 = 正しく分類された件数 / 総サンプル数

モデルが文脈を理解し、正しく分類できていればスコアは高くなります。

#### 真実度スコア

TruthfulQAは自由記述形式の応答に対して、正解・不正解のリストが用意されています。ここでは、MC2（Multiple Choice – 2 Options）という仕組みをもとに、どちらにより近いかを自動判定する手法が用いられました。

まず、すべての正答に「1」、不正答に「2」のラベルを付けておきます。次に、モデルの生成した応答と、各ラベルの答えとの文章埋め込みのコサイン類似度を計算し、最も近いラベルをその出力に割り当てます。

最後に、予測されたラベルが実際の正解と一致しているかを集計し、モデルの「真実度」を次の式で評価します。

真実度スコア = 一致した件数の合計 / 総サンプル数

この指標により、もっともらしいけれど誤った答えにどれだけ引っかかっているかを測ることができます。

## 結果と考察

ここでは、段階的に導入してきた各手法が実際にどれだけ効果を発揮したのかを見ていきます。実験によって得られた結果は、でまかせ（ハルシネーション）の抑制にどの手法が貢献するのかを明らかにしており、実装の参考として活用できます。

### モデルの性能比較と全体傾向

検証に使用された3つのベンチマークデータセット（HaluEval、FEVER、TruthfulQA）において、各手法がどのような結果を示したかを順に見ていきます。

#### もっともらしい誤りを抑える

ハルシネーション率の観点から見ると、「RAG + CoT」を組み込んだGPT-3.5-Turboや、「自己検証」を導入した設定で特に大きな効果が見られ、誤った出力の割合が11%にまで低下しました。これは、何もしない状態（ベースライン）と比較して大幅な改善です。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_88124_6.png)

#### 主張の分類精度

FEVERでは、自己検証の手法が最も高い精度を示し、約90%の正答率を記録しました。主張が「支持される／否定される／情報不足」のどれに分類されるべきかを判断する能力が向上しており、事実に基づいた推論がしっかりできていることが示されました。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_88124_7.png)

#### 真実性の評価

TruthfulQAにおいても、自己検証が最も高いMC2スコアを達成し、約80%の真実度を記録しました。これは、正しい回答と誤った回答をモデルがしっかり区別できるようになってきていることを示しています。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_88124_8.png)

### 実験から見えてきたポイント

結果を通して、ハルシネーション対策としてどのような工夫が有効かが具体的に見えてきました。

#### ベースラインよりもすべての手法が改善

CoT、RAG、RAG + CoT、自己一貫性、自己検証といったどのアプローチも、ベースラインのモデルよりも優れた結果を示しました。推論を丁寧に進めること、外部情報で裏付けを取ること、自らの出力を見直すこと――どの視点からも、モデルの精度と安定性が確実に向上しています。

#### 外部知識を組み合わせた手法が安定して高性能

RAGやRAG + CoT、そして自己検証といった、外部知識を活用する方法が一貫して好成績を収めました。検索を通じて新しい情報を取り込み、それを踏まえて推論する設計が、でまかせを減らすうえで効果的であることが確認されています。

#### 段階的な推論との相性のよさ

RAGとCoTを組み合わせると、検索による知識補完と、段階的な思考誘導が相乗効果を発揮します。複雑なタスクでも論理が崩れにくくなり、構造のある正確な回答が得られるようになります。

#### データセットごとに最適な戦略が異なる

RAG + CoTと自己検証の効果はどちらも高かったものの、強みの出方には違いがあります。たとえば、RAG + CoTはHaluEvalで最も低いハルシネーション率を記録し、自己検証はFEVERとTruthfulQAで高精度を示しました。タスクの特性に応じて使い分ける視点が求められます。

#### モデルごとのわずかな違いも見逃せない

全体としては自己検証を取り入れたLLaMA-2の結果が安定しており、GPT-3.5-Turboよりやや良好なスコアを出す傾向が見られました。LLaMAはオープンウェイトで柔軟性が高く、検証のような構造的プロンプトに適応しやすいことが影響しているかもしれません。また、自己評価の工程で過信せず冷静な判断ができる点も、ハルシネーションの抑制につながっている可能性があります。

## 今後の展望

ここまで見てきたように、ハルシネーションを抑える手法は一定の成果を上げていますが、まだ改良の余地はあります。いくつかの方向性が今後の検討ポイントとして挙げられます。

まず、自己検証で使うモデルを、生成モデルとは別の [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") にするという考え方があります。あえて異なるモデル同士でチェックを行うことで、出力に対する客観性が強まり、信頼性の向上につながる可能性があります。

また、RAGのような検索拡張の分野では、取得する文書の質をさらに高める工夫も求められます。たとえばクエリの表現を調整したり、ドメインに特化した埋め込みモデルを使ったりすることで、ノイズを減らして精度を引き上げられるかもしれません。

さらに、推論プロンプトについても、問いの内容に応じて動的にスタイルを切り替えるような仕組みが考えられています。 [強化学習](https://ai-data-base.com/archives/26125 "強化学習") を使って、「どの問いに、どの思考プロセスが合うか」を学ばせると、さらに柔軟な運用が可能になるでしょう。

最後に、自己一貫性の [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") では、あらかじめ一定の一貫性が確認できた時点で処理を打ち切ると、計算負荷を抑えることができます。生成結果の類似度をもとに「もう十分」と判断する早期終了の仕組みも、実用上のコスト削減につながりそうです。

これらはいずれも、LLMを現場でより信頼して使うために取り組む価値のあるテーマです。技術が洗練されるにつれ、ハルシネーション対策も「使える選択肢」へと近づいていくはずです。

## まとめ

本記事では、LLMのハルシネーション抑制に向けた複数の手法を段階的に検証した研究を紹介しました。

CoTやRAG、自己一貫性、自己検証といったアプローチは、それぞれ異なる観点から出力の信頼性向上に寄与していました。中でも、外部知識の活用や出力の見直しを加える構成が安定した効果を示しています。

ただ、モデルやタスクの特性によって適した手法は異なり、一律の正解があるわけではありません。取り組んでいるユースケースに合わせて、無理のない範囲で一部を試してみるところから始めるとよさそうです。

**参照文献情報**

- タイトル：Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification
- URL： [https://doi.org/10.48550/arXiv.2505.09031](https://doi.org/10.48550/arXiv.2505.09031)
- 著者：Adarsh Kumar, Hwiyoon Kim, Jawahar Sai Nathani, Neil Roy
- 所属：Texas A&M University

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMベンチマークは現場の実用性を捉えているか？モデルを選ぶ前に確認したい評価スコアの盲点](https://ai-data-base.com/archives/89851)

[人間らしさに近づくAI、その“内面”を探る旅へ　ほか、AI科学ニュースまとめ](https://ai-data-base.com/archives/90122)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)