---
title: "外部LLMを利用するときに機密情報を含む可能性のあるプロンプトからの情報漏洩を防ぐ方法"
source: "https://ai-data-base.com/archives/86644"
author:
  - "[[AIDB Research]]"
published: 2025-03-12
created: 2025-06-13
description: "本記事では、オンライン上でLLMを利用する際に懸念される、プロンプト経由の情報漏洩を防ぐために開発された新しい手法について紹介します。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、オンライン上でLLMを利用する際に懸念される、プロンプト経由の情報漏洩を防ぐために開発された新しい手法について紹介します。

LLMは便利である反面、機密情報や個人のプライバシーを意図せず外部に漏らしてしまうリスクがあります。今回取り上げるのは、そのようなリスクを回避するために考案された技術です。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86644-1024x576.png)

参照論文情報は記事の下部に記載されています。

## 背景

LLMのオンライン利用が一般的になるにつれ、企業や個人が扱う情報のプライバシーをどう保護するかという問題が注目されています。LLMは業務効率化を可能にしていますが、一方で、入力した内容が外部に流出してしまうリスクも指摘されています。

例えば、社員が業務上の指示や質問を作成する際に、企業内部の機密事項や個人情報といった外部に知られては困る情報を意図せず含めてしまうことがあります。こうしたプロンプトがオンライン上のサービスへ送信されると、情報漏洩やプライバシー侵害につながる恐れが生じます。

送信したテキストそのものが、サービス提供者側のデータベースに保管される可能性もあり、情報流出のリスクは軽視できない状況になっています。

これまでにも差分プライバシーという技術が情報漏洩対策として研究されてきました。差分プライバシーとは、データを加工する際に、元のデータに関する個々の情報が特定されないよう統計的にノイズを加えることでプライバシーを保護する手法のことです。

しかし既存の方法は文書全体を一括して書き換えるもので、意味や文脈を損ないやすいという弱点があります。また単語レベルでの細かなプライバシー保護については、十分な配慮が行われてこなかったため、漏洩リスクが完全には取り除けないという課題もあります。

企業が安心して外部サービスを使えるようにするために、プロンプトに含まれる情報をもっと柔軟に、かつ細やかにコントロールする技術が求められてきました。

そこで今回研究者らは、文書全体をざっくり書き換えるのではなく、文章の意味を保ちながらも重要な単語単位で情報漏洩を防ぐような方法を考案しました。

以下で詳しく紹介します。外部のLLMを使用しているが情報漏洩が気になる方にとっては参考になる可能性があります。

## そもそも差分プライバシーとは何か

まずは、本研究の理解を深める上で重要な差分プライバシーに関する基本概念から見ていきます。

差分プライバシー（Differential Privacy: DP）とは、データを公開するときに、データに含まれる個人や企業の機密情報を守るための仕組みです。簡単に言うと、元のデータにほんの少しの「ノイズ」（人工的なゆらぎ）を混ぜることで、データから個人が特定されるリスクを減らします。

こうすると、元の情報はぼやけるものの、全体として見るとデータの特徴や統計的な性質は大きく損なわれません。そのため、分析や統計処理に使う際も安全にデータを公開することができます。

差分プライバシー技術の中にもいくつか種類があります。また、関連する重要な考え方があります。

### 純粋差分プライバシー（Pure Differential Privacy）

純粋差分プライバシーは、あるデータの集合（データセット）があり、その中に特定の1人のデータが含まれている場合と含まれていない場合を考えたときに、その差を見分けられないようにする仕組みです。たとえば、「社員の給与の平均」を公開するとき、1人分の給与が含まれているかどうかを他人が推測できると、その人の給与情報が漏れてしまいます。

そこで、この手法ではデータに少量のランダムなノイズを追加して、「1人分のデータ」が含まれているかどうかを外から見てもわからないようにします。こうして、個人が特定されるリスクを抑えます。

### 局所差分プライバシー（Local Differential Privacy）

局所差分プライバシーは、データを集める段階であらかじめノイズを加える方法です。つまり、データを持っている人や企業が、データを外部に渡す前にすでに少量のノイズを加えてしまいます。

そのため、データが外部のサーバに送られる時点では、個別の情報がはっきりとは分からない状態になっています。たとえば、アンケートを行う場合、回答する人が回答を微妙に変化させた上で提出するイメージです。

外部に渡ったデータから個人や特定の情報が逆算されるリスクを避けられます。

### 距離ベース差分プライバシー（Metric Differential Privacy）

距離ベース差分プライバシーは、データ同士の「似ている度合い」（距離）を考慮して、ノイズを調整する手法です。元のデータに似ている場合はノイズを小さくし、大きく異なる場合はノイズを大きめにすることで、データの意味を極力失わずに情報を守ります。

たとえば、ユーザーの位置情報をぼかして公開する際、近くにいる人同士のデータは似た結果になりやすくしますが、離れた場所にいる人同士のデータはより異なる結果が出るように調整します。この仕組みによって、データの有用性を守りつつ、プライバシーも保護できます。

### エクスポネンシャルメカニズム（Exponential Mechanism）

エクスポネンシャルメカニズムとは、数値ではないデータ（たとえば単語やカテゴリなど）に対しても差分プライバシーを適用できる方法です。特定の選択肢（単語や回答）に対して、それがどれくらい「望ましい」かを数値化し、その望ましさに応じてランダムに選択する仕組みです。

つまり、望ましい選択肢ほど高い確率で選ばれやすくなりますが、必ずしも一番良い選択肢が毎回選ばれるとは限りません。これを活用すると、元のデータをそのまま公開せずに、安全に似た意味を持つデータを生成することができます。

### 差分プライバシーの組み合わせ

差分プライバシーには、「組み合わせ」によってプライバシー保護の度合いが変化する特性があります。たとえば、複数の差分プライバシー処理を順番に行った場合、各段階での保護レベル（ノイズの量など）が合計され、最終的な保護の強さが決まります。

つまり、複数回差分プライバシーを適用すると、個々の処理が弱い保護であっても、全体では強い保護になることがあります。逆に言えば、処理を重ねすぎるとノイズが増えてデータの精度が下がるため、注意が必要です。

### 差分プライバシーの後処理

差分プライバシーの重要な性質の一つに「後処理（ポストプロセッシング）」があります。これは、一度差分プライバシーを適用したデータに対して、その後どのような処理を加えても、元の差分プライバシーが保証され続けるという性質です。

つまり、一旦差分プライバシーで加工したデータであれば、その後に分析したり、公開したりしても、元の保護レベルが低下することはありません。

### 差分プライバシーを使った文章生成

文章を生成する際にも、差分プライバシーを適用することができます。特に、LLMは単語を順番に予測しながら文章を作りますが、その各予測の段階で、差分プライバシーのノイズを加えることによって、生成された文章から元の情報が漏れないようにします。

この場合、単語の選択に「ランダムさ」を持たせることで、元の文章が特定されにくくなります。つまり、同じ内容を表現しつつも細かな単語選びを微妙にずらすことで、プライバシーを保護します。

以上の知識は、本手法ですべて活用するわけではありませんが、知っておいて損はないと考えられます。本手法では、LLMの持つ力を利用して差分プライバシーを実現するといった内容になります。

## 提案手法DP-GTR

※Differentially Private Prompt Protection via Group Text Rewritingの略。

今回研究者たちは、プロンプトのプライバシーを守りながら、意味や文脈をできるだけ保つために新しい仕組みを提案しています。「文書全体としての意味」と「個々の単語レベルの情報」の両方を考慮した上で、プロンプトを外部に安全に送る方法です。

プロンプトをそのまま使うのではなく、単に「ノイズを加えてぼかす」のでもなく、重要単語のみを炙り出します。

### 方法の全体像

提案された方法は、全体として3つのステップで構成されています。

まず第1段階として、元のプロンプトを複数の異なるバージョンに書き換え、「書き換えた文章のグループ」を作ります。

この複数バージョンの文章を活用して、単語レベルのプライバシーリスクを分析し、第2段階で情報漏洩のリスクが高いキーワードを見つけ出します。

そして、第3段階でそれらのキーワードを使わないように指示を与えつつ、最も自然でわかりやすい書き換え文を例示して、安全性の高い新たなプロンプトを作成します。

この新しいプロンプトをLLMに入力することで、情報漏洩リスクを抑えながら、元の質問の意図を十分に反映した回答を得ることができます。それぞれの段階をもう少し詳しく見ていきます。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86644_1-1024x482.png)

方法論の全体イメージ

### 第1段階　元のプロンプトから複数の安全な「書き換え文」を作る

まずは外部のLLMサービスに送りたいプロンプトを書き換えます。ただし、1回だけ書き換えるのではなく、あえて複数のバージョンを作成します。なぜ複数のバージョンを作成するかというと、繰り返し登場する「特に重要な単語」や「外部に漏れると危険な単語」を特定するためです。

ここで、書き換えを外部サービスに任せてしまうと、その段階で機密情報が漏れてしまいます。そのため、この書き換え作業は安全な内部環境（自分のPCや組織内で動かせるオープンソースLLMなど）で実施します。

#### 実践例

1. **元のプロンプトを準備する**  
	例：「次回の役員会議は2025年3月15日に東京本社で開催されます。」
2. **内部LLMによる書き換えを行う** （安全な環境内で）  
	まず、このプロンプト全体を内部LLMに入力し、「同じ内容で別の言い回しを生成してください」という指示を与えて、LLMにパラフレーズを作成させます。 例えば、LLMが生成したパラフレーズの例として次のような文章が得られます。
	- 書き換え①「来月の役員会議は3月15日に東京都内の本社で行われる予定です。」
	- 書き換え②「次回の役員会は2025年3月中旬に東京の本社で開催されます。」
	- 書き換え③「次の役員ミーティングは2025年3月15日、東京の本社にて実施されます。」
	- 書き換え④「次回の幹部会合は2025年3月15日に都内本社で開かれます。」
	（内部LLMに「同じ意味の異なる表現」を複数回要求することで、このような複数パターンの文章が簡単に生成できます。）
3. **重要な単語を見つけるために書き換えを繰り返す**  
	このように複数のパターンを生成すると、それぞれ微妙に表現は異なりますが、共通して繰り返される重要な単語が自然に見えてきます。

#### 実践的ポイント

実践する際は、難しい差分プライバシー理論を手作業で計算する必要はありません。オープンソースLLM（例えばLlama系）に「元のプロンプトと同じ意味だが違う言い回しで文章を作成してください」という指示を数回与えるだけで、この第1段階の「複数の書き換え文」は実際に簡単に作れます。

「単語ごとの予測確率」を無理に意識する必要はなく、LLMに単純にパラフレーズを依頼することで、安全性向上のための第1段階の準備が十分に行えます。

要するに、

- 自分の安全な環境内で動く内部LLMを使う。
- 元のプロンプトを数回パラフレーズ（別の表現）で生成する。
- 得られた複数の文章を第2段階に渡す。

というステップです。

### 第2段階　複数の書き換え文から、安全なプロンプトを作るための準備

第1段階で複数の書き換え文を作った後は、それらの文章を分析して、LLMに最終的に渡すプロンプトの安全性と実用性を高めるための準備を行います。この段階で行う作業は2つあり、「自然で読みやすい文章を選ぶこと」と「頻繁に現れる単語を特定すること」です。

#### 実践ステップ1　書き換え文の中から自然な文章を1つ選ぶ

第1段階で作成した複数の書き換え文を並べて比較し、もっとも自然で読みやすい文章を1つ選びます。この作業は、自分自身や第三者が読んで、「意味が通じやすい」「違和感が少ない」と感じる文章を直感的に選ぶだけで十分です。

これは後でLLMに「模範例（参考例）」として与えることで、LLMが自然で意味の通った文章を生成できるようにするための準備となります。

具体的な実践イメージ

- 第1段階で得られた書き換え文を5つ程度並べる。
- 読んで違和感が少なく意味が通じやすいものを選び、メモしておく。

#### 実践ステップ2　書き換え文に頻繁に現れる重要な単語を探す（コンセンサス単語）

第1段階で作成した複数の書き換え文を比較し、何度も登場する単語を探します。なぜ頻繁に登場する単語を探すかというと、表現を変えても繰り返し現れる単語は、元のプロンプトの意味にとって特に重要か、または機密情報を含んでいる可能性が高いからです。このような単語を本研究では「コンセンサス単語」と呼びます。

具体的な実践イメージ

- 書き換え文（複数）の単語をリストアップする（メモ帳やExcelで）。
- 各単語が複数の書き換え文の中で何回ずつ現れるか数える。
- 最も頻繁に現れる単語（上位数個）を抽出し、リスト化して記録しておく。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86644_2.png)

重要キーワードを抽出する流れをアルゴリズムで示すとこのようになる

### 第3段階　最終プロンプトの生成

第2段階で準備した「もっとも自然でわかりやすい文章」と「頻繁に現れる重要な単語（コンセンサス単語）」を使って、実際に外部LLMサービスへ送るための安全なプロンプトを作成します。

ここで行うことは、主に2つあります。

- 第2段階で選んだ自然な文章をLLMに「例文」として与える。
- 特定した重要な単語（機密性が高い可能性のある単語）を、LLMが使わないよう明示的に指示する。

#### 実践ステップ1：「自然でわかりやすい例文」をLLMに示す

まずは、第2段階で選んだ「もっとも自然で読みやすい書き換え文」をそのままLLMに示します。これはLLMがプロンプトの意図を理解しやすくするための参考例（模範例）になります。

例えば、第2段階で次の文章を選んだとします。

- 「次回の役員会は2025年3月15日に東京本社で開催される予定です。」

これをそのまま、最終プロンプトの中で「参考文」として使います。

#### 実践ステップ2：LLMに使わせたくない単語を明確に伝える

次に、第2段階で特定した「頻繁に現れる重要な単語（コンセンサス単語）」を、LLMが使用しないように明示的に指定します。

例えば、第2段階で特定した単語が以下の3つだったとします

- 「役員会」
- 「東京」
- 「3月15日」

最終プロンプトではLLMに以下のような指示を出します。

> 「以下の例文を参考にして、同じ意味の文章を新たに生成してください。ただし、次の単語を使わずに表現してください：『役員会』『東京』『3月15日』
> 
> 参考例文：『次回の役員会は2025年3月15日に東京本社で開催される予定です。』」

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86644_3.png)

プロンプトテンプレート

#### 最終的なプロンプトの完成例

上記の指示をそのままプロンプトとしてLLMに渡します。するとLLMは、「役員会」「東京」「3月15日」の3つの単語を避けながらも、意味を保った新しい表現を生成します。

生成結果の一例として、次のような文章が得られるかもしれません。

- 「来月中旬に本社で幹部ミーティングを開催予定です。」

以上によって、外部のLLMに機密情報そのものを送ることなく、安全性の高いプロンプトを得ることができます。

この第3段階で実践する内容は、技術的に複雑な処理を必要としません。  
第2段階で得られた結果（自然な文章とコンセンサス単語）をそのまま利用するだけで、すぐに実践できます。

また、LLMへの指示は単純明快なため、誰でも簡単に試すことが可能です。

全体を通して、「そこまでするか？」と思われるかもしれませんが、例えば非常に長い文を外部LLMに入力することになったとき、 [自然言語処理](https://ai-data-base.com/archives/26319 "自然言語処理（NLP）") の力によってそのプロンプトを安全なもの（機密情報を含まないもの）にしたくなることは往々にしてあるでしょう。そんなときに思い出してほしい方法論です。

## 実験による評価

本研究で提案された手法の有効性を確かめるために、研究者らは複数の実験を実施しました。実験がどのような目的で行われ、どのような結果を示しているのかを見ていきます。

### 実験の目的と評価のポイント

実験の主な目的は、「プロンプトを書き換えることで、どのくらいプライバシーが守られるか」と、「書き換えられた文章がどのくらい自然で使いやすいか（実用性）」の2つを確認することでした。

前者・プライバシーを守る能力は、「元のプロンプトの情報が書き換え後の文章にどの程度残ってしまっているか」で評価されます。一方で後者・実用性は、書き換えられた文章が不自然になってしまわないか、読み手が違和感なく読めるかという観点から評価されます。

評価は客観的な数値で示され、異なる書き換え手法を公平に比較できるようになっています。

### 評価に用いたデータと基準

実験では実際の文章データセットが用いられ、プロンプトの書き換えを行いました。例えば、ニュース記事やウェブサイトで使われるような一般的な文章データが対象となりました。

さらに、公平に比較できるように、既存のさまざまな書き換え手法（従来型の差分プライバシー手法やパラフレーズ手法など）も用いて同じデータセット上で比較評価が行われました。

プライバシー保護性能の評価では、元の文章に含まれる特定の情報が、書き換え後の文章にどれほど残っているかが定量的に計算されました。

実用性の評価では、書き換えられた文章がどのくらい自然で、読みやすいかが数値的に測定されました。一般的には、文章の自然さを示す「パープレキシティ（Perplexity）\*」という指標が使われました。この指標は、低ければ低いほど文章が自然であることを示します。

\*同名（Perplexity）のサービスがあるので万が一にも混乱してしまわないように補足ですが、パープレキシティ（Perplexity）はもともとこのように [自然言語処理](https://ai-data-base.com/archives/26319 "自然言語処理（NLP）") の評価指標として使われている概念の名称です。

### プライバシー保護性能の評価結果

実験結果を見ると、本研究で提案された手法は、従来の差分プライバシー手法に比べて、より高いプライバシー保護性能を示しました。つまり、元のプロンプトの機密情報が外部に漏れるリスクが明確に低減されることが確認されました。

これは、提案手法が単純に文書全体を一括して書き換えるのではなく、「複数回の書き換えを通じて重要な単語を特定し、重点的に隠す」という精緻な仕組みを取っているためです。単語レベルでリスクの高い情報を効果的に隠せたため、結果として情報漏洩リスクが従来より大きく下がったと考えられます。

### 実用性（文章の自然さ）の評価結果

プライバシーの保護性能だけが優れていても、書き換えられたプロンプトが不自然で使いにくければ意味がありません。そのため、実験では、書き換え後の文章の「自然さ」や「意味の伝わりやすさ」についても詳細に評価されました。

評価の結果、本研究の手法で生成された文章は、従来の差分プライバシー手法に比べて「パープレキシティ」の数値が明確に低くなりました。パープレキシティとは文章がどれくらい自然かを示す指標であり、この数値が低いほど文章が人間にとって読みやすく自然であることを意味します。

要するに、提案手法は、単語を機械的にランダム化するだけでなく、「元の意味をできるだけ保ちながら危険な単語のみを特定して隠す」という細やかな仕組みを採用したため、書き換え後の文章が従来手法よりも格段に読みやすく、自然な文章に仕上がっています。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86644_4-786x1024.png)

本研究の手法（DP-GTR）と既存のさまざまな手法について、プライバシー漏洩リスク（縦軸）と、文章の自然さを示すパープレキシティ（横軸）を比較したもの。パープレキシティが小さいほど、より自然で読みやすい文章であることを示す。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86644_5-758x1024.png)

本研究の手法（DP-GTR）において、パープレキシティのしきい値を変化させた場合に、プライバシー漏洩リスク（青い線）と単語単位でのプライバシー予算（赤い線）がどのように変化するか。実用性とプライバシー保護の間のバランスが調整できることを確認。

### 既存の手法との比較についての考察

さらに実験では、差分プライバシーを用いずに単純にパラフレーズ（表現を変える）をする手法や、従来型の差分プライバシー手法との比較も行われました。

比較の結果、本研究の手法は、両者の良い点をうまく取り入れた形となっています。

従来型の差分プライバシー手法では、プライバシー保護はある程度達成できますが、文章が不自然になる欠点がありました。一方で単純なパラフレーズ手法では、自然な表現は得られますが、プライバシー保護性能が低く、機密情報が意図せず漏れてしまうリスクがあります。

本研究の手法は、単語ごとに丁寧な分析を行うことで、これら二つの手法の長所をバランスよく組み合わせました。その結果、機密情報の保護という観点と、読みやすく自然な文章を作るという観点の両方でバランスの取れた性能を示すことができています。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86644_6-1024x463.png)

本研究の手法（DP-GTR）と既存手法において、文書レベルおよび単語レベルで消費されるプライバシー予算を比較したもの。それぞれの手法がどのようにプライバシー保護を行っているかを明確に示す。

### 解釈

一連の実験をまとめると、本研究の手法は、外部LLMを利用するときにプロンプトからの情報漏洩リスクを抑えつつ、同時に書き換え後の文章が人間にとって自然で読みやすいという理想的な結果を得ることができました。

重要な情報を効果的に隠しつつ、プロンプトの意味が崩れることなく保持されるため、ユーザーは安全かつストレスなくLLMを利用できる環境を整えられることになります。従来の手法で抱えていた「安全性を高めると実用性が低下する」という問題が、この手法によって大きく改善されることが分かったのです。

単に情報をぼかすだけの差分プライバシー手法に比べ、複数回のパラフレーズを活用して重要単語を特定・抑制する方法は、実用性と安全性の両方において効果的であるということが示されました。

## まとめ

本記事では、外部LLMを安全に利用するため、機密情報を含むプロンプトを書き換える手法に関する研究を紹介しました。

本手法は、複数回の書き換えを通じて、プロンプト内の危険な単語を特定し、それを重点的に隠す仕組みになっています。

実験の結果、従来の方法よりも機密情報の漏洩リスクを抑えつつ、書き換え後の文章が自然で読みやすいことが確認されました。外部LLMの利便性と情報漏洩リスクのバランスをうまく調整するために有用な方法論だと言えそうです。

ただし本手法を手動で実践しようとすると効率面で課題が出てしまうため、自動的に処理されるような仕組みを整えることも大事かもしれません。

**参照文献情報**

- タイトル：DP-GTR: Differentially Private Prompt Protection via Group Text Rewriting
- URL： [https://doi.org/10.48550/arXiv.2503.04990](https://doi.org/10.48550/arXiv.2503.04990)
- GitHub： [https://github.com/FatShion-FTD/DP-GTR](https://github.com/FatShion-FTD/DP-GTR)
- 著者：Mingchen Li, Heng Fan, Song Fu, Junhua Ding, Yunhe Feng
- 所属：University of North Texas

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMは自分の回答を組み合わせることで精度が向上する　検証結果と実践方法](https://ai-data-base.com/archives/86593)

[専門分野の翻訳をLLMで上手に行う方法　専門用語や専門表現をいかにしてカバーするか](https://ai-data-base.com/archives/86681)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)