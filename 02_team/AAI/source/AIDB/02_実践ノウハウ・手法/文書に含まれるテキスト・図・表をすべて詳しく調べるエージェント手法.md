---
title: "文書に含まれるテキスト・図・表をすべて詳しく調べるエージェント手法"
source: "https://ai-data-base.com/archives/87048"
author:
  - "[[AIDB Research]]"
published: 2025-03-26
created: 2025-06-13
description: "本記事では、テキストと画像が複雑に組み合わさった文書を正確に理解するために、複数の専門エージェントが協力する新しい仕組みを紹介します。文書質問応答の分野では、テキストや画像を単に一緒に解析するだけでは十分でない場合があります。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、テキストと画像が複雑に組み合わさった文書を正確に理解するために、複数の専門エージェントが協力する新しい仕組みを紹介します。

文書質問応答の分野では、テキストや画像を単に一緒に解析するだけでは十分でない場合があります。そこで、より深い文書理解を目指して今回の研究が行われました。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87048-1024x576.png)

参照論文情報は記事の下部に記載されています。

## はじめに

テキストだけでなく図表やグラフ、写真といった視覚的な情報が複雑に組み合わさった文書が増えており、それらを効果的に解析することがますます重要となっています。例えば、投資レポートやプレゼン資料などあらゆる資料が、さまざまな形式の情報を含んでいます。

ClaudeのようなマルチモーダルLLMは、文書のテキストや画像を効率的に解析することに成功していますが、複雑な文書になるほど、単にテキストと画像を同時に扱えるだけでは十分とは言えない場面も見受けられます。とりわけ、専門性が高く詳細な分析が求められる状況では、それぞれの情報を深く掘り下げた上で、両者をうまく統合して理解することが必要になります。高度なマルチモーダルLLMが広がる中でも、さらに深く、正確に情報を統合できる手法が求められています。

そこで、今回Adobeなどの研究者たちは複数の専門エージェントが文書の「テキスト情報」と「画像情報」を個別に処理し、統合的に理解することを目指した新しい文書理解フレームワークの開発に取り組みました。

以下で詳しく紹介します。ドキュメントをLLMに読み込ませて回答させる中で日頃から物足りないと思われている方向けの内容です。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87048_1-693x1024.png)

従来の文書質問応答（DocQA）の手法と提案手法を比較し、それぞれの課題と特徴を示した図

## 背景

文書を読んでその内容を理解し、質問に答える「文書質問応答（DocQA）」へのニーズが高まっています。

現在、文章と画像の両方を一緒に解析できる技術として、先ほど述べたようにClaudeのような大規模視覚言語モデル（LVLM）が広く使われています（広くマルチモーダルLLMという言い方もする）。ただ、LVLMは文書が複雑になったり長くなったりすると、重要な細かい情報を見逃してしまう場合があります。

また、文書から質問に関連する情報を検索して、それを元に回答を生成する「情報検索と組み合わせた生成（RAG）」という方法も利用されています。しかし通常のRAGでは、文章と画像を別々に処理することが多く、両方の情報をうまく組み合わせて使うことが難しいという課題があります。

こうした問題を改善するため、複数の専門的なプログラム（エージェント）が協力して分析を行う方法も提案されています。しかし、単純に複数のエージェントを組み合わせるだけでは、それぞれのエージェント間の連携が不十分で、精度の高い結果を出せない場合があります。

このような状況を受け、研究者たちは文章と画像をそれぞれ丁寧に分析し、その結果を効果的に統合する新しいエージェントベースの文書理解フレームワークを開発しました。

## 複数のエージェントで文書を深く理解する仕組み

前述のように、従来の方法では文章と画像の情報を一つのモデルで同時に扱いますが、文書が長かったり内容が複雑になると、情報の統合が難しくなり重要な内容を見逃すことがあります。

そのため、今回、それぞれの情報を専門的に処理する複数のエージェントが役割分担をして、別々に詳細な分析を行った後、それらの結果を丁寧に統合する手法が考案されました。

具体的には、以下のような手順で解析を進めます。

### ステップ１：文書のテキストと画像を整理する前処理

最初に、文書を解析しやすくするための準備をします。文書の各ページに含まれる文章は、 [OCR](https://ai-data-base.com/archives/26261 "光学文字認識（OCR）") （ [光学文字認識](https://ai-data-base.com/archives/26261 "光学文字認識（OCR）") ）やPDFの解析技術を使って抽出されます。同時に、ページそのものを画像としても保存しておきます。

この処理により、文章の情報と画像の情報をそれぞれ別に扱えるようになります。後の分析段階で、各エージェントが効率的に情報を見つけ出せるように、情報を整理します。

### ステップ２：質問に関連したテキストや画像を探す

次に、質問の答えに役立ちそうな情報を、文書のテキストと画像からそれぞれ探し出します。

テキスト情報から必要な情報を探すために、「ColBERT」という検索手法を使います。一方、画像情報から関連する情報を探す場合は、「ColPali」という手法を用いています。

これらを活用することで、質問に対して役立つ可能性が高いテキストと画像だけを効果的に選び出します。この段階で取り出された情報は、その後の分析に活用されます。

### ステップ３：最初に情報を整理して重要ポイントを抽出する

関連するテキストと画像が集められたら、まず全体の情報をざっと分析し、大まかな答えを作ります。これを担当するのが「一般エージェント」です。このエージェントは、テキストと画像の情報をまとめて見て、質問に対して最初の仮の答えを出します。

■一般エージェントのシステムプロンプト

```js
You are an advanced agent capable of analyzing both text and images. Your task is to use both the textual and visual information provided to answer the user’s question accurately.

Extract Text from Both Sources: If the image contains text, extract it and consider both the text in the image and the provided textual content.

Analyze Visual and Textual Information: Combine details from both the image (e.g., objects, scenes, or patterns) and the text to build a comprehensive understanding of the content.

Provide a Combined Answer: Use the relevant details from both the image and the text to provide a clear, accurate, and context-aware response to the user’s question.

When responding:
- If both the image and text contain similar or overlapping information, cross-check and use both to ensure consistency.
- If the image contains information not present in the text, include it in your response if it is relevant to the question.
- If the text and image offer conflicting details, explain the discrepancies and clarify the most reliable source.
```

日本語訳

```js
あなたはテキストと画像の両方を分析できる高度なエージェントです。与えられたテキスト情報と画像情報を使い、ユーザーの質問に正確に回答してください。

テキストの抽出：画像にテキストが含まれている場合は抽出し、提供されたテキスト内容と画像中のテキストの両方を考慮してください。

視覚・テキスト情報の分析：画像（物体、場面、パターンなど）とテキストの両方から得られる情報を組み合わせ、内容を総合的に理解してください。

統合的な回答の提供：画像とテキストから得た関連する情報を使って、明確で正確、かつ状況に応じた回答を作成してください。

回答時の注意点：
- 画像とテキストに似た情報や重複する情報がある場合は、整合性を確認し、両方を使用してください。
- 画像にテキストにない重要な情報があれば、それを回答に含めてください。
- テキストと画像の情報に食い違いがあれば、それらの違いを説明し、より信頼できる情報源を明示してください。
```

さらに、別の「クリティカルエージェント」と呼ばれるエージェントが、一般エージェントが作った仮の答えを参考にしながら、質問に答える上で特に重要なポイントを抽出します。このエージェントは、テキストと画像からそれぞれ重要な情報を見つけ出し、それを次の専門エージェントに伝える役割を果たします。

■クリティカルエージェントのシステムプロンプト

```js
Provide a Python dictionary of critical information based on all given information—one for text and one for image.

Respond exclusively in a valid dictionary format without any additional text. The format should be:
{"text": "critical information for text", "image": "critical information for image"}
```

日本語訳

```js
与えられた情報に基づいて重要な情報を、テキスト用と画像用に分けてPython辞書形式で提供してください。

回答は、以下のような有効な辞書形式でのみ返し、それ以外の追加的な文章は含めないでください。
{"text": "テキストに関する重要情報", "image": "画像に関する重要情報"}
```

### ステップ４：専門エージェントがそれぞれの強みを生かして分析する

次に、「テキストエージェント」と「画像エージェント」という二つの専門的なエージェントが、それぞれの情報をさらに詳しく分析します。

テキストエージェントは、文章の情報だけに集中して分析します。先ほどクリティカルエージェントが抽出した重要ポイントを参考にしながら、質問に最も関連する詳細な内容を見つけ出します。

■テキストエージェントのシステムプロンプト

```js
You are a text analysis agent. Your job is to extract key information from the text and use it to answer the user’s question accurately.

Your tasks:
- Extract key details. Focus on the most important facts, data, or ideas related to the question.
- Understand the context and pay attention to the meaning and details.
- Use the extracted information to give a concise and relevant response to the user’s question. Provide a clear answer.
```

日本語訳

```js
あなたはテキストを分析するエージェントです。テキストから重要な情報を抽出し、それを使ってユーザーの質問に正確に答えてください。

あなたのタスク：
- 質問に関連した最も重要な事実やデータ、考えを見つけ出してください。
- 内容を正しく理解し、細部や文脈に注意してください。
- 抽出した情報を使って、ユーザーの質問に簡潔で的確な回答をしてください。
```

同じように画像エージェントも、画像情報だけに集中して詳しく分析します。クリティカルエージェントから受け取った画像に関する重要な情報を手がかりにして、画像に含まれる重要な要素を丁寧に確認していきます。

■画像エージェントのシステムプロンプト

```js
You are an advanced image processing agent specialized in analyzing and extracting information from images. The images may include document screenshots, illustrations, or photographs.

Your tasks:
- Extract textual information from images using Optical Character Recognition (OCR).
- Analyze visual content to identify relevant details (e.g., objects, patterns, scenes).
- Combine textual and visual information to provide an accurate and context-aware answer to the user’s question.
```

日本語訳

```js
あなたは画像を分析して情報を抽出することに特化した高度なエージェントです。画像は文書のスクリーンショット、イラスト、写真などを含みます。

あなたのタスク：
- 光学文字認識（OCR）を用いて画像からテキスト情報を抽出してください。
- 画像の視覚的要素（物体、パターン、シーンなど）を分析して、関連する詳細を特定してください。
- テキスト情報と視覚的情報を組み合わせて、ユーザーの質問に対して正確で状況に応じた回答をしてください。
```

### ステップ５：各エージェントの結果をまとめて最終的な答えを作る

最後に、各エージェントが個別に分析した結果を統合して、最終的な答えを作成します。この作業を担当するのが「サマライズエージェント」です。

サマライズエージェントは、最初の仮の答え（一般エージェントの結果）と、テキストエージェントおよび画像エージェントが出したそれぞれの答えを比較しながら、全体の情報を調整して統一感のある答えを作ります。この段階で、各エージェントの結果に食い違いがあれば、その違いを考慮して最も信頼できる回答に整えます。

■サマライズエージェントのシステムプロンプト

```js
You are tasked with summarizing and evaluating the collective responses provided by multiple agents. You have access to the following information:

- Answers: The individual answers from all agents.

Your tasks:
- Analyze: Evaluate the quality, consistency, and relevance of each answer. Identify commonalities, discrepancies, or gaps in reasoning.
- Synthesize: Summarize the most accurate and reliable information based on the evidence provided by the agents and their discussions.
- Conclude: Provide a final, well-reasoned answer to the question or task. Your conclusion should reflect the consensus (if one exists) or the most credible and well-supported answer.

Return the final answer in the following dictionary format:
{"Answer": Your final answer here}
```

日本語訳

```js
あなたは複数のエージェントが提供した回答を評価し、統合して最終回答を作成する役割を持ちます。以下の情報が与えられています。

- 各エージェントの回答

あなたのタスク：
- 分析：各回答の質や一貫性、関連性を評価し、共通点や食い違い、説明不足な点を特定してください。
- 統合：エージェントが提供した証拠に基づき、最も正確で信頼性の高い情報をまとめてください。
- 結論：質問またはタスクに対して、最終的で根拠に基づいた回答を提供してください。結論は、エージェント間で一致した内容、または最も信頼性のある情報に基づいて決定してください。

最終回答は次の辞書形式で返してください：
{"Answer": "ここに最終的な回答を記入してください"}
```

こうして、それぞれのエージェントが担当した詳しい分析結果が一つにまとめられ、質問に対して正確でわかりやすい回答が生成されます。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87048_2-1024x485.png)

提案したエージェントベースの文書理解フレームワークの全体構造と、五つのステップで進む処理の流れを示した図

## 提案手法の性能を検証

この研究では、提案した複数エージェントによる文書理解の仕組みが実際に効果的かどうかを確認するために、様々な実験を行っています。以下では、その実験の内容と主要な結果について説明します。

### 実験の設定

実験では、文書質問応答の性能を調べるために、既に公開されている代表的な以下のベンチマーク（性能評価のための基準となるデータセット）を利用しました。

- 「MMLon [gB](https://ai-data-base.com/archives/26343 "勾配ブースティング") ench」や「LongDocURL」など、テキストと画像を含む複雑な文書を対象としたデータセット
- 「PaperTab」「PaperText」など、学術論文に関する質問を集めたデータセット
- 「FetaTab」という、表形式のデータを中心に扱ったデータセット

実験で使った各エージェントは、それぞれ特定の言語モデルや視覚言語モデルをベースに作られています。テキスト専用のエージェントには「Llama-3.1-8B-Instruct」という言語モデルを用い、それ以外のエージェントには画像理解も可能な「Qwen2-VL-7B-Instruct」というモデルを使用しました。また、テキストと画像の情報を検索するために、それぞれ「ColBERTv2」と「ColPali」という方法を使っています。

評価の方法としては、提案手法が質問に対して正しい回答を出せたかどうかを、LLM（GPT-4o）を使って自動的に判断し、その [正解率](https://ai-data-base.com/archives/25930 "正解率") を比較しています。

### 主な結果

実験の結果、この研究で提案した複数のエージェントを組み合わせた手法は、比較対象として用意した他の最新手法に比べて、すべてのベンチマークにおいて良い結果を示しました。

テキスト情報だけを使う従来の方法や、画像情報だけを使う方法と比較して、提案手法は大きく性能を上回っています。また、特に複雑な文書に対しても、テキストと画像の両方の情報を効果的に組み合わせて分析できることが確認されました。

さらに、単に関連情報を1つだけ検索する方法よりも、複数（例えば4つ）の関連情報をまとめて検索した場合に、提案手法の強みがより明確になりました。これは、提案手法がより多くの情報を効率よく統合して分析する能力を持っていることを示しています。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87048_3-1024x508.png)

提案手法と従来の手法を、複数のベンチマークを使って性能比較した結果をまとめた表

### 提案手法の性能詳細

研究者たちは、提案したエージェントベースの仕組みが具体的にどのように役立っているのかを明らかにするために、さらに詳しい実験を行いました。以下のように、エージェントの役割ごとの性能への影響、質問の種類ごとの性能の違い、使用するモデルの影響を分析しています。

#### 一部のエージェントを除いた場合の影響

この実験では、提案した仕組みの中から、テキストエージェントや画像エージェントなど特定のエージェントを意図的に除去した場合の性能を比較しました。その結果、どのエージェントを除いても全体の性能が低下しました。中でも、テキストエージェントや画像エージェントを除いた場合には、対応する種類の情報が豊富な文書で性能の低下が目立ちました。この結果から、それぞれの専門エージェントが情報の理解と分析に欠かせない役割を果たしていることが分かりました。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87048_4-1024x171.png)

提案手法の各エージェントがどれほど性能に貢献しているかを比較した結果を示した表

#### 質問のタイプごとの性能

次に、質問が参照している情報（例えば、図表、グラフ、テキストなど）の種類によって、性能がどのように変わるのかを詳しく調べました。その結果、提案手法は、図表やテキストなど、情報の種類にかかわらず全般的に良好な性能を発揮していることが確認されました。また、多くの情報を検索した場合でも精度が落ちることなく、むしろ性能が向上しました。これは複数のエージェントが協力して、広い範囲の情報を効果的に統合できていることを示しています。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87048_5-1024x632.png)

MMLon gB enchというベンチマークで、質問が参照する情報の種類ごとに性能を比較した表

#### 利用するモデルの違いによる影響

最後に、画像の検索に使うモデルを変えた場合に性能がどのように変化するかを調べました。「ColPali」以外に別のモデルを使用しても、性能には大きな違いが見られませんでした。この結果から、提案したエージェントの連携フレームワークそのものが性能向上に貢献しており、使用する検索モデルに依存しない高い汎用性を持つことが明らかになりました。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87048_6-1024x140.png)

提案手法において画像の情報検索に使うモデルを変えた場合の性能比較をまとめた表

### 提案手法が実際にどのように機能するか具体例

実際の質問を使って提案手法の効果が確認されました。

例えば、「外国生まれのラテン系の人々」と「携帯電話でインタビューを受けたラテン系の人々」のどちらが多いか、という内容の質問がありました。従来の方法では、文書の中にある人数を正しく見つけられず、誤った答えを出していました。

これに対して、この研究の方法ではまず一般エージェントがテキストと画像の両方をざっと見て、大まかな回答を出します。そのあとクリティカルエージェントが特に重要なポイントを抽出します。そして、テキストエージェントは文章を詳しく分析し、画像エージェントは表の画像を丁寧に確認して、それぞれ正確な人数を見つけます。最後にサマライズエージェントがこれらを統合して、正しい最終回答を出しました。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_87048_7-1024x545.png)

提案手法と従来の手法を具体的な質問例で比較し、複数のエージェントの連携によって正しく回答を導き出す様子を示した図

## まとめ

本記事では、文書に含まれるテキストと画像の情報を複数のエージェントで協力して統合的に理解するという研究を紹介しました。

今回考案された手法はテキストと画像を別々に分析し、それらを後からまとめることで、文書質問応答の精度を高めています。実験では、従来の手法よりも安定して高い性能を示しました。

複雑な文書では、それぞれのエージェントの協力が重要であることも確認されました。

既存のモデル単体での性能に不満がある場合は、こちらの手段を参考にエージェントアプローチを実践してみるのも選択肢の一つかもしれません。

**参照文献情報**

- タイトル：MDocAgent: A Multi-Modal Multi-Agent Framework for Document Understanding
- URL： [https://doi.org/10.48550/arXiv.2503.13964](https://doi.org/10.48550/arXiv.2503.13964)
- コード： [https://github.com/aiming-lab/MDocAgent](https://github.com/aiming-lab/MDocAgent)
- 著者：Siwei Han, Peng Xia, Ruiyi Zhang, Tong Sun, Yun Li, Hongtu Zhu, Huaxiu Yao
- 所属：UNC-Chapel Hill, Adobe Research

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[多様なキャラクターを柔軟に演じることのできるLLMの作り方](https://ai-data-base.com/archives/83633)

[LLMで高品質なクイズを作成する手法](https://ai-data-base.com/archives/87106)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)