---
title: "LLMエージェントに人間のような欲求を持たせてシミュレーションする手法"
source: "https://ai-data-base.com/archives/80804"
author:
  - "[[AIDB Research]]"
published: 2024-12-18
created: 2025-06-13
description: "本記事では、人間らしい行動を再現するLLMの新しい仕組みを紹介します。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、人間らしい行動を再現するLLMの新しい仕組みを紹介します。これまでのLLMベースのシミュレーションは外からの指示や決められた目標に従うだけのものが多かったですが、この研究では「自分の欲求」に基づいて自律的に行動する方法が提案されています。この仕組みを活用して、より自然で人間らしい行動が再現できることが期待されています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_80804-1024x576.jpg)

**発表者情報**

- 研究者：Yiding Wang et al.
- 研究機関：北京大学、香港大学、北京師範大学、BIGAI

**LLMによるシミュレーションの関連研究**

- [実在する人間1052人の態度と行動をAIでモデル化　インタビューベースのエージェントが人間の回答を85%再現](https://ai-data-base.com/archives/80107)
- [10億人のペルソナ（人物像）で多様な合成データを作成するための技術](https://ai-data-base.com/archives/72498)
- [100万体のLLMエージェントによるシミュレーションを実験できる環境が登場](https://ai-data-base.com/archives/76640)

## 背景

「人間らしく自分で考えて行動する知的な対話システム」の実現に向けた取り組みは、長年続けられてきました。人工知能研究の初期から、人間と区別がつかないような応答ができることは重要だと考えられてきました。人間らしい行動ができることは、例えば対話システムやゲームのキャラクターなどで、ユーザーが親しみやすく信頼できるものにするために大切な要素です。

人工知能分野以外でも、心理学、社会学、経済学など多くの分野では「人間の日常生活の行動パターンを理解すること」が研究されてきました。例えば、人々がどのように判断を下すのか、市場がどう動くのかを予測するのに役立ってきました。ただし、実際の人間の行動データを集めるのは大変です。費用がかかるうえ、プライバシーの問題もあるためです。  
そこで、コンピューター上で人間らしい行動を再現する技術が注目されています。これまでは仮想空間での行動記録や、深層学習による行動パターンの生成が試されてきましたが、LLMを使った日常行動の再現はまだ十分に研究されていません。

既存の研究では、限られた環境でしか動作しない対話システムや、決められた計画を実行するだけのシステムが主でした。また、ほとんどは「与えられた目標を達成する」ことだけに注目しており、人間が持っているような「自分の欲求に基づいて行動する」という面は考慮されていませんでした。

そこで今回北京大学などの研究者らは、外から指示されるのではなく、自分の欲求に基づいて自律的に行動を選ぶ新しい仕組みを提案しています。人間の欲求に関する心理学理論を参考に、システムが自分で考えて行動を選択し実行する方法を開発しました。そうして、より自然な人間らしい行動が再現できるようになり、様々な分野での応用が期待されています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_80804_1-1024x310.png)

人間らしい活動生成手法の比較。提案手法D2Aは「内発的欲求の満足」を重視。

## 研究課題

研究者らは、日常的な室内空間で人間らしい活動を自然に再現するという課題に取り組みました。従来の対話システムとは異なり、具体的な指示を与えることなく、自律的に行動を選択し実行するシステムが目指されています。

### 環境とエージェントの設定

研究環境は、部屋の様子などの環境の説明と、エージェントの個人情報（名前、年齢、性格特性など）を含むプロファイルから構成されています。研究者らは異なるタイプのエージェントに対応できるよう、補足情報を用意しました。例えば指示に従うタイプのエージェントには具体的な目標が、欲求に基づいて行動するタイプには欲求の度合いが与えられます。

### シミュレーションの流れ

シミュレーションは複数の段階に分けて実行されます。各段階で、エージェントはそれまでの行動履歴と観察結果などの情報を基に、次の行動を決定します。行動の選び方はエージェントのタイプによって異なり、性格に合わせた調整や、タスクの分解、計画立案、行動選択などの処理が行われます。

最後に、生成された一連の行動はLLMによって自然な文章に書き直されます。研究者らは、異なるエージェント間で公平に比較できるよう、表現スタイルを統一することを重視しました。シミュレーション全体の目的は、人間らしい多様な行動パターンを生成することに置かれています。

## シミュレーションシステムの設計

基礎となる [Concordiaライブラリ](https://github.com/google-deepmind/concordia) （Google DeepMind製）には、目標と計画の管理、時間の管理、記憶の保存と呼び出しといった機能が備わっています。また、様々な言語モデルやAPIに対応できるインターフェースが用意され、異なるモデル間での切り替えが可能にされました。

研究者らはさらに人間らしい日常活動をより自然に再現するため、以下のような機能強化を行いました。

1. より現実的な行動を実現するために、エージェントは環境内に存在するものとだけやり取りができるように制限されました。
2. また、環境内のアイテムを必要に応じて追加・削除できる機能も実装されました。
3. さらに、エージェントが行動を実行する際や状態が変化する際に、自己の行動を振り返る機能が追加されました。
4. 加えて、エージェントの行動が自然かつ一貫しているかを評価・可視化する機能も備えられました。

### 欲求に基づく行動システムの概要

研究者らは人間の欲求階層説を参考に、エージェントが内的な動機に基づいて行動を選択する仕組みを開発しました。システムは「価値体系」と「欲求駆動型プランナー」という2つの主要な部分で構成され、記憶や環境情報などの要素によって支えられています。

欲求階層説では、人間の行動は安全、社会、自尊心、自己実現などの様々な欲求によって動機づけられると考えられています。個人の性格や特徴によって、これらの欲求の優先順位は異なります。研究者らはこの考えを取り入れ、エージェントの性格プロファイルから欲求の強さを決定する仕組みを開発しました。

屋内での一人用環境では、11種類の欲求が定義されています。

生理的欲求として「空腹」「喉の渇き」「眠気」「清潔さ」「快適さ」「健康」が含まれます。安全への欲求は「安全性」として表現されます。人間関係への欲求は「社会的つながり」として定義され、個人の成長に関する欲求は「喜び」「情熱」「精神的満足」として表現されています。

エージェントの性格特性は、これらの欲求の目標値に影響を与えます。例えば「非常に社交的」なエージェントは社会的つながりへの欲求が高く設定され、「やや社交的」な場合は中程度に設定されます。シミュレーション開始時、各欲求の初期値はランダムに設定され、その後の行動によって変化していきます。

### 価値体系の仕組み

価値体系は各シミュレーションステップで3つの重要な処理を実行します。

「量的価値控除」  
時間経過による欲求の自然な変化を表現します。各ステップの開始時に、エージェントの性格特性に基づいて決められた確率で欲求の値が減少していきます。この仕組みにより、エージェントは継続的に欲求を満たすための行動を取るよう動機づけられます。

「質的価値記述」  
数値で表された欲求の状態を文章に変換します。LLMは数値の解釈が苦手なため、各シミュレーションステップで定められた質問に答える形で、欲求の状態が文章として表現されます。実験によると、この文章化のプロセスはエージェントが適切な行動を選ぶために重要な役割を果たしています。

「価値の更新」  
エージェントの行動とその結果を受けて欲求の値が更新されます。エージェントが行動を実行し、環境からその結果が返されると、価値体系はそれらの情報に基づいて各欲求の値を調整します。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_80804_2-1024x530.jpg)

D2Aフレームワークの全体像。価値システムと欲求駆動プランナーが連携して行動を決定。

### 欲求に基づく行動決定の仕組み

欲求駆動型プランナーは、価値体系から受け取った欲求の状態と記憶の情報を組み合わせて、エージェントの次の行動を決定します。行動の選択は3段階のプロセスで行われ、各段階で合理的な判断が行われるよう設計されています。

第一段階の「活動の提案」では、Tree of Thoughtsという手法を参考に、複数の候補となる行動を生成します。通常は3つの候補が提案され、それぞれがエージェントの現在の欲求状態、過去の行動、環境の状況などを考慮して選ばれます。

第二段階の「活動の評価」では、提案された各行動について、実行した場合にどのような結果が予測されるかを検討します。プランナーは各行動を実行した後のエージェントの欲求状態を予測し、それぞれの行動の効果を評価します。

第三段階の「活動の選択」では、評価結果を比較して最適な行動を選びます。エージェントの内的欲求を最もよく満たすと予測された行動が選択され、実行に移されます。行動が実行された後、環境からその結果が返され、価値体系が欲求の状態を更新して次のステップに進みます。

実験の結果（詳細は後述のセクション）、このシステムは従来の方法と比べて、より自然で一貫性のある人間らしい行動を生成できることが示されました。

## 実験

研究者らは提案した欲求駆動型エージェント（D2A）の性能を評価するため、実験を行いました。主に3つの点について検証が行われました。

1. 従来の手法と比べて、より人間らしい行動を生成できるか
2. 人間のように適切に欲求を満たすことができるか
3. システムの各要素が性能と行動にどのような影響を与えるか

### 実験環境

主な実験は室内環境で行われ、キッチン、リビング、寝室、バスルームを備えた家の中で「アリス」というキャラクターが生活するという設定です。アリスには11種類の欲求に関する詳細な性格設定が与えられました。

家の中には必要な日用品が配置され、アリスがそれらを使って快適に生活し、欲求を満たせるような環境が整えられました。

また、より複雑な社会的な状況での検証のため、「セントラルパーク」という屋外環境も用意されました。ここでは「Enjoy Your Life」というパーティーが開催され、様々な施設やアクティビティが提供されています。この環境には他の2つのキャラクターも配置され、社会的な交流に関する欲求への対応も検証できるようになっています。

### 比較対象と評価方法

研究者らはD2Aの性能を、3つの既存システムと比較しました。

- ReAct（行動を起こす前に推論を行うシステム）
- LLMob（キャラクターの性格から動機を読み取り、それに基づいて行動を選ぶシステム）
- BabyAGI（タスクの優先順位リストを管理し、それに従って行動を選択するシステム）

実験では、これらのシステムには欲求の状態は直接伝えず、代わりに目標や性格設定、行動習慣などの情報が与えられました。すべてのシステムは同じ言語モデル（LLaMA3.1-70B）を基盤として使用し、より高性能な言語モデル（Qwen2.5-72B）での実験も行われました。

### 評価の仕組み

システムの性能は主に2つの指標で評価されました。

（１）人間らしさの度合い

システムが生成した行動の記録を2つずつ比較し、どちらがより人間らしいかを判定します。この比較はGPT-4oによって100回ずつ行われ、その勝率が計算されました。

（２）欲求の充足度

システムが持つ欲求の現在値と目標値との差を「不満度」として数値化し、時間とともにどのように変化するかを観察しました。

### 実験の結果

研究者らは各システムで15回ずつ実験を行いました。システムごとに出力される文章の表現スタイルが異なるため、公平な比較ができるよう、すべての出力を言語モデル（Llama3.1）で統一的な表現に書き直しました。

人間らしさの評価では、以下の3つの観点から判断が行われました。

1. その行動がキャラクターの能力や習慣、環境に合っているか
2. 行動の順序や組み合わせが論理的につながっているか
3. 状況や文脈から見て、その行動が合理的といえるか

実験の結果、D2Aは他のシステムより優れた性能を示しました。これは人間の判断とも一致することが確認されています。D2Aが優れていた理由として、以下の点が挙げられます。

- 人間の欲求理論に基づいて設計されているため、より自然な行動選択ができる
- 現在の欲求状態を考慮しながら継続的に行動を選択するため、一貫性が保たれる
- キャラクターの性格特性を反映した行動が取れる

一方、他のシステムにはそれぞれ以下のような課題が見られました。

- ReAct：論理的な判断はできるが、長期的な一貫性に欠ける
- BabyAGI：一貫性はあるが、同じような行動を繰り返す傾向がある
- LLMob：多様な行動はできるが、行動間のつながりが弱い

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_80804_3.png)

4つのエージェントの比較結果。D2Aが最も高い「人間らしさ」を示す。

### 詳細な検証

研究者らはD2Aの性能をより詳しく検証するため、2つの実験を行いました。

1つ目の「ランダム8ステップ実験」では、欲求に基づいて適切に行動を選択できるかを検証しました。その結果、D2Aは他のシステムと比べて明らかに低い不満度を示し、より効果的に欲求を満たせることが分かりました。

2つ目の「固定12ステップ実験」では、人間の行動パターンとの比較を行いました。朝の時間帯を想定し、すべての欲求を中程度の値に設定して実験を開始。3人の人間にも同じ環境で行動を選んでもらい、結果を比較しました。実験の結果、人間の行動が最も効果的に欲求を満たせることが分かり、D2Aは他のシステムの中で最も人間に近い結果を示しました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_80804_4-1024x357.png)

（a）欲求不満の平均値の変化。D2Aが他手法よりも効率的に不満を解消。 （b）人間の行動シーケンスに最も近い不満解消パターンをD2Aが示す。

研究者らはD2Aの各機能の重要性も検証しました。その結果、欲求の状態を文章で表現する機能を省くと、適切な行動選択が難しくなりました。また、複数の候補から行動を選ぶ際、候補数が多いほど良い結果が得られました。事前に行動計画を立てる機能は、逆に柔軟な対応を妨げる可能性が示されました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_80804_5-1024x718.png)

各フレームワーク構成要素の有効性を検証。D2Aの設計が不満解消に寄与していることを確認。

しかし現在のシステムには、まだいくつかの制限があります。例えば、欲求の変化を単純な計算で処理しており、より自然な変化の表現が必要です。また、人間の複雑な動機づけを十分に表現できていない可能性があります。

今後の研究では、より複雑な環境での実験や、欲求の自然な変化の表現、様々な状況への適応能力の向上などが計画されています。また、実際の人間の行動データをより多く集めて分析することで、システムの改良を進めていく予定です。

## まとめ

本記事では、LLMエージェントが人間らしい日常活動を内発的な欲求に基づいて生成する「Desire-driven Autonomy (D2A)」フレームワークを紹介しました。

研究者らはLLMが自然で合理的な行動を選択し、欲求を満たす過程を模倣することを可能にしました。今後はモデルの精緻化を通じて、より高度で柔軟な人間模倣が期待されます。

**参照文献情報**

- タイトル：Simulating Human-like Daily Activities with Desire-driven Autonomy
- URL： [https://arxiv.org/abs/2412.06435](https://arxiv.org/abs/2412.06435)
- 著者：Yiding Wang, Yuxuan Chen, Fangwei Zhong, Long Ma, Yizhou Wang
- 所属：Peking University, The University of Hong Kong, Beijing Normal University, State Key Laboratory of General Artificial Intelligence.
- プロジェクトページ： [https://sites.google.com/view/desire-driven-autonomy](https://sites.google.com/view/desire-driven-autonomy)
- GitHub： [https://github.com/zfw1226/D2A](https://github.com/zfw1226/D2A)

## 理解度クイズ（β版）

Q1. この研究で開発されたシステムが、従来のシステムと大きく異なる点は次のうちどれですか？

従来のシステムは目標達成に重点を置いていたのに対し、この研究のシステムは人間の欲求に基づき自律的に行動を選択します。

解説を見る

Q2. この研究で、エージェントの行動選択に影響を与える要素として挙げられていないものは次のうちどれですか？

このシステムは、エージェントの性格、過去の行動、環境情報に基づいて行動を選択しますが、外部からの具体的な指示には依存しません。

解説を見る

Q3. この研究で開発された「価値体系」の主な機能として、次のうち間違っているものはどれですか？

価値体系は、時間経過による欲求の変化、数値から文章への変換、行動結果による欲求値の更新を行います。外部からの指示に基づいて行動を選択する機能はありません。

解説を見る

Q4. この研究で実施された実験において、システムが生成した行動の「人間らしさ」を評価する際に用いられた観点として、適切でないものはどれですか？

人間らしさの評価では、キャラクターの能力や習慣への合致、行動の論理的つながり、状況に対する合理性が評価されました。行動が言語モデルによって生成されているかは評価対象ではありません。

解説を見る

Q5. 実験結果で、欲求に基づき行動するシステム（D2A）が他のシステムより優れていた主な理由として、次のうち適切でないものはどれですか？

D2Aが優れていた理由は、人間の欲求理論に基づく設計、現在の欲求状態の考慮、性格特性の反映です。行動決定に複雑な計算は使用していません。

解説を見る

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[文脈内学習は「少数事例からの単純な学習だけでなく、言語モデルが持つ幅広い適応能力」](https://ai-data-base.com/archives/80765)

[動画を理解する軽量なLLM『Apollo』、オープンソースで登場（商用利用も可能）](https://ai-data-base.com/archives/80871)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)