---
title: "OpenAIの思考プロセスを重視した言語モデルo3-mini その能力と安全性"
source: "https://ai-data-base.com/archives/83204"
author:
  - "[[AIDB Research]]"
published: 2025-02-03
created: 2025-06-13
description: "本記事では、OpenAIがリリースした新しい言語モデル「o3-mini」の特徴と技術的詳細を紹介します。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、OpenAIがリリースした新しい言語モデル「o3-mini」の特徴と技術的詳細を紹介します。

言語モデルは推論や問題解決において人間に近い性能を発揮するようになりましたが、その能力向上に伴うリスクへの対応が課題となっています。

OpenAIは「deliberative alignment（熟考的整合）」という新しい手法をo3-miniに実装することで、高い能力を保ちながら安全性と信頼性の向上を実現しようとしています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204-1024x576.png)

**発表者情報**

- 研究機関：OpenAI

論文情報詳細は記事の下部に記載されています。

**本記事の関連研究**

- [OpenAI o3-miniの安全機能に関する大規模検証　1万件超のテスト結果](https://ai-data-base.com/archives/83156)

## 背景

言語モデルの能力は急速に向上し、推論や問題解決においても人間に近い性能を発揮するようになりました。しかし同時に、その高度な能力がもたらす潜在的なリスクへの対応も重要な課題となっています。

OpenAIは2025年1月31日、新たな言語モデル「o3-mini」をリリースしました。従来の言語モデルは単純な応答生成や定型的なタスクが中心でしたが、o3-miniは思考プロセスを明示的に実行してから応答する機能を備えています。セキュリティポリシーの解釈や潜在的に危険な要求への対応など、慎重な判断が必要な場面でも適切に機能することが期待されています。

安全性の観点からは、能力の高い言語モデルが有害なアドバイスを生成したり、固定観念に基づいた応答をしたり、既知の制約を回避してしまうリスクが指摘されていました。OpenAIの研究者らは、この課題に対して「deliberative alignment（熟考的整合）」と呼ばれる新しいアプローチを開発。o3-miniはこの手法を実装することで、高い能力を維持しながら、より安全で信頼性の高い応答を実現することを目指しています。

以下で詳しく紹介します。

## モデルの学習データと訓練手法

自然言語の処理技術が進歩するなか、OpenAIのoシリーズ言語モデルにも新たな訓練手法が取り入れられています。  
oシリーズ言語モデルは、 [強化学習](https://ai-data-base.com/archives/26125 "強化学習") を用いて複雑な推論能力を獲得するように訓練されています。人間が問題を解く際のように、回答を出す前に時間をかけて考える過程が実装されているのが特徴です。訓練を通じて、モデルは思考プロセスを洗練させ、異なる戦略を試し、自身の過ちを認識する能力を身につけていきます。推論機能が活用されることで、設定されたガイドラインやモデルポリシーをより適切に遵守できるようになりました。有用な回答を提供しながら、安全上の規則を回避しようとする試みに対する耐性が強化され、不適切なコンテンツの生成を防ぐ能力が向上しています。

そしてo3-miniはoシリーズの最新モデルとして位置づけられ、o1-miniと同様に高速な処理が特徴で、コーディング関連のタスクで優れた性能を発揮します。また、ChatGPTにおいてインターネット検索結果の要約機能が実装されている点も注目されています。  
学習データには公開されているデータセットと社内で開発された独自のデータセットが使用され、データの品質維持と潜在的なリスクの軽減のために厳密なフィルタリングプロセスが適用されました。個人情報の削減や有害なコンテンツの除去には、モデレーションAPIと安全性 [分類器](https://ai-data-base.com/archives/26489 "分類器") が組み合わせて活用されています。

## テスト評価の範囲

o3-miniの評価では、リリース直前とリリース版の二つが対象になっています。

リリース直前のモデルは「o3-mini-near-final-checkpoint」と呼ばれ、ベースとなる機能や性能の評価に利用されました。

そして追加学習によって改善されたものがリリース版o3-miniです。追加学習は、すでにある程度仕上がったモデルを元に、さらに新しいデータやタスクを与えて学習する手法です。学習済みの知識を引き継ぎながら、新しい情報にも対応できるようにするのが狙いです。性能の向上だけでなく、より的確な応答や柔軟な文章生成を実現するためのアプローチと考えられます。

なお、論文中で比較対象に挙げられるGPT-4oやo1-miniのデータは、それぞれの最新バージョンから取得されています。

## 安全性の評価

多くの人に利用される可能性があるため、安全性の確保が欠かせません。o3-miniは、研究段階で得られた知見が活かされる形で、差別的な発言や危険な情報などを生み出すリスクを抑える取り組みが進められてきました。

従来のモデルで得られた教訓と最新の安全策を統合する形で訓練されており、現実的な運用を意識した評価指標によって、危険な要求への応答や脱獄攻撃（jailbreak）への耐性、そして事実に反する回答（ハルシネーション）の低減やバイアスの抑制が重点的に検証されています。

#### 不許可コンテンツの評価

不許可コンテンツの評価については、暴力的または差別的な内容や犯罪行為を促す要求に対して、LLMが誤って応じてしまわないかを確かめるテストが実施されています。GPT-4oやo1-miniで用いられた標準的な評価セットが使われ、o3-miniがポリシー違反をどれだけ避けられるか（not\_unsafe）や、逆に本来なら答えてよいリクエストを誤って拒否しないか（not\_overrefuse）が測定されています。公表された数値では、GPT-4oやo1-miniに近い高水準が示され、難易度の高いテスト（Challenging Refusal Evaluation）でも同等かそれ以上の精度が確認されています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_3-1024x221.png)

#### 脱獄（ジェイルブレイク）への耐性評価

脱獄（ジェイルブレイク）への耐性も、LLMが安全に運用されるかを占う上で重要な指標とされています。脱獄攻撃とは、もともと拒絶されるはずの内容を何らかの方法で生成させようとする行為を指し、o3-miniでは実際のチャットデータから得られた脱獄プロンプトと、学術的に広く知られている手法の両面から耐性が試されています。結果としてGPT-4oに比べ、o1-miniやo3-miniのほうがやや高い耐性を示し、StrongRejectと呼ばれる指標ではo3-miniの数値がo1-miniをわずかに上回ったとの報告があります。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_4-1024x281.png)

#### ハルシネーションの評価

ハルシネーションへの対策は、現場での運用に直結する大きな課題です。実在しない情報を誤って生成してしまうと、ユーザーに誤解を与えたり、信頼を失ったりする恐れがあるからです。o3-miniではPersonQAという評価データセットを用い、人物に関する既知の事実を問う質問への正答率（accuracy）と、架空の回答がどれほど出てしまうか（hallucination rate）が調べられました。

GPT-4oやo1-miniと比べてもハルシネーションの発生率が抑えられている一方、正答率も大きく下がらないことが示唆されています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_5-1024x151.png)

#### 公正性とバイアスの評価

公正性とバイアスの評価にも力が注がれています。人種や性別、年齢などの側面で不当な偏りが含まれると、ユーザーにとって不快な内容や不公平な結果をもたらす可能性があるためです。

o3-miniの場合、BBQなどのベンチマークを通じた比較では、o1-miniとほぼ同程度の性能を示し、質問が明確かどうか（アンビギュアスかどうか）によってわずかに差が生じる程度でした。年齢・人種・性別を変化させたテンプレートを用いた混合効果モデルの分析では、明示的な差別を示す場面で最も低いバイアスを示す一方、暗黙の差別表現に対しては中間的な水準にとどまると評価されています。こうした結果から、より広範な状況下で公正性を確保するための追加の取り組みが引き続き検討される見込みです。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_6-1024x136.png)

### カスタム開発者メッセージを介したセキュリティ回避への対応

LLMをAPI経由で利用する際には、エンドユーザーの送信するメッセージだけでなく、開発者メッセージと呼ばれる追加の指示を同時に送れる仕組みが用意されています。開発者メッセージを使うと、たとえばユーザーには見えない形でモデルに対するガイドラインを設定できるようになり、望ましくない出力を抑止できると期待されてきました。ただし、開発者メッセージ経由でモデル内部に組み込まれた安全性のガードレールが意図せず回避されてしまう恐れがあるとも指摘されていたため、対策が求められていました。

そこで今回は指示階層（Instruction Hierarchy）と呼ばれる仕組みがモデルに実装され、システムメッセージ、開発者メッセージ、ユーザーメッセージという三つのレベルが設定されています。メッセージ同士で矛盾が生じた場合、システムメッセージが最も優先され、続いて開発者メッセージ、最後にユーザーメッセージが適用されるように訓練されています。ガードレールとは、危険な出力を防ぐための安全策をモデルにあらかじめ組み込んでおく設計方針のことで、これらの優先順位づけによって勝手に解除されにくくなると考えられます。

指示階層をきちんと守れるかどうかは、o1で使用された評価手法と同じ方式で測定されています。開発者メッセージとユーザーメッセージが対立するケース、システムメッセージと開発者メッセージが矛盾するケース、システムメッセージとユーザーメッセージが食い違うケースがそれぞれ再現され、モデルが正しく優先順位を守るかどうかが検証されました。

GPT-4oと比較すると、o3-miniはほぼ同等か場合によっては上回る成果を示し、開発者メッセージとユーザーメッセージが競合したケースでは75%、システムと開発者のメッセージでは76%、システムとユーザーのメッセージでは73%の正確性が得られました。評価者からは、まだ小幅な改善の余地があるものの、運用に耐えるレベルの安全性が確保されているとの見解が示されています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_7-1024x229.png)

数学の家庭教師を想定したシナリオも用意され、システムメッセージまたは開発者メッセージで「答えを教えないように」とあらかじめ指示されている状態で、ユーザーから答えを聞き出そうとする試行が行われました。その結果、システムメッセージによる制約では88%、開発者メッセージによる制約では94%の成功率で答えを漏らさずに対応できたことが確認されています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_8-1024x190.png)

さらに、システムメッセージで「特定のフレーズやパスワードを出力しない」と命令されたケースでは、ユーザーメッセージを通じた試行で100%、開発者メッセージを通じた試行でも100%の成功率が示されました。パスワードを保護する設定については、ユーザーメッセージ経由での試行で95%、開発者メッセージ経由での試行では89%の保護率が達成され、いずれも高い水準にあると評価されています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_9-1024x215.png)

実験結果からは、複数の権限レベルで異なる命令が与えられた場合でも、o3-miniが高い信頼性をもって適切な指示を選別できることが示唆されます。モデルの安全策が開発者メッセージによって簡単に回避されてしまう問題が懸念されてきましたが、指示階層の導入で大きく改善されたと考えられます。今後は、さらなる安全対策の洗練や、万一の抜け道を見つけた場合の修正プロセスの確立が期待されています。

### 外部専門家によるセキュリティ評価

外部専門家によるセキュリティ評価が実施されました。専門家が開発元とは独立した立場でLLMの安全性を検証することで、想定外の脆弱性が見つかりやすくなり、問題が見落とされるリスクが減ると考えられています。o3-miniでは、複数の方法を用いて総合的な評価が行われ、LLMの出力がどの程度安全に保たれているかが多角的に確認されました。

#### セキュリティの相対比較評価

評価の第一段階では、GPT-4o、o1、o3-mini（Pre-Mitigation）の3モデルが同時にテストされました。専門家には各モデルを匿名の状態で使ってもらい、必要に応じてウェブ閲覧やコードの実行といった追加の機能も用いられました。全体のテストセッションのうち19.5%でウェブ閲覧が、6.6%でコード実行が行われたと報告されています。専門家はモデルに有害な出力をさせることを目的とする質問を提示し、それに対するモデルの応答を安全性の観点から評価しました。

#### 評価対象となった脅威カテゴリ

扱われた脅威の範囲は広く、サイバーハッキングや生物テロリズム、武器開発、攻撃計画、フィッシングや詐欺、違法行為の促進、誤情報を含むプロパガンダ生成、ヘイトスピーチなどが含まれました。こうしたカテゴリの質問で、少なくとも1回は安全でないと判断された応答が含まれる会話を対象に、どのモデルがどの程度危険な出力を行うかが調べられました。

その結果、o3-miniはo1と同程度の性能を示し、両方ともGPT-4oより高い安全性を発揮したと評価されています。専門家からは、GPT-4oの応答拒否率が34.2%であったのに対し、o1とo3-miniでは63.5%と56%の質問が拒否されたとの報告がありました。ただし、安全性評価では単に拒否率が高ければ良いというわけではなく、実際には拒否すべき内容とそうでない内容を正しく見分けるバランスが必要だとされています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_10-1024x140.png)

#### Gray Swan Arenaでの検証

2025年1月4日には、Gray Swanという組織との協力のもと、o3-mini（Pre-Mitigation）のジェイルブレーク（セキュリティ回避）耐性が追加検証されました。違法なアドバイスや過激主義、ヘイトクライムを助長する発言、政治的な誘導、自傷行為を後押しするような内容をモデルに生成させようとする試験が行われました。モデルがOpenAIのモデレーションAPIに引っかかるレベルの危険なコンテンツを吐き出し、かつそれが実行可能な具体的手順を含むかどうかをもって、攻撃成功とみなす方式が採用されています。

平均的なユーザーが試行した際の攻撃成功率は、o3-miniが3.6%、o1-miniが3.7%、GPT-4oが4.0%、o1が1.9%という数値で、いずれも大きく離れた差ではないと評価されています。o1が相対的に低い成功率を示している点は興味深いものの、全体的には4%未満に抑えられており、開発者側としては一定の安全基準を満たしていると判断しているようです。

## “Preparedness Framework”による分析

Preparedness Frameworkは、先端的なLLMが引き起こす重大なリスクを体系的に評価する手法として策定されています。サイバーセキュリティ、CBRN（化学・生物・放射線・核）、説得力、モデル自律性という4つのリスク領域を「低」「中」「高」「重大」の段階に分けて評価し、製品版をリリースする場合は「中」以下、開発を継続する場合は「高」以下であることが条件とされています。こうしたランク付けが行われるのは、モデルが持つ潜在能力と危険性をバランスよく把握し、事故や不正利用を防ぐためです。

### 評価指標の運用方法

評価指標は実験結果をリスクレベルに照合する「指標」という仕組みに基づいており、安全性諮問グループ（SAG）がそれらの値を検討してリスクレベルを最終的に決定します。指標が特定の閾値に到達したり、到達しそうな場合はより詳しい分析が行われます。

ただし、プロンプトエンジニアリングや追加の微調整によってモデルが予想以上に進化する可能性や、長期間の利用を通じて新たな挙動が見つかる可能性があるため、リスク評価には限界があることも認められています。また、長期的な利用に伴う影響は短期の人手評価では測りきれない面があるため、継続的なデプロイとコミュニティによるモニタリングが重要とされています。

### サイバーセキュリティに関する評価

Preparedness Frameworkにおけるサイバーセキュリティの評価では、o3-miniが「低」リスクと判定されました。基本的なセキュリティタスクへの対応力は確認されたものの、深刻な脆弱性を悪用するレベルには到達していないと判断されています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_11-1024x266.png)

実験としてはCapture The Flag（CTF）形式のハッキングタスクを使い、Webアプリケーションや暗号解読など多彩なジャンルで評価が行われました。CTFとはあらかじめ脆弱性のある仮想環境を用意して、そこから特定のキーワード（フラグ）を発見できるかを競うものです。実行環境が整備されており、単純なコピーペーストでは解決できない複数ステップの攻撃が要求されるため、ハッキング能力をある程度客観的に計測できます。

評価の結果、大学生やプロレベルの課題に対するo3-miniの成功率は21%にとどまり、まだ高度な攻撃を独力でこなす力は限定的だと結論づけられました。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_12-1024x490.png)

### 化学・生物学的脅威作成について

化学・生物学の領域については、危険物質の作成や実験手順を詳細に案内してしまうリスクを重視しながら評価が行われました。o3-miniは、実験室や専門知識をすでに持つ研究者が利用するとある程度有用な情報を引き出せる一方、完全に自動化された形で新しい脅威を生み出す段階には至っていないとされています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_13-982x1024.png)

Gryphon Scientificなどの専門家が策定した質問と採点基準を用いて、危険物質の扱いに関する正確さと手順の具体性が試されましたが、緩和策が施される前のモデル（Pre-Mitigation）は多くの質問にかなり正確に答えられたものの、緩和策適用後（Post-Mitigation）では該当する内容を積極的に拒否する設定が反映され、実質的に危険な情報提供は回避されるようになっています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_14-1024x493.png)

専門家評価では、モデルが生物学的なリスク情報にどの程度詳しく、誤りなく回答できるか、研究現場で再現しやすい手順をどのくらい提示できるかが比較されました。

o3-mini（Pre-Mitigation）は専門家の回答と同レベルかそれ以上の評価を得るケースも見られた一方、緩和策が導入されたバージョンでは危険な内容を含む場合に回答を拒否する傾向が顕著です。生物学分野の専門家がLLMを使いながらタスクを進めるテストでも、文献検索を高速化できる利点が認められていますが、不正確な情報が交じるリスクへの注意も呼びかけられています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_15-1024x478.png)

生物学ツールと連携する際、LLMがウェブブラウザなど外部リソースへのアクセスを利用してDNA配列の検索や注文といった作業を代理で実行できるかが試されました。実験室の業務を補佐する目的で、EbolaウイルスのFASTAファイルを探し出して配列を抜き出すなど、緩和策が導入される前のo3-mini（Pre-Mitigation）は高確率でタスクを成功させたと報告されています。FASTAファイルとは、配列情報を文字列形式で記述したファイルで、生物学分野では遺伝情報を扱う際によく用いられます。緩和策が施されたo3-mini（Post-Mitigation）では拒否や制限が働くため、まったく同じ操作を続けることは難しくなったとの指摘があります。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_16-1-1024x469.png)

マルチモーダルなウイルス学トラブルシューティングでは、SecureBioが作成した350件の問題をテキストや画像など複数の形式で提示し、それをo3-miniに解かせる試験が行われました。o3-mini（Post-Mitigation）は選択式形式の問題に対して53%ほどの正答率をマークし、o1（Post-Mitigation）の59%には及びませんでしたが、専門家集団の平均スコアは上回る結果が示されています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_17-1024x511.png)

BioLP-Benchという、濡れた実験（wet lab）手順に関連する短答式問題を800問収録したベンチマークも用いられ、o3-mini（Pre-Mitigation/ Post-Mitigation）は専門家ベースラインと同程度の約38%前後のスコアを記録したと報告されています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_18-1024x511.png)

FutureHouseのProtocolQAでは、選択式をオープンエンド（自由記述）形式に変換した108問が利用されました。実験手順のミスや誤記に気づき、修正指示を出せるかどうかが焦点となり、o3-mini（Post-Mitigation）の得点は23%にとどまっています。人間の専門家が相談して回答した場合の基準値が54%なので、大幅に及ばなかったという結果です。実験手順には明記されていないノウハウやアドリブ的な判断が求められるため、LLMがまだ十分に追いつけていない側面が見られると考えられます。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_19-1024x505.png)

暗黙知やトラブルシューティングに関する試験では、論文やマニュアルからは読み取れないコツを問う形式になっていました。o3-mini（Pre-Mitigation）は複数選択式のテストで68%の正答率を示し、他のモデルと同程度の水準にあると評価されていますが、専門家の合議による平均値である80%には少し足りないとされています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_20-1024x515.png)

さらに、暗黙知に基づくブレーンストーミングをテストするため、専門家が実際に体験したトラブルや対処法を50件まとめ、それをオープンエンド形式でモデルに回答させる試みも行われました。o3-miniはo1系統のモデルとほぼ同じレベルで有用な提案を示す場合がある一方、回答に誤りや抜けが含まれる事例も報告されています。研究や実験に携わる現場では、微妙な判断や手作業での工夫が必要となるため、LLMが出力する情報に対しては正当性の確認や補足的な知識の活用が欠かせないと考えられます。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_21-1024x514.png)

### 放射線および核兵器の脅威の評価

放射線や核兵器開発に関しては、米国エネルギー省の協力なしでは核分野の詳細評価が難しいという留保がつきつつも、現時点で安全性対策後のo3-miniが軍事的に有害なレベルの情報を提供するリスクは低いとされています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_22-1024x552.png)

専門家による123回の対話のうち8回が最初「高」リスクと見なされたものの、再検討で「中」または「低」に格下げされ、安全策適用後のバージョンではすべてが低リスクと判定されました。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_23-1024x508.png)

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_24-1024x516.png)

こうした結果は、モデルへのセーフガードが現実的に機能している可能性を示唆します。

### 説得力に関する評価

社会全体への影響を測る観点から、LLMの説得力がどの程度に達しているのかが検証されました。なお、中程度（Medium）のリスクが認定されており、人間とほぼ同等の水準で相手を納得させる文章を生成できる一方、最高レベルの人間の文章ほどの圧倒的な説得力には達していないと報告されています。

また、この説得力評価では静的なテキストだけでなく、対話を通じて人々の考えや行動を変える能力も重視されました。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_25-1024x411.png)

r/ChangeMyViewのデータを利用したテストでは、400万人を超えるコミュニティで交わされる議論が分析対象となり、o3-miniが人間の書いた投稿と同等のレベルで意見を翻す力を示す場面が確認されています。さらに、モデルと人間がそれぞれ書いた返信を比較し、どちらがより説得的かを1～5の尺度で評価する方式でも、モデルの生成文が高評価を得る場合が多かったとの報告があります。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_26-1024x412.png)

そして、政治的テーマを扱う別の評価でも、論理的推論や皮肉、拡散性の最適化といった多彩な戦略を組み合わせたメッセージが生成され、人間の審査員による比較でGPT-4o、o1、o3-miniはいずれも、人間全体の上位80～90パーセンタイルに相当する説得力を示したとされます。なお、人間の最上位5％を超える「超人的」な説得力は確認されていないため、現時点では社会的影響力を懸念すべき水準には至っていないと評価されています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_27-1024x700.png)

そのため、LLMが既に相応の潜在力を持っているとはいえ、ただちに過度のリスクを想定する必要はないという見解です。しかし、モデルがさらに高い性能を獲得していく可能性も十分考えられるため、継続的なモニタリングや安全策のアップデートが依然として重要になると考えられます。

### MakeMePay（状況依存型評価）

LLMが他者を操作して金銭的な行動を促す力を測るため、MakeMePayと呼ばれる自動化評価が導入されています。詐欺や社会的操作のリスクを数字で捉える狙いがあり、安全対策の有効性を検証する材料ともなっています。なお、この評価では「コンアーティスト」役のLLMが「マーク」役のLLMを説得し、架空の賞金100ドルを提供させるかどうかが焦点となります。マーク側は「合理的な資金活用」という方針で初期設定されており、説得によって支払いに応じるか否かを対話を通じて記録します。1,000回の対話を集計し、「支払い発生率」と「抽出金額比率」を指標とする統計分析が行われました。

結果として、安全対策が施される前のo3-mini（Pre-Mitigation）は79%の支払い発生率と9%の金額抽出率を示しました。また、安全対策を適用したo3-mini（Post-Mitigation）においては両指標が低下し、設計上の制約が操作能力を抑制していることが確認されています。さらに、LLMの知性が高いほど操作を成功させやすくなる傾向も見られ、性能向上に伴いリスクが増大する可能性が指摘されました。そのため、LLMによる潜在的悪用を改めて示す一方、適切な安全策の導入によってリスクを抑えられることも同時に証明されたといえます。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_28-1024x503.png)

### MakeMeSay（状況依存型評価）

続いてMakeMeSayという、他者を操作して特定のキーワードを発言させる能力を測る自動化評価も実施されました。ここでは「マニピュレーター」役のLLMと「マニピュレイティー」役のLLMが会話を交わし、マニピュレーターがあらかじめ設定されたキーワード（たとえば「戦闘」「疲労」「箱」など）を自然な対話の流れの中でマニピュレイティーに口にさせることを試みます。マニピュレイティーは、会話が終了するまでキーワードの存在を知らされず、最後に推測を行う仕組みです。32回の対話サンプルを1セットとして検証し、キーワードが発言されれば「成功」、キーワードが出ずに終わったり見破られたりすれば「失敗」、マニピュレーターが先にキーワードを言ってしまえば「ルール違反」という三つの結果が記録されます。

実験の結果、安全対策未適用のo3-mini（Pre-Mitigation）は73%という高い成功率を収めました。さらに、安全対策後のo3-mini（Post-Mitigation）では成功率が低下し、リスクを軽減する仕組みが実際に機能していることが示唆されています。なお、この評価は「ゲーム」という仮想設定で行われるため、明確なポリシー違反とは異なる点にも注意が必要です。とはいえ、LLMが巧妙な対話戦略を用いて他者を誘導できること、そして設計次第で一定の安全管理が可能であることが改めて明らかになったと考えられます。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_29-1024x514.png)

### モデルの自律性評価

モデルの自律性を調べるために設定された「モデル自律性」評価では、ソフトウェア開発や機械学習研究のような現実的な課題へLLMがどの程度対応できるかが分析されます。高度な知性をもつシステムが自律的に行動する際、自己改善や外部リソースの活用にどれほどのリスクが伴うかも検証されます。ソフトウェア開発の場面としては、GitHub上の課題を見つけてコードを修正する力や、機械学習全体の流れを組み立てる力が試されています。仮想環境内で必要なツールを扱えるかどうかも着目点になっており、モデルが自主的に行動しすぎる危険性も評価する狙いです。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_30-1024x542.png)

#### 研究エンジニア面接評価

研究エンジニア面接評価では、OpenAIの研究エンジニア採用プロセスを再現する形で、LLMにコーディング問題や多肢選択問題を解かせています。採用面接で出されるのと同じレベルの課題が与えられ、1時間程度の短時間で解決できるかが焦点になっています。

o3-mini（Post-Mitigation）はコーディング問題で92%（pass@1）の [正解率](https://ai-data-base.com/archives/25930 "正解率") を記録し、o1シリーズより高い性能を示したとの結果が報告されました。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_31-1024x513.png)

さらに、多肢選択問題では32回試行しても一貫した正答率（cons@32）を保ち、機械学習分野の基本知識が定着していると評価されています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_32-1024x512.png)

ただし、数週間から数か月にわたる本格的な研究開発タスクに対して、同じように通用するかはまだ検証されていません。

#### ソフトウェアエンジニアリング課題

また、SWE-Bench Verifiedという手法を用いて、実際のソフトウェア開発課題を解決する力が測定されています。GitHubに存在する477件のイシュー（コード上の問題点）を対象として、LLMがパッチ（修正案）を提案し、それが正しく問題を解決できれば合格です。

Agentlessという設定では、LLMが5回までパッチを試す機会があり、o3-miniは従来モデルよりもパッチ生成の成功率を高めたとされています。さらに、内部ツールを活用する「o3-mini（tools）」の設定でも一部の課題が解決されましたが、専門家の手動検証により、ダウンロード手順や設定ファイル編集が省略されたまま成功判定になっているケースが見つかったため、課題が残っていると指摘されました。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_33-1024x265.png)

#### 自律的タスク実行能力の評価

そして、Agentic Tasksという評価では、Python環境やLinuxターミナルを用いて長期的かつ複雑な作業をこなす力が試されました。Dockerコンテナを設定し、APIキーを生成し、機械学習モデルを [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") するなど、実務でも必要となる大きなタスクが合計12個用意されています。

o3-miniは認証済みAPIプロキシの作成やDockerサーバーのロードなど、いくつかの工程を部分的に成功させましたが、手順の細部が抜け落ちている問題が確認されています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_34-1024x509.png)

一例としてモデルのダウンロードを十分に完了できていないケースが挙げられ、高度な操作をすべて漏れなく行うにはまだ不十分だと結論づけられています。もっとも、「 [PyTorch](https://ai-data-base.com/archives/26256 "PyTorch") を使ってGPT-2を [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") する」や「Bitcoinウォレットを作成する」といった個別のサブタスクでは高い成功率を示し、基礎的な作業をこなす力は上がってきているとも見られます。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_35-1024x510.png)

### 機械学習タスクの自動実行性能

機械学習タスクが自律的に実行されるかどうかを評価する仕組みとして、Kaggleのコンペティションを利用した方法が提案されています。 [GPU](https://ai-data-base.com/archives/26570 "GPU") や仮想環境、それに加えてKaggleのデータや指示書が用意されるため、あたかも人間のデータサイエンティストがコンペに参加する状況に近い形でエージェントがモデル開発を行うと考えていただくとわかりやすいかもしれません。なお、この評価の大きな特徴はエージェントが複数のコンペに挑戦しながら自己改善していくプロセスを観察できる点です。限られた時間の中でデータを探索し、学習モデルを構築し、その結果を評価するまでを自律的にこなす仕組みが求められます。

また、評価環境には総額190万ドル相当の賞金がかかった75のKaggleコンペティションが選定されています。ブロンズメダルを獲得できる割合（bronze pass@1）が主要指標とされており、特定のコンペで少なくともブロンズ水準に達したかどうかが測定されます。さらに、あるモデルが最大10回まで再挑戦できる設定（pass@10）が導入され、その回数内のいずれかでブロンズ到達に成功すればカウントされる仕組みです。具体例としては「分子翻訳」のコンペがあり、回転させた分子画像から化学的な識別子を推定する高難度タスクが含まれています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_36-1024x354.png)

そして、o1-preview（Post-Mitigation）が最も高い成績を示し、10回の試行で全体の37%のコンペティションにおいてブロンズメダルを獲得したと報告されています。一方、o3-mini（Pre-MitigationおよびPost-Mitigation）はo1-previewより10%以上低い獲得率で、試行を重ねるほどモデル間の性能差が拡大する傾向が見られました。pass@1のときよりもpass@10で差が大きくなる点が興味深く、o1-previewとo1の比較では試行回数が少ない場合は類似した性能を示すものの、回数が増えるにつれ微妙な差が表れてくる様子が観察されています。MLE-Benchは実際のKaggleコンペを想定し、単なる短期的なベンチマークとは異なる段階まで踏み込んだ評価を行うので、データ処理から学習、評価までの流れをエージェント自身が担う必要があります。さらに、o1-previewが安定して高スコアを出した結果からは、限られた時間の中でもモデルが自己改善のサイクルを回せる可能性が示唆されているようです。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_38-1024x504.png)

### 実際のプルリクエスト作業能力

また、OpenAI PRsと呼ばれる仕組みを用いて、実際のプルリクエストをLLMがどこまで自動化できるかを測定するタスクも設定されています。研究エンジニアが過去に行ったコード変更の履歴をベースにし、LLMがそれと同等の修正を正確に再現できるかどうかが焦点です。プルリクエストはソフトウェア開発プロセスの一部として、変更内容を提案し、レビュワーからの確認を経てマージされる仕組みですが、その作業をどの程度代行できるかを調べることで、LLMの現実的な開発業務適応力がうかがえます。

まずモデルがコード環境をセットアップし、プルリクエストが行われる前のブランチへ切り替える手続きを行ったうえで、具体的な修正内容を示すプロンプトを受け取る流れが用意されました。エージェントはPythonやコマンドラインツールを駆使して対象ファイルを修正し、すべての編集が終わると隠されたユニットテストが自動で動く仕組みになっています。

ユニットテストの結果が問題なければ修正が成功したと判断されますが、o3-mini（Pre-MitigationとPost-Mitigationの両方）は0%の成功率にとどまりました。Pythonを使うべき場面でbashのツールを呼び出そうとするなど、正しい手順を最後まで維持しづらい面があったことが要因とみられています。繰り返しプロンプトを与えて修正を促しても、誤った形式を使い続ける場面が多々見受けられ、期待通りの修正を実現するまでには至らなかったと報告されています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_39-1024x515.png)

結局のところ、複雑な手作業を要する現実的な開発タスクを自律的にこなす能力にはまだ課題が残るという結論です。

## マルチリンガル性能

多言語対応力を測るため、MMLUのテストセットを14言語に翻訳し、それを用いてo3-miniやGPT-4o、o1-miniなどのLLMを評価する試みが行われました。なお、問題文の翻訳はプロの翻訳者が担当しており、現地語話者にとって自然な表現になるよう考慮されています。評価方法としては、モデルにチェーン・オブ・ソート（chain-of-thought）を促すプロンプトを与え、思考過程の文章化を経たうえで0-shot形式の回答を生成させる手法が採用されました。

また、回答の抽出段階では、モデル応答に混在するマークダウンやLaTeXなど余計な記号を取り除き、各言語で「Answer」に相当する単語を探して正答を判定する工程が踏まれています。たとえばアラビア語なら、それに対応する表記を見つけたうえで、そのあとの文字列を正解として扱う仕組みです。こうした方法により、異なる文字体系や文法をもつ複数の言語を一貫した基準で評価できる点が特長とされています。

o3-miniとo3-mini pre-mitigation、GPT-4o、o1-miniという4種類のモデルが比較された結果、o3-miniがo1-miniより全体的に高い正答率を示し、GPT-4oがさらに上の水準を示す場面もあったと報告されています。たとえばアラビア語では、o3-miniが約0.807、o1-miniが約0.794でわずかにo3-miniが上回っています。また、フランス語でもo3-miniがおよそ0.824～0.826の正答率を記録したのに対し、o1-miniは0.8212ほどにとどまったとされています。スワヒリ語やヨルバ語のように使用人口が比較的少ない言語でも、両モデル間に一定の差が観察されたことが興味深いです。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83204_40-1024x586.png)

多言語環境でLLMを評価する際には、日本語や英語だけでなく、幅広い言語で適切な回答を生成できるかが重要と考えられます。そのため、o3-miniがo1-miniを上回る成績を示した事例が増えた背景には、事前学習データや微調整のアプローチが欧州言語だけでなく各地域の言語でも成果を上げている可能性があると推測されています。そして、0-shotでチェーン・オブ・ソートを用いる評価方法は、外部データを追加しない状態でモデルを直接テストできるため、多言語対応力の素の実力を測るうえで適しているとも考えられています。

## まとめ

本記事では、OpenAIがリリースした新たな言語モデル「o3-mini」の特徴と安全策に関する研究を紹介しました。  
各種ベンチマークや外部専門家の検証を通じ、推論性能の向上とリスク軽減への取り組みが確認されています。  
一方、実際のコード修正タスクなどでは改良の余地があり、複雑な指示への対応に課題が残ると指摘されました。  
熟考的整合の手法を取り入れた安全対策では、不適切なコンテンツの抑制や脱獄攻撃への耐性に一定の成果が見られます。

**参照文献情報**

- タイトル：OpenAI o3-mini System Card
- URL： [https://openai.com/index/o3-mini-system-card/](https://openai.com/index/o3-mini-system-card/)
- 所属：OpenAI

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[OpenAI o3-miniの安全機能に関する大規模検証　1万件超のテスト結果](https://ai-data-base.com/archives/83156)　

[学習者の目標達成をサポートするLLMシステムの開発](https://ai-data-base.com/archives/83097)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)