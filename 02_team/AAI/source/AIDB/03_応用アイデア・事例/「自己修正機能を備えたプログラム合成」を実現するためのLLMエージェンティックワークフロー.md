---
title: "「自己修正機能を備えたプログラム合成」を実現するためのLLMエージェンティックワークフロー"
source: "https://ai-data-base.com/archives/83400"
author:
  - "[[AIDB Research]]"
published: 2025-02-05
created: 2025-06-13
description: "本記事では、プログラム合成の最新手法を紹介します。プログラム合成は1940年代から続く研究課題で、最近はLLMを活用したアプローチが主流となっています。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、プログラム合成の最新手法を紹介します。プログラム合成は1940年代から続く研究課題で、最近はLLMを活用したアプローチが主流となっています。これまでに考案されてきた手法では計算コストの増大や品質管理の課題が指摘されていましたが、今回、複数のLLMエージェントを協調させる新しい手法によって課題を克服する道筋が見えてきました。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83400-1024x576.png)

**発表者情報**

- 研究者：Yaojie Hu et al.
- 研究機関：アイオワ州立大学, アマゾン ウェブ サービス, カリフォルニア大学アーバイン校

論文情報詳細は記事の下部に記載されています。

**本記事の関連研究**

- [ノーコードでLLMマルチエージェントを操る『AUTOGEN STUDIO』Microsoftが新開発](https://ai-data-base.com/archives/75716)
- [スクショからHTMLとCSSのコードをLLMが生成する『Design2Code』タスク、プロンプト手法やファインチューニングで高い性能を確認](https://ai-data-base.com/archives/65294)
- [GPT-4などLLMのコード生成能力にデバッグ機能を追加する『SELF-DEBUGGING（セルフデバッギング）』と実行プロンプト](https://ai-data-base.com/archives/57709)

## 背景

プログラム合成は1940年代から続く長年の研究課題で、人工知能技術の発展により実用化への期待が高まっています。現在はLLMを用いた手法が注目を集めています。

基本的なプログラム合成では、自然言語による問題記述とユニットテストが与えられ、それらの要件を満たすプログラムを生成することが求められます。MBPPやHumanEvalといったベンチマークが整備され、多くの研究機関で評価に使用されています。

LLMを用いたプログラム合成では、モデルの規模を大きくすることで性能向上が図られてきましたが、計算コストが膨大になる課題があります。しかし最近では、複数のLLMエージェントを組み合わせて協調動作させる「エージェンティックワークフロー」というアプローチが登場し、追加学習なしで性能向上を実現できる可能性が示されています。

既存のプログラム合成手法には3つの問題があります。1つ目は、生成されたコードが与えられたテストに従うことを暗黙的に仮定している点です。2つ目は、生成されたテストの品質がボトルネックとなっている点です。3つ目は、自己デバッグの過程で誤った方向に進んでしまう可能性がある点です。

アイオワ州立大学やAWSなどの研究者らはこれらの問題に取り組むため、LLMによる品質チェック機能を備えた新しいエージェンティックワークフローの開発に取り組みました。品質チェッカーがワークフローを制御し、正しいプログラムを選別しながら段階的に改善を進める手法です。

以下で詳しく紹介します。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83400_1.png)

MBPPコード生成ベンチマークの問題例とQuality Checkerの役割。生成コードがユニットテストに適合するかを明示的に検証。

## 複数のLLMエージェントが協力しながらプログラムを生成・改善していくシステム「QualityFlow」の提案

プログラム合成では、”自然言語で書かれた問題文”と”ユニットテスト”が入力として与えられます。今回の提案手法「QualityFlow」は、それらをもとに正しいプログラムを生成することを目指したシステムです。

QualityFlowは以下のような複数のエージェントから構成されます。

- プログラム生成を担当する「コードジェネレーター」
- テストケースを設計する「テストデザイナー」
- 自己デバッグを行う「セルフデバッガー」
- 全体の制御を担う「品質チェッカー」
- 問題文の再解釈と説明を行う「問題クラリファイア」

下の図は構成の全容です。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83400_2-1024x566.png)

QualityFlowの全体的なエージェントワークフロー。コード生成、テスト設計、セルフデバッグ、品質チェックの関係を図示。

#### 全体的な処理の流れ

最初にコードジェネレーターが初期プログラムを生成し、品質チェッカーがその品質を評価します。品質が十分であれば、そのプログラムが最終出力として採用されます。品質が不十分な場合、テストデザイナーが追加のテストケースを生成し、セルフデバッガーがプログラムの改善を試みます。

改善されたプログラムは再度品質チェッカーによって評価され、品質が確認されるまでこのプロセスが繰り返されます。品質チェッカーが複数回にわたって不合格と判定した場合は、問題文の解釈に誤りがあった可能性を考慮し、「問題クラリファイア」と呼ばれるエージェントが問題文の再解釈を行います。

最終的に品質の高いプログラムが得られない場合は、最初に生成されたプログラムに立ち返る仕組みが考えられています。

下記はこの流れを表現したアルゴリズムです。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83400_3.png)

処理の流れをアルゴリズムで表現

以下ではQualityFlowの各エージェントの詳しい役割を見ていきます。

### プログラムジェネレーター

QualityFlowの最初のステップを担うプログラムジェネレーターは、問題文とテストケースから最初のプログラム案を生成します。LLMに問題文を入力し、プログラムコードを出力させる基本的な処理です。追加の文脈情報は与えられず、問題文とテストケースのみを参照してプログラムを生成します。

### テストデザイナー

プログラムの品質を高めるために不可欠なのがテストケースです。テストデザイナーは、最大50個の新しいテストケースを生成します。

今回は5種類の異なる指示（プロンプト）を用いて、各10個のテストケースを生成することで検証され、実験結果から、エッジケース（極端なケース）よりも一般的なケースのテストの方が最終的なプログラムの品質向上に効果的だということが分かりました。

### セルフデバッガー

生成されたプログラムの改善を担当するのがセルフデバッガーです。セルフデバッガーにおける処理の流れは以下のようになります。

1. まず、問題文、現在のプログラム、生成されたテストケースを入力として受け取ります
2. Pythonインタープリターを使って、プログラムに対してテストケースを実行します
3. テスト失敗時のエラーメッセージや、プログラムの実際の出力結果を収集します
4. LLMが段階的に思考しながら（Chain-of-Thought方式）、なぜテストが失敗したのかを分析します
5. 分析結果に基づいてプログラムを修正します

セルフデバッガーは、単純に新しいプログラムを生成するのではなく、既存のプログラムの問題点を理解し、的確な修正を加えることを目指します。テストケースの実行結果を基に、プログラムの振る舞いを論理的に分析し、一貫性のある改善を行います。

### 品質チェッカー

そしてプログラム生成の各段階で重要な役割を果たすのが品質チェッカーです。生成されたプログラムを評価し、次のステップへ進むかどうかを判断します。

品質チェッカーは通常のプログラム実行とは異なる方法でプログラムの正しさを判定します。MBPPなど一部のベンチマークでは、評価用のテストケースを直接実行することが禁止されているためです。そこで「想像実行」と呼ばれる手法が採用されました。

想像実行は、LLMが人間のように段階的に思考しながらプログラムの実行結果を予測する手法です。テストケースの入力値に対して、プログラムがどのような出力を返すか、ステップバイプステップで推論していきます。予測された出力が期待される結果と一致した場合のみ、プログラムは正しいと判断されます。

生成されたテストケースの品質も重要な評価対象となります。不適切なテストケースによってプログラムが誤った方向に修正されることを防ぐため、テストケースそのものの正しさも検証されます。テスト品質チェッカーは問題文のみを参照してテストケースの妥当性を判断します。

### 問題クラリファイア

プログラミングの問題文は、しばしば曖昧さや解釈の余地を含んでいます。そこで、プログラムの修正が何度も失敗した場合に問題クラリファイアが起動されます。

問題クラリファイアは単に問題文を再確認するだけでなく、テストケースの生成過程や品質チェックの結果、デバッグの履歴など、それまでの全ての情報を考慮に入れます。最初の問題解釈で見落とされていた点を発見し、異なる視点からプログラム生成を試みる機会を提供します。

修正されたプログラムは再度品質チェッカーによって評価されます。合格すれば採用され、不合格の場合は最初に生成されたプログラムに戻ります。

実験結果から、品質チェッカーは98%以上の高い精度で判定を行えることが確認されており、正しいプログラムを確実に選別できます。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83400_image2-416x1024.png)

QualityFlowにおける処理の流れ（AIDBが作成）

以上のようにQualityFlowでは、品質チェッカーの高精度な判定能力を活かし、「多様な生成戦略」というアプローチを導入しています。同じ問題に対して微妙に異なる複数の指示（プロンプト）を用意し、並行してプログラム生成を試みます。例えば今回の実験では、6つの異なる指示でプログラムを生成し、それぞれについて3回のデバッグを行います。

従来の「自己一貫性」アプローチが同じ指示で複数回の生成を試みるのに対し、この手法は多様な生成戦略は意図的に異なる指示を用います。生成されるプログラムの多様性を高めることで、正しい解に到達する可能性を最大化します。実験では、MBPPベンチマークで4%の性能向上が確認されました。

各エージェントの中でも、品質チェッカーが高精度な判定を行えることが、多様な生成戦略の成功の鍵となっています。様々なアプローチで生成されたプログラムの中から、確実に正しいものを選び出すことができるためです。

## 実験結果

QualityFlowの性能評価では、プログラミング分野で標準的な「pass@k」という指標が使用されました。与えられた問題に対してk個のプログラムを生成し、そのうち1つでも全てのテストをパスすれば成功とみなされます。特にpass@1（1回の試行での成功率）は、実用的な観点から重要な指標とされています。

プログラムの正しさは、完全な一致ではなく機能的な同等性で判断されます。つまり、コードの書き方が異なっていても、同じ入力に対して同じ出力を返せば正解とみなされます。

多様な生成戦略の実装では、各プログラミング問題に対して6つの異なる指示を用意し、並行してプログラムを生成します。生成されたプログラムが品質チェックを通過しない場合、3回のデバッグサイクルが試みられます。テストケースの生成においては、温度パラメータ（ランダム性の度合い）を0.1に設定し、多様なテストケースが生成されるようにしています。

評価には主に4つのベンチマーク（MBPP、MBPP-EvalPlus、HumanEval、HumanEval-EvalPlus）が使用されました。MBPPは評価用テストケースの直接実行が禁止されているため、想像実行による品質チェックの有効性を検証するのに適しています。

QualityFlowの評価では、同一のLLM（Claude）を使用した場合の比較を重視し、手法自体の優位性を公平に検証することが心がけられました。

### プログラム合成性能向上を達成

実験結果から、QualityFlowは既存手法と比較して優れた性能を示しました。MBPPベンチマークでは94.2%のpass@1精度を達成し、従来の最高性能を4.8%上回りました。HumanEvalでも98.8%という高い精度を記録しています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83400_4.png)

MBPPとHumanEvalにおけるQualityFlowの各ステップのpass@1精度。逐次的な精度向上を確認。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83400_5.png)

QualityFlowの最終的なpass@1精度と従来のSOTA手法との比較。全ベンチマークで新記録を達成。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83400_6.png)

MBPPベンチマークにおけるQualityFlowのpass@k性能。最高のパフォーマンスを記録。

### 品質チェッカーの有効性を確認

品質チェッカーの性能は、プログラムが正しいかどうかを判定する2値分類タスクとして評価されました。MBPPでの実験では、98%という高い精度で正しいプログラムを識別できることが示されました。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83400_7.png)

Code Quality Checkerの混同行列データ。高精度でコードの正誤を分類可能であることを示す。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83400_8-1024x623.png)

MBPPおよびHumanEvalベンチマークの各ステップでのpass@1精度の変遷。Code Quality Checkerがパフォーマンス向上に貢献。

実験では「想像実行」と「単純なYes/No判定」を比較しています。単純な判定では正しくないプログラムも合格としてしまう傾向が見られましたが、想像実行では段階的な思考プロセスを経ることで、より信頼性の高い判定が可能になりました。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83400_9-1024x169.png)

Imagined Executionと単純なYes/No分類モデルの比較。QualityFlowの手法がより正確であることを確認。

全体として、品質チェッカーの導入により、プログラム合成の精度が向上しただけでなく、システムの振る舞いがより制御しやすくなったことが実験から明らかになりました。

### テスト品質チェッカーのさらなる効果検証

実験では、生成されたテストケースの品質が重要な課題として浮かび上がりました。MBPPベンチマークでは、生成されたテストケースの約62%が不適切であることが判明しています。テスト品質チェッカーは、不適切なテストケースの約80%を検出することに成功しました。

テスト品質チェッカーの評価は、より難しい問題に焦点を当てて行われました。品質チェッカーが不合格としたプログラムや、実行時にエラーを引き起こしたテストケースを対象としています。評価結果は使用するLLMの性能に依存する傾向が見られ、より高性能なLLM（Sonnet）では全体の性能が0.8%向上しましたが、比較的性能の低いLLM（Opus）では逆効果となる場合もありました。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83400_10-1024x461.png)

Test Quality Checkerが不正確なテストを識別する精度。MBPPで約80%の誤ったテストを除去可能であることを報告。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83400_11-1-1024x536.png)

Test Quality Checkerをワークフローから削除した場合のpass@1精度の変化。LLMの種類によって影響が異なることを確認。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83400_12-1024x629.png)

Test Quality Checkerを導入しない場合のpass@1精度の変化。MBPPで1%の精度改善を確認。

### QualityFlow全体の有効性を追加実験で検証

QualityFlowの有効性を確認するため、複数の追加実験が実施されました。

例えば、DeepSeek（おそらく古いバージョン）をQualityFlowのフレームワークに組み込んで比較実験を行いました。結果として、QualityFlowの手法を適用することで、DeepSeekの性能も向上することが確認されました。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83400_13.png)

DeepSeek LLMをQualityFlowに統合した場合の精度測定。

システムから問題クラリファイアやリバート機能を除外した実験も行われました。HumanEvalベンチマークでは、これらの機能を除外すると性能が0.78%から2.44%低下することが分かりました。機能の有効性が数値で裏付けられています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83400_14.png)

ClarifierおよびRevertメカニズムを削除した場合の影響測定。それぞれの要素がパフォーマンス向上に貢献していることを確認。

実験結果全体を通じて、プログラム合成における品質管理の重要性が明らかになりました。特に、テストケースの品質管理は今後さらなる改善の余地があることも示唆されています。

## 本研究の妥当性について

### 実験結果は一般化できるか

公開されているAnthropicのClaudeモデルと公開ベンチマークデータセットを用いて実験が行われました。多様なプログラミング問題が含まれるデータセットが使用されましたが、他のデータセットでも同様の結果が得られるか検証する必要があります。複数のベンチマークで評価を行うことで、より広い適用可能性が確認されました。

### 実装の信頼性について

実験を容易にするためのツールやスクリプトが開発されましたが、バグが含まれる可能性は否定できません。これらの課題に対しては、厳密なコードレビューが実施され、実験が複数回繰り返されて一貫性が確認されました。コードは公開され、誰でも検証できる状態となっています。

### 評価指標は妥当性か

標準的なpass@k基準を用いて生成されたプログラムの正確性が測定されました。全てのテストケースに合格した場合、プログラムは正しいと判断されます。pass@1指標はプログラムの機能的な正確性を示しますが、有限個のテストケースではエッジケースを完全にカバーできない制約があります。ただし、この指標は広く使用されており、本研究の性能評価には適切であると判断されています。

## まとめ

本記事では、LLMを用いたプログラム合成の精度を向上させるQualityFlow研究を紹介しました。

品質チェッカーという新しい要素を導入することで、プログラムの正確性を確認しながら生成を進められる手法が提案されています。

なお、プログラム合成以外の分野でも応用できる可能性もありそうですね。

**参照文献情報**

- タイトル：QualityFlow: An Agentic Workflow for Program Synthesis Controlled by LLM Quality Checks
- URL： [https://arxiv.org/abs/2501.17167](https://arxiv.org/abs/2501.17167)
- 著者：Yaojie Hu, Qiang Zhou, Qihong Chen, Xiaopeng Li, Linbo Liu, Dejiao Zhang, Amit Kachroo, Talha Oz, Omer Tripp
- 所属：Iowa State University, Amazon Web Services, University of California, Irvine

## 理解度クイズ（β版）

1\. QualityFlowシステムの主な目的は何ですか？

QualityFlowは品質チェッカーを用いてプログラム合成の正確性を向上させるシステムです。複数のLLMエージェントが協調して動作し、プログラムの品質を段階的に改善します。

解説を見る

2\. 品質チェッカーが「想像実行」を採用する理由は何ですか？

MBPPなどのベンチマークでは評価用テストケースの直接実行が禁止されています。LLMが人間のように段階的に思考しながらプログラムの実行結果を予測する「想像実行」が代替手段として採用されました。

解説を見る

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[学習者の目標達成をサポートするLLMシステムの開発](https://ai-data-base.com/archives/83097)

[LLMを活用した「Text to CAD」 テキスト指示から高品質な3Dモデルを作成する](https://ai-data-base.com/archives/83475)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)