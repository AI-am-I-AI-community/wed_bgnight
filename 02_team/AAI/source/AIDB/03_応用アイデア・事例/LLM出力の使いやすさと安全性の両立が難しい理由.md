---
title: "LLM出力の使いやすさと安全性の両立が難しい理由"
source: "https://ai-data-base.com/archives/88574"
author:
  - "[[AIDB Research]]"
published: 2025-04-22
created: 2025-06-13
description: "本記事では、LLMの出力を安全に制御するための「ガードレール」の限界と構造的な課題を分析した研究を紹介します。安全性・有用性・使いやすさの3つをどのように両立させるかは、実運用に直結する関心事です。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、LLMの出力を安全に制御するための「ガードレール」の限界と構造的な課題を分析した研究を紹介します。

安全性・有用性・使いやすさの3つをどのように両立させるかは、実運用に直結する関心事です。本研究は、そのバランスの取り方に対して経験的な知見を提供しています。

LLMを業務に取り入れようとする際の判断材料として、参考になる内容です。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_88574-1024x576.png)

## 背景

LLMが広く使われるようになる中で、その出力を安全に保つための「ガードレール」が欠かせない仕組みとして注目されています。ガードレールとは、AIの出力をチェックして、不適切な内容を防ぐためのシステムです。

ただ、ガードレールを導入するときには、必ず何かしらのバランス調整が必要になります。たとえば、安全性を重視しすぎると、ユーザーにとって使いにくくなってしまったり、逆に柔軟な仕組みにすると、悪意ある攻撃に対して弱くなったりします。実際には問題のない内容でも、有害な表現に似ているという理由で止められてしまう「疑似有害」といったケースもあります。

加えて、プロンプトインジェクションといった攻撃、さらには生成される文章の多様さそのものが、ガードレールの設計をより難しくしています。

こうした背景を受けて、研究チームは「完璧なガードレールは存在しない」という前提に立ち、どこにどのようなトレードオフがあるのかを見極めるための評価を行いました。安全性、使いやすさ、柔軟性といった要素のバランスをどう取るかを測る取り組みです。

以下で詳しく説明します。

## ガードレールの基礎概念

上述の通り、LLMの活用が進む中で、出力内容の安全性をどう確保するかは大きな関心事になっています。以下では、ガードレールの基本的な仕組みと、それがどのように評価されるかを紹介します。

### ガードレールとは何か

LLMは、入力プロンプトに対して確率的に出力を生成する仕組みです。通常は自然な応答や有用な情報を返しますが、まれに事実と異なることを言ったり、不適切な表現を含むこともあります。こうしたリスクを抑えるために導入されるのが、ガードレールと呼ばれる出力チェックの仕組みです。

モデルの出力に対してガードレールを通すことで、最終的にユーザーに届ける内容が適切かどうかを判断できます。このとき、内容がポリシーに沿っていれば通過し、問題があれば拒否または修正されます。ガードレールは、 [分類器](https://ai-data-base.com/archives/26489 "分類器") を使ったチェックや、LLMによる推論評価など、さまざまな形で実装されます。

### ガードレールが持つ3つの評価軸

ガードレールの性能は、次の3つの観点で評価されます。

（１） **残存リスク**  
本来はブロックすべき有害な出力がそのまま通ってしまう可能性を指します。完全に防ぐことは難しく、自然言語の曖昧さがこのリスクを高めます。

（２） **有用性の損失**  
フィルタリングが厳しすぎると、正確で有益な出力まで削られてしまう恐れがあります。情報の削減によって、本来得られたはずの価値が失われることがあります。

（３） **使いやすさの低下**  
良性の出力が誤って拒否されてしまう（偽陽性）ケースや、ガードレールによる処理の遅延が生じることで、ユーザー体験が損なわれることもあります。

### 代表的な3つのガードレール方式

研究チームは、現実に広く使われている以下の3種類のガードレール方式を対象に評価を行いました。

（１） **プロバイダー型**  
OpenAIやAmazonなどのAPIサービスが提供する、安全性レイヤーです。ブラックボックス的な設計のため中身は見えませんが、効率的で幅広い用途に対応しています。ただし、文脈への細やかな対応には弱さがあります。

（２） **BERTベースの [分類器](https://ai-data-base.com/archives/26489 "分類器") 型**  
害があるかどうかを判定する [分類器](https://ai-data-base.com/archives/26489 "分類器") を使った方式で、シンプルで高速です。ただ、言い回しの工夫や敵対的な表現には対応しきれないことがあります。

（３） **LLMベース型**  
モデレーターのようにLLM自身が出力を評価する方式です。柔軟で強力ですが、プロンプトの設計に大きく左右されやすいという特性があります。

### ガードレールに「無料のランチ」はない

ガードレールの設計には、理想的な解決策は存在しません。悪意あるプロンプトや、難読化された表現による攻撃はガードレールの回避を狙ってきます。また、医療や法律のように、言葉づかいが一見センシティブに見える正当なコンテンツが誤ってブロックされることもあります。

こうした現実をふまえ、研究チームは「ガードレールには無料のランチ（＝全てを両立する解）はない」という仮説を立てました。つまり、安全性を高めようとすると、どうしても有用性や使いやすさに影響が出てしまうという考え方です。

なお、「無料のランチ」はない（”No Free Lunch”）という概念は経済学発祥で、何かを得るには何かを捨てなければいけないという意味です。機械学習分野ではこれまでも何度か引用されている概念です。

## 研究方法

研究者らは、「安全性・有用性・使いやすさのすべてを同時に満たす完璧なガードレールは存在しない」という仮説を、実際のデータに基づいて検証しました。つまり、ある面を強化すれば他の面にしわ寄せがくるという構造が本当に存在するのかを確かめることが目的です。

そのために、複数のガードレール方式を用意し、さまざまな入力に対してどのように反応するかを比較しました。とくに、攻撃的な入力への対応、役立つ回答の維持、ユーザーにとっての負担といった3つの観点から、各方式のふるまいを丁寧に評価しています。

### 評価のためのデータセット構築

まず、安全性や使いやすさの観点からガードレールの特性を測るために、2種類の専用データセットが用意されました。

#### 1\. 敵対的シナリオの評価（Dattack）

こちらは、有害な出力を引き出そうとする、いわゆる「ジェイルブレイク」入力の集まりです。たとえば、

- 明らかにポリシー違反の回答を引き出そうとするストレートな質問
- 有害と判断されにくい言い回しや符号化された表現を使ってルールの裏をかく攻撃
- モデルの安全設計そのものを意図的に上書きしようとするプロンプトインジェクション

といった、巧妙で多様な手法が含まれています。

このデータセットは、既存の攻撃系データセットから幅広く抽出されており、さらに長文脈攻撃への耐性テスト、最先端ジェイルブレイクの再現実験といった以下の2つの新しいテストケースも追加されています。  
長文脈攻撃への耐性テストは、重要な指示が数千トークンの中に埋め込まれた状態でモデルがどのように動作するかを調べるものです。文脈の長さによってガードレールが効かなくなる問題に注目しています。

最先端ジェイルブレイクの再現実験はClaudeなどの主要なモデルに対して実際に成功した攻撃プロンプトを再利用し、ガードレールが本当に突破されてしまうのかを検証しています。

#### 2\. 実用性と使いやすさの評価（Dutility+usability）

こちらは、有用な回答をどれだけ保てるか、安全なはずの入力が誤ってブロックされてしまわないか、といった「日常使いのしやすさ」を見るためのデータセットです。

この中には、2つのタイプの入力が含まれています。

（１） **知識や推論を必要とする高度なタスク**  
たとえば、複数の情報をつなげて答えるマルチホップ推論や、プログラミング系の課題、複雑な質問応答など。これらを通じて、ガードレールが役に立つ回答まで削っていないかを確認します。

（２） **「疑似有害」な入力**  
ヘルスケアや法律、社会的な話題など、実際には有益で無害な内容でありながら、キーワードや言い回しが「有害っぽく」見えてしまうようなプロンプトです。ここで誤ってブロックが起きると、ユーザーの体験が大きく損なわれます。

### 評価の進め方

各入力について、まずベースモデルが回答を生成し、その出力を複数のガードレールに通します。そこで、

- ガードレールがそれを受け入れるか拒否するか
- 回答の質がどう変化したか（有用性の評価）
- 処理にかかった時間（レイテンシー）

を記録し、評価のためのデータを蓄積していきます。

LLMを使ったガードレール（GLLM）については、どんな指示（プロンプト）で評価をさせるかによって結果が変わってくるため、3パターンを比較しています。

1. 単純な安全チェックのプロンプト
2. 危険性のカテゴリや判断基準を明記した詳細なプロンプト
3. 判断前に一度考えさせるChain-of-Thought型の推論付きプロンプト

### 評価指標

評価では主に2つの軸が使われています。

（１） **重み付き [F1スコア](https://ai-data-base.com/archives/26112 "F1スコア（F値）")**  
有害な出力を見逃さずに止められたか（ [再現率](https://ai-data-base.com/archives/26095 "再現率") ）と、正しい出力を誤って止めなかったか（精度）をバランスよく見る指標です。クラスの偏り（良性な入力が多い）を考慮して、実運用に即した重み付けがされています。

（２） **レイテンシー（処理遅延）**  
モデレーション処理がどの程度追加の時間を必要としたかを測定します。とくにリアルタイムでの応答が求められるような場面では重要な指標です。

### 3つの方式の比較

最終的に、以下の3つの異なるガードレール方式について、上記の観点から横断的な比較が行われました。

（１） **API型**  
OpenAIやAmazonなどが提供する、安全性評価が組み込まれたAPI方式。高速で汎用的ですが、内部構造がブラックボックスであるため、文脈に応じた判断が難しいケースもあります。

（２） **BERTベース [分類器](https://ai-data-base.com/archives/26489 "分類器") 型**  
害があるかどうかを事前学習済みの [分類器](https://ai-data-base.com/archives/26489 "分類器") で判定する方式です。スピードが速く、解釈性も高い一方で、敵対的な表現や複雑な文脈には弱さがあります。

（３） **LLMベース評価型**  
モデレーターのようにLLM自体が回答を判断する方式。柔軟で高性能ですが、プロンプト設計の影響を大きく受け、処理時間も長くなりがちです。なお、今回の研究では、単純な評価から段階的な思考を取り入れた方法（Chain-of-Thought）まで、複数のプロンプト形式が検討されました。最もシンプルなプロンプトとして使用されたのは以下です。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_88574_3-1024x689.png)

これらの方式を、攻撃耐性、実用性、使いやすさのそれぞれの観点から評価することで、どこにどんなトレードオフがあるのかを可視化します。とくに「リスクを減らそうとすると、有用性やスピードにしわ寄せが来る」という構造がどこまで避けられないものなのかを、丁寧に検証しています。

## 結果と考察

以上のようなアプローチに基づき、各ガードレール方式を「安全性」「有用性」「使いやすさ」という3つの観点で比較した結果を紹介します。検証の中で明らかになったのは、それぞれの方式に明確な強みと弱みがあるということです。すべてを完璧に満たす万能な選択肢はなく、用途や優先順位に応じてバランスを取る必要があるという現実が見えてきました。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_88574_1-1024x466.png)

各ベンチマークにおけるモデルのフィルタリング性能

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_88574_2-1024x501.png)

5 種類の攻撃に対するF1スコアと平均レイテンシ（秒）

### 有用性と使いやすさのバランス

まずは、ガードレールが「役に立つ出力」をどこまで維持できるかという視点から見ていきます。

日常的な質問応答や雑談のような、比較的穏やかなプロンプトに対しては、どのシステムも安定して高いスコアを示しました。たとえば [Arena](https://lmarena.ai/) や [Awesome](https://github.com/SihyeongPark/Awesome-LLM-Benchmark) 、NoRobotsといった一般的なベンチマークでは、ほとんどのモデルが良好なパフォーマンスを発揮しています。

一方で、医療や法律など、センシティブな話題に対する「疑似有害」プロンプトでは、対応の仕方に差が出ます。たとえば、 `[llama-guard](https://github.com/meta-llama/PurpleLlama)` や `[think-guard](https://arxiv.org/abs/2502.13458)` のような静的 [分類器](https://ai-data-base.com/archives/26489 "分類器") は、かなり厳しめのフィルタリングを行うため、安全性と有用性の両方で高いスコアを出す一方で、応答にかかる時間（レイテンシー）が3秒を超えることもありました。

逆に、 `iad-v3` や `vijil-mbert-prompt-injection` のような軽量な [分類器](https://ai-data-base.com/archives/26489 "分類器") は、0.1秒以下の高速応答を実現しています。ただしそのぶん、微妙なケースではやや判断が粗くなり、誤って良性な回答を弾いてしまう場面も見られました。

プロバイダーが提供するAPI型のガードレール（たとえば [Enkrypt AIのModeration API](https://www.enkryptai.com/) など）は、この2つの中間に位置します。応答速度と判定精度のバランスがよく、実用性は高めですが、センシティブな事例では判定が揺れることもありました。

LLM自体を使って判断する方式は、より複雑な文脈にも対応できる点で優れています。特に、ステップごとの推論を促す「Chain-of-Thought」型のプロンプトを使うと、判断の精度が大きく向上しました。たとえばClaude 3.5 SonnetのReasoningモードでは、非常に高いスコアを出しています。ただし、1回の応答に7〜8秒を要するケースもあり、即時性が求められる場面には不向きです。

### 敵対的な入力への耐性

ガードレールがどこまで「悪意のある入力」に耐えられるかという観点では、システムの構造の違いがより明確に現れました。

たとえば、 `[nemo-guard](https://github.com/...)` や `iad-v3` は比較的軽量なモデルでありながら、複数のタイプの攻撃（パラフレーズ、難読化、プロンプトインジェクションなど）に対して高いF1スコアを記録しています。レイテンシーも短く、実用性の高い構成です。

一方、 `vijil-mbert-prompt-injection` は特定の攻撃（インジェクション型）には強いのですが、長文プロンプトが相手になると性能が落ちる傾向が見られました。

API型のガードレールでは、Enkryptのシステムが安定感のある結果を出しており、とくに長文や複雑なプロンプトへの対応で効果的でした。対照的に、Azureのような一部のAPI型では、あるデータセットでは高スコアを記録する一方で、別の場面では精度が極端に下がるなど、汎用性に課題が残ります。

GLLM方式でCoTを使ったモデレーションは、敵対的入力の検出において最先端の性能を示しています。ClaudeやGeminiなどのモデルが、SAGEやXTRAMのような厳しいテストセットでも高得点を出しています。ただしその代償として、応答に8秒以上かかるなど、重たい計算コストがかかります。

### レイテンシーと処理効率

全体の中で、実用性を大きく左右するのが「どれだけ速く応答できるか」という観点です。

`iad-v3` や `enkrypt-api` のようなシステムは、処理時間が0.05秒前後と非常に高速です。これに対して、LLMベースの方式、特に推論強化されたGLLMは、1回の応答に最大8秒以上かかるケースもあります。

たとえば、Claude 3.5 SonnetやMistral-LargeにCoTを加えた構成では、高精度なモデレーションが可能になる反面、対話型のアプリケーションではストレスになるほどの遅延が発生します。スピードが求められる環境では、導入の可否そのものに影響を与えるほどの差です。

### 見えてきた構造的なトレードオフ

この一連の評価を通じて、研究チームが当初から想定していた「無料のランチはない」という仮説が裏付けられる結果となりました。つまり、ガードレールには以下のような構造的なトレードオフが存在するということです。

- 敵対的な入力をしっかり防ぐよう設計されたシステムは、反面、レイテンシーが高くなりやすく、誤って無害な出力もブロックしてしまう傾向がある
- スピードと操作性を優先すると、敵対的なプロンプトへの対応力が弱まる場合がある
- バランス型のシステムは、総合的には安定していますが、長文や分布の変化に弱くなるなど、極端な状況では性能が崩れる可能性がある

このように、安全性・有用性・使いやすさの3つのバランスをどう取るかは、導入する場面や目的によって変わってきます。

少し身もふたもない結論に見えるかもしれませんが、ここをスタート地点にしてプロジェクトごとにバランスを検討していくのが得策と言えそうです。

## まとめ

本記事では、ガードレール設計におけるトレードオフ構造を明らかにした研究を紹介しました。

安全性、有用性、使いやすさの3つを同時に最適化することの難しさが、実証的に検証されています。軽量な [分類器](https://ai-data-base.com/archives/26489 "分類器") やLLMベースの評価など、異なる手法がそれぞれ異なる強みと制約を持っていることが示されました。

用途やリスク許容度に応じた選択が、実用的な運用には欠かせないという点が強調されています。  
自分自身のユースケースにあわせて、どの観点を優先すべきかを見極める助けになる内容です。

**参照文献情報**

- タイトル：No Free Lunch with Guardrails
- URL： [https://doi.org/10.48550/arXiv.2504.00441](https://doi.org/10.48550/arXiv.2504.00441)
- 著者：Divyanshu Kumar, Nitin Aravind Birur, Tanay Baswa, Sahil Agarwal, Prashanth Harshangi
- 所属：Enkrypt AI

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMで仮想的な顧客を生成しA/Bテストを行うためのペルソナ設計](https://ai-data-base.com/archives/88185)

[「賢くしゃべる家電」は実現できるか？LLMを用いて、頭脳を現実のモノに宿す](https://ai-data-base.com/archives/88618)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)