---
title: "自動コードドキュメント生成を行うLLMエージェント設計論"
source: "https://ai-data-base.com/archives/89120"
author:
  - "[[AIDB Research]]"
published: 2025-05-06
created: 2025-06-13
description: "本記事では、コードドキュメントを自動生成するLLMエージェント設計に関する研究を紹介します。複雑なコードベースに対応するため、エージェント間で役割を分担し、依存関係を踏まえた段階的な処理を行うアプローチが提案されています。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、コードドキュメントを自動生成するLLMエージェント設計に関する研究を紹介します。  
複雑なコードベースに対応するため、エージェント間で役割を分担し、依存関係を踏まえた段階的な処理を行うアプローチが提案されています。  
あわせて、生成されたドキュメントを多面的に評価するための指標設計についても検討されています。  
ドキュメント自動化を目指す際に参考となる設計上の考え方を整理していきます。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_89120-1024x576.png)

## 背景

ソフトウェア開発において、コードのドキュメントは欠かせない存在です。コードの理解や保守を支えるだけでなく、LLMを使ったコード解析や生成の精度にも大きく関わってきます。

ただ実際には、ドキュメント作成と管理は手間がかかり、多くの開発現場で品質や量にばらつきが出ています。

この課題を受けて、近年では自動生成に取り組む動きが活発になっています。チャット型エージェントなどでコードの周辺情報をもとにドキュメントを補完する仕組みが開発されてきました。

GitHubリポジトリをURLを差し替えるだけでWiki形式に可視化し、依存関係図やチャットUIまで提供するDeepWikiのようなツールも登場しました。こちらは初見リポジトリを素早く把握するには便利です。

ただ、商用ツールはSaaSとしての操作性や体験に主眼が置かれており、内部構造や評価手法については詳細が公表されていません。

ビジネスとしては、プライベートリポジトリや巨大なモノリスコードにも対応できる方法が望まれています。

こうした背景を受け、Metaの研究チームからは、コード全体をトップダウンで順序立てて処理し、ドキュメントを自動生成しながら品質を多面的に評価する新しい手法が提案されています。

## （先に補足）公開リポジトリについて

この記事では、自分自身でコードドキュメント自動生成に向けて設計の考え方のヒントが得られるよう、論文をもとにプロセス構成や役割分担について整理していきます。

一方で、そうしたシステム「DocAgent」の実装はGitHub上で [こちら](https://github.com/facebookresearch/DocAgent) に公開されています（MITライセンス）。

もし既成ツールを動かしてみたり、あるいはカスタマイズを試みたりしたい場合には、このリポジトリを活用することも可能です。ただし、ツールをそのまま動かす場合でも、中身の設計思想を理解しておくことには意味があります。うまく動かなかったり、自分の環境に合わせて改善したい場面で、仕組みを理解していないと対応が難しくなるからです。

そのため、以下ではまず設計の本質に目を向け、エージェント構成をどのように組み立てるかを見ていきます。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_89120_1-1024x337.png)

「DocAgent」の アーキテクチャ

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_89120_2.png)

「DocAgent」がコードドキュメント生成を行う様子

## LLMエージェント同士の協力でコードドキュメントを生成する

コードドキュメントを自動生成するうえで直面する課題は、決して単純ではありません。  
コンテキストをどう特定するか、複雑な依存関係をどう整理するか、そして生成したドキュメントをどう評価するか。これら三つを同時に解決する場合、エージェントによるアプローチが役立つと考えられました。

まず「Navigator」がコード全体の処理順序を決め、続いて「マルチエージェントシステム」がドキュメントを生成する、二段構えのプロセスを採用しています。以下では、その設計を紐解いていきます。

### まずは依存関係を見据えた順序作りを行う

大規模なコードベースでは、すべての依存関係を一度に追うことは現実的ではありません。そこで、リポジトリ全体の静的解析を行い、抽象構文木（AST）から関数・クラス・メソッドの関係性を抽出するエージェントを動かします。

そのエージェントには、「どのコンポーネントがどれに依存しているか」を表す有向グラフを構築する役割を持たせます。循環がある場合はTarjanのアルゴリズムで検出し、超 [ノード](https://ai-data-base.com/archives/26470 "ノード") にまとめます。こうして依存関係が整理されたうえで、トポロジカルソートを実行します。

この順序付けが持つ意味は重要です。  
ドキュメント化する際、「依存先がすでに説明されている」という状況を確保することで、必要最小限の情報だけで済みます。無限に背景情報をたどる必要がなくなり、ドキュメント生成が破綻しづらくなります。

まとめると、「依存関係を整理して、階層的に処理する」という視点になります。今回の研究ではこの段階におけるエージェントは「Navigator」と名付けられました。

### エージェントによる分業でドキュメントを生成する

コードコンポーネントごとにドキュメントを生成する際、すべてを一度にこなそうとすると、情報不足や内容のばらつきが生まれがちです。  
そこで、プロセスを分業し、それぞれの段階で「何に集中するか」をはっきりさせる考え方が有効になります。

以下では、ドキュメント生成に必要な役割をどう整理すべきかについて考えていきます。

#### まず必要な情報を見極める仕組みを作る

最初に重要になるのは、「このコードを正しく説明するには、どんな情報が必要なのか」を判断する仕組みです。  
コード本体だけで完結する場合もあれば、別のコンポーネントへの依存や、外部のドメイン知識を補足しないと十分な説明にならない場合もあります。

このため、まずは対象となるコードを読み取り、

- 追加で知るべき内部情報（依存先コンポーネントなど）
- 補足すべき外部知識（アルゴリズムの定義など）

をリストアップする工程を設けるとよいでしょう。

今回の研究ではこのエージェントを「Reader」と呼んでいます。

重要なのは、「いきなりドキュメントを書くのではなく、まず何が不足しているかを整理する」段階を意識的に設計に組み込むことです。

#### 見極めた情報を確実に取りに行く仕組みを作る

必要な情報が整理できたら、次に求められるのは、それを確実に収集する工程です。

リポジトリ内を静的解析して依存先の定義を探し出したり、必要に応じて外部の知識ソース（たとえば、公開されているアルゴリズムの説明など）を参照したりすることになります。

この情報収集を担うこのエージェントは「Searcher」と呼ばれています。

設計上意識しておきたいのは、

- 内部リポジトリと外部知識を区別して扱うこと
- 必要な対象をピンポイントで取りに行くこと

です。

#### 収集した情報をわかりやすくまとめる仕組みを作る

情報が揃ったら、いよいよドキュメントとしてまとめていく段階に入ります。  
このとき意識したいのは、「単に情報を列挙するだけ」では実用的なドキュメントにはならないという点です。

たとえば関数やメソッドなら、

- 概要
- 引数の説明
- 戻り値の説明
- 例外処理
- 使用例

といった要素を適切な構成でまとめる必要があります。

このまとめ役は「Writer」とされています。

このように、あらかじめ「こういう要素を揃える」というガイドラインを用意し、それに沿って生成させる仕組みを作るとよいでしょう。

#### 出来上がったドキュメントをチェックする仕組みを作る

生成されたドキュメントも、そのまま受け入れてしまうのではなく、きちんと検証することが重要です。

検証の観点としては、

- 必要な情報がすべて網羅されているか
- 記述が適切なレベルの詳しさになっているか
- 構成や表現に違和感がないか  
	などが挙げられます。

この検証役は「Verifier」と名付けられています。

一言で言うと、問題があれば修正指示を出すフローを組み込んでいく役割です。

検証の際には、単なる形式チェックにとどまらず、「このドキュメントを読んだ開発者にとって本当に役立つか」という視点を持たせると、実践的な品質管理につながります。

#### 全体をつなぎ、調整する仕組みを作る

最後に、こうした各プロセスを単独で動かすだけではなく、全体の流れを一貫して管理する役割が必要になります。

たとえば、

- どのタイミングで次のプロセスに移るか
- 取得した情報が多すぎる場合にどう制御するか  
	といった判断は、どこかで集中管理しなければなりません。

このエージェントは「Orchestrator」と呼びます。

プロセスの進行や例外処理を一元管理するこうした「指揮者」的な仕組みを置くことを検討するとよいでしょう。

以上の設計から学べるのは、  
「単純にエージェントを並べるのではなく、ドキュメント生成に必要な段階をきちんと切り分け、各段階ごとの責任と目的を明確にする」  
という実践的な発想です。

自分自身でコードドキュメント自動生成に取り組む際も、こうした発想をベースに設計を組み立てると、失敗のリスクを減らしながら、安定した運用を目指せると考えられます。

## ドキュメント品質をどう測るか

コードドキュメントを自動生成する技術に取り組むと、避けて通れない問題があります。  
それは「生成されたドキュメントの品質を、どうやって客観的に評価するか」という課題です。

自然言語生成でよく使われるBLEUやROUGEといった指標は、コードドキュメントには向きません。なぜなら、正解となるリファレンスが存在しないからです。また、単純にドキュメントの長さだけを測っても、本当に役に立つ内容かどうかはわかりません。

一方で、人手による評価は最も正確ですが、コストが高く、特に継続的な運用や大規模実験には向きません。

こうした背景を踏まえ、今回、自動生成されたドキュメントを体系的に評価するための独自フレームワークが提案されています。下記の評価指標はそのまま使用してもよいですし、独自の状況に合わせて作り変えてもよいかと思います。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_89120_3.png)

評価フレームワークの全体像

### 基本構成を網羅できているかの指標「完全性（Completeness）」

まず押さえておきたいのは、生成されたドキュメントが必要最低限の構成要素をきちんと含んでいるか、という視点です。

たとえば関数やクラスについては、

- 概要
- 引数（Args）
- 戻り値（Returns）
- 発生しうる例外（Raises）
- 使用例（必要に応じて）  
	などが期待されます。

この完全性をチェックするために、以下のような手順を自動化します。

1. コード全体を抽象構文木（AST）として解析し、対象コンポーネントと生成ドキュメントを対応づける
2. コードのシグネチャや本体（return文、raise文など）から「必要なドキュメント要素」を動的に決定する
3. 正規表現などを使って、ドキュメント内に各要素が含まれているかを判定する
4. 必要な要素のうち、どれだけが実際に含まれているかの割合（0〜1のスコア）を計算する

上記のプロセスにより、ドキュメントが基本的な形式要件を満たしているかどうかを客観的に測定します。

このように「最低限の構成チェック」を独立させて設計するのが実用的な出発点になります。

### 開発者にとって役立つかの指標「有用性（Helpfulness）」

次に問うべきは、「このドキュメントは本当に開発者の助けになっているか」という点です。

たとえば、

- 概要が明快で、コードの目的がすぐに理解できるか
- 拡張説明で、なぜこのコードがこうなっているかを説明できているか
- パラメータや戻り値の説明に、具体的な型や制約が明記されているか
- いつ、どのようにこのコンポーネントを使うべきか、適切なガイダンスがあるか

といった観点から見ます。

有用性の評価は難しい問題ですが、「LLMを評価者として活用する」アプローチが有効です。  
ただし、単に「LLMに評価させる」のではなく、次のような工夫を加えます。

- ドキュメントの各部分（概要、説明、パラメータ説明など）を分け、それぞれ別々に評価する
- 5段階リッカート尺度（5段階評価のこと）に基づく明示的な評価基準（ルーブリック）を提示する
- 良い例・悪い例を提示し、評価のぶれを抑える
- ステップバイステップで考えさせ、単なる直感評価を防ぐ
- 出力フォーマットも標準化し、後から分析しやすくする

このように、評価対象を分解して、ルールを明示し、プロセスを固定化することが、意味的品質を安定して測るうえで重要な考え方となります。

### 事実に基づいているかの指標「真実性（Truthfulness）」

どれだけ構成や説明が整っていても、事実と異なる内容を含んでいればドキュメントとして致命的です。  
とくに、LLMによる生成では、存在しない関数や属性を”それらしく”記述してしまう「ハルシネーション」がよく問題になります。

そこで、「ドキュメントに登場するエンティティ（関数・クラス・属性など）が、本当にリポジトリ内に存在するか」を検証する仕組みを設けます。

手順は次の通りです。

1. まず、ドキュメント文中からコードエンティティ（関数名やクラス名など）をLLMを使って抽出する
2. Navigatorモジュールで作成された依存関係グラフを「正解」として参照する
3. 抽出したエンティティが本当に存在するかを突き合わせる

この結果、「実在するエンティティ数 ÷ 言及されたエンティティ数」という「存在比率（Existence Ratio）」を算出し、真実性を数値化します。

この設計のポイントは、「表面的な整合性チェック」ではなく、「対象リポジトリに実在するか」という確かな基準に基づいていることです。

### 3つの軸を組み合わせて全体像を捉える

完全性、有用性、真実性。これら3つの軸を合わせて見ることで、単なる「きれいな文」や「長い説明」とは違う、本当に役立つドキュメントかどうかを多面的に評価できるようになります。

もしコードドキュメント生成を本格的に設計・運用するなら、

- 最低限、どこから着手するか
- どこにコストをかけて精緻化していくか

を決めるうえでも、こうしたフレームワーク的な考え方を取り入れておくと、無理のない運用につながるでしょう。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_89120_4.png)

「DocAgent」が評価を行う様子

## 実験と結果から見えてくるもの

ここまで設計と評価方法を整理してきましたが、実際に提案された仕組みがどれほど効果を発揮するのかも重要です。

今回Metaの研究チームは、本フレームワーク（DocAgent）の有効性を確かめるため、複数のベースラインと比較する実験を行いました。さらに、設計の要である処理順序の効果についても検証が加えられています。

### 比較対象

DocAgentの性能は、次の二つのベースラインと比較されました。

まず、FIM（Fill-in-the-middle）です。これはコードの周囲を手がかりにドキュメントを補完するアプローチで、CodeLlama-13B（FIM-CL）が使われました。

次に、チャット型アプローチです。コードスニペットをチャットベースのLLMに渡してドキュメントを生成する方法で、GPT-4o mini（Chat-GPT）とCodeLlama-34B-instruct（Chat-CL）が比較に使われています。

加えて、DocAgent自体も二つのバリエーションが検証対象となっています。  
GPT-4o miniを使ったバージョン（DA-GPT）と、CodeLlama-34B-instructを使ったバージョン（DA-CL）です。

### 実験設計

対象データは、サイズや複雑さ、ドメインの多様性を考慮して選ばれたPythonリポジトリ群です。関数、メソッド、クラスなど、依存関係の密度が異なるコンポーネントも含まれています。

評価には、完全性、有用性、真実性の三つの軸が用いられました。  
統計的検定にはペアワイズt検定が使われ、p値が0.05未満で有意と見なされています。

### 完全性に関する結果

DocAgentは、どちらのバリエーションでもChatベースラインを大幅に上回りました。

DocAgentのCodeLlama版はスコア0.953を記録し、Chatより0.229ポイント高い結果となりました。GPT-4o mini版もスコア0.934を達成し、Chatの0.815を大きく上回りました。

FIMのスコアは0.314に留まり、単純なコード補完型ではドキュメントの構成要素を十分にカバーできないことが改めて示されました。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_89120_5.png)

### 有用性に関する結果

意味的な品質を測る有用性の指標でも、DocAgentの優位性は明確でした。

DocAgentのGPT-4o mini版が最も高い有用性スコアを記録し、Chatベースラインを大幅に上回りました。CodeLlama版も改善を示しており、とくに要約生成において強いパフォーマンスを示しました。

一方で、有用なパラメータ記述を生成することは依然として難しい課題であり、最高スコアを達成したDocAgentでも、さらなる精緻化の余地があることが示されています。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_89120_6.png)

### 真実性に関する結果

事実に基づいた記述ができているかを測る真実性の指標でも、DocAgentは際立った結果を示しました。

DocAgentのGPT-4o mini版は存在比率95.74パーセント、CodeLlama版は88.17パーセントを記録しています。

一方、Chat型アプローチではGPT-4o miniが61.10パーセント、CodeLlamaが68.03パーセントに留まりました。  
FIMはさらに低く、45.04パーセントでした。

これは、単にコードスニペットをLLMに渡すだけでは、ハルシネーションや事実誤認が頻発することを裏付けています。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_89120_7.png)

### トポロジカルソートの重要性

トポロジカルとは、「どれが先で、どれが後か」という順番や依存の関係を正しく保つことです。

DocAgentでは、依存関係に従った順序で処理を行うことが設計の要となっています。  
その効果を確かめるため、依存関係を無視してランダムな順序で処理するバリアント（DocAgent-Rand）も作成され、比較されました。

#### 有用性への影響

Navigatorによる順序付けがある場合、全体的な有用性スコアが大きく向上しました。

GPT-4o mini版では通常版が3.69、ランダム版が3.44  
CodeLlama版では通常版が2.39、ランダム版が2.18  
と、どちらも順序制御による明確な改善が確認されました。

とくに、ドキュメントの要約部分でこの差が大きく現れました。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_89120_8.png)

#### 真実性への影響

トポロジカルソートを省略すると、真実性にも悪影響が出ました。

GPT-4o mini版では存在比率が94.64パーセントから86.75パーセントに低下し、  
CodeLlama版では87.76パーセントから83.06パーセントに低下しました。

依存先がきちんと整備された文脈に基づいてドキュメントを生成することが、事実に基づく正確な記述を支えているとわかります。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_89120_9.png)

### 実験結果から学べること

DocAgentの実験結果は、マルチエージェント型アプローチの有効性を強く裏付けました。

単にLLMにコードを渡すだけでは、完全性、有用性、真実性のすべてにおいて限界があることが明らかです。  
段階的な分業と依存関係管理を取り入れることで、生成されるドキュメントの品質が大きく向上しました。

とくに、大規模リポジトリや複雑なコードベースでは、依存関係に沿ったトポロジカルな順序制御が不可欠です。  
適切な順序でコンテキストを構築しながらドキュメントを生成することが、実用的な自動生成システムを支える鍵になるでしょう。

また、基盤となるLLMの違いによる性能差は多少見られたものの、DocAgentの設計自体が優れた結果をもたらしていることは一貫して示されました。

これらの知見は、今後コードドキュメント生成に取り組む際の実践的な設計指針として活用できるはずです。

## まとめ

本記事では、コードドキュメント自動生成システムの研究を紹介しました。

エージェントの分業と依存関係に基づく処理順序を組み合わせることで、生成ドキュメントの完全性、有用性、真実性を向上させることを目指しています。  

実験では、従来手法と比較して一貫した品質向上が示されました。また、依存関係に配慮した設計が、特に事実性の確保に大きく貢献することが確認されました。

開発現場でも、規模や目的に応じて、こうした段階的な生成プロセス設計の考え方は柔軟に取り入れられる可能性があります。

**参照文献情報**

- タイトル：DocAgent: A Multi-Agent System for Automated Code Documentation Generation
- URL： [https://doi.org/10.48550/arXiv.2504.08725](https://doi.org/10.48550/arXiv.2504.08725)
- Github： [https://github.com/facebookresearch/DocAgent](https://github.com/facebookresearch/DocAgent)
- 著者：Dayu Yang, Antoine Simoulin, Xin Qian, Xiaoyi Liu, Yuwei Cao, Zhaopu Teng, Grey Yang
- 所属：Meta AI

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[ビジネスプロセス評価におけるLLMの使いどころ](https://ai-data-base.com/archives/88825)

[LLMエージェントに必要なメモリーの選び方と残し方　抽出と構造化で蓄積される記憶のかたち](https://ai-data-base.com/archives/89188)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)