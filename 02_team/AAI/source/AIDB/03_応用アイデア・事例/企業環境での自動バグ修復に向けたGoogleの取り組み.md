---
title: "企業環境での自動バグ修復に向けたGoogleの取り組み"
source: "https://ai-data-base.com/archives/82409"
author:
  - "[[AIDB Research]]"
published: 2025-01-22
created: 2025-06-13
description: "本記事では、Googleが取り組むエージェントベースのプログラム自動修復に関する研究を紹介します。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、Googleが取り組むエージェントベースのプログラム自動修復に関する研究を紹介します。

プログラムの自動修復は長年の研究課題でしたが、近年のLLMの発展により、バグの特定から修正まで一連の作業を自律的に行えるようになりつつあります。

しかし、その有効性は主にオープンソースのプロジェクトで実証されており、Googleのような企業環境での実用性は未知数でした。そこで同社は実際の社内での取り組みをまとめて報告しています。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82409-1024x576.jpg)

**発表者情報**

- 研究者：Pat Rondon et al.
- 研究機関：Google

詳細は記事の下部に記載されています。

## 背景

プログラムの自動修復（APR：Automated Program Repair）は、長年にわたりプログラミング言語とソフトウェア工学の研究分野で取り組まれてきました。これまでのAPRシステムでは、バグを再現するテストスイートが与えられ、それを用いてバグの修正が試みられてきました。

近年、LLMの進歩により、APRシステムの性能も大きく向上しています。LLMをエージェントとして活用することで、 **バグの説明から自動的にバグ再現テストを生成** し、 **不具合箇所を特定** し、 **修正候補を生成** し、それを **検証して解決策を提案する** といった一連の作業を自律的に実行できるようになりました。

LLMのバグ修正性能を示す一例として、オープンソースのPythonプロジェクトから収集されたSWE-Benchデータセットでは、エージェントベースのAPRシステムが高い性能を示しています。

しかし、実際の企業環境での有効性はまだ明らかになっていない部分が大きいです。企業のコードベースは多言語で構成され、プロジェクトが複数の領域にまたがり、企業独自のインフラストラクチャで動作するなど、オープンソースとは異なる特徴を持つためです。

そのような状況を受け、Googleの研究者らは社内バグ追跡システムから収集した178件のバグを対象に、エージェントベースのAPRシステムを開発し、その有効性を検証することに取り組みました。

以下で詳しく紹介します。

研究チームはまず、評価に適したバグデータセットの構築から着手しました。自動修復システムの性能を適切に評価するには、現実のバグを反映しつつも、システムが扱える形式のデータセットが必要だからです。データセットの構築方法と、その過程で得られた知見を見ていきましょう。

## バグデータセットの収集方法

Googleの社内バグ追跡システム（GITS）には膨大なバグ報告が蓄積されています。しかし、単純にランダム [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") を行うだけでは、自動修復システムの性能を適切に評価できません。例えば、スクリーンショットを含むバグ報告は現状のシステムでは扱えないため、評価用データセットから除外される必要があります。そこで今回特別なフィルタリングが施されました。

### フィルタリングの4段階プロセス

バグ報告は4段階のフィルタリングプロセスを経て選別されます。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82409_1.png)

GITS評価セットを収集するための多段階フィルタリングプロセスの概要。対象バグを絞り込む工程を示す。

第0段階では、アクセス権限の確認やバグステータスの確認など、基本的なフィルタリングが行われます。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82409_2.png)

フィルタリングフェーズ0で使用されたバグ選定基準。基本的な条件。

第1段階では、テスト可能なソースファイルに関連するバグのみが選択されます。

今回実際に使用されたのはSANというシステムとTODというシステムです。

SAN（Sanitizer-based Analysis）システムは、プログラム実行中に発生するランタイムエラーを自動で検出するツールです。例えば、メモリの境界外アクセスや未初期化の値の使用などを特定します。エラーの再現方法や実行ログが含まれるバグ報告を作成し、修正作業をサポートします。

TOD（Test Order Dependency Analysis）システムは、テストケースの実行順序によって発生するバグを検出します。特定のテストケースの組み合わせや実行順序がエラーを引き起こす場合、それを明確にし、再現手順を含む報告を提供します。

SANは主に低レベルのメモリエラーやスレッドエラーに焦点を当て、プログラムの安全性を確保するのに役立ちます。一方で、TODは高レベルのテスト依存性の問題を明らかにし、ソフトウェアのテスト品質を向上させる助けとなります。これらは実験結果でも再度登場します。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82409_3.png)

フィルタリングフェーズ1での基準。特にソースファイルとテストファイルの変更を条件に追加。

第2段階では、長時間実行されるテストを含むバグや、マルチメディアコンテンツを含むバグなど、現実的な評価が困難なバグが除外されます。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82409_4.png)

フィルタリングフェーズ2と3での基準。パッチサイズ制限や実行可能なテストの適用性を含む。

### 人手による最終確認

第3段階では、自動化された評価が可能かどうかを人手で確認します。例えば、テストケースに「マジックコンスタント」（バグ報告や既存コードから導出できない新しい定数値）が含まれるバグは除外されます。最終的に、78件の人間が報告したバグと100件の自動検出されたバグが評価用データセットとして選定されました。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82409_5.png)

SANとTODで報告されたバグの例。再現方法やエラー情報を含む典型的な内容。

### SWE-Benchとの比較

オープンソースのバグデータセットであるSWE-Benchと比較すると、GITSのバグは以下のような特徴を持ちます。

1. バグ説明文中にコード関連の用語が少なく、不具合箇所の特定が難しい
2. 修正が必要なファイルが多岐にわたり、変更箇所が分散している
3. 修正に必要なコード行数が多い
4. 多様なプログラミング言語（Java、C++、TypeScript、Kotlin、Python）が使用されている

つまり実際の企業環境に合わせたバグデータセットを作ることに成功したようです。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82409_6-1024x172.png)

GITSバグとSWE-Benchバグの修正特性比較。コード識別子、変更されたファイル数、パッチスプレッドなどの違いを示す。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82409_7-1024x169.png)

GITS-EvalとSWE-Bench-Liteのバグ比較。機械報告バグは類似するが、人的報告バグは複雑。

研究チームは、企業環境に特有のこれらの課題に対応するため、専用の自動バグ修正システムの開発に着手しました。

オープンソース向けに開発された既存システムをそのまま適用するのではなく、Googleの開発環境に最適化された新しいシステムが必要だったのです。

開発されたシステムは「Passerine」と名付けられました。以下では、Passerineの設計思想と具体的な実装について説明します。

## Passerineシステムの設計と実装

Passerineは、バグ修正を自動化するエージェントシステムです。 [SWE-Agent](https://github.com/SWE-agent/SWE-agent) と呼ばれる先行研究をベースに、Google独自の開発環境で動作するように設計されています。

システムは、

「思考」「行動」「観察」

という3つのステップを繰り返しながら動作します。思考ステップでは次に取るべき行動を計画し、行動ステップでは実際にコマンドを実行し、観察ステップではその結果を確認します。

### 利用可能なコマンド

Passerineが利用できるコマンドは以下の5種類に限定されています。

- ファイルの内容表示（cat）：指定されたファイルの内容を表示します
- コード検索（code\_search）：指定された検索語でコードを検索します
- ファイル編集（edit）：ファイルの特定行を編集します
- テスト実行（bazel）：指定されたテストを実行します
- 終了（finish）：修正成功または失敗で処理を終了します

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82409_8.png)

PasserineのReActスタイルのダイナミックループとUnixスタイルのコマンド概要。

### 評価フレームワーク

蓄積された過去のバグを用いて評価を行うため、専用のフレームワークが実装されています。フレームワークは以下の手順で動作します。

1. バグが報告された時点のコード状態を再現します
2. バグが実際に再現できることを確認します
3. Passerineによる修正を試みます
4. 修正後のコードでテストを実行し、バグが解消されたかを確認します

### システムの特徴

従来のシステムと異なり、Passerineは事前に定められた修正手順に従うのではなく、状況に応じて柔軟に行動を選択します。複雑な仮想環境を必要とせず、Googleの既存インフラストラクチャを活用することで、スケーラブルな運用が可能となっています。また、修正の過程で生成される履歴を保存することで、後からの分析や改善に活用できます。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82409_9.png)

Passerineのシステムプロンプトテンプレートとコマンド文書化テンプレート。

Passerineのソースコードは現時点で公開されていませんが、その設計思想と実装アプローチには、独自の自動修正システムを開発する際に参考となる要素が多く含まれています。利用可能なコマンドやシステムプロンプトなどは具体的に模倣できる情報と考えられます。

そして、どれほど優れたシステムでも、その有効性は実際の評価によって示される必要があります。研究チームは、システムの評価方法自体についても、再現可能で信頼性の高い手法を確立することを目指しました。以下では、評価方法の詳細とその結果について説明します。

## 自動修正の評価方法

エージェントにより自動生成された修正は2つの基準で評価されます。

1つ目は「もっともらしさ」で、バグを再現するテストが正常に実行できるかどうかで判断されます。

2つ目は「有効性」で、人間が作成した修正と意味的に同等かどうかを評価します。なお、テストの実行だけでは修正の正しさを完全に判断できないため、有効性の判断には研究者による手動での確認が必要とされました。

### 修正プロセスの分析

修正の成功率だけでなく、Passerineがどのように修正を行うのかも分析されました。具体的には以下の観点から評価が行われています。

#### 観点１：コマンド使用パターン

Passerineがどのような順序でコマンドを使用するのかが分析されました。人間が報告したバグの場合は、まずコード検索や内容表示を行い、問題箇所の特定から始めます。一方、自動検出されたバグの場合は、バグ報告に再現手順が含まれているため、すぐにテスト実行から始められることが明らかになりました。

#### 観点２：不具合箇所の特定精度

修正が必要なファイルをどの程度正確に特定できるかが評価されました。ファイルシステム上での距離（ディレクトリ階層の差）を用いて、Passerineが編集したファイルと実際に修正が必要だったファイルの近さが測定されました。

#### 観点３：問題のある修正パターン

修正プロセスで発生する問題のあるパターンも分析されました。

（例えば、テストを一度も実行せずに修正を試みる、同じファイルを何度も読み直す、連続して同じような検索を繰り返すといった非効率な動作が観察されました。これらの問題は、システムの改善に向けた重要な知見となっています）

### ※評価手法の限界

評価には一定の制約があることも認識されています。例えば、テストファイルの修正は評価対象外とされ、プロジェクト固有のテストは使用されていません。また、生成された修正と人間による修正の類似性は、テキストレベルではなく意味レベルでのみ評価されています。

## 評価結果

実験は178件のバグ（自動検出バグ100件、人間報告バグ78件）を対象に実施されました。基盤となるLLMにはGemini 1.5 Proが使用されました。各バグに対して20回の修正試行が行われ、1回の試行では最大25ステップまでの処理が許可されました。

### 修正成功率

自動検出されたバグに対しては高い成功率が達成されました。SANシステムで検出されたバグでは78%が「もっともらしい」修正に成功し、62%が「有効な」修正を生成できました。TODシステムで検出されたバグでは68%が「もっともらしい」修正に成功し、24%が「有効な」修正となりました。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82409_10-1024x306.png)

Passerine修正の正確性とLLM使用状況の概要。各バグタイプの修正成功率を含む。

一方、人間が報告したバグに対する成功率は相対的に低く、「もっともらしい」修正は25.6%、「有効な」修正は17.9%にとどまりました。ただし、人間が報告したバグの場合、「もっともらしい」修正と「有効な」修正の差が小さいことから、テストによる評価がより厳密に機能していることが示唆されています。

つまり、 **自動システムと人間を比較すると、自動検出されたバグ（SANおよびTODシステム）に対する修正成功率は、人間が報告したバグに対する成功率を大きく上回っています** 。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82409_11.png)

修正成功率（プラウジブル/バリッド）の詳細な評価。バグタイプごとの軌跡サンプルの分析。

### エージェントの行動分析

Passerineは、バグの種類に応じて異なる戦略を採用することが判明しました。 **人間が報告したバグに対しては、まず問題箇所の特定から始め、徐々に修正を試みる傾向が見られます。** 一方、自動検出されたバグに対しては、報告に含まれる再現手順を活用して、早い段階からテストと修正を繰り返す傾向が観察されました。

つまり、Passerineはバグ報告に含まれる情報量や特性に応じて、柔軟に行動を切り替える能力を持っています。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82409_12-1024x243.png)

Passerineの軌跡における「スメル」の発生率。修正の成功・失敗に関連する行動パターンをリスト化。

### 課題点の発見

分析を通じて、改善が必要な点も明らかになりました。

例えば、 **不具合箇所の特定に苦戦する** ケースや、 **非効率な操作を繰り返す** ケースが観察されています。また、 **LLMへの入力が長大になり処理に支障をきたす** ケースも確認されており、 **履歴管理の最適化が今後の課題** として挙げられています。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82409_13-1024x234.png)

Passerineのコマンド使用頻度の軌跡分布。バグタイプごとのエージェントの行動変化を示す。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82409_14.png)

バグ修正が失敗した場合のファイルシステム距離。人的バグよりも機械的バグのほうが高い位置特定率を示す。

## 考察と今後の課題

現在のPasserineシステムでは、いくつかの技術的な制約に直面しています。例えば、 **テストログやソースコードファイルが大きい場合、LLMへの入力が制限を超えてしまう** 問題があります。また、 **人間が報告したバグの多くは再現手順が明確でない** ため、 **バグの再現自体が困難** となっています。

### 改善の方向性

技術的な課題に対して、複数の改善案が検討されています。LLMへの入力制限問題に対しては、 **直近のステップのみを保持する方式** や、 **履歴を要約する機能の導入** が提案されています。バグ再現の難しさに対しては、 **バグ再現テストを自動生成する機能の開発** が進められています。

### システムツールの拡張

現状のPasserineは限られたコマンドセットしか持っていませんが、ユーザーとの対話機能や外部ドキュメントへのアクセス機能など、新しいツールの追加が検討されています。実際の使用状況を観察すると、システムがユーザーに質問しようとしたり、バグ報告に含まれるリンクにアクセスしようとする場面が確認されています。

### 実用化に向けた課題

実際の開発環境への導入を見据えると、 **修正能力を超えるバグに対する処理** が課題となります。無駄な計算リソースの消費を避けるため、 **修正が困難なバグを事前に判別する機能の開発** が必要とされています。過去のバグ修正履歴を活用して、成功の見込みを予測する手法が検討されています。

### バグ報告の改善提案

分析結果から、 **バグ報告の質がシステムの性能に大きく影響する** ことが判明しました。自動検出されたバグは再現手順が明確で修正成功率が高い一方、 **人間が報告したバグは情報が不足しがち** です。将来的には、 **人間がバグを報告する際に、自動システムが扱いやすい形式でバグ情報を入力できるようなガイドラインや支援ツール** の整備が望まれます。

## まとめ

本記事では、Googleが開発したエージェントベースのプログラム自動修復システム「Passerine」に関する研究を紹介しました。

社内のバグ追跡システムから収集した178件のバグを評価対象として、自動検出されたバグの73%、人間が報告したバグの25.6%で修正に成功し、企業環境での実用可能性が示されました。

一方で、バグ報告の質がシステムの性能に大きく影響することも判明し、今後はバグ報告のガイドライン整備などが課題として挙げられています。

**参照文献情報**

- タイトル：Evaluating Agent-based Program Repair at Google
- URL： [https://arxiv.org/abs/2501.07531](https://arxiv.org/abs/2501.07531)
- 著者：Pat Rondon, Renyao Wei, José Cambronero, Jürgen Cito, Aaron Sun, Siddhant Sanyam, Michele Tufano, Satish Chandra
- 所属：Google

## 理解度クイズ（β版）

1\. Passerineシステムの基本的な動作サイクルはどれですか？

Passerineは「思考」で次の行動を計画し、「行動」でコマンドを実行し、「観察」で結果を確認するサイクルで動作します。このシステム設計は人間の問題解決プロセスを模倣したものです。

解説を見る

2\. 企業環境のバグと比較して、オープンソースプロジェクトのバグの特徴は？

オープンソースプロジェクトのバグは、企業環境のバグと比べてコード関連の用語が多く含まれ、不具合箇所の特定が容易です。企業環境のバグは説明が不明確で、修正箇所が分散している特徴があります。

解説を見る

3\. Passerineの評価で、自動検出バグと人間報告バグで大きく異なった点は？

自動検出バグの修正成功率は約70%以上だったのに対し、人間が報告したバグは約25%と大きな差がありました。自動検出バグは再現手順が明確で扱いやすい特徴がありました。

解説を見る

4\. Passerineシステムの現状における主要な技術的課題は？

テストログやソースコードが大きい場合にLLMの入力制限を超えてしまう問題が主要な課題として挙げられています。この問題に対して履歴の要約機能などの導入が検討されています。

解説を見る

5\. バグ報告の選別プロセスにおいて、第3段階で確認される重要な点は？

第3段階では人手で自動評価が可能かどうかを確認し、マジックコンスタントなど自動化できない要素がないかを精査します。この段階で評価対象として適切なバグだけが選別されます。

解説を見る

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[DeepSeek R1が実現した教師なし強化学習による推論性能の向上](https://ai-data-base.com/archives/82540)

[LLMエージェント間で観察された人間のような「意見の二極化」](https://ai-data-base.com/archives/82487)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)