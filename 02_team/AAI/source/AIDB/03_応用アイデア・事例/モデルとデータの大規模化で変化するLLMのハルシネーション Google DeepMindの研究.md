---
title: "モデルとデータの大規模化で変化するLLMのハルシネーション Google DeepMindの研究"
source: "https://ai-data-base.com/archives/74778"
author:
  - "[[AIDB Research]]"
published: 2024-08-26
created: 2025-06-13
description: "まるで人間のように文章を作ることができるLLMの能力が注目を集めています。しかし、LLMが時に事実と違うことを言ってしまうことがあり、これは「ハルシネーション」と呼ばれる問題となっています。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

まるで人間のように文章を作ることができるLLMの能力が注目を集めています。しかし、LLMが時に事実と違うことを言ってしまうことがあり、これは「ハルシネーション」と呼ばれる問題となっています。

今回Googleの研究者らは、このハルシネーションが、LLMの大きさと、学習に使ったデータの量によってどう変わるのかを調べました。また、LLMが嘘をついているかどうかをどうやって見つけるか、という問題についても研究しています。

この記事では、研究で使われた方法や、わかったこと、そして将来どのように役立つのかについて詳しく説明します。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_74778-1024x576.jpg)

**参照論文情報**

- タイトル：Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability
- 著者：Jiri Hron, Laura Culp, Gamaleldin Elsayed, Rosanne Liu, Ben Adlam, Maxwell Bileschi, Bernd Bohnet, JD Co-Reyes, Noah Fiedel, C. Daniel Freeman, Izzeddin Gur, Kathleen Kenealy, Jaehoon Lee, Peter J. Liu, Gaurav Mishra, Igor Mordatch, Azade Nova, Roman Novak, Aaron Parisi, Jeffrey Pennington, Alex Rizkowsky, Isabelle Simpson, Hanie Sedghi, Jascha Sohl-dickstein, Kevin Swersky, Sharad Vikram, Tris Warkentin, Lechao Xiao, Kelvin Xu, Jasper Snoek, Simon Kornblith
- 所属：Google DeepMind

**本記事の関連研究**

- [ファインチューニングがLLMの幻覚（ハルシネーション）に与える影響　Googleなどによる検証結果](https://ai-data-base.com/archives/69421)
- [マルチモーダルLLMにおける幻覚（ハルシネーション）の原因と対策　クリエイティブでの活用も推奨　AWSなどが網羅的に調査](https://ai-data-base.com/archives/68720)
- [LLMの内部状態を観察することで「出力がハルシネーションか否かを判別する」手法『LLMファクトスコープ』](https://ai-data-base.com/archives/61651)
- [LLMの誤り（ハルシネーション）発生原因と、「創造性と事実性のバランス」などの対策ロードマップ](https://ai-data-base.com/archives/58767)

## 背景

LLMはどんどん賢くなっていますが、「ハルシネーション」の問題はまだあります。LLMが生成する”事実と異なる情報”は一見正しそうに見えるため、影響を考慮すると解決するのが望ましいとされています。

LLMの性能向上に関しては「スケーリング則」という経験則が知られており、データセットやモデルのサイズが大きくなるにつれてLLMの性能が向上するという法則です。つまり「大きくすればするほど良くなる」という考え方です。しかし、ハルシネーションがスケールにどう依存するかについては、まだ十分に理解されていません。

ハルシネーションにはさまざまな種類があり、皆が「これがハルシネーションだ」と言える定義はありません。そこで今回研究者らは「正しい答えが学習データにそのまま書いてある場合」だけを調べました。それがハルシネーションを最も正確に検出する手段だからです。

なお、普通の文章だとどんな知識が入っているかを正確に知るのは難しいという問題があります。そのため研究者らは、「ナレッジグラフ」というデータの形式を使用して実験を行いました。ナレッジグラフなら、どんな事実が入っているかを完全にコントロールでき、LLMが言ったことが本当にデータの中にあるかどうかを簡単に確認できます。そのためナレッジグラフを使ってLLMを訓練すれば、LLMが学習データをどれくらい間違って覚えているか、そしてLLMの大きさによってそれがどう変わるかを調べられます。

このようにして、LLMのハルシネーションと大きさの関係について今までよりもっとよく分かるようになると期待された研究が行われました。詳しい研究アプローチや実験結果は以下で紹介します。

## LLMの知識を制御する

LLMのハルシネーションを研究する上で一番難しいのは、LLMが学習中にどんな情報を見たのかを正確に知ることです。問題を解決するために、次の方法が使われました。

### ナレッジグラフデータセット

ナレッジグラフは、LLMが学ぶ情報を完全にコントロールするための方法として選ばれました。ナレッジグラフは事実を正確に整理したデータで、多くの会社がさまざまな用途に使っています。

ナレッジグラフが選ばれた理由は以下の通りです。

1. 矛盾のないデータを提供する
2. 実際の世の中の情報の仕組みを真似ている
3. LLMが何を学んだのかを正確に知ることができる
4. データが整理されているため、LLMの答えが正しいかどうかを簡単に確認できる

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_74778_2-1024x290.png)

データセットの概要。各データセットの使用目的とサイズ

今回使われたナレッジグラフは、「誰が」「何を」「どうした」という3つの部分からなる情報のかたまりでできています。情報のかたまりは特殊なトークンでつなげられ、LLMの学習に使われました。  
このアプローチでは、言葉のあいまいさがなくなります。ただし、これはLLMにとって良い影響だけでなく悪い影響があります。

**良い影響  
**情報がきちんと整理されているので、LLMは文法や言い回しの違いを理解する必要がなく、事実を学ぶことに集中できる

**悪い影響  
**それぞれの情報の固まりがほとんど関係ないので、ある事実を学んでも他の事実の理解にはあまり役立たない

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_74778_1-1024x413.png)

データと訓練パイプラインの概要。知識グラフから言語モデル訓練、評価、幻覚検出までのプロセス

### ナレッジグラフでLLMを訓練する

LLMの訓練には、様々な大きさ（315万から16億のパラメータ）の特別な仕組み（デコーダーのみ）のモデルが使われました。そして、異なるデータ量のナレッジグラフ（全体の1%から100%）で学習させました。

訓練は、トークンがついた情報の固まりから作られた文字列を使って、モデルが次の単語を予測する能力を高めるように行われました。

1つの情報の固まりが決められた長さ（256個の単語）に満たない場合、複数の情報をまとめて使いました（平均して約20の情報の固まりが一度に使えます）。

なお、学習の調整には、「Adam」という特別な方法が使われました。「Adam」は、最初はゆっくり学習し、その後徐々に学習のスピードを落としていくアプローチです。学習の回数は、繰り返し学ぶ効果を調べるために変えられました。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_74778_3.png)

異なるデータセットサイズに対する訓練 エポック 数。データセットが大きくなるにつれて訓練 数を減らしている

また学習の速さは、モデルの大きさに合わせて計算して決められました。

このようにして、モデルが間違った情報を出してしまう問題と、モデルの大きさの関係を詳しく調べることができるようになりました。

## ハルシネーション率とそのスケーリング

### スケーリング則との相違

スケーリング則とは、LLMの性能が、モデルと学習データの大きさに応じて一定の割合で良くなるという経験則です。しかし今回、間違った情報を生成する（ハルシネーション）率は、この法則に従わないことが分かりました。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_74778_4-1024x534.jpg)

モデルサイズや訓練データ量によるハルシネーション率の変化

同じ大きさの学習データを使う場合、より大きく長い時間学習したモデルの方が間違いが少なくなる傾向がありました。ところが、 **学習データの量を増やすと、予想に反して間違いが増えました** 。この不思議な現象は、モデルの性能を測る別の指標でも同じように見られました。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_74778_5-1024x359.png)

モデルサイズやデータセットサイズによる損失の変化

この予想外の結果には、次の2つの理由があると考えられています。

1. ナレッジグラフの中の多くの情報は、そのまま覚えなければいけない
2. 各情報は学習データの中で1回しか出てこない

### 学習データの重要性

学習中に見たことのないデータに対しては、どのモデルも50%以上の確率で間違った情報を生成しました。これは、単純に覚えることが大切だということを示しています。例えば、CDの曲名や生まれた日などの情報は、他のデータから推測するのが難しいので、そのまま覚える必要があります。

### 長期学習の必要性

そのまま覚える必要があることと、繰り返し学習できないことから、与えられたサイズのLLMで最も間違いを少なくするには、20回以上の繰り返し学習が必要だと分かりました。これは、現在の1～数回という一般的な学習方法とはかなり違います。

### バランスの難しさ

20回以上繰り返し学習すると、新しいデータに対する対応力が下がってしまう可能性があることが分かりました。この問題は、モデルの設定を変えるとさらに顕著になります。つまり、間違った情報を減らすことと、他の能力を高めることの間でバランスを取るのが難しいということです。

### 現状のLLMとの比較

最も大きく長時間学習したモデルでも、学習中に見たデータに対して約5%の確率で間違った情報を生成しました。この確率は、設定を変えることで約1.5%まで下げられますが、正解が複数ある場合（例：論文を書いた人全員の名前）、モデルが出す正解の数が減ってしまうという新たな問題が起きます。

### 正確さと網羅性のバランス

設定を変えることで、正確さ（間違いの少なさ）と網羅性（正解をどれだけ多く出せるか）のバランスを調整できることが分かりました。この事実は、間違いの少なさだけに注目することの問題点を明らかにしています。間違いを減らす簡単な方法は、あまり多く出力させないことですが、これは必ずしも良い手段とは言えません。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_74778_6-1024x698.png)

1%データにおける温度に対する精度と 再現率 。モデルサイズと訓練 エポック 数による変化を示す

### 最適なモデルサイズの見直し

最も性能の良いモデルは、学習に使った言葉の数から予想される最適な大きさより10倍ほど大きいことが分かりました。これは、学習中に見たデータに対してもめったに間違いを起こさないモデルを作るには、今最適だと考えられている計算量の数倍が必要だということを示しています。

## ハルシネーションの検出可能性とそのスケーリング

LLMが大きくなり、長く学習するほど、間違った情報を生成する確率は一般的に下がります。しかし、（前項までの実験結果から）最適だと思われる大きさよりもずっと大きなモデルでも、学習したデータに対して約5%、見たことのないデータに対して約50%の確率で間違いを起こすことが分かりました。また、学習したデータと新しいデータでの間違いの起こりやすさには、バランスの難しさがあります。

そのため、間違いをさらに減らせる他の方法があるかを調べる必要が出てきました。

### 検出タスクの設計

LLMの間違いを見つけるために、次の2つの方法が考えられました。

（１）文全体を見る方法  
元の「誰が」「何を」と、モデルが予測した「どうした」を入力として、その「どうした」が間違いかどうかを判断します。

（２）単語ごとに見る方法  
学習済みのモデルの特定の部分から単語の情報を取り出し、その単語が間違いかどうかを予測します。

### 検出器の種類

それぞれの方法に対して、2種類の間違い検出器が試されました。それぞれ、モデルが出力した結果やモデルの内部の情報から間違いを見つけ出そうとします。

（１）ヘッド部分だけ追加する検出器  
既に学習済みのモデルの上に新しい判断する部分を追加します（モデルの他の部分は変えません）。

（２）全体を調整する検出器  
学習済みのモデルを基に、すべての部分を少しずつ調整します。

### 訓練と評価データ

それぞれの学習済みのLLMと検出方法の組み合わせに対して、別々の検出器が訓練されました。訓練データは、LLMの学習データの中のすべての「誰が」「何を」に対して、5つの「どうした」を予測させて作りました。

### 結果の分析

実験の結果、次のような重要なことが分かりました。

1. モデル全体を調整する検出器の方が、ヘッド部分だけ追加する検出器よりも良い成績を示した
2. 単語ごとに見る方法は、一般的に文全体を見る方法よりも高い精度で間違いを見つけられた
3. 検出器の精度は、基になるLLMが大きくなるほど良くなる傾向があった
4. しかし、この結果は基になるLLMがどれくらい間違いを起こすかによって影響を受ける
5. 精度と網羅性のバランスを測る指標（ [AUC](https://ai-data-base.com/archives/26250 "AUC") -PR）を使って評価すると、文全体を見る方法の方が優れていることが分かった
6. LLMが大きくなるほど、その間違いを見つけることが難しくなるという、予想とは逆の関係が見られた

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_74778_7-1024x362.png)

言語モデルのサイズに対する幻覚検出の精度。異なるタスク形式と検出器タイプによる精度の変化を示す

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_74778_8-1024x588.png)

言語モデルの幻覚率に対する検出器の AUC -PR。モデルサイズやデータ量による変化を示す

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_74778_9-1024x461.png)

文章タスクにおけるfull検出器の精度-再現率曲線。言語モデルのサイズによる検出の難しさの変化を示す

以上の実験結果は、LLMの大きさと間違いを見つける難しさの間に複雑な関係があることを示しています。より大きなLLMは間違いを起こす頻度が低くなりますが、間違いを見つけることはより難しくなります。

## まとめ

この記事では、LLMが間違った情報を生成する問題と、モデルの大きさの関係を調べた研究を紹介しました。ナレッジグラフを使った特別な実験により、次のことが分かりました。

1. より大きなLLMは間違いを起こすことが少ない
2. しかし、間違いをほとんど起こさないLLMを作るには、より多くの計算が必要
3. さらに、LLMが大きくなるほど、残りの少ない間違いを見つけることが難しくなるという、意外な関係も発見された

今回の発見は、LLMの開発や使用方法に対して重要な示唆をもたらすものだと考えられています。

- 参照論文URL： [https://arxiv.org/abs/2408.07852](https://arxiv.org/abs/2408.07852)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMにおける現状のリスクと対策に関するまとめ](https://ai-data-base.com/archives/74734)

[プロンプトの影響によるLLMの性能のばらつきを考慮した評価指標「Sharpeスコア」　NAIST研究者ら考案](https://ai-data-base.com/archives/74842)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)