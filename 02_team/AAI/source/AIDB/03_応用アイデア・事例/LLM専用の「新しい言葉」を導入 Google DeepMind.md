---
title: "LLM専用の「新しい言葉」を導入 Google DeepMind"
source: "https://ai-data-base.com/archives/84361"
author:
  - "[[AIDB Research]]"
published: 2025-02-17
created: 2025-06-13
description: "本記事では、LLMの理解と制御に向けて新たな語彙を導入する研究を紹介します。人間の言葉だけでモデル内部を説明しようとすると、両者の概念がずれてしまい、誤解や使いにくさが生じる可能性が高いと指摘されています。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、LLMの理解と制御に向けて新たな語彙を導入する研究を紹介します。

人間の言葉だけでモデル内部を説明しようとすると、両者の概念がずれてしまい、誤解や使いにくさが生じる可能性が高いと指摘されています。

そこで、新たに定義された言葉を使えば人間とLLMの間にある概念の隔たりを解消できると考えられ、取り組みが進められています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_84361_thum2-1024x576.png)

参照論文情報は記事の下部に記載されています。

**本記事の関連研究**

- [LLM同士による人工言語コミュニケーションで発見された「言語構造の創発」](https://ai-data-base.com/archives/80658)
- [LLMの記号推論タスク（化学式や絵文字の理解など）で記号を自然言語に変換することの有効性を確認](https://ai-data-base.com/archives/65784)
- [「人間の自然言語を超えて」LLMにタスク実行時の思考を非自然言語フォーマットで行わせるプロンプト手法『AutoForm（オートフォーム）』](https://ai-data-base.com/archives/65090)

研究者のあいだでは、LLMがまるで人間と同じように振る舞うように見える場合でも、その内部ではまったく異なるかたちで情報が処理されていると考えられています。つまり、LLMは大量のデータから統計的な傾向を学習しているため、あたかも「理解」しているかのように文章を生成できる一方、人間の直感とはかけ離れた内部構造をもつことがあるのです。

人間がすでに使ってきた語彙や概念をそのまま用いてLLMの内部構造や挙動を説明しようとすると、誤解が生じる恐れが指摘されています。たとえば、「真実」や「安全性」といった言葉は、一般的に人間が共有している意味合いを前提として使われます。

しかしLLMの内部表現では、こうした概念が同じようには対応していないかもしれません。外部からの見た目（文章の出来ばえや回答内容）だけを頼りに説明しようとすると、LLMならではの特徴を見落としてしまい、制御や理解が十分に行えない可能性が高まります。

なお、「内部表現」という言葉は、LLMが学習を通じて自分の中に保持している情報の組み合わせや、出力に至るまでのプロセスを指します。人間にとっては日常的な感覚や理屈が通用する領域ではありません。

にもかかわらず、人間の言葉でLLMをとらえようと試みられてきた結果、モデルが示す反応を「嘘をついている」とか「矛盾している」といった形で説明してしまいがちです。つまり、意図や文脈が見えにくいLLM特有の仕組みと、人間の常識に基づく語彙のあいだに大きな隔たりがあるのです。

そのため、今回Google DeepMindの研究者らは新たに独自の語彙を定義し、LLMの内部表現や振る舞いをより正確に言い表そうとしています。従来の言葉だけではうまく説明しきれないため、LLMの学習プロセスで何が起きているのかを把握するうえで、まったく新しい概念を導入する試みです。

たとえば、モデルが特定のパターンを検出・強調するプロセスを示す用語や、文章生成の際に用いられる重みづけの仕組みを説明する言葉が考案されつつあります。つまり、人間とLLMのあいだにある認識のギャップを少しでも埋めるための、わかりやすさと正確さを両立できるような新語の整備というわけです。

ただし、新しい語彙を定義すればただちに問題が解決するわけではありません。モデル内部を直接「見る」ことが難しい現状では、どうしても推測や仮説に頼る部分があるからです。

とはいえ、既存の言葉にしばられるよりは、LLMならではの特徴をより実態に近いかたちで記述できる可能性が高まります。研究者らはこのアプローチを通じて、LLMの制御や理解を深める一助になると期待しています。以下で詳しく紹介します。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_84361_1.png)

人間とLLMの概念の違いによるコミュニケーションの障害を示す図。新語を導入することで、両者の理解の溝を埋める可能性を表現

## AIを理解するには新しい言葉（造語）が必要となる

同じ言葉で両者を語ると誤解が生じやすく、新しい概念を説明するための適切な新語（ネオロジズム）が必要だと考えられています。実際に行われた後述の実験によると、新しい語彙をモデルに学習させることで、出力の長さや多様性などを制御しやすくなる傾向が示されています。

### LLM理解における課題

LLMを正しく解釈するには、人間側の視点とモデル側の視点の不一致が大きな障害となる場合があります。見かけ上は自然な文章を返していても、それがLLMの内部の概念を人間にとって分かりやすい形で表しているとは限らないのです。

つまり、専門家がモデル内部をくまなく調べようとしても、それを的確に言い表す単語が不足するおそれがあると考えられています。ここでは、大きく三つの問題が指摘されています。それぞれをもう少し噛み砕いて説明します。

#### 問題１：概念化の相違による問題

そもそも、人間は「感情」「論理」「信念」などの言葉を日常的に使い、これらの概念をある程度共有できています。とはいえ、高度なLLMが内部で形成している概念は、人間のそれと違う構造を持つ可能性があります。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_84361_2-1024x332.png)

人間とLLMが異なる概念を持つことを可視化。未解明の機械的概念の存在や、人間が持たない新たな概念がモデル内に生じることを説明

囲碁やチェスなどで、システムが生み出した「人間には思いつかない戦略」に人間がまだ名前を与えていないのと同じように、LLMが内部で扱う概念にも未定義なものが含まれているかもしれません。人間の言葉をそのまま流用しても、実は同じものを指していないというズレが生まれるおそれがあり、これが相互理解を難しくしていると考えられています。

#### 問題２：抽象化がもたらす難しさ

さらに、LLMの無数のパラメータや結合を完全に分析するのは、情報量が膨大すぎて現実的ではないといわれます。しかし、あまりに抽象的すぎる言い回しだけでLLMを説明すると、具体的な制御や予測ができなくなるという問題が残ります。

つまり、どの程度の抽象度でLLMの特徴を捉えるかという点が大きな課題で、研究者たちは適切な落としどころを模索しているのです。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_84361_3-1024x248.png)

機械学習の解釈手法における抽象度の違いを示す図。機械的な解釈と行動観察の中間に、新語を用いた概念理解が位置付けられることを提案

#### 問題３：確証バイアスの懸念

また、LLMの内部に人間が期待するような構造や概念を見いだそうとすると、「安全性モジュール」や「真実を識別する仕組み」など、人間に都合のよい解釈を無意識に当てはめてしまう危険があります。

つまり、観測された一部の結果が自己流の解釈を強化してしまい、誤解が広がるおそれがあるのです。そのため、新たな言語表現によって既存の概念から距離をとることができれば、こうした思い込みを防ぎやすくなると考えられています。

### 新語がもたらす効果

ということで、高度なLLMの内面を人間が理解しやすい形で説明するには、新語をつくり出すのが良いかもしれません。なお、一見すると「単なる名前の付け替え」に思えるかもしれませんが、言葉の再編を通じてLLM特有の概念を人間にとって理解しやすい形で抽象化できるという利点があるのです。

#### 利点１：未知の概念を手短に指し示す

人間がまだ名前をつけていない複雑なアイデアを表すには、一から説明する手間がかかることが多いのですが、新語を設ければその概念を短い呼び名で指し示せます。将棋やチェスの妙手など、複雑な戦略に一言で言及できるようになるのと同様、LLM内部の未知の特徴を参照しやすくなるのです。

その結果、モデルのふるまいを確認したり議論したりする作業がスムーズになると期待されています。

#### 利点２：有用な抽象度を保ちやすくする

さらに、新語は「すべてを細部まで分析しきる」か「大ざっぱにまとめる」かという二択を避けるための中間レベルを提供しやすいとされています。細部をすべて追おうとすれば扱いきれないほど膨大な情報量になりますが、あまりに抽象的すぎると制御しづらくなります。

つまり、ちょうどよい抽象度を選択するための足がかりとして、新語が役立つ可能性があるのです。

#### 利点３：確証バイアスを軽減する

人間の既存の言葉をそのまま使ってしまうと、以前から持っていた概念とLLM内部の仕組みが似ていると感じるだけで、「やはりそうだったのか」と決めつけてしまう危険があります。そのため、新しい呼称を採用することで、先入観から離れた視点を保ちやすくなるわけです。

「人間の把握している意味」とは異なる可能性がある以上、まったく別の呼び名を与えておくことで、似ている部分と異なる部分を早い段階で検討しやすくなると考えられています。

#### 利点４：複合的な表現を可能にする

さらに、複数の新語を組み合わせることで、言語特有の柔軟な表現力を活かせる点も強調されています。たとえば、「一定時間持続する学習傾向」と「注意配分の偏り」をそれぞれ新語で定義した場合、それらを掛け合わせて「一定時間続く注意の偏り」といった複雑な概念を手短に表現できるようになるのです。

LLMが内部でどのように概念を組み立てているかを人間が細かく知る必要はないものの、外部からのコントロールや観察に役立つ指標として活用できます。

#### 利点５：制御のためのインターフェースとして機能する

最後に、新語を活用すれば、LLMへの指示や要望をより具体的かつ分かりやすい形で伝えられる可能性が高まります。

たとえば「もっと多様な解答を出してほしい」と要求する際も、「多様」という言葉だけでは曖昧です。このとき、「多様性」を示す特定の指標を新語として定義すれば、その概念を簡潔に呼び出せるようになります。

つまり、ユーザがモデルに意図を明確に伝えやすくなり、得られた応答を評価しやすくなる手段として、新語が有効そうというわけです。

## 様々な見解

LLMへの新単語導入がもたらす可能性やリスクをめぐっては、多種多様な意見が提示されています。たとえば「モデルをさらに大規模化していけば、人間が日常的に使う概念とのずれは自然に解消される」と楽観視する立場があったり、「わざわざ新しい言葉を導入しなくても、すでにある語彙だけで十分に説明できる」という主張も根強かったりします。「新語を本当に導入しなければならないのか」、それぞれの見解が検討されているのです。

ここでは四つの代表的な意見と、それに対して指摘されている問題点を順に整理します。規模拡大で自然に人間の概念へと近づくという見解、既存の言葉だけで充分だという考え方、内部構造のすべてを網羅的に明らかにすれば解決するという主張、そして抽象化を排除すべきだという立場が取り上げられています。

### トピック１：規模拡大が問題を解決するのではないか

まず、LLMの性能が上がり続けていることを背景に、「そのまま計算資源を増やしていけば、いずれ人間の常識的な概念や高次の知識とも一致するようになる」という意見があります。昔は拙い文章しか生成できなかったモデルが、今では人間並みに巧みな言葉遣いを身につけている事実が、その根拠になっているのです。

しかし、性能の向上と人間の概念との一致が必ずしも直結しているわけではありません。チェスや将棋などでLLMが示す超人的な戦略は、人間が理解しきれないロジックや着想を含んでいる可能性があるからです。

つまり、出力が優秀になるほど、人間側がその思考回路を追いかけられなくなる懸念があるのです。にもかかわらず、「規模さえ大きくすれば、自然に溝が埋まる」と楽観視しすぎると、モデルならではの独自性に気づかないまま制御不能の状態に陥るリスクが考えられます。

そのため、ただ大きくすればすべて解決するわけではなく、説明可能性や制御の観点は別途検討されるべきだという声が上がっています。

### トピック２：既存の語彙だけで充分に説明できるのではないか

人間の言語にはすでに十分な語彙が備わっているため、あえて新たな造語を使わなくてもLLMの動きを説明できるだろうという意見も見られます。たとえば英語や日本語には豊富な表現力があり、複雑な現象や概念も何らかの比喩や説明文を組み合わせれば、ある程度は伝えられるはずだと考えられているのです。

つまり、「よく使われる言葉」を組み合わせるだけで、LLMの振るまいも整理できるのではないかというわけです。

ただし、そうした方法は冗長性が高まりやすく、共通の概念を素早く参照しにくいという問題が指摘されています。LLMが生み出した独特のパターンや行動原理を、既存の言葉を寄せ集めながらいちいち解説していると、説明が膨大になってしまい、別の文脈で再利用する際も手間がかかります。

一度しっかりと新しいラベルを付けておけば、同じ概念を手短に指示したり再利用したりしやすくなるのです。つまり、「既存の語彙だけでも不可能ではないが、効率の面からいえば新語の導入が有効」という見解が成り立つわけです。

### トピック３：LLMを徹底的にマップ化すれば完全に理解できるのではないか

LLMの構造をすべて余すところなく分解し、徹底的にマッピングすることで説明可能性を高められるという主張もあります。つまり、モデル内部のパラメータや結合の状態を一つひとつ調べ上げ、どこでどんな計算が行われているのかを「地図」のように作り上げれば、不透明な部分はなくなると考えられているのです。

膨大な要素を網羅的に整理できれば、LLMの思考プロセスを完全に可視化できるのではないか、というわけです。

とはいえ、あまりに情報量が多すぎると、かえって人間には扱いきれない問題が生じると指摘されています。

生物学の領域でも、ごく単純な生物の神経回路図が完全にわかっていても、その生物がどう行動を決定しているかを十分に説明しきれない状況があります。LLMにおいても、すべてを並べ立てるだけでは「最終的にどこが重要か」を見極めづらくなるおそれがあるといえます。

その膨大なマップをどう使うのかが具体的に定義されていないかぎり、説明可能性や制御のしやすさに結びつくとは限らないのです。

### トピック４：抽象化は不要ではないか

最後に、抽象化をできるだけ排除し、LLMの振るまいを完全に追いかけるべきだという意見も取り上げられています。

たとえば脳科学の分野では、神経回路の構造を物理的レベルで徹底的に分析するアプローチがあるように、LLMでも重みや演算を一つひとつリバースエンジニアリングしさえすれば、抽象的な概念は不要ではないかというわけです。

つまり、根本原理を直接解き明かせば、曖昧な言葉やレイヤーに頼る必要がなくなるはずだという主張です。

しかし、そこまで低いレイヤーの情報をすべて把握したところで、人間が具体的にどう制御すればいいのかが見えにくくなる懸念が払拭されていません。トランジスタがどのように動作しているかを詳細に知っていても、実際にソフトウェアを設計・運用するときには別の抽象度が必要になるのと同様です。つまり、あるレベルの抽象化を導入しないかぎり、実用の現場や研究応用のフェーズで有益な指針を得にくい可能性があります。

## LLMの解釈可能性をアップデートする

LLMの理解を深めるための研究分野では、これまでも可視化技術や説明手法、内部表現の分析など、さまざまなアプローチが検討されてきました。ただし、それらの研究が生み出した知見を、最終的にどのように人間の言葉に落とし込むかという問題が依然として残るとされてきました。

そこで、新しい語彙を導入すれば、既存のアプローチが見出した内部の特徴をより正確に表現したり、人間が把握しやすい形で要約したりするチャンスが増えると考えられています。

### どの入力がどれだけ影響したのかを示す

LLMの出力にどの入力要素がどの程度影響しているかを可視化する「特徴帰属」系の手法は、説明可能性の観点から非常に重要だと認識されています。たとえば、単語やフレーズごとの重みづけをヒートマップのように示す技術がこれに該当します。

ただし、人間がその結果を見ても、色の濃淡だけでは「モデルが何を考えているのか」具体的に想像しにくいという課題がありました。

新しい単語を導入できれば、ヒートマップ上の部分的なパターンに対して固有の名前を付与し、そのパターンの意味を論じやすくすることができるかもしれません。つまり、ただ色の強弱を見るだけでは漠然とした印象にとどまっていた情報を、新たな語彙によって明確に参照しやすくなるわけです。

### モデルが学んでいる未知の特徴を見つける

ネットワークの内部から未知の概念を自動的に探し出す「概念発見」の研究は、画像モデルなどで活発に行われてきました。たとえば、すでに知っている概念（犬の耳や車のタイヤなど）をネットワークの中から検出する手法は、その概念の存在を可視化するうえで有効です。

ただし、人間が認識していない新しい概念がLLMの中に潜んでいる場合は、既存の語彙に対応する呼び名がまだ用意されていない可能性があります。

新しく作った言葉をあてがうことで、モデルが内包している潜在的な特徴をもっと自由に表すことができるようになります。つまり、新語を使ってLLMの未知の領域を呼び出し、整理し、議論にかけるといった取り組みに発展する余地があるという考え方です。

### 説明が本当に合っているかを確かめる

LLMの信頼性や安全性を確保するうえで、モデルの説明が本当にモデル内部のプロセスを正しく反映しているかを検証することは欠かせない要素だといわれてきました。ただし、数値で評価して「一定の指標を満たしたから大丈夫」とするだけでは、人間にとって直感的にわかりにくい場合があります。

既存の語彙で示すには微妙なズレが生じる概念について、新語を導入すれば、より厳密に呼び出して検討できる可能性が高まります。言い換えれば、人間が「何を意味するのか」を明確にイメージできる名前を用意しておくことで、モデルがどの程度正しい説明を行っているかを詳細にチェックできるようになるわけです。

### 内部表現を探る

ネットワーク内部のベクトル表現を [分類器](https://ai-data-base.com/archives/26489 "分類器") などで読み取る手法は「プロービング」と呼ばれています。ある層でどんな情報が符号化されているかを調べることで、LLMの知識の断片を外部から観測できるのが特長です。

この分野では、得られた結果をどのように人間の言葉で参照し、モデルの制御に反映させるかが明確でないという課題が指摘されていました。

新しい語彙を導入すれば、プロービングで見いだされた特徴を短い呼び名で呼び出しやすくなり、それらを活用した「表現エンジニアリング」の範囲を広げられると考えられています。たとえば、特定のベクトルの方向性を強調する処理を新語と結びつけることで、モデルに対する細やかな調整を人間が直接指示しやすくなるかもしれません。

## 実証

新しい語を単に作るだけではなく、その語をモデルに学習させ、限られた変更で応答を制御する具体的な方法が考案されました。人間が名付けた新語をモデル側に認識させ、必要に応じて振る舞いを切り替えられるかどうかを試す取り組みです。

大規模なパラメータをほとんど変えず、新たに追加した単語だけの埋め込みベクトルを学習させることで、LLMが未知の概念を使いこなすようにする方法が提案されています。

モデルの内部構造や既存の重みはなるべく固定しておき、新語に対応するベクトルだけを調整するのです。そうすれば、元のモデル機能を損なわずに新しい命令を学習できる可能性が高まると考えられています。

### アプローチ

プロンプトと似た発想を用いながら、新語として明確に定義した単語をトークナイザに追加し、その単語の埋め込みベクトルだけを学習させる方法がとられました。言語モデルに対して、人間があらかじめ作った新語を含む文章を入力し、そこから得られる出力を正解・不正解に振り分けて学習を進めます。

ただし、更新をかけるのは新語のベクトルのみです。そうすると、新語が使われない限りモデルは元の状態を維持し、新語を利用したときだけ新たな振る舞いを示すようになります。

この手法はパラメータ全体を再調整する必要がないことが利点だとされています。またプロンプトとの違いとしては、ユーザが自然言語の文章の中で自由に新語を呼び出せる点が大きいと説明されています。たとえば、複数の新語を組み合わせて使用することができるため、モデルが学習した複数の振る舞いを柔軟に組み合わせることも可能になるわけです。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_84361_4.png)

新語の埋め込み学習の概念図。新語を追加した場合としない場合でのモデルの振る舞いの違いを示す

### 本手法のメリット

学習するパラメータの数が非常に少なく、既存のモデル全体を再学習しなくて済む点は大きなメリットだと考えられています。プロンプトやLoRAといった手法と同じく、大掛かりな再学習を避けながらモデルの出力を変化させられる方法ですが、文字列として扱える新語が使えるところに特徴があるという見方がされています。

そのため、専門領域の概念を追加で覚えさせたり、特殊な振る舞いを後から付与したりする応用が期待されます。既に完成度が高いベースモデルに対して、新たな単語を挿入し、それを少量の学習で使いこなせるようにする仕組みは、LLMを部分的に拡張するうえで効率的です。

### 実験：文量制御

文章の長さを思い通りに制御するのは、高性能なLLMでも意外と難しい課題だといわれてきました。たとえば「長めに書いて」と頼んでも、モデルが必ずしも意図した長さに収めるわけではないからです。

そこで研究者は「指定した範囲の長さに文章を収める」という概念を新語として定義し、その埋め込みを学習させる実験を行いました。

実際の学習では、あらかじめ決めた長さ制限を守った出力を正解、制限を超えた出力を不正解として設定し、新語のベクトルを調整し続けます。すると、新語が含まれたプロンプトで指示を与えた場合にのみ、モデルが文量をコントロールしやすくなる傾向が観測されました。

つまり、元の性能を維持しながら、新語を使う場面でだけ「長さをコントロールする機能」が追加される形になるわけです。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_84361_5.png)

新語を用いた長さ制御の実験結果。従来のプロンプトでは長い文章を生成できなかったが、新語を使うことで意図した長さの出力が可能になることを示す

### 実験：出力の多様性を高める

同じ質問を何度も投げても、似たような回答ばかり返ってくると創造的な応用が難しくなります。そこで、人間が「もっと多彩な回答を見たい」と思うときに使える新語を学習させる実験も行われました。

多様性の高い回答を正解、似通った回答ばかりを不正解として設定し、新語の埋め込みを学習していきます。

その結果、新語を含めたプロンプトで指示を与えると、以前よりもバリエーションに富んだ応答が返るようになったと報告されています。つまり、パラメータ全体を大幅にいじることなく、最小限の調整だけで「多様さ」を強化できる点が注目されています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_84361_6.png)

新語を用いた多様性制御の効果を示すグラフ。従来の方法では偏った応答が生成されがちだったが、新語を使うことで応答のバリエーションが向上することを確認

### 実験：モデルの好みを引き出す

モデル自身が「良い回答」と判断するパターンを、人間側で逆に学習し、新語にまとめる手法も検討されています。たとえば、ある指示に対して複数の回答を生成させ、モデルが高評価をつけたものを正例、低評価のものを不正解として集めます。

すると、モデルがどんな回答を「好ましい」と感じているのかをデータ化できるわけです。

それを新語として学習させた結果、その新語を使って「モデル自身が良いと思う回答を出してほしい」と指示すると、モデルの好みが反映された応答が得られやすくなりました。逆に「モデル自身が良くないと判断する回答を出してほしい」と命じれば、モデルの好みに合わないパターンを引き出すことも可能になるとのことです。

つまり、新語を介してモデルの内面的な基準を観察・利用できるようになり、モデルと人間のあいだで「何をよしとするのか」という意識のすり合わせを行う手がかりにもなると考えられています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_84361_7.png)

「モデルが良いと考える応答」に関する実験結果の一例。新語を使うことで、モデルが内部で評価する「良い回答」と「良くない回答」の違いを引き出せることを示す

## まとめ

新しい言葉（ネオロジズムと呼ばれます）を定義し、それをモデルに学習させる試みが行われました。その背景には、従来の語彙だけではLLMが内包する概念を十分に表現しきれない可能性が指摘されてきたという経緯があります。

今回の実験によると、新語を導入することで応答の長さや多様性を制御しやすくなり、モデル内部の概念をより正確に参照できる傾向が示唆されました。

とはいえ、こうした手法がどの程度まで一般化できるのか、また複雑な概念に対しても適用可能なのかは今後の検討課題といえます。

LLMとのやりとりを滑らかに行ううえで、新しい語彙が果たす役割はさらに注目を集めそうです。

**参照文献情報**

- タイトル：We Can’t Understand AI Using our Existing Vocabulary
- URL： [https://doi.org/10.48550/arXiv.2502.07586](https://doi.org/10.48550/arXiv.2502.07586)
- 著者：John Hewitt, Robert Geirhos, Been Kim
- 所属：Google DeepMind

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[「すべてのソフトウェアをエージェントとして使う」ビジョンと実践例](https://ai-data-base.com/archives/84295)

[LLMを擬人化することは開発や評価にどんな影響を及ぼすか](https://ai-data-base.com/archives/84526)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)