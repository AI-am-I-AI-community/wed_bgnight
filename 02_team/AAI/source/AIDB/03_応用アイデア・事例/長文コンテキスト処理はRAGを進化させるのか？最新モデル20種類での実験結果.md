---
title: "長文コンテキスト処理はRAGを進化させるのか？最新モデル20種類での実験結果"
source: "https://ai-data-base.com/archives/79561"
author:
  - "[[AIDB Research]]"
published: 2024-12-02
created: 2025-06-13
description: "本記事では、最新のLLMにおける長文コンテキスト処理能力の進化と、従来のRAG（情報検索＋生成）技術への影響について紹介します。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、最新のLLMにおける長文コンテキスト処理能力の進化と、従来のRAG（情報検索＋生成）技術への影響について紹介します。近年、OpenAIのo1やClaude、Geminiなど、膨大なトークンを処理できるモデルが登場し、それに伴ってRAGワークフローの在り方が問い直されています。そこで今回、20種類のモデルを対象とした大規模な実験が行われました。長文コンテキストRAGの可能性と課題について見ていきましょう。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_79561_thumnail-1024x576.jpg)

**発表者情報**

- 研究者：Quinn Leng et al.
- 所属：Databricks Mosaic Research

## 背景

LLMの長文コンテキスト処理能力は飛躍的に向上しており、例えばOpenAIのo1が12.8万トークン、AnthropicのClaudeが20万トークン、そしてGoogle Gemini 1.5 Proは200万ものトークンを処理することができるようになっています。

そのため従来のRAGワークフローが変わるのではないかという議論が生まれています。RAGとは、外部ソースから情報を取得することでLLMの精度を向上させる重要な技術です。特定のタスクに関連する情報や非公開データをLLMのワークフローに組み込む際には必須です。これまでの研究では、機械翻訳、意味解析、質問応答、そしてオープンエンドなテキスト生成など、多くの分野でRAGの有効性が実証されてきました。

なお、RAGと長文コンテキストの2者を単純に比較する研究も行われてきました。知識ベースから適宜情報を取り出して処理するのが良いのか、 [コーパス](https://ai-data-base.com/archives/26324 "コーパス") 全体をLLMのコンテキストウィンドウに含めるのが良いのかということです。ただしどちらのアプローチが正確な結果をもたらすのか、またコスト効率が良いのかについては、まだ結論が出ていない状況です。

長文コンテキストモデルそのものの課題も指摘されています。これまでの研究では、長いテキストにおける中間部分の情報の保持・活用に苦労することや、コンテキスト長が増加するにつれて性能が低下することが示されています。”性能が低下しない範囲のコンテキストの長さ”は、プロバイダーが提供する最大のコンテキスト長よりも実ははるかに短いと言われています。

ここで、RAGと長文コンテキストLLMを組み合わせるのが良いのではないかという発想が生まれてきます。それを確かめるには、長文コンテキストモデルがRAGの性能をどのように向上させられるのか、また、その限界や課題は何かを調査する必要があります。

そこで今回研究者らは、長文処理能力を持つ代表的なモデル20種類を対象に実験を行いました。

なお、 [以前の記事](https://ai-data-base.com/archives/71774) でLongRAGという手法を紹介しましたが、本稿で紹介する研究は「シンプルなRAGを使って様々なLLMで比較実験」しているのに対し、以前紹介したLongRAGは独自のコンポーネントを新たに提案した新しいRAG [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") の「提案」が主題である点が大きな違いです。

## RAGとコンテキストウィンドウの基礎

まずは、そもそもRAGとはなにか、コンテキストウィンドウ（文脈長）とはなにかといった部分から説明します。

### RAGの基本原理

RAG（Retrieval Augmented Generation）の基本的な仕組みは以下の2段階で構成されます。

1. 検索（Retrieval）：ユーザーの質問に関連する情報をデータベースから取り出す
2. 生成（Generation）：取り出した情報とユーザーの質問を組み合わせて、LLMが回答を生成する

### コンテキストウィンドウとその重要性

コンテキストウィンドウとは、LLMが一度に処理できる文脈の長さを表します。文脈の長さは「トークン」という”言葉の単語やその一部”を表す単位で測定します。

モデルによって処理可能な長さは異なり（例：GPT-3.5は4,000トークン、最新モデルでは200万トークンまで）、入力（質問や参照文書）と出力（回答）の両方に影響します。

### シンプルなRAGアーキテクチャの採用理由

本研究では、以下の理由からシンプルなRAG [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") が採用されました。

- 基本性能の正確な評価が可能
- モデル間の公平な比較ができる
- 実用的なシナリオを反映している

（”シンプルでない”RAG [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") とは、例えば検索結果の再ランキングを行う多段階検索や、複数のLLMを組み合わせたハイブリッド処理などが挙げられます）

### 長文コンテキスト性能を検証することの重要性

長文コンテキストでの性能検証が重要視された理由は、まず第一により多くの関連文書を参照できる可能性があるためです。また、単一の長い文書をより深く理解できる可能性があります。

そのため従来のRAGワークフローを置き換えられる可能性が指摘されており、コスト効率の観点から最適なコンテキスト長を見極める必要もあります。

ただし、この研究では（結論から言えば）長いコンテキストウィンドウを活用できる最新のLLMであっても、それが必ずしもRAGの性能向上につながるとは限らないことが示されました。つまり、単にコンテキストウィンドウを長くすれば良いわけではなく、適切な長さの見極めが重要であることが明らかになっています。

## 研究手法

以下のような方法で実験が実施されました。

### データセットと評価対象

以下3つのデータセットで評価が行われました。

- Databricks DocsQA（データブリックス社の技術文書に関する質問応答データセット）
- FinanceBench（金融関連の質問応答データセット）
- Natural Questions（一般的な質問応答データセット）
![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_79561_table2.png)

評価された20のLLMは以下の通りです。

商用モデル

| モデル名 | 最大コンテキストウィンドウ | バージョン（リリース日） |
| --- | --- | --- |
| o1-mini | 128k | o1-mini-2024-09-12 |
| o1-preview | 128k | o1-preview-2024-09-12 |
| Gemini 1.5 Pro | 2,000k | gemini-1.5-pro-001 (2024-06-27) |
| Gemini 1.5 Flash | 2,000k | gemini-1.5-flash-001 (2024-06-27) |
| GPT-4o | 128k | gpt-4o-2024-05-13 |
| Claude 3.5 Sonnet | 200k | claude-3-5-sonnet-20240620 |
| Claude 3 Opus | 200k | claude-3-opus-20240229 |
| Claude 3 Haiku | 200k | claude-3-haiku-20240307 |
| Claude 3 Sonnet | 200k | claude-3-sonnet-20240229 |
| GPT-4o-mini | 128k | gpt-4o-mini-2024-07-18 |
| GPT-4-turbo | 128k | gpt-4-turbo-2024-04-09 |
| GPT-4 | 128k | gpt-4-0125-preview |
| GPT-3.5-turbo | 16k | gpt-3.5-turbo-0125 |

オープンソースモデル

| モデル名 | 最大コンテキストウィンドウ | バージョン（リリース日） |
| --- | --- | --- |
| Llama 3.1 405B | 128k | meta-llama/Llama-3.1-405B-Instruct (2024-07-23) |
| Llama 3 70B | 8k | meta-llama/Meta-Llama-3-70B (2024-03-18) |
| Llama 3.1 70B | 128k | meta-llama/Llama-3.1-70B (2024-07-23) |
| Llama 3.1 8B | 128k | meta-llama/Llama-3.1-8B-Instruct (2024-07-23) |
| Qwen-2-72B | 128k | Qwen/Qwen2-72B-Instruct (2024-06-06) |
| Mixtral-8x7B | 32k | mixtral-8x7b-instruct-v0.1 (2023-12-11) |
| DBRX | 32k | databricks/dbrx-instruct (2024-03-27) |

### 検索手法の詳細

RAGにおける検索段階では、一貫性を保つため全ての設定で同じ埋め込みモデル（ [OpenAI text-embedding-3-large](https://openai.com/index/new-embedding-models-and-api-updates/) ）が使用されました。

他の設定は以下の通りです。

- チャンクサイズ：512トークン
- ストライド：256トークン
- ベクトルストア：FAISS（IndexFlatL2インデックスを使用）

なお、「ストライド (stride)」とは、文書やデータを小さなチャンク（部分）に分割する際のオーバーラップの具合を示します。ストライドが小さいほど、チャンク間のオーバーラップが大きくなり、多くの重複する情報を含むことになります。重複が増えると、文脈の保全には役立ちますが、計算コストも増大します。

### 評価方法

生成パフォーマンスの評価は、以下の条件で実施されました。

- コンテキストウィンドウ：2,000トークンから128,000トークン（可能な場合は200万トークン）まで変化させて測定
- 温度パラメータ：0.0に設定
- 最大出力シーケンス長：1,024トークンに設定

質問に対する回答は、キャリブレーションされたGPT-4oが「Judge LLM」として使用されて評価しました。

参考： [LLMを「評価者」として活用する『LLM-as-a-judge』の基本](https://ai-data-base.com/archives/79428) （AIDB）

## 研究結果

主要な発見を見ていきます。

### 長文コンテキストとRAG性能の関係性

実験の結果、長文コンテキストの活用が必ずしもRAG性能の向上につながらないことが明らかになりました。とはいえ、一定の長さまでは性能が順調に上がることもあります。

まず、最新の商用モデルは、コンテキスト長の増加に応じて着実な性能向上を示しました。一方、オープンソースモデルの多くは、最初は性能が向上するものの、その後低下する傾向が観察されました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_79561_fig1-1024x555.jpg)

長文コンテキストRAG性能の比較結果。3つのデータセット（Databricks DocsQA, FinanceBench, Natural Questions）での評価。

以下のモデルが100,000トークンまで一貫した精度の向上を示しました。

- o1シリーズ（preview、mini）
- GPT-4oシリーズ（標準、mini）
- Claude 3.5 Sonnet、Claude 3 Opus
- Gemini 1.5 Pro

これらのモデルは、性能のピーク後も大幅な低下を示すことなく、安定した結果を維持しました。

### モデル別の特徴的な挙動

オープンソースモデルの中では、Qwen 2 70Bが64,000トークンまで安定した精度を維持しました。一方、Llama 3.1 405Bは32,000トークン以降で性能低下が始まり、GPT-4-0125-previewは64,000トークン以降で低下が観察されました。

OpenAI o1モデルはGPT-4およびGPT-4oと比較して顕著に高い性能を示しています。また、Google Gemini 1.5（ProおよびFlash）モデルは、128,000トークンまでの範囲ではo1やGPT-4oと比べて全体的な正答率は低いものの、2,000,000トークンという極めて長いコンテキストでも一貫した性能を維持するという独特な特徴を示しました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_79561_b_s2-1024x569.png)

Natural Questionsでの長文コンテキストRAG性能グラフ

なお、金融ドメインに特化した質問応答タスクも検証されました。実験の結果、商用モデルとオープンソースモデルの間で明確な性能差が観察されています。最も高い性能を示したのはo1-mini（2024年9月版）で、64,000トークン付近で約0.75の正答率を達成しました。これに続いてClaude 3.5 SonnetとClaude 3 Opusが0.65から0.7程度の正答率を記録しています。

コンテキスト長と性能の関係については、興味深い傾向が確認されました。2,000から4,000トークンまでの初期段階では、ほとんどのモデルが急激な性能向上を示しました。しかし、8,000から32,000トークンの中間段階では、性能向上が緩やかになり、その後のプラトー（性能の頭打ち）が観察されています。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_79561_fig2-1024x556.jpg)

FinanceBenchデータセットにおける長文コンテキストRAG性能の結果。

また、64,000トークン以上の長文コンテキスト領域では、モデル間で大きな挙動の違いが見られました。商用モデルが比較的安定した性能を維持したのに対し、オープンソースモデルの多くは性能の低下を経験しています。  
以上の結果は、金融という専門性の高い領域において、モデルの基本的な性能差がより顕著に表れることを示唆しています。

### 失敗パターンの分析

長文コンテキストシナリオにおける各モデルの失敗パターンにも、顕著な違いが観察されました。Natural Questionsデータセットでの分析によると、Claude 3 Sonnetは著作権への懸念から回答を拒否する傾向が強く、特に長いコンテキストでこの傾向が顕著でした。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_79561_s6.png)

Claude 3 Sonnetの指示に従えなかった例

またGemini 1.5 Proは極めて長いコンテキスト（最大200万トークン）でも一貫した性能を示しましたが、厳格な安全性フィルターにより、一部のタスクで失敗する事例が確認されました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_79561_s10.png)

Gemini 1.5 Proの誤答例

オープンソースモデルについては、Llama 3.1 405Bが64,000トークンまで一貫した失敗パターンを維持した一方、Mixtral-8x7Bは長いコンテキストで内容の重複やランダムな生成が増加する傾向を示しました。また、DBRXは16,000トークン以上のコンテキストで指示に従わず、質問に直接答えるのではなく内容の要約を行う傾向が観察されました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_79561_fig3-1024x604.jpg)

Natural Questionsデータセットにおける、Gemini 1.5 Pro、Claude 3 Sonnet、Mixtral 8x7B、Llama 3.1 405Bの失敗分析。安全性フィルターや著作権の懸念による失敗を示している。

以上が、「長文コンテキストLLMはRAGの性能を向上させることができるのか」というシンプルな問いに対応する実験結果です。

## 考察

実験から得られる考察は以下の通りです。

### 考察１：最新モデルの可能性と限界

o1、GPT-4o、Claude 3.5、Gemini 1.5、そしてQwen 2 70Bといった最新のモデルにおいて、長文コンテキストの活用は一貫してRAGの性能向上につながることが確認されました。しかし、この傾向はすべてのモデルに当てはまるわけではありません。分析対象となったモデルの大多数では、16,000から32,000トークンまでの範囲でのみ性能向上が観察されました。

### 考察２：o1モデルの優位性に関する考察

o1モデルが示した優れた性能について、研究チームは「テスト時の計算能力の向上」が要因であると推測しています。紛らわしい質問への対処や、関連性の低い検索文書に惑わされることなく適切な回答を生成できる可能性が指摘されています。

### 考察３：アラインメントと安全性の課題

興味深い発見として、Natural Questionsデータセットにおけるモデルの失敗パターンの多くが、アラインメント（Claude 3 Sonnet）や安全性フィルタリング（Gemini 1.5 Pro）に起因していました。研究チームは、これらの機能が短いコンテキストでの学習に基づいており、長文コンテキストでは必ずしも適切に機能しない可能性を指摘しています。

### 考察３：実用化に向けた考察

128,000トークン（あるいはGeminiの場合200万トークン）未満の [コーパス](https://ai-data-base.com/archives/26324 "コーパス") に対しては、理論的には検索ステップを省略し、データセット全体をLLMに直接入力することが可能です。しかし、必ずしもそれが最適ではない理由は以下の通りです。

- コストが著しく高くなる可能性がある
- 性能が低下する可能性がある
- 開発者体験の簡素化とコストのトレードオフが発生する

### 備考：コストに関する現状分析

2024年10月時点での128,000トークンの処理コストは以下のとおりです。

- GPT-4o：$0.32
- o1-preview：$1.92
- Claude 3.5 Sonnet：$0.384
- Gemini 1.5 Pro：$0.16

長文コンテキストでのRAGは、従来のベクトルデータベースを用いた手法と比較して大幅にコストが高くなります。

ただし、バッチ処理や [コーパス](https://ai-data-base.com/archives/26324 "コーパス") のキャッシュによってコストを軽減できる可能性があります。また、この1年でGPT-4の入力トークンあたりのコストが$30から$2.5まで低下したことを考えると、将来的にはより実用的なコストで128,000トークンの処理が可能になると期待されます。

## まとめ

本記事では、長文コンテキスト処理が可能なLLMがRAGの性能向上にどう影響するかを検証した研究を紹介しました。  
研究から得られた主な知見は以下の通りです。

まず、最新のLLM（o1、GPT-4o、Claude 3.5、Gemini 1.5など）では、RAGのコンテキスト長を伸ばすことで一貫して性能が向上することが確認されました。ただし、これは一部の最新モデルに限った傾向で、多くのオープンソースモデルでは16k-32kトークン程度が実用的な上限となっています。この範囲を超えると性能が低下する傾向が見られました。

長文処理時の失敗パターンは各モデルで異なり、例えばClaude 3 Sonnetは著作権への懸念から回答を拒否する傾向が、Gemini 1.5 Proは安全性フィルターが過度に厳格になる傾向が観察されています。

以上から、RAGシステムの設計においては「単にコンテキスト長を増やせば良いわけではなく、使用するモデルの特性に応じた適切な設定が重要である」と示唆されています。今後は、より多くのモデルが長文処理能力を向上させていく中で、RAGシステムの [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") もさらなる進化を遂げていくことが予想されます。

**参照文献情報**

- タイトル：Long Context RAG Performance of Large Language Models
- URL： [https://arxiv.org/abs/2411.03538](https://arxiv.org/abs/2411.03538)
- 著者：Quinn Leng, Jacob Portes, Sam Havens, Matei Zaharia, Michael Carbin
- 所属：Databricks Mosaic Research

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[『LLM-as-a-judge』のさまざまな応用と分野の展望](https://ai-data-base.com/archives/79535)

[時系列データをグラフにしてLLMに見せると文字だけより最大120%性能向上　トークンも節約](https://ai-data-base.com/archives/79686)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)