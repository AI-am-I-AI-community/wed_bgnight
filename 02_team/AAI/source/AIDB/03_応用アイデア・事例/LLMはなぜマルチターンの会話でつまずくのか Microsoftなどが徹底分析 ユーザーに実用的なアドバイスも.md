---
title: "LLMはなぜマルチターンの会話でつまずくのか Microsoftなどが徹底分析 ユーザーに実用的なアドバイスも"
source: "https://ai-data-base.com/archives/89709"
author:
  - "[[AIDB Research]]"
published: 2025-05-19
created: 2025-06-13
description: "本記事では、LLMがマルチターンの会話でどのようにつまずくのかを検証した研究を紹介します。マルチターンの応答精度が期待どおりに保たれないことは、実運用においてもしばしば課題になります。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、LLMがマルチターンの会話でどのようにつまずくのかを検証した研究を紹介します。

マルチターンの応答精度が期待どおりに保たれないことは、実運用においてもしばしば課題になります。  
MicrosoftとSalesforceの研究チームは、その背景にある構造的な要因を大規模な実験で分析しました。

ユーザーや開発者が今できる対応についても、実践的な視点から示されています。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89709-1024x576.png)

## 背景

ChatGPTをはじめとするLLMは、いまや日常の業務や調査で使われるツールとして多くの人に利用されています。問いかけに自然に答えるだけでなく、複数のやり取りを通じて目的を一緒に整理していく存在として期待されています。

使い方としては、最初はぼんやりとした要望を伝えるところから始まり、「そういえばこういう条件も入れてください」「やっぱりこういう方向に変えてください」と、やり取りを重ねる中で指示が具体的になっていく場面も多く見られます。  
LLMの対話ログを分析した研究では、会話の初期は「情報不足」がちであることも確認されています。

こうした使い方が現実には当たり前になってきている一方で、LLMがそうしたやり取りにどれだけ対応できているかは、あまりハッキリしていません。というのも、LLMの能力を測る際の評価の枠組みが、そもそも実際の使い方とはズレた軸を立てがちだからです。

たとえば、多くの評価では「最初から条件がすべてそろった一発勝負の指示」が用いられます。要するに、会話の中で少しずつ情報が明らかになっていくようなプロセスに対するモデルの柔軟さや持続力を測っていないのです。

業務への応用を考えるなら、こうした状況にどこまで付き合えるかは無視できない要素です。少し曖昧な問いかけにも丁寧に対応しながら、次第に相手の意図を汲み取っていけるのか。それができるかどうかで、モデルに任せられる範囲も変わってきます。

この記事は、そうした疑問に正面から向き合おうとしている事例を紹介します。あえて情報が出そろわない状態から始まるマルチターンの会話を設計し、実際にLLMがどのように応じるのかが検証されたものです。対象となったのは、オープンソースのモデルから商用の高性能モデルまで（GPT-4.1やClaude-3.7 Sonnetなども含む）。幅広いタスクを用いて横断的な比較が行われています。

以下で詳しく見ていきます。ユーザーが今できる妥当な対処法や対策案についてもまとめています。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89709_1-1024x537.png)

今回紹介する研究の実験デザイン全体像

## LLMが本当に人の話をうまく汲み取れているのか、という問い

もちろん、「マルチターンでも評価しよう」という動きもこれまでにありました。とはいえ、まだわからない部分が多くあります。実際のやり取りに近い形で評価できているかというと、まだ課題が残っています。

過去の研究では、「一応マルチターンではあるけれど、毎回それぞれのターンに明確なタスクが与えられていて、順に答えていけばよい」といった形式が多く、現実のような曖昧さや揺れには対応しきれていません。

今回紹介する研究は、その点を見直しています。あえて「情報が足りないまま始まるやり取り」を設計し、LLMがどんなふうに反応するかを調べています。評価に使われたのは、文章生成やコード生成といった実用性の高いタスクです。

さらに、ユーザー役にはLLMを活用したシミュレーターを使い、本当のやり取りに近いパターンを大量に再現しています。完全に現実そのものとは言えないものの、かなり実際に寄せています。

## 情報が足りない会話をどう再現したのか

今回の研究では、「最初の指示が不完全で、やり取りを通じて情報が明らかになっていく」という状況を実験としてどう再現するかがひとつのポイントになっています。

研究チームは、もともと単一ターン向けに設計されていた既存のタスクを活用しながら、マルチターン評価に適した形式へと変換する仕組みをつくりました。その中核となっているのが「シャーディング」という手法です。

### シャーディングとは

もとの指示文をいくつかの「かけら（シャード）」に分解し、それを順番に明かしていくことでマルチターン会話を構成します。

たとえば、数学の問題があったとします。

**完全な指示**  
ジェイは妹との雪合戦に備えて雪玉を作っています。彼は1時間で20個の雪玉を作れますが、15分ごとに2個溶けてしまいます。彼が60個の雪玉を持つまでにどれくらいの時間がかかるでしょうか？

これを次のように分けて提示していきます。

- シャード1　ジェイが雪合戦の準備ができるまでにどれくらいかかる？
- シャード2　彼は妹との雪合戦に備えています。
- シャード3　彼は1時間に20個の雪玉を作れます。
- シャード4　彼は合計60個を目指しています。
- シャード5　問題は15分ごとに2個溶けることです。

このようにして、最初のシャードではざっくりとした問いだけが与えられ、その後のやり取りで細かい前提が徐々に明かされていきます。すべてを通して読めば、もとの指示と同じ内容になります。

シャーディングの作業はGPT-4oによって自動化されつつ、人間が確認・編集する形で実施されました。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89709_2-1024x281.png)

### 会話の流れはこう進む

シャーディングされた指示を使った会話では、以下の3つの役割が設定されています。

- ユーザー役のシミュレーター（LLM）はシャード全体を把握しており、会話の流れに応じてどのシャードを次に出すかを決めます。
- 評価対象となるアシスタント（LLM）は実際に評価されるモデルであり、何も知らない状態から会話に参加します。
- 評価システムはアシスタントの応答を分類し、成功かどうかを判断します。

最初のターンでは、ユーザーシミュレーターが最初のシャードだけを提示します。アシスタントはその情報だけをもとに自由に返答し、システムがその応答を次のような7つのタイプに分類します。

1. 明確化
2. 拒否
3. 回避
4. 質問
5. 議論
6. 無応答
7. 回答の試行

アシスタントが何かしらの答えを出そうとしたと判断されれば、その回答を抜き出してスコア化します。会話は、次のいずれかの条件を満たした時点で終了します。

- モデルが正しい回答にたどり着いた場合
- ユーザーシミュレーターがもう明かすシャードを持っていない場合

ユーザーシミュレーターは単に順番に情報を出すわけではなく、アシスタントの返答を見ながらどのシャードを次に出すべきかを判断します。たとえば、アシスタントが「何を目指しているのか？」と聞いてきたら、「60個を目指しています」といったシャードを選び直す必要があります。この制御には、軽量版のGPT-4oが使われました。

また、アシスタントには「これは情報不足の会話です」といった前提は与えられていません。あくまで自然な会話として扱われています。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89709_3.png)

### 5つの評価パターン

会話の形式は1つだけではなく、次の5種類のパターンで実験が行われました。

1. FULLY-SPECIFIED　最初からすべての情報が与えられる単一ターンのやり取りであり、モデルの基本性能を測る基準になります。
2. SHARDED　情報が少しずつ与えられるマルチターン会話であり、今回の研究の中心的な設定です。
3. CONCAT　シャーディングされた情報をすべて並べて1ターンで提示する形式で、文章は分割されたままですが情報の不足はありません。
4. RECAP　SHARDEDのやり取りの最後にすべての情報をまとめて再提示し、エージェントが要点を整理し直す効果を持つかどうかを調べます。
5. SNOWBALL　各ターンで新しい情報を出しつつ、これまでの情報も毎回まとめて伝える形式で、文脈の積み重ねによるサポート効果を測ります。
![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89709_4.png)

以上のように設計された評価フレームによって、情報の出し方やタイミングがLLMの応答にどんな影響を与えるのかが比較されました。

## 実験の規模とパラメータ

今回の研究では、結果の信頼性を高めるために、かなり大きな実験設計が組まれました。単一ターンとマルチターンで実際にどれだけ性能に差が出るのかを正確に捉えるため、モデルの種類やデータ量にも幅を持たせています。

### 大規模シミュレーション実験

メインの実験では、6つのタスクから合計600個の指示が選ばれました。

使用されたLLMは全部で15種類。各モデルごとに、すべての指示とシミュレーションタイプの組み合わせについて10回ずつ実行されました。全体としては、20万回を超える会話シミュレーションが行われた計算になります。

なお、LLMの温度設定はT = 1.0に固定して実施されています。また、温度の違いが性能や信頼性にどう影響するかを調べる補足実験も、後のパートであらためて紹介されています。

1つの条件につき10回の繰り返しを行うことで、単なる平均スコアだけでなく、モデルの基本的な能力と安定性・信頼性についても丁寧に見られました。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89709_5-1024x616.png)

### 評価対象となったLLM

今回の実験で評価されたLLMは下記の通りです。

- GPT-4o-mini（OpenAI）
- GPT-4o（OpenAI）
- o3（OpenAI）
- GPT-4.1（OpenAI）
- Claude 3 Haiku（Anthropic）
- Claude 3.7 Sonnet（Anthropic）
- Gemini 2.5 Flash（Google）
- Gemini 2.5 Pro（Google）
- Llama3.1-8B-Instruct（Meta）
- Llama3.3-70B-Instruct（Meta）
- Llama 4 Scout（Meta）
- OLMo-2-13B（AI2）
- Phi-4（Microsoft）
- Deepseek-R1（Deepseek）
- Command-A（Cohere）

最先端のモデルが重点的に取り上げられており、8Bクラスの比較的小さなモデルから300B超の大規模モデルまで幅広く含まれています。オープンウェイトとクローズドウェイトの両方が対象となっているほか、追加の推論時間を活用するモデル（たとえばo3やR1）も含まれており、マルチターンへの適応力がどのように違ってくるのかを多角的に観察できる構成になっています。

この規模の実験にかかった推定コストは約5,000ドルとされています。小さくない金額ですが、得られる知見の広さや深さを考えれば、それに見合う価値のある取り組みと言えるでしょう。

以上のように、研究チームは実験の設計において、規模の大きさだけでなく、モデルの多様性や条件の再現性にも気を配っています。

## 実験結果

ここからは、研究チームが行った大規模シミュレーションの結果を見ていきます。

### どのモデルも、マルチターンでスコアが下がる

まず最初の大きな発見は、どのモデルも例外なく、マルチターンになるとスコアが下がるということでした。最初からすべての情報がそろっている単一ターン形式（以下、完全指示形式）と、情報が少しずつ出てくるマルチターン形式（以下、分割指示形式）を比べたとき、平均して39%ものスコア低下が見られたそうです。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89709_6-1024x515.png)

研究チームはこの現象を「会話で迷子になる（Lost in Conversation）」と呼んでいます。単一ターンでは90%以上の正答率を出していたようなモデルでも、現実的なマルチターンの会話に移ったとたん、大きくつまずいてしまうという傾向です。

一方、すべての情報を一括で提示する箇条書き形式（以下、連結指示形式）では、完全指示形式とほぼ同じスコアが保たれていました。具体的には、約95.1%に相当します。これは、性能の落ち込みが「情報の言い換え」や「シャードの分割」にあるのではなく、情報が段階的に出されることそのものに原因があると考えられます。

小規模モデル（Llama3.1-8B-Instruct、OLMo-2-13B、Claude 3 Haiku）では、連結指示形式でもやや性能が落ちていました。表現が少し違うだけでも理解が難しくなる傾向があり、大規模モデルほど表現の違いに強くないことが見て取れます。

ただ、注目すべきは、性能が高いはずのClaude 3.7 SonnetやGemini 2.5 Pro、GPT-4.1でも、30〜40%の性能低下が確認された点です。完全指示形式でのスコアが高ければ高いほど、相対的に下がり幅も大きく見えてしまうという側面はありますが、マルチターンになると苦戦しているのは事実です。

タスクごとの違いにも興味深い傾向が見られました。Command-AはActionsタスクで比較的安定しており、Claude 3.7 SonnetとGPT-4.1はCodeタスク、Gemini 2.5 ProはData-to-Textタスクで良い結果を出しています。マルチターンでの耐性は、モデルごとに得意不得意の分野があるということがわかります。

また、追加の思考時間を使えるモデル（o3やDeepseek-R1）でも、同様にマルチターンではスコアが下がっていました。長く考えられる＝うまくいく、というわけではなかったようです。これらのモデルは、平均して33%長めの回答を出す傾向があり、そこに余計な仮定が入り込みすぎて会話の整合性を崩してしまうことが確認されています。

### スコアの低下は「能力」ではなく「信頼性」の問題

研究チームは、スコアの平均だけでなく、「能力」と「信頼性」という2つの視点からもモデルを分析しています。

単一ターン形式や連結指示形式のように、情報が一度に与えられる設定では、能力の高いモデルほど信頼性も高い傾向が見られました。GPT-4.1やGemini 2.5 Proはスコアも安定しており、逆にLlama3.1-8BやOLMo-2-13Bのような小型モデルは、結果がブレやすく不安定でした。性能が高いモデルほど細かい入力の違いにも強く、プロンプトの調整にあまり依存しないという従来の知見とも一致しています。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89709_7-1024x540.png)

ところが、分割指示形式になると様子が変わります。モデルの「能力」自体はそれほど落ちないにもかかわらず、「信頼性」が大きく下がるのです。平均すると、信頼性の欠如は2倍以上に増加していました。

しかもこれは、どのモデルにも共通して見られた傾向でした。つまり、能力が高くても、マルチターンでは応答の安定性を保つのが難しいということです。実際、同じモデルでも会話の流れによってはスコアが50ポイント以上変わるケースもありました。

このように、性能低下の主な原因は能力そのものの問題ではなく、「安定して正解にたどり着けるかどうか」に関わる信頼性の問題である、というのがこの分析の結論です。

### モデルが「迷子になる」原因とは

研究チームは、なぜモデルがマルチターンになると迷いやすくなるのか、その理由も掘り下げています。主に次の4つの傾向が見つかっています。

1. 情報が足りないうちに、勝手に前提を立てて早まって答えてしまう
2. 一度出した間違った答えを引きずり、長くてややこしい説明にハマる
3. 会話の最初や最後ばかりを重視し、中間のやりとりを軽視する
4. 回答が冗長になり、ユーザーの問いから焦点がずれてしまう

実際にLLMと会話していて「なんか話がズレてきた」と感じたことがある人には、心当たりのある現象かもしれません。

### 情報の出し方が信頼性に影響する

ここまでの実験では「1ターンにつき1シャードずつ」情報を出していく形式でしたが、実際のやりとりではもう少し柔軟に情報が与えられることもあります。そこで研究チームは、情報の出し方を変えた場合にモデルの動きがどう変わるかを検証する追加実験を行いました。

この「段階的シャーディング実験」では、元の31の指示を使い、それぞれを2〜8個のシャードに分けてテストが行われました。GPT-4oとGPT-4o-miniの2つのモデルで評価されています。

結果として、2個以上のシャードに分けた時点で、どちらのモデルもスコアが落ち、特に信頼性の低下が顕著になることが確認されました。つまり、情報が2回以上に分けられただけで、モデルは迷子になりやすくなるということです。

この結果は、ユーザーが安定した出力を得たいなら「できるだけ情報は一度にまとめて伝えたほうがいい」という実践的なヒントにもつながります。

## 研究から見えてくること

今回の研究結果からは、LLMを活用するうえで注意しておきたいポイントがいくつか見えてきます。LLMを取り巻くさまざまな立場（システム開発者、モデル開発者、研究者、ユーザー）それぞれに向けたヒントを紹介します。

### システム開発者やエージェント構築者にとって

LLMを活用したアプリケーションの多くでは、問題の分解や情報の取得、ツールの呼び出しなど、複数のプロセスを組み合わせて構成する必要があります。LangChainやAutogenのようなフレームワークでは、LLMをステップごとの処理ブロックとして活用する設計が主流です。

そこで出てくるのが、次のような問いです。マルチターンのやりとりは、LLMが自力で処理できるようにすべきなのでしょうか？ それとも、会話の流れはエージェント側で整え、LLMは1ターンずつ情報を受け取って処理する役割にとどめるべきなのでしょうか？

この疑問を検証するために、研究チームは2つの「エージェント風」な会話の流れを用意しました。ひとつは、会話の最後にこれまでの内容をまとめる「要約追加型」、もうひとつは、各ターンで新しい情報とともに過去の内容も再提示する「繰り返し型」です。

この2パターンを、GPT-4oとGPT-4o-miniに対して実験したところ、どちらの手法も「少しは」効果がありました。ただし、最初から情報がそろっている単一ターン形式や、一括提示形式のような安定した結果には届きませんでした。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89709_8.png)

要約追加型の方が若干良い結果を出しましたが、これは「会話の最後にまとめてくれる」という前提で進められており、実際の会話では想定しづらい構成です。繰り返し型はもう少し現実的ですが、性能低下を15〜20%ほど緩和するのが限界でした。

つまり、フレームワーク側で工夫しても限界があり、マルチターンで自然に振る舞えるよう、LLMそのものに対応力を持たせる必要があると研究チームは指摘しています。

### LLM開発者にとって

これまでのLLM開発では「何ができるか」に注目が集まり、難しい課題を解ける能力や汎用性の高さが重視されてきました。しかし本研究は、それだけでは足りないと訴えています。とくに「信頼性」の重要性が浮き彫りになっています。

LLMは確率的な仕組みを持っており、テキストの出力には一定のゆらぎが生じます。温度というパラメータを下げれば出力のばらつきは減りますが、それで安定性の問題が解決するのかどうかは、あらためて検証が必要です。

この疑問に対して、研究チームはアシスタント側の温度（AT）と、ユーザーシミュレーター側の温度（UT）をそれぞれ3段階（1.0、0.5、0.0）に変えて比較実験を行いました。

その結果、情報が一度にすべて与えられる形式では、温度を下げることで信頼性が大きく向上しました。ところが、情報が徐々に出てくる形式では、温度を下げても信頼性はほとんど改善されませんでした。GPT-4o-miniでは変化がなく、GPT-4oでさえ改善はごくわずかだったそうです。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89709_9.png)

つまり、温度を0.0にしてもマルチターンでは出力のブレが残るということです。モデル内部での状態の揺らぎが積み重なり、安定性の乱れにつながっている可能性があると指摘されています。

こうした結果をふまえて、研究チームは「能力と信頼性の両方をバランスよく備えたモデルづくり」を呼びかけています。マルチターンで安定したパフォーマンスを維持できるモデルこそが、今後のLLMに求められる方向だとしています。

### NLP研究者にとって

この研究は、単一ターンとマルチターンでモデルのふるまいが大きく変わることを示しました。とくに、情報が分割されて提示されるような会話では、スコアの低下や応答の不安定さが顕著に現れます。

ただし、どんなタスクでもそうなるわけではありません。研究チームは、LLMでも扱いやすいタイプのタスクと、迷子になりやすいタスクの違いについても分析しています。

たとえば、翻訳のような「文ごとに進めていける」タスクでは、情報を少しずつ出しても性能が落ちにくい傾向が見られました。これは、1ターンごとに独立した出力ができる「エピソード型」の構造が影響していると考えられます。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89709_10.png)

一方で、以下のような特徴を持つタスクでは、マルチターンになると迷子になりやすいことが確認されています。

1. 出力が新しいテキストの生成を含む「生成系タスク」である
2. 複数の仕様を含み、内容を細かく分けていく必要がある
3. 出力全体がつながっており、一部の情報だけでは完成しない

こうした特性を持つタスクでは、会話が長引くほど混乱しやすくなる傾向があります。研究チームは、今後タスクを設計・公開する際には、「シャード化した指示」も併せて用意することで、マルチターン性能の評価を広げていくことを提案しています。

### 会話システムのユーザーにとって

LLMを日常的に使っている人にとっても、この研究は実践的な示唆を与えてくれます。とくに、マルチターンでやり取りしているときに「なんだか話がズレてきた」と感じたことがある人には、心当たりのある現象かもしれません。

研究チームは、そうした状況に対して次の2つの対処法を提案しています。

**対処法（１）うまくいかないときは、いったん仕切り直す  
**うまく返ってこなかったときは、会話をそのまま続けるより、最初から同じ内容でやり直したほうがうまくいくことがあります。LLMは確率的に動いているため、別の会話にすると別のルートに入ることがあるからです。

**対処法（２）指示を整理してから伝える**  
情報を複数のターンに分けて伝えるのではなく、最初にまとめて伝えたほうが安定した結果が得られやすいです。もし途中で話がごちゃごちゃしてきたら、「今まで話したことをまとめてください」とLLMに頼み、その出力を使って新しい会話を始めるのも有効です。

こうした工夫は一見まわりくどく感じられるかもしれませんが、現状ではこれが一番確実な方法です。今後のLLMがマルチターンでも迷わず対応できるようになれば、こうした手間は少なくなっていくと期待されます。

## まとめ

本記事では、LLMがマルチターンの会話でどのように性能を落とすかを調べた研究を紹介しました。

情報が段階的に与えられる状況では、モデルの能力そのものよりも応答の安定性が大きく揺らぎやすいことが確認されています。

温度の調整やフレームワークによる補助では限界があり、現時点では根本的な解決には至っていません。

そのため、指示をなるべく一度にまとめて伝えるなど、ユーザー側の工夫が依然として重要です。

**参照文献情報**

- タイトル：LLMs Get Lost In Multi-Turn Conversation
- URL： [https://doi.org/10.48550/arXiv.2505.06120](https://doi.org/10.48550/arXiv.2505.06120)
- 著者：Philippe Laban, Hiroaki Hayashi, Yin [gb](https://ai-data-base.com/archives/26343 "勾配ブースティング") o Zhou, Jennifer Neville
- 所属：Microsoft Research, Salesforce Research

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[NECの決算に見る、現場起点のAI活用と人材育成の今](https://ai-data-base.com/archives/89794)

[LLMアプリが安全に動くという思い込み　外部から守るセキュリティ設計](https://ai-data-base.com/archives/89743)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)