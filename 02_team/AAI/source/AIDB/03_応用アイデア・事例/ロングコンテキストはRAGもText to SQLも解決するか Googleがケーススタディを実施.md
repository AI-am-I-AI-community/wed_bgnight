---
title: "ロングコンテキストはRAGもText to SQLも解決するか Googleがケーススタディを実施"
source: "https://ai-data-base.com/archives/71486"
author:
  - "[[AIDB Research]]"
published: 2024-06-24
created: 2025-06-13
description: "ロングコンテキストのLLMは、単体でも多様なタスクをこなせる可能性が示唆されています。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

ロングコンテキストのLLMは、単体でも多様なタスクをこなせる可能性が示唆されています。DeepMindの研究チームは、100万トークンという膨大なコンテキストを一度に処理するLLMが、情報検索や質問応答、さらにはデータベース操作までをこなせるかどうかを検証しました。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71486-1024x576.jpg)

LOFTベンチマークのタスクとデータセットの詳細。

**参照論文情報**

- タイトル：Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?
- 著者：Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Boratko, Yi Luan, Sébastien M. R. Arnold, Vincent Perot, Siddharth Dalmia, Hexiang Hu, Xudong Lin, Panupong Pasupat, Aida Amini, Jeremy R. Cole, Sebastian Riedel, Iftekhar Naim, Ming-Wei Chang, Kelvin Guu
- 所属：Google DeepMind

## 背景

LLMのコンテキスト長が大幅に拡大し、これまで外部ツールに頼っていたタスクをLLMが直接処理できるようになってきています。例えば、従来は膨大な文書から情報を抽出するために専用の検索エンジンが必要だった情報検索や質問応答システムも、長文コンテキストを扱えるLLMなら文書全体を一度に処理できる可能性があります。また、専門的なSQLを使わずに、自然言語でデータベースを操作できるようになるかもしれません。

長いコンテキストを扱えるロングコンテキストLLMには、使いやすさ向上、エラーの減少、柔軟性の向上など、多くの利点が期待されています。

しかしどのような可能性が現実的なのか、長文コンテキストを扱えるLLMの性能を厳密に評価する必要があります。そこで今回、最大100万トークンのコンテキストを扱えるベンチマークLOFTが開発されました。情報検索、質問応答、SQL風の推論など、実用的なタスクを含みます。

そしてLOFTを用いた評価により、長文コンテキストを扱えるLLMの可能性と限界が明らかになってきました。例えば、情報検索ではLLMが専用システムに匹敵する性能を示す一方で、複雑な推論タスクではまだ課題があることがわかりました。

以下で詳しく紹介します。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71486_1-1024x608.jpg)

LOFTベンチマークの概要。ロングコンテキスト言語モデルと専門モデルの性能比較を示す。

## LOFTベンチマーク

今回作成されたLOFTベンチマークは、ロングコンテキストLLMの能力を多角的に評価するために設計されたベンチマークです。実世界のアプリケーションを想定しており、テキスト、画像、音声など多様なデータタイプを扱う6種類のタスクが含まれています（詳細は後述）。各タスクは32k、128k、1Mトークンの3段階のコンテキスト長で評価可能であり、将来的には10億トークンまで拡張できる柔軟性を備えています。

コンテキスト長を段階的に拡大できる設計により、モデルの性能がどのように変化するかを詳細に分析できるようになっています。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71486_2-1024x483.jpg)

LOFTベンチマークのタスクとデータセットの詳細。

LOFTベンチマークのタスクとデータセットの詳細。

### タスク1：テキスト検索

大規模な文書 [コーパス](https://ai-data-base.com/archives/26324 "コーパス") から関連情報を見つけ出すタスクです。単一文書の検索から、複数文書を組み合わせて回答する必要がある複雑なケースまで、様々な難易度の問題が用意されています。評価指標としては、Recall@1（正解文書が1位に検索される割合）やMRecall@k（複数の正解文書がトップkに含まれる割合）が使用されます。

### タスク2：視覚検索

テキストに基づいて関連する画像や動画を見つけ出すタスクです。Flickr30kやMS COCOといった一般的な画像検索から、MSR-VTTを用いた動画検索、OVENデータセットによる画像とテキストの複合検索まで、幅広いシナリオが想定されています。評価には主にRecall@1が使用されます。

### タスク3：音声検索

与えられた文字起こしに最も適合する音声を見つけ出すタスクです。FLEURSデータセットを使用し、英語、ヒンディー語、中国語、スペイン語、フランス語の5言語で評価が行われます。Recall@1で性能が測定されます。

### タスク4：検索拡張生成（RAG）

検索と生成を組み合わせたタスクです。モデルは関連情報を検索し、それを基に回答を生成することが求められます。評価には、生成された回答が正解の一部と完全に一致する割合（部分一致 [正解率](https://ai-data-base.com/archives/25930 "正解率") ）が用いられます。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71486_4-1024x646.png)

検索とRAGタスクのための コーパス 作成方法。

### タスク5：SQL風推論

自然言語の質問からSQLクエリを生成し、データベースから正しい情報を抽出するタスクです。SpiderとSparCという2つのデータセットが使用され、生成された回答の正確さが測定されます。

### タスク6：多ショットコンテキスト内学習

モデルが多数の例から学習し、新しい問題に対処するタスクです。Big-Bench HardとLongICLBenchから選ばれたデータセットが使用され、分類の正確さで評価が行われます。

## ロングコンテキストLLMの能力を最大限に引き出すプロンプト手法

今回、新しいプロンプト手法Corpus-in-Context Prompting（CiC）が設計されました。従来は専門的なシステムや複雑な処理が必要だったタスクを、LLMのコンテキストウィンドウ内に直接組み込むことで効率的に処理することを目指すものです。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71486_5-1024x451.png)

検索タスクにおけるCorpus-in-Context Promptingの例。

### CiCプロンプトの主な要素

1. **タスク指示  
	**まず、LLMに具体的なタスク内容を指示します。例えば、「文書を注意深く読み、質問に関連する文書を見つけてください」といった形で与えます。
2. **[コーパス](https://ai-data-base.com/archives/26324 "コーパス") のフォーマット  
	**次に、処理対象となる全 [コーパス](https://ai-data-base.com/archives/26324 "コーパス") がプロンプトに挿入します。各文書や画像、音声データには一意の識別子（ID）を割り当て、必要に応じて参照できるようにします。
3. **少数ショット例  
	**タスクの理解を助けるため、少数の例示を提供します。例は全て同じ [コーパス](https://ai-data-base.com/archives/26324 "コーパス") を参照するよう設計し、LLMが [コーパス](https://ai-data-base.com/archives/26324 "コーパス") の詳細も学習できるようにします。
4. **クエリのフォーマット  
	**最後に、実際に回答すべきクエリが提示します。多段階の質問の場合、前の質問とその回答も含めて提示します。

### 設計のポイント

CiCプロンプトの設計では、いくつか重要なポイントがあります。まず、様々な形式のプロンプトを柔軟に扱うために、十分な余裕を持ってコンテキスト長を設定します。

次に、モデル間の性能差を正確に評価するため、各コンテキスト長（例：32kや128k）で同じ [コーパス](https://ai-data-base.com/archives/26324 "コーパス") と例を使用することが推奨されています。

さらに、100万トークンものコンテキストを処理する際の計算コストの高さに対処するため、プレフィックスキャッシングという技術が活用されています。プレフィックスキャッシングは、 [コーパス](https://ai-data-base.com/archives/26324 "コーパス") 部分を一度だけ処理し、クエリ部分のみを繰り返し処理します。

研究者らはCiCプロンプトの有効性を確認するため、様々な要素を変更した実験を行いました。指示の一般化、クエリの位置の変更、文書IDの形式変更などが試され、CiCプロンプトの各要素が性能にどのような影響を与えるかが明らかにされました。説明は後述します。

## LOFTタスクと主要結果の概要

LOFTベンチマークでは、下記3つの最先端のロングコンテキストLLMが評価されました。

- Google社のGemini 1.5 Pro
- OpenAI社のGPT-4o
- Anthropic社のClaude 3 Opus

また、比較対象としてタスク専用に調整されたモデルが用意されました。

下の表は主な結果一覧です。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71486_6-1-1024x584.jpg)

LOFTベンチマークのタスクとデータセットの詳細。

LOFT 128kコンテキストテストセットの主要結果。

Gemini 1.5 ProがほぼすべてのタスクにおいてGPT-4oやClaude 3 Opusを上回り、多くの場合で専門モデルと同等以上の性能を示しています。特に視覚検索や音声検索タスクでの優位性が顕著です。一方で、SQLタスクでは専門モデルに及ばず、複雑な推論能力にはまだ改善の余地があることが示唆されています。

以下でタスクごとに詳しく見ていきます。

### テキスト検索タスクの結果

Geckoという最新の双符号化器（文書とクエリを別々にベクトル化し、その類似度で検索を行うモデル）が比較対象として使用されました。主な結果は以下のとおりです。

1. Gemini 1.5 Proは、128kトークンのコンテキスト長でGeckoと同等の性能を示しました。
2. 他のLLMも予想以上に良好な性能を発揮しました。
3. [コーパス](https://ai-data-base.com/archives/26324 "コーパス") サイズが100万トークンに拡大すると、LLMの性能は低下する傾向が見られました。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71486_10-1024x745.jpg)

コーパス サイズを32kから100万トークンまでスケールしたときのLLMと専門モデルのスケーリング結果比較。

なお、LLMの性能がコンテキスト内の位置によってどのように変化するかが分析され、以下のことがわかりました。

1. テストクエリの正解文書が後ろに配置されるほど、性能が低下しました。
2. 少数ショット例の正解文書を後ろに配置すると、性能が向上しました。
3. テストクエリと少数ショット例の正解文書を近くに配置すると、一貫して性能が向上しました。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71486_9-1024x271.jpg)

位置分析。 コーパス 内のクエリの正解文書の位置を変えた際の平均リコール@1の変化。

つまりテキスト検索タスクにおいては、LLMがコンテキストの後半部分に注意を向けにくい傾向があること、そして少数ショット例の配置を工夫することでこの問題を緩和できる可能性が示唆されました。

### 視覚検索タスクの結果

視覚検索タスクでは、CLIP-L/14という広く使用されているテキストから画像への検索モデルが比較対象として採用されました。主な結果は以下のとおりです。

1. Gemini 1.5 ProはGPT-4oを全ての視覚ベンチマークで上回りました。
2. Gemini 1.5 ProはCLIPを全てのコンテキスト長とベンチマークで一貫して上回る性能を示しました。

視覚検索タスクにおいては汎用モデルが特化モデルを上回る可能性が示された形です。

### 音声検索タスクの結果

音声検索タスクでは、PaLM 2 DEという音声とその書き起こしの類似性を最大化するよう訓練された双符号化器が比較対象として使用されました。主な結果は以下のとおりです。

1. Gemini 1.5 Proは5つの言語全てでPaLM 2 DEと同等以上の性能を示しました。
2. 特にヒンディー語でGemini 1.5 ProはPaLM 2 DEを大きく上回りました。
3. 様々なコンテキスト長でも安定した性能を維持しました。

LLMの現在の能力を示すとともに、より難易度の高い音声データセットの必要性も示唆する結果です。

### 検索拡張生成（RAG）タスクの結果

RAGタスクでは、Geckoを用いた検索と読解のパイプラインが比較対象として設定されました。Geckoが上位40件の文書を検索し、Gemini 1.5 Proのコンテキストに入れて回答を生成します。主な結果は以下のとおりです。

1. Gemini 1.5 Proは、全 [コーパス](https://ai-data-base.com/archives/26324 "コーパス") をコンテキストに含めた状態で、多段階推論を要するデータセット（HotpotQAとMusiQue）においてRAGパイプラインを上回りました。
2. 専門的な検索モデルであるGeckoは、トピックに関連する全ての文書をランク付けする能力に優れており、特に複数の回答を要するデータセット（QUESTやQAMPARI）で高い性能を示しました。
3. 128kトークンのコンテキストではLLMがRAGパイプラインと同等の性能を示しましたが、100万トークンに拡大すると性能が低下しました。

また、Gemini 1.5 Proの能力をさらに検証するため、 [コーパス](https://ai-data-base.com/archives/26324 "コーパス") を除いた状態（閉じた本の状態）での実験も行われました。結果として、閉じた本での性能はロングコンテキストモデルや専門モデルを大きく下回りました。これは、外部 [コーパス](https://ai-data-base.com/archives/26324 "コーパス") を利用することでLLMの推論能力が大幅に向上することを示しています。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71486_11-1024x382.png)

RAGタスクにおけるGeminiの閉じた本での性能。

### SQL風の複合的推論タスクの結果

このタスクでは、自然言語の入力をSQLクエリに変換する訓練を受けた [セマンティック](https://ai-data-base.com/archives/26143 "セマンティック") パーサーが比較対象として使用されました。なおDAIL-SQLというLLMにSQLクエリを提供させるプロンプト手法が採用されました。主な結果は以下のとおりです。

1. LLMは妥当な性能を示しましたが、専門的なパイプラインには及びませんでした。
2. この結果は、LLMの複合的推論能力にはまだ改善の余地があることを示しています。
3. しかし、タスク特有の調整なしで構造化データを扱える可能性も示唆されました。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71486_12-1024x768.png)

SQL推論分析。SQLクエリの演算子ごとにSpiderクエリをビン分けし、Geminiの性能を報告。

なおLLMの複雑な複合推論における短所を理解するため、ゴールドSQLクエリの演算子に基づいてクエリを分類し、Gemini 1.5 Proの性能が測定されました。

1. 平均化が最も困難な演算であることが判明しました。
2. カウントは比較的容易な演算でした。
3. 等価性に関する推論は、不等式に関する推論よりもかなり容易でした。

### 多ショットコンテキスト内学習（ICL）タスクの結果

各LLMの分類精度が全てのICLデータセットで比較されました。主な結果は次のとおりです。

1. Gemini 1.5 ProはBBH-tracking7を除く全てのベンチマークでGPT-4oを上回りました。
2. Claude 3 Opusが平均して最高の性能を示しました。

なお、Geminiにおける例の数の増加の影響も調査されました。

1. LIB-dialogでは、例の数が増えるほど精度が単調に向上しました。
2. BBHでは結果が混在していました。BBH-dateやBBH-salientのような知識集約型タスクではLIB-dialogと同様の改善が見られましたが、BBH-tracking7やBBH-webのような推論集約型タスクでは改善が見られませんでした。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71486_14-916x1024.png)

プロンプティングに使用される例の割合をスケールしたときのICL性能。

複雑なタスクではコンテキスト内の例を増やすことによる学習効果に早期の限界があると解釈できます。

## CiCプロンプトの詳細分析

Corpus-in-Context (CiC) プロンプティングの異なる要素がロングコンテキストLLMの性能にどのように影響するかを明らかにするアブレーション実験が行われました。

まず、下記の表は、Gemini 1.5 ProをLOFT 128kコンテキスト長で評価した際の様々なアブレーション結果を示しています。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71486_13-1024x417.png)

少数ショット例の数の効果。例の数が増えるにつれて、検索タスクの集計性能が向上していることを示しています。

以下のような洞察が得られています。プロンプトの各要素が性能に重要な影響を与えることを示しています。

1. タスク固有の指示を一般的な指示に置き換えると、性能が低下する。
2. クエリをプロンプトの最初に配置すると、一貫して性能が低下する。
3. 文書IDを単調増加の数字からランダムな英数字に変更すると、多くのデータセットで性能が低下する。
4. 文書のタイトルのみを使用し、内容を除去すると、大幅な性能低下が見られる。
5. 文書IDをエコーしない（前後に繰り返さない）と、性能が低下する。
6. 各少数ショット例に独自の [コーパス](https://ai-data-base.com/archives/26324 "コーパス") を使用すると、性能が低下する。
7. Chain-of-Thought（CoT）推論を除去すると、わずかな性能低下が見られる。

さらに下の図は少数ショット例の数が性能に与える影響を示しています。例の数を増やすにつれて検索タスクの性能が向上することが分かります。ゼロショットの0.76から5ショットの0.81まで性能が向上しています。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71486_15-916x1024.png)

少数ショット例の数の効果。例の数が増えるにつれて、検索タスクの集計性能が向上していることを示しています。

つまり次のことが導けます。

1. タスク固有の指示とChain-of-Thought推論は、LLMの性能向上に重要な役割を果たす。
2. プロンプトの構造、特にクエリの位置や文書IDの表示方法が性能に大きく影響する。
3. [コーパス](https://ai-data-base.com/archives/26324 "コーパス") の内容を完全に含めることが重要で、タイトルだけでは不十分である。
4. 少数ショット例の数を増やすことで、一定の性能向上が見込める。

以上のような知見をもとにした適切なプロンプト設計により、LLMの性能を最大限に引き出せると考えられます。ただし、タスクの複雑さや性質によって最適なプロンプト設計が異なる可能性があるため、個々のアプリケーションに応じた調整が必要となるでしょう。

## まとめ

本記事では、ロングコンテキストLLMの能力を評価するためのLOFTベンチマークに関する研究を紹介しました。検索、検索拡張生成、SQLライクな推論、インコンテキスト学習など、パラダイムシフトの可能性がある6つのタスクで構成されています。100万トークンまでの動的なコンテキスト長のスケーリングを可能にし、LCLMの進化に合わせて評価を継続できる設計になっています。

実験の結果では、ロングコンテキストLLMが検索や検索拡張生成タスクにおいて、特化モデルに匹敵する性能を示すことが明らかになりました。しかし、SQLのような複雑な推論タスクではまだ課題が残されています。また、プロンプト戦略が性能に大きな影響を与えることも示されました。

今後、コンテキスト長がさらに拡大するにつれて、LLMの能力がどのように向上していくか、実務者にとっても注目に値する研究分野と言えますね。

- 参照論文URL： [https://arxiv.org/abs/2406.13121](https://arxiv.org/abs/2406.13121)
- GitHub： [https://github.com/google-deepmind/loft](https://github.com/google-deepmind/loft)
理解度クイズ

## 理解度クイズ

#### 初級：LOFTベンチマークの主な目的は何ですか？

A) 短コンテキストLLMの性能を評価する

B) 画像認識モデルの性能を評価する

C) ロングコンテキストLLMの能力を多角的に評価する

D) [自然言語処理](https://ai-data-base.com/archives/26319 "自然言語処理（NLP）") の新しいアルゴリズムを開発する

LOFTベンチマークは、ロングコンテキストLLMの能力を多角的に評価するために設計されました。テキスト検索、視覚検索、音声検索、RAG、SQL風推論、多ショットICLなど、様々なタスクを含み、最大100万トークンの文脈長で評価を行います。LLMの実世界のアプリケーションにおける性能と限界を包括的に理解することが目的です。

#### 中級：Corpus-in-Context Prompting（CiC）の特徴として、正しくないものはどれですか？

A) タスク指示、 [コーパス](https://ai-data-base.com/archives/26324 "コーパス") 、少数ショット例、クエリの順でプロンプトが構成される

B) [コーパス](https://ai-data-base.com/archives/26324 "コーパス") 全体がプロンプトに含まれる

C) 少数ショット例は毎回異なる [コーパス](https://ai-data-base.com/archives/26324 "コーパス") を参照する

D) クエリは通常プロンプトの最後に配置される

Corpus-in-Context Prompting（CiC）では、少数ショット例は同じ [コーパス](https://ai-data-base.com/archives/26324 "コーパス") を参照するように設計されています。モデルが [コーパス](https://ai-data-base.com/archives/26324 "コーパス") の詳細も学習できるようになっています。選択肢Cは、この設計と矛盾するため、正しくありません。A, B, Dはすべて、CiCの正しい特徴を述べています。

#### 上級：LOFTベンチマークでのテキスト検索タスクにおける「位置分析」の結果として、正しいものはどれですか？

A) テストクエリの正解文書を後ろに配置するほど性能が向上した

B) 少数ショット例の正解文書を前に配置するほど性能が向上した

C) テストクエリと少数ショット例の正解文書を離して配置すると性能が向上した

D) テストクエリの正解文書を前に、少数ショット例の正解文書を後ろに配置すると性能が向上した

論文の位置分析の結果によると、テストクエリの正解文書を [コーパス](https://ai-data-base.com/archives/26324 "コーパス") の後ろに配置するほど性能が低下し、少数ショット例の正解文書を後ろに配置すると性能が向上しました。また、テストクエリと少数ショット例の正解文書を近くに配置すると一貫して性能が向上しました。これらの知見から、テストクエリの正解文書を前に、少数ショット例の正解文書を後ろに配置することで最も高い性能が得られると推測できます。

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[オープンソースモデルでも力を合わせれば先端モデルに匹敵することを示す「Mixture-of-Agents（MoA）」アーキテクチャ](https://ai-data-base.com/archives/71419)

[タスクを一度視覚化して取り組ませることで、LLMの推論能力を大きく向上させるプロンプト手法『Whiteboard-of-Thought（ホワイトボード思考法）』](https://ai-data-base.com/archives/71633)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)