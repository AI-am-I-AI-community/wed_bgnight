---
title: "LLMの事前学習とファインチューニングの関係についての新視点 まるで「アムロ」と「シャア」？"
source: "https://ai-data-base.com/archives/73532"
author:
  - "[[AIDB Research]]"
published: 2024-08-19
created: 2025-06-13
description: "本記事では、LLMの事前学習と微調整の相互作用を分析した研究を紹介します。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、LLMの事前学習と微調整の相互作用を分析した研究を紹介します。

従来は個別に最適化されていた事前学習と微調整の関係性に焦点を当て、各段階での能力獲得と微調整効果を詳細に分析することで、LLMの学習プロセスに新たな視点をもたらしています。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73532-1024x576.jpg)

**参照論文情報**

- タイトル：Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models
- 著者：Kaiser Sun, Mark Dredze
- 所属：Johns Hopkins University

**本記事の関連研究**

- [「LLMはプロンプトから新しいタスクを学べるのか？」 という根本的な問いに対する3つの仮説を検証](https://ai-data-base.com/archives/74020)
- [LLMに専門的なドメイン知識を学ばせるのに有効な「読解タスクテキストに変換する」テクニック](https://ai-data-base.com/archives/73575)
- [ファインチューニングがLLMの幻覚（ハルシネーション）に与える影響　Googleなどによる検証結果](https://ai-data-base.com/archives/69421)
- [LLMのプロンプトに数百から数千の例を含める超長尺のコンテキスト内学習（In-context learning）とファインチューニングの性能比較](https://ai-data-base.com/archives/68564)

## 背景

LLMの開発には、通常2つの段階があります。まず「事前学習」で、モデルは多くの文章を読んで言葉の基本を学びます。次に「微調整」で、特定のタスクや人の好みに合わせて調整されます。

事前学習では、モデルを大きくしたり、読ませる文章を増やしたりする試みが続いています。最近のモデルは、かなり多くの言葉（なんと15兆！）を学んでいます。また、どんな文章を選ぶか、どれだけ質の良い文章を使うか、そしてモデルの構造をどう工夫することなども研究されています。

微調整では、人やLLMの評価を使ってモデルを改善する方法や、特定のタスクを教える方法、複数のタスクを同時に教える方法などが使われています。

最近では、事前学習と微調整がどう関係しているのか、もっと詳しく知りたいという声が出てきています。例えば、事前学習の途中でモデルが何を学んでいるのか、微調整でどんな能力が伸びるのか、逆に何を忘れてしまうのかなどです。

そこで今回、事前学習の途中経過を何回か取り出して、それぞれを微調整してみるという方法が考え出され実験が行われました。研究結果から、モデルがどのように学習していくのか、より詳しく分かるかもしれません。

下記は本研究における実験計画の図解です。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73532_1-1024x397.jpg)

## 実験内容

### モデルの選択

実験には、OLMo-1Bという高性能なオープンソースLLMが採用されました。このモデルが選ばれた主な理由は、中間的な事前学習チェックポイントを公開している唯一のモデルだったためです。OLMo-1Bは、以下のように望ましい特性を持っています。

1. トレーニングの詳細、事前学習データ、微調整データを含め、完全にオープン
2. 比較的小さなモデルサイズのため、単一のA100 [GPU](https://ai-data-base.com/archives/26570 "GPU") で効率的にトレーニングできる
3. より大きなバージョンと非常に良好な比較結果を示している

### トレーニング手順

選択された各モデルチェックポイントに対して、2つの異なる手順で微調整が行われました。

1. 各モデルチェックポイントとデータセットに対して教師ありファインチューニングを個別に実施
2. 指示データセットを用いて指示チューニングが一度行われました。

なお、教師ありファインチューニングは特定のタスクに対する正確な予測を目指し、指示チューニングは与えられた指示に基づいた適切な反応を目指します。それぞれ異なる種類ノンデータセットを使用します。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73532_2-935x1024.png)

データセット情報と評価指標

#### 教師あり微調整

モデルの学習には、一つの分野のデータに対して、1〜2つの別の分野のデータも用意しました。モデルの学習は3回繰り返し、一度に8個のデータを使って学習しました。学習の速さ（学習率）は、少し試して一番良さそうな値が選ばれました。

#### 指示微調整

先行研究に従ってTÜLU（チューリュー）というデータセットを使いました。モデルの学習は5回繰り返し、一度に8個のデータを使いました。学習の速さは0.000002に設定しました。

### 評価

モデルがさまざまな種類の仕事をどの程度上手にできるか調べるため、いくつかのデータセットを選びました。モデルの学習途中の状態と微調整後の状態、それぞれについて評価しました。

#### データセット

モデルに、文章の要約を作ったり、質問を作ったり、文章の関係を推測したり、似た意味の文を見つけたりするタスクをさせて評価しました。公平に比較するため、それぞれの学習データは6,000個に調整しました。

指示に従うように調整したモデルは、ARC、OpenbookQA、Hellaswag、BoolQ、SciQといったデータセットで評価しました。

1. **ARC (AI2 Reasoning Challenge)** ：科学に関する常識的な質問に対する複数選択肢からの解答を求めるデータセット
2. **OpenbookQA** ：中学校レベルの科学の教科書に基づく質問に対して、知識と推論を活用して解答するデータセット
3. **Hellaswag** ：日常的なシナリオにおける次に起こる事象を予測するための選択肢問題を含むデータセット
4. **BoolQ** ：自然言語で記述された質問に対して、与えられた文脈に基づき「はい」または「いいえ」で答える [バイナリ](https://ai-data-base.com/archives/26314 "バイナリ") クエスチョンデータセット
5. **SciQ** ：科学に関する質問と、それに対応する詳細な解答を含むデータセット

#### 評価指標

正解を選ぶ問題では「精度」（正解の割合）を、文章を作る問題では「ROUGE-L」（人間が作った文章とどれくらい似ているか）を評価指標として使いました。

正解を選ぶ問題では、いくつかの方法を試しました。最終的に、モデルが最も自信を持っている答えを選ぶ方法が一番良い結果になりました。

## モデルは事前学習中にどう変わるのか

### 調べ方

モデルの学習途中の変化を見るため、少数の例が見せられてテストする方法（few-shot評価）が使われました。普通、学習途中のモデルは例なしでは上手く働かないため、この方法が選ばれました。それぞれのデータセットから4つの例がランダムに選ばれ、モデルの能力が学習の各段階で測られました。

### 見つかった変化のパターン

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73532_3-1024x327.jpg)

事前学習中のモデルの性能変化

テストの結果、データセットが大きく2つのグループに分かれることが分かりました。

1. 事前学習中に学ばれるタスク
	- 学習の初めの方で急に改善が見られなかった
	- およそ342,000回目の学習で上達が止まり、そのあとはあまり変わらなかった
	- 最初の急な上達は、学習の仕方が大きく変わったためだと考えられている
2. 学習中に上手くならないタスク
	- 学習の最初から最後まで、能力が変わらなかった
	- MNLI、XSum、BoolQというデータセットがこのグループに入った
	- 例が見せられても見せられなくても、結果は同じだった

### データの問題？

事前学習で上手くならなかったタスクデータセットについて、学習に使われたデータに問題（テストに使われるデータが混ざっている）がないかが考えられました。しかし、次の理由から、そういう問題はないと判断されました。

1. テストに使われるデータセットは、そのタスクの人気度と学習データの内容を考えて選ばれた
2. 学習中に上手くなったタスクは、学習データには入っていないことが確認された
3. 学習データに入っている可能性が高いデータセット（MNLIとXSUM）でも、学習中に上手くならなかった

### 結論

結論としては、今回の実験結果から、タスクには「学習中に上手くなれるタスク」と「学習中には上手くならないタスク」の2つの種類があることが分かりました。このことはモデルの学習の仕方について重要なヒントを与えるかもしれません。

次に、学習中に上手くならないタスクについて、モデルが実際に何を学んでいるのかを調べるため、さらに細かい調整をされたモデルが分析されました。

## 多く事前学習させると、その後の微調整も上手くいく？

先行研究によると、OLMo（オルモ）を微調整すると、苦手だったことも上手くできるようになることが分かっています。

しかし前述したように、学習中に上手くなるものと、ならないものがあることが今回明らかになりました。

そこで、次の研究課題が立ち上がりました。

1. モデルは、事前学習で上手くならないものについても何か役立つことを学んでいるのか
2. 上手くならないものは、後から調整しないとダメなのかか
3. 役立つことを学んでいるとしても、微調整しないと使えないのか

上記を調べるために、学習の途中経過を取り出して、それぞれを調整してみました。

### 主な発見

調査の結果、興味深い発見がなされました。

学習中に上達が見られたタスクについては、後からの調整によってほとんど改善が見られませんでした。この傾向は学習過程のどの段階でも一貫して観察されました。

一方、学習中に上達が見られなかったタスクでは、すべての段階で調整後に大幅な改善が確認されました。特に、学習初期の段階ほど調整の効果が顕著でした。

### 具体例の分析

具体的な例として、Hellaswag と MNLI という二つのタスクが詳しく分析されました。Hellaswag では、学習中に上達が見られ、その後の調整による改善はありませんでした（下図a）。対照的に、MNLI では学習中の上達は見られませんでしたが、調整後に劇的な改善が確認されました（下図b）。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73532_4-1024x879.png)

事前学習とファインチューニングの関係性を示す例。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73532_5-1024x605.png)

ファインチューニングによる性能向上の比較

### 結論と示唆

これらの発見から、いくつかの重要な示唆が得られました。

モデルによって学習中に重要な情報が獲得されているものの、調整なしではその情報が十分に活用できない可能性が示唆されました。また、学習を早期に終了しても、後からの調整によって高い性能が得られる可能性も示されました。さらに、モデルの調整が長期の学習よりも効果的である可能性も指摘されました。

ただし、最適な学習終了時点を特定することは、実際に調整を行わずに判断するのは困難であることも明らかになりました。今後の研究では、調整なしで効果を予測できる方法が発見される可能性も示唆されました。

この知見から、大規模なモデルを一から構築することが難しい場合、比較的少ない学習を行ったモデルを後から調整することで、高品質なモデルが得られる可能性が示されました。  
リソースが限られた状況下での効果的なモデル開発の新たな選択肢となる可能性があります。

## 教師ありファインチューニングによるモデルの学習と忘却

教師あり微調整（モデルに正解例を示して学習させる方法）を通じて、モデルによって何が学ばれ、何が忘れられるかが分析されました。この分析は、タスクフォーマット、タスク間の転移、ドメイン知識という3つの観点から行われました。

### タスクフォーマットに関する仮説

LLMはプロンプトの小さな変化に非常に敏感であることが示されています。

この知見から、「微調整によってモデルが特定のタスクフォーマットに適応し、そのフォーマットに一致する評価で高い性能を示すのではないか」と考えられました。つまり、慣れた形式のテストでは高得点が取れるのではないか？ということです。

この仮説を検証するため、3つの異なるプロンプトフォーマットが用意されました。トレーニングで使用されたものと同じフォーマット、入力と出力のみを含むシンプルなフォーマット、そして人間が読みやすい指示を含むフォーマットです。

事前学習の初期段階では、タスクフォーマットをトレーニングデータと一致させることが重要でした。しかし、後期の事前学習段階で微調整されたモデルは、徐々に異なるタスクフォーマットに柔軟に対応できるようになりました。

このことから、微調整によってモデルがタスクへの応答方法を学習し、より多くの学習を経ることでフォーマットへの敏感さが低下する可能性が示唆されました。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73532_6-1024x377.jpg)

異なるタスク形式でのモデル性能の例

つまり、LLMが学習していくと、最初は指示文のちょっとした違いに敏感であるものの、だんだん柔軟になっていくことがわかったということです。

### タスク間の転移

次に、モデルの忘却現象（あるタスクを学ぶと別のタスクの能力が低下すること）を評価するため、あるタスクで微調整された後の他のタスクでの性能変化が調査されました。

分類タスク（正解を選ぶタスク）で微調整されたモデルは、生成タスク（文章を作るタスク）での性能が大きく低下しました。  
一方、生成タスクで微調整されたモデルは、分類タスクでほぼ同等の性能を維持しました。

この結果は、分類が生成の一部と見なせることや、モデルが単純なタスクを記憶しやすい傾向があることから説明できます。また、微調整によって、目的のタスクに不要な能力が失われる可能性も示唆されました。

### ドメイン知識

モデルの一般化能力への微調整の影響を調査するため、ドメイン外（学習したデータとは異なる分野）での性能が分析されました。自然言語推論タスクでは、MNLIという特定のデータセットでの微調整後に全てのデータセットで性能が向上しました。一方、Pawsというデータセットでの微調整は、他のパラフレーズ検出タスクにとって不利な結果となりました。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73532_7-734x1024.png)

ドメイン外性能のファインチューニング効果の例

この結果から、モデルは特定の分野の知識を用いてタスクを学習しますが、微調整で学んだ内容から遠いものほど忘れられやすくなる可能性が示唆されました。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73532_8-1024x713.png)

タスクごとのドメイン外性能変化率

実験結果をまとめると、微調整によってモデルはタスクの実行方法を学習しますが、それと引き換えに、微調整されたタスクに必要でない一般化能力が失われる可能性があることが明らかになりました。

事前学習だけで学習できるタスクと、微調整が必要なタスクの違いは、タスクフォーマットの理解しやすさや、事前学習の目的との一致度に関係している可能性があります。

## 考察

この研究では、LLMの学習過程が詳しく調べられました。研究者たちは、モデルの学習途中の状態（チェックポイント）を取り出し、それぞれを微調整して、モデルの能力がどう変化するかを観察しました。

しかし、この研究には制約もありました。比較的小さな1つのモデルと10個ほどのデータセットしか使用されませんでしたが、それでも数千時間もの計算時間と多額の費用がかかりました。より大きなモデルやより多くのデータで同じような実験を行うのは、現実的には非常に難しいのです。

### 主要な発見とその意義

モデルは、一部のタスクを事前学習だけでマスターしました。事前学習で習得したタスクは、後から微調整しても性能が上がりませんでした。このことから、事前学習中にモデルが見るデータの中に、これらのタスクをうまくこなすヒントが含まれていたと考えられます。もし他のタスクもこのようなヒントを含む形で提示できれば、モデルの学習がもっと効率的になるかもしれません。

一方で、事前学習中には上手くできるようにならなかったタスクもありました。しかし、そのパターンでも実は事前学習の恩恵を受けていたのです。それは、微調整を行った後に初めて明らかになりました。つまり、モデルは事前学習中に重要な情報を学んでいたものの、それをすぐには使えなかったようです。この「見えない進歩」を予測できる方法があれば、モデルの学習をより効果的に進められる可能性があります。

微調整によって、モデルは特定のタスクの形式や解き方を学びます。学習が進むにつれて、モデルはより柔軟に様々な形式に対応できるようになります。しかし、微調整には代償もあります。特定のタスクに特化すると、他のタスクでの性能が下がることがあるのです（人間が一つのことに集中すると他のことが疎かになるのと似ています）。この知見は、複数のタスクを同時に学習させる際の難しさを示唆しています。

## まとめ

本記事では、LLMの事前学習と微調整の関係を探る研究を紹介しました。OLMo-1Bモデルの複数のチェックポイントを分析した結果、一部のタスクは事前学習で十分に学習され、他のタスクは微調整で大幅に改善されることが分かりました。微調整はタスクフォーマットの学習に役立つ一方で、他の能力の低下を招く可能性も示されました。

今回得られた知見は、事前学習の終了時期やタスクの提示方法など、モデルの訓練方法の改善に役立つ可能性があります。

研究者たちは、研究のために他のモデルの学習途中の状態も公開するよう呼びかけています。

- 参照論文URL： [https://arxiv.org/abs/2408.06663](https://arxiv.org/abs/2408.06663)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMから「LLMエージェント」へ　ソフトウェアエンジニアリングにおける今後の展開](https://ai-data-base.com/archives/74375)

[Google DeepMindがリリースした新世代の画像生成モデル「Imagen 3」テクニカルレポート](https://ai-data-base.com/archives/74508)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)