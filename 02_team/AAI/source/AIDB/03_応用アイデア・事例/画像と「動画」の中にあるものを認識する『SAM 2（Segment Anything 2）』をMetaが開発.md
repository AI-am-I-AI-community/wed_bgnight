---
title: "画像と「動画」の中にあるものを認識する『SAM 2（Segment Anything 2）』をMetaが開発"
source: "https://ai-data-base.com/archives/73710"
author:
  - "[[AIDB Research]]"
published: 2024-08-02
created: 2025-06-13
description: "本記事では、Metaが開発した画像・動画セグメンテーションモデル「SAM 2」を紹介します。SAM 2は、SAMの機能を動画に拡張し、動画内の任意のフレームでオブジェクトを追跡できる機能を持ちます。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、Metaが開発した画像・動画 [セグメンテーション](https://ai-data-base.com/archives/26353 "セグメンテーション") モデル「SAM 2」を紹介します。

SAM 2は、SAMの機能を動画に拡張し、動画内の任意のフレームでオブジェクトを追跡できる機能を持ちます。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73710-1024x576.jpg)

**参照論文情報**

- タイトル：SAM 2: Segment Anything in Images and Videos
- 著者：Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, Christoph Feichtenhofer
- 所属：Meta FAIR

## 背景

2023年4月にSegment Anything Model（SAM）が登場しました。画像内の任意のオブジェクトをセグメンテーション（分割）でき、画期的なシステムとして注目されました。  
しかし研究者らは「画像だけでは不十分であり、動画もセグメントできるようにすべきだ」と考えていました。

これまでに研究されてきた動画オブジェクトセグメンテーションは、例えば、オブジェクトが途中で隠れたり再出現したりする場合にはオブジェクトの追跡が困難でした。また長時間動画も不得意でした。

そこで研究者らは、SAMの原理を動画に拡張した『SAM2』を開発しました。そして動画全体を通してオブジェクトを追跡することが可能になりました。

また、開発の中で、大規模なビデオセグメンテーションデータセット「SA-V」が作成されました。様々な動画から構成されており、SAM 2の学習に使用されています。

下はSAM 2の概要として、タスク、モデル、データエンジンを示す図です。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73710_1-1024x363.jpg)

## タスクの概要

SAM 2が行うタスクは『Promptable Visual Segmentation（PVS）』タスク＝プロンプタブル視覚セグメンテーションタスクと呼ばれています。Segment Anything（SA）タスクを静止画から動画へと拡張したものです。入力された動画に対して、様々な種類のプロンプト（指示）を用いてインタラクティブにセグメンテーションを行います。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73710_2-1024x376.jpg)

インタラクティブセグメンテーションの例を示す図

### プロンプトの種類と機能

以下のようなプロンプト（操作指示）が使用されます。

1. クリック
2. バウンディングボックス
3. マスク

セグメンテーションしたいオブジェクトを定義したり、モデルの予測を修正したりするための操作です。動画内の任意のフレームに対して適用されます。

### 特徴１：インタラクティブな操作

ユーザーがあるフレームにプロンプトを提供すると、モデルは即座にそのフレームでのセグメンテーション結果を表示します。ユーザーはリアルタイムで結果を確認し、必要に応じて修正を加えることができます。

### 特徴２：動画全体へのセグメンテーション

初期のプロンプト（1つまたは複数のフレームに対して）を受け取ると、モデルはプロンプトを動画全体に伝播させ、各フレームでターゲットオブジェクトのセグメンテーションを行います。この一連のセグメンテーション結果は「マスクレット」と呼ばれます。

### 特徴３：柔軟な修正機能

ユーザーは任意のフレームで追加のプロンプトを提供することで、マスクレット全体を洗練させることができます。オブジェクトの追跡が失敗した場合や、セグメンテーションの精度を向上させたい場合に柔軟に対応することができます。

### 従来のタスクとの関係

PVSタスクは、既存の複数のセグメンテーションタスクを含んだ、より汎用的なアプローチです。

例えば、PVSを単一フレームの動画に適用すると、Segment Anything（SA）タスク、つまり一枚の画像に対してセグメンテーションを行うタスクと同等になります。

また、PVSで最初のフレームにのみマスクプロンプトを提供する場合、従来の半教師あり動画オブジェクトセグメンテーション（VOS）タスクと同様の機能を果たします。

さらに、PVSはインタラクティブVOSタスクの特徴も備えており、複数のフレームでプロンプトを提供してセグメンテーションを改善できます。

ただし、PVSではより多様なプロンプトタイプがサポートされているため、より柔軟な操作が可能です。

## SAM 2モデルの概要

モデルについての情報はユーザー目線では役立つシーンが限られるかもしれませんが、例えば将来的にSAM 2をファインチューニングして使用したいケースが出てきた際には参照することになる可能性があります。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73710_3-1024x262.jpg)

SAM 2の アーキテクチャ を示す図

### 主要コンポーネント

SAM 2は以下の主要コンポーネントから構成されています。

1. 画像エンコーダー
2. メモリアテンション
3. プロンプトエンコーダー
4. マスクデコーダー
5. メモリエンコーダー
6. メモリバンク

#### 1\. 画像エンコーダー

動画のリアルタイム処理を実現するため、ストリーミングアプローチが採用されています。画像エンコーダーは、各フレームに対して一度だけ実行され、フレームを表現する非条件付きトークン（特徴埋め込み）を生成します。 [MAE](https://ai-data-base.com/archives/26526 "平均絶対誤差（MAE）") （Masked Autoencoder）で事前学習されたHiera画像エンコーダーが使用されています。

#### 2\. メモリアテンション

このコンポーネントの役割は、「現在のフレーム」の特徴を、「過去のフレーム」の特徴や予測、新しいプロンプトに基づいて条件付けることです。複数のトランスフォーマーブロックが積み重ねられ、各ブロックは自己注意、メモリに対するクロス注意、 [MLP](https://ai-data-base.com/archives/26372 "多層パーセプトロン（MLP）") から構成されています。

#### 3\. プロンプトエンコーダーと4. マスクデコーダー

プロンプトエンコーダーはSAMと同一で、クリック（正または負）、バウンディングボックス、マスクなどのプロンプトを処理します。

マスクデコーダーもSAMと類似していますが、いくつかの変更が加えられています。

1. （オクルージョン予測ヘッドが追加され、）対象オブジェクトが現在のフレームに存在するかどうかを予測します。
2. 画像エンコーダーからの高解像度情報を取り込むスキップ接続が導入されました。
3. 動画フレーム間での曖昧さに対処するため、各フレームで複数のマスクが予測されます。

#### 5\. メモリエンコーダーと6. メモリバンク

メモリエンコーダーは、出力マスクを画像エンコーダーの埋め込みと組み合わせて、メモリを生成します。

メモリバンクは、最大N個の最近のフレームと最大M個のプロンプトされたフレームのメモリを保持します。また、オブジェクトポインターと呼ばれる軽量ベクターも保存され、セグメンテーション対象のオブジェクトに関する高レベルの意味情報を表現します。

### トレーニング

モデルの学習目標は、与えられたプロンプトを基に、動画全体の正確なセグメンテーション（マスクレット）を順番に予測することです。

SAM 2は、静止画と動画の両方を使って同時に学習しますが、その過程で、実際のユーザー操作を模倣するような工夫がされています。  
8フレームの動画クリップを取り出し、そのうち最大2フレームに対して「プロンプト」と呼ばれる指示を与えており、ユーザーが動画の特定のフレームで操作を行う状況を再現しています。

## データエンジンと SA-V データセット

「動画内のオブジェクトを何でもセグメンテーションする」を実現するため、動画セグメンテーションデータセット（SA-V）を収集するためのエンジンが構築されました。

### ①データエンジン

データエンジンは、人間のアノテーターとモデルを組み合わせです。セグメンテーション対象は、全体のオブジェクト（例：人物）だけでなく、その一部（例：人物の帽子）も含まれています。データエンジンは、モデルの支援レベルに基づいて3つのフェーズに分けられました。

データエンジンのフェーズは次のとおりです。

#### フェーズ1: フレームごとのSAM

このフェーズでは、画像ベースのインタラクティブSAMを用いて [アノテーション](https://ai-data-base.com/archives/26297 "アノテーション") 作業が支援されました。アノテーターは6FPSの速度で各フレームのマスクを作成しましたが、時間的な伝播を補助するトラッキングモデルは使用されませんでした。その結果、平均 [アノテーション](https://ai-data-base.com/archives/26297 "アノテーション") 時間は1フレームあたり37.8秒となり、1.4K本の動画から16Kのマスクレットが収集されました。

#### フェーズ2: SAM + SAM 2 Mask

次のフェーズでは、マスクのみをプロンプトとして受け入れるSAM 2 Maskが導入されました。アノテーターはSAMを使用して最初のフレームのマスクを生成し、それをSAM 2 Maskで時間的に伝播させました。また、後続フレームでの修正も可能となりました。この改善により、平均 [アノテーション](https://ai-data-base.com/archives/26297 "アノテーション") 時間はフレームあたり7.4秒まで短縮され、フェーズ1と比較して約5.1倍の速度向上が実現しました。このフェーズでは63.5Kのマスクレットが収集されています。

#### フェーズ3: SAM 2

最終フェーズでは、点やマスクなど様々なプロンプトを受け入れる完全な機能を持つSAM 2が使用されました。このモデルはオブジェクトの時間的なメモリを活用し、アノテーターは中間フレームで修正クリックを提供するだけで済むようになりました。その結果、平均 [アノテーション](https://ai-data-base.com/archives/26297 "アノテーション") 時間はフレームあたり4.5秒にまで短縮され、フェーズ1と比較して約8.4倍の速度向上が達成されました。このフェーズでは197.0Kものマスクレットが収集され、データセットの大幅な拡充が実現しました。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73710_4-1024x160.png)

データエンジンフェーズの進化を示す表

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73710_5.png)

各データエンジンフェーズからのデータ追加による性能改善を示す表

また、各 [アノテーション](https://ai-data-base.com/archives/26297 "アノテーション") されたマスクレットの品質を維持するため、別のアノテーターグループによる検証ステップが導入されました。「満足」または「不満足」の評価が行われ、不満足なものは修正のため [アノテーション](https://ai-data-base.com/archives/26297 "アノテーション") パイプラインに戻されました。

なお、 [アノテーション](https://ai-data-base.com/archives/26297 "アノテーション") の多様性を確保するため、自動生成されたマスクレット（「Auto」と呼ばれる）が追加されました。SAM 2に規則的なグリッドの点でプロンプトを与え、候補マスクレットを生成し、検証ステップでフィルタリングされました。

### ②SA-V データセット

データエンジンを通じて収集されたSA-Vデータセットは、50.9K動画と642.6Kマスクレットで構成されています。既存のVOSデータセットと比較して、 [アノテーション](https://ai-data-base.com/archives/26297 "アノテーション") されたマスクの数が53倍（自動生成を除いても15倍）多くなっています。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73710_6-1-770x1024.jpg)

SA-Vデータセットからの例示動画と、手動および自動生成されたマスクレットのオーバーレイを示す図

#### データセットの特徴

- 動画解像度：240pから4K（平均1,401 × 1,037）
- 動画時間：4秒から2.3分（平均13.8秒）
- 総 [アノテーション](https://ai-data-base.com/archives/26297 "アノテーション") フレーム数：4.2M
- 総動画時間：196時間
- マニュアルマスクレット：190.9K
- 自動生成マスクレット：451.7K
- 地理的多様性：47カ国にわたる撮影場所
- 消失率（再出現するマスクレットの割合）：42.5%

SA-Vデータセットは、多様な環境や日常的なシナリオを含む「実世界の」動画で構成されています。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73710_8-1024x256.png)

データセットの分布を示す図。(a)マスクレットのサイズ分布、(b)動画の地理的多様性、(c)クラウドワーカーの人口統計

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73710_7-1024x290.png)

SA-Vデータセットと他のVOSデータセットの比較を示す表

## ゼロショット実験

SAM 2の性能を評価するため、様々なゼロショット実験が行われました。なお、モデルが学習時に見たことのないデータセットや設定での性能が測定されました。

### 動画タスク

#### プロンプタブル動画セグメンテーション

ユーザー体験に近い形でのインタラクティブな設定が評価されました。オフラインとオンラインの2つの方式が採用されています。

（１）オフライン評価

この方式では、動画全体を何度も繰り返し分析します。まず、モデルが最も大きな誤りを犯しているフレームを特定します。次に、そのフレームに対してユーザーの操作（プロンプト）をシミュレートします。このプロセスを繰り返し、誤差の大きなフレームから順に修正していきます。これでモデルの性能を徹底的に評価し、改善します。

（２）オンライン評価

この方式は、より実際の使用状況に近い評価を行います。動画を一度だけ最初から最後まで見ていき、モデルの予測精度が低いフレーム（具体的には、正解との一致度を示すIoUが0.75未満のフレーム）で一時停止します。そのフレームでユーザーが追加のクリックプロンプトを提供する状況をシミュレートします。  
このオンライン評価ではリアルタイムでのユーザー操作に対するモデルの反応性を評価します。

上記2つの評価は、9つの密に [アノテーション](https://ai-data-base.com/archives/26297 "アノテーション") されたゼロショット動画データセットで実施されました。各フレームで3つのクリックが使用されました。

結果として、SAM 2は2つの強力なベースライン（SAM+XMem++とSAM+Cutie）を上回る性能を示しました。3倍以上少ない相互作用で、より高品質なセグメンテーション精度を生成することができました。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73710_9-1024x335.png)

インタラクティブなオフラインおよびオンライン評価設定での9つのデータセットにわたるゼロショット精度を示すグラフ

#### 半教師あり動画オブジェクトセグメンテーション

最初のフレームでのみプロンプト（クリック、ボックス、またはマスク）を提供する半教師あり設定も評価されました。

17のゼロショット動画データセットで評価が行われ、SAM 2は様々なプロンプトタイプにおいて両方のベースラインを上回る性能を示しました。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73710_10-1024x159.png)

17の動画データセットでの半教師ありVOS評価での異なるプロンプトを使用したゼロショット精度を示す表。

#### 公平性評価

Ego-Exo4Dデータセットの人物カテゴリを用いて、SAM 2の公平性が評価されました。性別や年齢といった人口統計学的グループ間での性能差が分析されました。3クリックおよび [グラウンドトゥルース](https://ai-data-base.com/archives/26293 "グラウンドトゥルース") マスクプロンプトでは、最小限の性能差が観察されました。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73710_11.png)

保護された人口統計グループに対するSAM 2の公平性評価結果を示す表

### 画像タスク

Segment Anythingタスクにおいて、SAM 2は37のゼロショットデータセット（SAMで使用された23データセットを含む）で評価されました。1クリックと5クリックのmIoU（平均Intersection over Union）が報告されました。

SAM 2（Hiera-B+）は、SAMと比較して以下の点で優れた性能を示しました。

- 1クリック精度が向上（58.9 mIoU vs 58.1 mIoU）
- 6倍高速な処理速度
- より効果的なHiera画像エンコーダーの使用

さらに、SA-1B（SAMの開発用に開発されたデータセット）と動画データの混合で学習することで、23データセットの平均精度が61.4%まで向上しました。中でも、動画ベンチマークでの大幅な改善が見られました。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73710_12-1024x189.png)

37のデータセットでのSegment Anythingタスクにおけるゼロショット精度を示す表。

タスクの評価結果をまとめると、SAM 2は動画および画像セグメンテーションの両方で高い能力を持つことを示しています。そしてそれは多様なトレーニングデータに起因していると考えられています。

## 動画オブジェクトセグメンテーションにおける最先端手法との比較

SAM 2は主に柔軟な動画セグメンテーション（PVS）のために開発されましたが、従来の動画オブジェクトセグメンテーション（VOS）タスクでも評価されました。

VOSタスクでは、動画の最初のフレームで正確なマスク（セグメンテーション）が与えられ、それを基に動画全体のセグメンテーションを行います。これは長年使われてきた標準的な方法です。

今回、SAM 2の異なるバージョン（処理能力の異なる2種類）が、既存の最新技術と比較されました。評価では、セグメンテーションの正確さと処理速度の両方が測定されました。

結果として、SAM 2は既存の最高性能の手法を大きく上回りました。また、より高性能なバージョンではさらに良い結果が得られました。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73710_13-1024x426.png)

VOSの比較結果を示す表。SAM 2が動画セグメンテーションの精度と速度で優れていることを示す

SAM 2は5つの異なるベンチマーク（MOSE、DAVIS、LVOS、SA-V、YTVOS）で評価されました。特に注目すべきは、SA-Vベンチマークでの性能です。このベンチマークは「動画内のオブジェクトを何でも」セグメンテーションする能力を測るもので、SAM 2は他の手法を大きく上回る性能を示しました。

また、長い動画でのセグメンテーション（LVOSベンチマーク）でも、SAM 2は優れた性能を発揮しました。

上記の結果は、SAM 2が何らかのタスクに特化していないにもかかわらず、幅広い動画セグメンテーションタスクで高い性能を発揮できることを示しています。SAM 2は、動画内のあらゆるオブジェクトを柔軟にセグメンテーションする能力を持っており、これは従来の手法では難しかったことです。

## 追加実験

研究チームは、SAM 2の性能を向上させるために、様々な要素を変えて実験を行いました。主に、モデルの構造や学習データの影響を調べるために実施されました。

実験の評価には、複数のデータセットと指標が使用されました。主な指標は、動画の最初のフレームに3回クリックした場合のセグメンテーション精度（J&F）です。また、画像タスクでは1回クリックした場合の精度（mIoU）も測定されました。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73710_14-1024x436.png)

異なるデータミックスでSAM 2を訓練した際の性能比較。VOS、内部データ、SA-V、SA-1Bの組み合わせによる影響

データに関する実験では、以下のことがわかりました。

1. 多様なデータセットを組み合わせて学習させると、モデルの性能が大幅に向上する
2. 学習データの量を増やすと、性能が徐々に向上する
3. 高品質のデータ（編集回数の多いデータ）を選んで学習させると、性能が向上する

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73710_15-1024x273.png)

SA-Vの量に対するSAM 2の性能をプロットしたグラフ

モデルの構造に関する実験では、以下の点が明らかになりました。

1. 入力サイズを大きくすると、性能が向上する
2. メモリ機能を強化すると、性能が向上する
3. モデル全体のサイズを大きくすると、画像と動画の両方で性能が向上する
4. 位置情報の扱い方を工夫すると、わずかに性能が向上する
5. 特定の注意機構（オブジェクトポインターへのクロスアテンション）を使うと、一部のデータセットで大きな改善が見られる

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73710_16-1024x178.png)

SA-V手動データの異なるサブセットで訓練したSAM 2の性能比較。ランダム サンプリング 、最も編集されたマスクレット、全データセットの結果を比較

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73710_17-1024x533.png)

モデルの容量に関するアブレーション実験の結果。入力サイズ、メモリサイズ、モデルサイズなどの影響

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_73710_19-1024x179.png)

メモリ設計に関するアブレーション実験の結果。オブジェクトポインターとGRUメモリの使用の影響

上記の実験結果を基に、研究チームはSAM 2の最終的な設計と学習方法を決定しました。

## まとめ

本記事では、Segment Anything Model（SAM）を動画領域に拡張したSAM 2の研究を紹介しました。

SAM 2は、メモリ機能を追加することで動画内の任意のオブジェクトをセグメンテーションできるようになっています。

また、開発のために大規模なSA-Vデータセットが構築されました。

SAM 2は既存の手法を上回る性能を示し、特筆すべきことに画像セグメンテーションタスクでもSAMを上回りました。

さまざまな活用や実験が進むことが期待されます。

- 参照論文URL： [https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/](https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/)
- デモ： [https://sam2.metademolab.com/](https://sam2.metademolab.com/)
- コード： [https://github.com/facebookresearch/segment-anything-2](https://github.com/facebookresearch/segment-anything-2)
- ウェブサイト： [https://ai.meta.com/sam2](https://ai.meta.com/sam2)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[Appleが「LLMエージェントの評価」に特化したベンチマーク『MMAU』を開発　5領域5能力で測る](https://ai-data-base.com/archives/73656)

[LLMは人間のような「共感的な対話」ができるか？実行プロンプトと検証結果](https://ai-data-base.com/archives/73786)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)