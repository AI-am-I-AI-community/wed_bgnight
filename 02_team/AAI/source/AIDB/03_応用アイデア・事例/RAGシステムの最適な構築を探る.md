---
title: "RAGシステムの最適な構築を探る"
source: "https://ai-data-base.com/archives/72121"
author:
  - "[[AIDB Research]]"
published: 2024-07-04
created: 2025-06-13
description: "本記事では、RAGの最適実装を探る研究を紹介します。研究者らはRAGの構成要素を洗い出し、最適なアプローチを考察しています。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、RAGの最適実装を探る研究を紹介します。

研究者らはRAGの構成要素を洗い出し、最適なアプローチを考察しています。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72121-1024x576.jpg)

**参照論文情報**

- タイトル：Searching for Best Practices in Retrieval-Augmented Generation
- 著者：Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, Ruicheng Yin, Changze Lv, Xiaoqing Zheng, Xuanjing Huang
- 所属：School of Computer Science, Fudan University, Shanghai Key Laboratory of Intelligent Information Processing

## 背景

検索拡張生成（RAG）技術が注目されています。外部知識ベースから関連文書を取得し、LLMに提供することで、最新の情報を含む正確な応答を生成する手法です。  
特定の組織や分野向けのアプリケーションを展開する際に、モデルのパラメータを更新せずに、クエリに関連する文書を提供するだけで対応できるという利点があります。

これまで多くのRAGアプローチが提案されてきましたが、実装が複雑で応答時間が長くなるという課題があります。クエリ分類、検索、再ランク付け、再パッケージング、要約など、複数の処理ステップがあり、各ステップには様々な実行方法があるため、最適なRAGの構造を決めるのは難しいと言わざるを得ません。

そこで今回研究者らは、既存のRAGアプローチを徹底的に調査し、ベストな組み合わせを探すことにしました。

その結果、パフォーマンスと効率性の両方のバランスを取るRAG戦略がいくつか提案されています。

以下で詳しくみていきます。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72121_1-1024x676.png)

RAGワークフローの概要図。各モジュールの方法を示している

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72121_2-1024x560.jpg)

異なるタスクのための検索要件の分類。情報が提供されない場合、モデルの機能に基づいてタスク処理する

## RAGにおける各要素

RAGのワークフローには、複数の要素があります。各要素ごとに、一般的な説明と本研究独自の取り組みを見ていきます。

### 【１】クエリ分類

RAGシステムの最初のステップとして、クエリ分類があります。入力されたクエリが検索拡張を必要とするかどうかが判断されるプロセスです。

LLMはクエリに対して事前知識で十分に対応できることも多く、また検索拡張を頻繁に行うと応答時間が長くなる可能性があります。そのため検索の必要性を判断するのが重要であり、そこでクエリ分類を始めに行います。

なお、トピックが同じでもタスクの種類によって検索の必要性が異なる場合があります。例えば、2023年までの学習データを持つLLMでは、”Soraはどの会社が開発したか”というリクエストは検索なしで処理できますが、別の要求だと関連情報の検索が必要となります。

そこでクエリの検索必要性を判断するため、今回タスクの種類に基づく分類方法が提案されています。15種類のタスクが、十分な情報が提供されているかどうかに基づいて分類されます。ユーザーが提供した情報のみに基づくタスクは「十分」とされ、検索が不要と判断されます。それ以外のタスクは「不十分」とされ、検索が必要となる可能性があります。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72121_3.png)

クエリ 分類器 の結果。精度、 適合率 、 再現率 F1スコア を示している

この決定プロセスを自動化するため、 [分類器](https://ai-data-base.com/archives/26489 "分類器") が訓練されました。クエリ分類がワークフローに与える影響の詳細は後のセクションで説明します。

### 【２】チャンキング

チャンキングは文書を小さなセグメントに分割する作業です。検索の精度を向上させ、LLMの長さ制限の問題を回避するために非常に重要な役割を果たします。

チャンキングの手法にはいくつかの種類があります。

**（１）トークンレベルのチャンキング**

単純な方法ですが、文章を分割してしまう可能性があり、検索品質に影響を与えてしまいます。

**（２）意味レベルのチャンキング**

LLMを使用して文脈を保持しながら分割点を決定します。時間がかかるという欠点があります。

**（３）文レベルのチャンキング**

テキストの意味を保持しつつ、シンプルで効率的な方法です。

本研究では、シンプルさと意味の保持のバランスを取るため、文レベルのチャンキングが採用されました。

次にチャンキングにおけるの4つの重要な側面を見ていきます。システムを作る際には、以下の側面全てを考慮する必要があります。

**1\. チャンクサイズ**

大きなチャンクは文脈理解を向上させますが、処理時間が長くなります。小さなチャンクは検索の再現率を向上させ時間を短縮しますが、十分な文脈が得られない可能性があります。

最適なチャンクサイズを見つけるには、忠実性（生成された応答が検索されたテキストと一致しているか）と関連性（検索されたテキストと応答がクエリに合致しているか）のバランスを取る必要があります。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72121_5.png)

異なるチャンクサイズの比較。忠実性と関連性の平均スコアを示している。

**2\. チャンキング技術**

小さいサイズから大きいサイズへ（small-to-big）や [スライディングウィンドウ](https://ai-data-base.com/archives/26349 "スライディングウィンドウ") などの高度な技術が、チャンクブロックの関係を整理し、検索品質を向上させるために使用されます。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72121_6.png)

異なるチャンキング技術の比較。忠実性と関連性の平均スコアを示している

**3\. 埋め込みモデルの選択**

自然言語をベクトル空間に変換する（埋め込む）作業を行うモデルを埋め込みモデルと言います。  
クエリとチャンクブロックの効果的な意味マッチングには、適切な埋め込みモデルの選択が重要です。

今回の研究では様々なオープンソースの埋め込みモデルが評価され、LLM-Embedderがそのパフォーマンスとサイズのバランスから選択されました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72121_4-1024x450.png)

異なる埋め込みモデルのnamespace-Pt/msmarcoデータセットでの結果比較

**4\. メタデータの追加**

タイトル、キーワード、仮想的な質問などのメタデータをチャンクブロックに追加することで、検索が改善され、検索されたテキストの後処理の方法が増え、LLMが検索された情報をより良く理解できるようになります。

### 【３】ベクトルデータベース

ベクトルデータベースは、埋め込みベクトル（テキストや画像などのデータを数値の配列で表現したもの）とそのメタデータを保存します。様々な索引付けや近似最近傍（ANN）手法を通じて、クエリに関連する文書を効率的に取得するための格納庫です。

本研究では、適切なベクトルデータベースを選択するために、以下の4つの主要な基準が設定されました。

1. **複数の索引タイプ  
	**異なるデータ特性や使用例に応じて検索を最適化できる
2. **10億規模のベクトル対応  
	**大規模なデータセットを扱う能力がある
3. **ハイブリッド検索  
	**検索の精度を向上させるため、ベクトル検索と従来のキーワード検索を組み合わせる
4. **クラウドネイティブ機能  
	**クラウド環境での統合、スケーラビリティ、管理が容易

上記の観点で5つのオープンソースのベクトルデータベース（Weaviate、Faiss、Chroma、Qdrant、Milvus）が詳細に比較されました。比較結果は以下の通りです。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72121_7.png)

様々なベクトルデータベースの比較。機能の有無を示している

評価の結果、Milvusが際立っていることが明らかになりました。Milvusは、すべての重要な基準を満たし、他のオープンソースオプションを上回る性能を示しました。様々なデータ特性や使用シナリオに対応できる柔軟性と、大規模なアプリケーションに必要なスケーラビリティを兼ね備えていると評価されています。

### 【４】検索モジュール

RAGにおける検索モジュールは、ユーザーのクエリに基づいて、事前に構築された [コーパス](https://ai-data-base.com/archives/26324 "コーパス") （大規模なテキストデータの集合）から最も関連性の高い上位k個の文書を選択します。文書は、クエリに対する適切な応答を生成するために使用されます。

オリジナルのクエリは、表現力の乏しさや意味情報の不足により、検索性能が低下することがあります。今回、この問題に対処するため、以下の3つのクエリ変換手法が評価されました。

**1\. クエリ書き換え**

クエリを洗練させ、関連文書とのマッチングを改善します。LLMを使用してクエリの書き換えを行います。

**2\. クエリ分解**

オリジナルのクエリから派生した副質問に基づいて文書を取得します。より複雑な理解と処理が必要になります。

**3\. 疑似文書生成**

ユーザーのクエリに基づいて仮想的な文書を生成します。そして埋め込みを使用して類似文書を検索します。HyDEという手法が注目されています。

なお最近の研究では、語彙ベースの検索（要するに [ルールベース](https://ai-data-base.com/archives/26614 "ルールベース") ）とベクトル検索を組み合わせることで、性能が大幅に向上することが示されています。今回の研究では、スパース検索にはBM25アルゴリズムが、密ベクトル検索には教師なし対照学習エンコーダであるContrieverが使用されました。

#### 実験結果と分析

TREC DL 2019と2020のパッセージランキングデータセットを用いて、各検索手法の性能が評価されました。評価指標には、mAP、nDCG@10、R@50、R@1kが使用され、各クエリあたりの平均レイテンシー（応答時間）も報告されました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72121_8-1024x284.png)

TREC DL19/20データセットにおける異なる検索手法の結果比較

結果として、教師あり手法が教師なし手法を大きく上回る性能を示しました。また、HyDEとハイブリッド検索を組み合わせたLLM-Embedderが最高スコアを達成しました。  
なお、クエリ書き換えとクエリ分解は、予想されたほど検索性能を向上させませんでした。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72121_9-1024x158.png)

HyDEにおける仮説文書とクエリの異なる連結方法の比較

推奨される検索手法としては、もしパフォーマンス重視ならHyDEを組み合わせたハイブリッド検索が最も高いRAGスコアを達成しているため推奨されます（ただし計算コストが高くなります）。一方で効率性重視なら、ハイブリッド検索またはオリジナルの埋め込み手法が推奨されます。

研究者らはHyDEの性能をさらに分析するため、仮説的文書とクエリの異なる連結戦略を検討しました。その結果、複数の疑似文書をオリジナルのクエリと連結することで、検索性能が大幅に向上することが示されました。ただし、これにはレイテンシーの増加が伴います。

また、スパース検索と密検索のバランスを調整するαパラメータの影響が調査されました。α=0.3が最適な性能を示し、適切なα値の調整により検索効果を向上させることができることが確認されました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72121_10-1024x198.png)

異なるαの値によるハイブリッド検索の結果比較

### 【５】再ランク付け

初期検索の後は、再ランク付けステップがあります。検索された文書の関連性をさらに高め、最も適切な情報がリストの上位に表示されるように順序が調整されるステップです。文書を効果的に並べ替え、クエリと上位にランク付けされた文書間の類似性を最大にします。

主な再ランク付け手法として、本研究では、2つのアプローチが検討されました。

**1\. DLM（Deep Language Model）再ランク付け**

分類に基づくアプローチで、性能を重視する手法です。

言語モデルを活用して文書の関連性を「真」または「偽」に分類します。  
なお言語モデルは以下のように微調整されます。

（１）連結されたクエリと文書の入力で訓練  
（２）関連性によってラベル付け

そして推論時には、「真」トークンの確率に基づいて文書がランク付けされます。

**2\. TILDE再ランク付け**

クエリの尤度に焦点を当てたアプローチで、効率性を重視する手法です。

以下のように各クエリ用語の尤度を独立して計算します。

（１）モデルの語彙全体にわたるトークン確率を予測  
（２）文書のスコアは、クエリトークンの対数確率の合計で算出

#### 再ランク付け手法の比較実験

MS MARCO Passage rankingデータセットが使用されました。機械読解用の大規模データセットで、880万以上のパッセージと100万のクエリを含みます。

評価モデルは次のとおりです。

1. monoT5
2. monoBERT
3. RankLLaMA
4. TILDEv2

評価指標はMRR@1、MRR@10、MRR@1k、Hit Rate@10で、MRR@10がMS MARCOの公式指標です。

#### 実験結果と分析

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72121_11-1024x309.png)

MS MARCO Passage rankingデータセットの開発セットにおける異なる再ランク付け手法の結果比較

すべての再ランク付け手法が、ランダムに並べ替えた順序とBM25検索のベースラインを大幅に上回る性能を示しました。  
中ではmonoT5とmonoBERTがほぼ同等の性能を示し、RankLLaMAが最高性能を達成しました。ただし、性能が高い順に処理時間（レイテンシー）が長くなります。  
一方で、TILDEv2は最速の処理速度を示し、クエリあたり約10〜20ミリ秒で処理が可能です。ただし、性能面では他の手法に劣ります。またTILDEv2は、新しいパッセージに対しては前処理が必要となり、効率性の利点が失われる可能性があります。

### 【６】文書の再パッケージング

データベースから取得された文書がモデルに提供される順序は、LLMによる応答生成などの後続プロセスの性能に影響を与える可能性があります。そこで、再ランク付けの後に簡潔な再パッケージングモジュールがワークフローに組み込まれることが推奨されます。

再パッケージング手法は以下3つです。

**1\. 順方向（Forward）**

再ランク付け段階で得られた関連性スコアに基づいて、文書を降順（高スコアから低スコア）に並べ替えます。

**2\. 逆方向（Reverse）**

関連性スコアに基づいて、文書を昇順（低スコアから高スコア）に配置します。

**3\. サイド（Sides）**

関連情報を入力の先頭または末尾に配置します。先行研究の結果に基づくテクニックです。

本セクションでは、「サイド（Sides）」方式がデフォルトの再パッケージング手法として選択されました。

適切な再パッケージング手法を選択する理由は以下の通りです。

- LLMの応答生成プロセスが最適化される
- 最も関連性の高い情報がLLMに効果的に提供される
- 生成される応答の質が向上する可能性がある

なお、さまざまなシナリオや入力タイプに対する各手法の効果を詳細に調査する必要があります。

### 【７】要約

検索結果には冗長または不要な情報が含まれていることがあり、LLMの正確な応答生成を妨げる恐れがあります。また、長いプロンプトは推論プロセスを遅くする原因となります。そのため、RAGのパイプラインにおいては、検索された文書を効率的に要約することが重要となります。

要約タスクは大きく2つのカテゴリーに分類されます。

**1\. 抽出的要約**

テキストを文単位で分割し、重要度に基づいてスコア付けとランク付けを行います。

**2\. 生成的要約**

複数の文書から情報を統合し、言い換えて一貫性のある要約を生成します。

通常、要約タスクはクエリベースとノンクエリベースに分けられます。RAGではクエリに関連する情報を取得するため、本研究ではクエリベースの手法に焦点が当てられました。

評価された要約手法は以下の通りです。

**（１）Recomp  
**抽出的圧縮器と生成的圧縮器の両方を備えています。抽出的圧縮器は有用な文を選択し、生成的圧縮器は複数の文書から情報を統合します。

**（２）LongLLMLingua  
**LLMLinguaを改良し、クエリに関連する重要な情報に焦点を当てています。

**（３）Selective Context  
**入力コンテキストから冗長な情報を特定し除去することでLLMの効率を向上させます。また、ベースとなる因果言語モデルで計算された自己情報量を使用して、語彙単位の情報量を評価します。この手法はノンクエリベースであり、クエリベースとノンクエリベースのアプローチを比較するために含められました。

#### 要約手法の評価実験設定

評価データセットはNQ、TriviaQA、HotpotQAの３つです。

また評価指標としてはF1スコアと要約後の変更トークン数（簡潔性の指標）が使用されました。

生成モデルとしてはLlama3-8B-Instructが選ばれました。要約比率は0.4に設定されました。

#### 実験結果と分析

実験結果は下記の表にまとめられています。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72121_12-1024x338.png)

主な知見は以下の通りです。

- Recompが全体的に最も優れた性能を示した
- LongLLMLinguaは期待されたほどの性能を示さなかったものの、実験データセットで訓練されていないため、より優れた汎化能力を持つ可能性がある

最高性能を示したRecompが推奨手法であり、その代替手法としてLongLLMLinguaは汎化能力の観点から認められています。

### 【8】ジェネレータの微調整

今後の研究課題として注目されているのがジェネレータの微調整、つまりLLMのファインチューニングです。微調整によって、関連性のある文脈や無関係な文脈が性能にどのように影響するかが今回調査されました。

#### 実験設定

実験データとしては、質問応答（QA）と読解のデータセットであるASQA、HotpotQA、NQ、TriviaQA、NarrativeQA、SQuAD、TruthfulQAが使用されました。

QAタスクの回答が比較的短いことを考慮し、正解のカバレッジ（ground-truth coverage）が評価指標として採用されました。

その他の実験設定は以下の通りです。

- 基本モデル：Llama-2-7B
- 学習方法：効率性を考慮し、LoRAと8ビット量子化を使用
- 学習設定：3 [エポック](https://ai-data-base.com/archives/26594 "エポック") 、最大シーケンス長1600、 [バッチサイズ](https://ai-data-base.com/archives/26582 "バッチサイズ") 4、学習率5e-5
- テスト設定：ゼロショット（文脈例なし）

#### 主な結果と分析

関連文書とランダム文書を混合して訓練したモデルが、ゴールドコンテキストや混合コンテキストを与えられた場合に最も良い性能を示しました。訓練中に関連文書とランダムに選択された文書を混ぜることで、ジェネレータの「無関係な情報に対する頑健性」「関連コンテキストの効果的な利用」が向上することを示唆しています。

この実験結果に基づき、訓練時に少数の関連文書とランダムに選択された文書を増強することが、最適なアプローチとして考えられます。

## 最適なRAGアプローチを検証する実験

以下のプロセスによって実践的に最適な手法が検討されました。

1. 上述のセクションで特定された各モジュール設定を使用する
2. 個々のモジュールを順次最適化
3. 代替案の中から最も効果的なオプションを選択
4. 最終的な要約モジュールの最適な実装方法が決定されるまで、プロセスを反復

### 生成器モデルの選択

（前セクションでの結果に基づき、）各クエリにランダムに選択された文書と関連文書を少数追加して微調整されたLlama2-7B-Chatモデルが採用されました。

### ベクトルデータベースの構築

Milvusを使用して、以下のコンテンツを含むベクトルデータベースが構築されました。

- 英語版Wikipediaから1000万テキスト
- 医療データから400万テキスト

### モジュールの貢献度評価

クエリ分類、再ランク付け、要約モジュールを個別に除去し、それぞれの貢献度が評価されました。

### 包括的な評価

#### 評価タスクとデータセット

以下の5つの [NLP](https://ai-data-base.com/archives/26319 "自然言語処理（NLP）") タスクとデータセットで広範な実験が実施されました。

1. 常識推論  
	MMLU、ARC-Challenge、OpenbookQAデータセット
2. 事実確認  
	FEVER、PubHealthデータセット
3. オープンドメインQA  
	NQ、TriviaQA、WebQuestionsデータセット
4. マルチホップQA  
	HotPotQA、2WikiMultiHopQA、MuSiQueデータセット（MuSiQueは回答可能な2ホップ質問のみ）
5. 医療QA  
	PubMedQAデータセット

各データセットからテストセット（または開発セット）から500エントリがランダムにサブ [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") されました。

#### RAG能力の評価

NQ、TriviaQA、HotPotQA、2WikiMultiHopQA、MuSiQueから合計500エントリが収集され、RAG能力が評価されました。各エントリは「質問、ゴールド文書、ゴールド回答」の三つ組で構成されています。

#### 評価指標

オープンドメインQAとマルチホップQAにおいてはトークンレベルのF1スコアとEM（Exact Match）スコアが採用され、その他のタスクにおいては正確性（Accuracy）が採用されました。

また、厳密な一致ではなく、モデル生成にゴールド回答が含まれているかで評価するスコアも測定されました。

#### RAG能力評価指標

RAGASフレームワーク（※）から採用された以下4つの指標が使用されました。

※RAGパイプラインを評価/テストするためのオープンソースフレームワーク

1. 忠実性（Faithfulness）  
	生成された回答が検索されたコンテキストと事実的に一致しているか
2. コンテキスト関連性（Context Relevancy）  
	検索されたコンテキストが元のクエリにどれだけ関連しているか
3. 回答関連性（Answer Relevancy）  
	生成された回答が元のクエリにどれだけ関連しているか
4. 回答正確性（Answer Correctness）  
	生成された回答が正解とどれだけ一致しているか

なお、GPT-4が判定者として評価を行いました。

さらに、検索された文書とゴールド文書間のコサイン類似度が検索類似度（Retrieval Similarity）として計算されました。

## 実験結果と分析

表11に実験結果が示されています。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72121_14-1-1024x618.png)

主な知見は以下の通りです。

クエリ分類モジュールは効果と効率の両方に貢献し、平均スコアが0.428から0.443に向上、クエリあたりのレイテンシーが16.41秒から11.58秒に短縮されました。

検索モジュールでは「HyDEを用いたハイブリッド」方式が最高のRAGスコア0.58を達成しましたが、計算コストが高くなります。効率性を考慮すると、「ハイブリッド」または「オリジナル」方式が推奨されます。

また、再ランク付けモジュールがないと性能が顕著に低下するため、必要不可欠です。MonoT5が最高の平均スコアを達成し、検索された文書の関連性を高める効果が確認されました。

再パッケージングモジュールではReverse構成が最も高いパフォーマンスを示し、RAGスコア0.560を達成しました。より関連性の高いコンテキストをクエリに近づけることで最適な結果が得られることを示唆しています。

要約モジュールとしてはRecompが優れた性能を示しましたが、要約モジュールを除去しても同等の結果が得られ、レイテンシーが低下しました。ただし、Recompは生成器の最大長制限に対処できるため、優先的に選択されます。時間が制約される応用では、要約を省略することでレスポンス時間を短縮できる可能性があります。

実験結果から、RAGシステムの各モジュールはそれぞれの役割で全体の性能に寄与していることが明らかになりました。クエリ分類モジュールは精度と遅延時間を改善し、検索および再ランク付けモジュールは多様なクエリに対するシステムの処理能力を大幅に向上させます。そして再パッケージングと要約モジュールは、システムの出力をさらに洗練し、様々なタスクにおいて高品質な応答を確保します。

## RAGの最適なアプローチと今後の課題

実験結果に基づき、RAGシステムの実装には2つの異なるアプローチがあると提案されています。なお、要件に応じてカスタマイズされます。

### 1\. 最高性能を追求するアプローチ

最高の性能を達成するためには、以下の構成が推奨されます。

- クエリ分類モジュールの導入
- 「HyDEを用いたハイブリッド」方式による検索
- monoT5を用いた再ランク付け
- Reverse方式による再パッケージング
- Recompを用いた要約

ただし、計算負荷が高くなる傾向があります。

### 2\. 効率性とパフォーマンスのバランスを取るアプローチ

性能と効率性のバランスを取るのであれば、以下の構成が良いと考えられています。

- クエリ分類モジュールの導入
- ハイブリッド方式による検索
- TILDEv2を用いた再ランク付け
- Reverse方式による再パッケージング
- Recompを用いた要約

システムの処理時間の大部分を占める検索モジュールをハイブリッド方式に変更することで、他のモジュールを維持しながら、レイテンシーを大幅に削減しつつ、同等の性能を維持することが可能となります。

### マルチモーダルRAGについて

RAGは、マルチモーダルアプリケーションにも拡張されはじめています。大量の画像とテキスト説明のペアをデータベースに持つシステムに、text2imageとimage2textで検索するのです。

#### text2image

ユーザーのクエリが保存されている画像の説明テキストと一致する場合に使用します。画像生成プロセスが高速化できます。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72121_16-1024x298.jpg)

#### image2text

ユーザーが画像を提供し、その画像について会話を行う場合に使用します。

マルチモーダルRAGは画像だけでなく他のモダリティ（例：動画や音声）にも拡張する計画が立てられています。また、効率的で効果的なクロスモーダル検索技術の探求も予定されています。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72121_17-1024x409.jpg)

## まとめ

本記事では、RAGシステムの最適実装を探る研究を紹介しました。

今回研究者らは各モジュールの効果的アプローチを評価し、包括的な評価ベンチマークを用いて検証しました。また、最後にマルチモーダルRAGについても触れられています。

情報検索と生成における可能性を広げるためにはRAGシステムの最適化と拡張が重要です。なお、この論文で表現されているのは一つの提案のため、個々のプロジェクトでの調整は必須です。

- 参照論文URL： [https://arxiv.org/abs/2407.01219](https://arxiv.org/abs/2407.01219)
- GitHub： [https://github.com/FudanDNN-NLP/RAG](https://github.com/FudanDNN-NLP/RAG)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMエージェントの評価はLLM単体の評価と大きく異なる](https://ai-data-base.com/archives/72074)

[人間のような内省メカニズムをLLMに導入することの効果 Google DeepMindなどが検証](https://ai-data-base.com/archives/72194)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)