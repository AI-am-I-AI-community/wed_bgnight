---
title: "オープンソースのコード生成LLMが商用LLMに追いつく Qwen2.5-Coderの能力値全容"
source: "https://ai-data-base.com/archives/78609"
author:
  - "[[AIDB Research]]"
published: 2024-11-18
created: 2025-06-13
description: "本記事では、コード生成に特化したLLM「Qwen2.5-Coder」の研究成果を紹介します。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、コード生成に特化したLLM「Qwen2.5-Coder」の研究成果を紹介します。

このモデルは、GPT-4oやClaude3.5に匹敵する性能を持つオープンソースのコードLLMとして注目を集めており、0.5Bから32Bまでの6つのサイズ展開で、40以上のプログラミング言語に対応しています。

5.5兆トークンという膨大なデータでの学習で、コード生成・デバッグ・SQL生成など多岐にわたる機能を備え、開発者の実用的なニーズに応えることを目指して開発されました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609-1024x576.jpg)

**参照文献情報**

- タイトル：Qwen2.5-Coder Technical Report
- 著者：Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, Junyang Lin
- 所属：Qwen Team of Alibaba Group

## 背景

プログラミングに特化した言語モデルの研究が活発に行われています。これまでStarCoderシリーズ、CodeLlamaシリーズ、DeepSeek-Coderシリーズ、CodeQwen1.5、CodeStralなど、多くのコードLLMが開発されてきました。

しかし、最新の非公開モデルであるClaude-3.5-SonnetやGPT-4oと比べると、オープンソースのコードLLMには、まだ改善の余地がありました。

このような背景から、今回研究チームは以前開発したCodeQwen1.5の経験を生かし、より性能の高い新しいモデルの開発に取り組むことにしました。5.5兆を超える大量のデータを用いて、プログラミングに特化した事前学習を行うことにしたのです。

データの収集には、GitHubなどのプログラミング関連サイトやウェブから得られた情報を活用したそうです。ただし、単にデータを集めるだけでなく、質の低いコンテンツを取り除くため、機械学習を使った選別も行っています。

さらに、プログラミングだけでなく、数学や一般的な文章の理解力を持つことも目指しています。そのため、コード、数学、一般テキストをバランスよく組み合わせてデータが作成されました。

そして0.5Bから32Bまでの異なる規模のモデルを開発することで、様々な用途に対応できるようにしています。

このように、既存のコードLLMの限界を超え、より使いやすく高性能なモデルを作ることが、本研究の出発点となっています。実際の開発現場での活用を強く意識し、コードアシスタントやプログラム関連のツールとして実用的なモデルを目指していることが特徴です。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_1.png)

以下で本モデルの構造やいかに優れているかといった実験結果をまとめて紹介します。

## モデルアーキテクチャ（モデルの基本設計構造）

Qwen2.5-Coderの [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") は、Qwen2.5から直接派生したものです。0.5B、1.5B、3B、7B、14B、32Bの6つのパラメータサイズ（モデルが学習に使用する数値の総数、数が多いほど高性能）で展開されています。

これらのモデルはすべて、ヘッドサイズ（注意機構における一つの注意ヘッドが処理できる情報量）が同じという共通の特徴を持っていますが、他の要素は異なります。たとえば1.5Bモデルは中間層（入力層と出力層の間の処理層）が大きく、3Bモデルはレイヤー数（モデル内の処理層の数）が多いという特徴があります。

基本的にパラメータ数が増えるにつれて、隠れ層サイズ（モデル内部の情報処理能力を決める数値）、クエリヘッド数とキー・バリューヘッド数（情報の関連性を計算する機構の数）、中間層サイズが拡大していきます。

具体例として7Bモデルと32Bモデルを比較すると、隠れ層サイズは7Bモデルが3,584、32Bモデルが5,120となっています。クエリヘッド数は7Bモデルで28個、32Bモデルで40個、キー・バリューヘッド数は7Bモデルで4個、32Bモデルで8個となっており、モデルの処理能力が向上しています。

また、小規模モデルでは埋め込み（単語をコンピュータが理解できる数値表現に変換する仕組み）の共有が行われますが、大規模モデルではその共有は行われません。すべてのモデルに共通する特徴として、語彙数が151,646トークンであり、5.5兆トークンで学習が行われているという点があります。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_2-1-1024x375.png)

Qwen2.5-Coderの6つのモデルサイズ（0.5B～32B）それぞれの設定値を示す構成表

### トークン化の仕組み

Qwen2.5-Coderは、Qwen2.5の語彙を継承しつつ、コードをより良く理解するために複数の特殊トークン（特別な意味を持つ記号や文字列）が追加されています。例えばテキストや配列の終端を示す<|endoftext|>、Fill-in-the-Middle技術（文章の中間部分を予測・補完する手法）のための<|fim\_prefix|>、<|fim\_middle|>、<|fim\_suffix|>、パディング用の<|fim\_pad|>、リポジトリ名を示す<|repo\_name|>、ファイル区切りのための<|file\_sep|>などがあります。

特殊トークンは、コードの構造を適切に理解し、ファイルレベルとリポジトリレベルの両方で、より長いコンテキスト（文脈）を効果的に扱うために重要な役割を果たしています。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_3.png)

コード処理用の特殊トークンとその用途の一覧

## 事前学習

事前学習モデル（学習の土台となる基礎モデル）の基盤となるのは大規模で高品質、かつ多様なデータです。今回、新しくQwen2.5-Coder-Dataと名付けられたデータセットが構築されました。5つの主要なデータタイプで構成されています。

1. プログラミングの元となるコードデータ
2. コードに関連する文書や説明文
3. 人工的に生成された学習用データ
4. 数学的な問題解決に関するデータ
5. 一般的な自然言語のデータ

詳しく見ていきましょう。

### データの構成

ソースコードについては、2024年2月以前に作成された92のプログラミング言語にわたるGitHubの公開リポジトリから収集されました。また、プルリクエスト（コード変更の提案）、コミット（コードの変更記録）、Jupyterノートブック（対話型プログラミング環境のファイル）、Kaggleデータセットなども収集され、それぞれに適切なクリーニング（データの選別や整理）が施されました。

テキストとコードの関連付けデータについては、Common Crawl（インターネット上の大規模なデータアーカイブ）から、コードに関連するドキュメント、チュートリアル、ブログなどが収集されました。従来のURL単位での段階的な収集方法とは異なり、粗から密へと階層的にフィルタリングする手法が開発されました。

上記のような方法をとった狙いは2つあります。

1. 各フィルターの役割を明確に制御でき、すべての側面を包括的に処理できる
2. データの品質スコアを自然に割り当てることができ、最終段階まで残るデータほど高品質となる

このデータのクリーニングには、fastTextなどの小規模モデルを用いたフィルタリングが段階的に適用されました。より大きなモデルでの実験も行われましたが、大きな利点は見られませんでした。おそらく、小規模モデルの方が表面的な特徴に集中でき、不要な意味的な複雑さを避けられるためと考えられます。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_4.png)

フィルタリング各段階でのトークン数の減少と、それに伴うモデル性能の向上を示すグラフ

合成データについては、将来的なデータ不足に対応するため、Qwen2.5-Coderの前身であるCodeQwen1.5を使用して大規模な合成データセットが生成されました。生成されたデータの品質を確保するため、実行可能なコードのみを保持する実行検証機能が導入されました。

数学データについては、Qwen2.5-Mathの事前学習データが統合されました。重要なのは、この数学データの追加がコードタスクの性能に悪影響を与えなかったという点です。

なおテキストデータについては、Qwen2.5モデルの事前学習データから高品質な一般的な自然言語データを含められました。既にQwen2.5のデータクリーニング段階で厳密な品質チェックを経ているため、追加の処理は行われませんでした。ただし、コードデータとの重複を避けるため、一般テキストデータからすべてのコード部分が削除されました。

## 事後学習

### 命令データの作成方法

多言語プログラミングコードの識別には、CodeBERTモデルが微調整して使用されました。約100のプログラミング言語を分類することができるモデルです。  
主要なプログラミング言語の命令データは保持し、マイナーな言語の命令データは一部を無作為に除外しています。もしサンプルにコードスニペット（コードの断片）がほとんど、あるいはまったく含まれていない場合、「プログラミング言語なし」というタグに分類される可能性があります。コード生成タスクの性能を維持するため、コードスニペットを含まないサンプルの大部分は除外されました。

GitHubからの命令データ生成については、1024トークン以内のコードスニペットから命令文を生成し、コードLLMで応答を生成して教師あり命令データセットを構築しました。最終的には、LLMによる品質評価を行い、低品質なものを除外してデータペアを作成します。

多言語コード命令データについては、多言語マルチエージェント協調フレームワークを提案し、多言語の命令データを生成しました。言語固有のエージェントを導入し、各プログラミング言語に特化したエージェントのセットを作成しました。エージェントは、既存の多言語命令データから抽出された言語固有の命令データで初期化されます。

命令データの品質評価のために、サンプルごとにチェックリストベースの採点を導入しました。評価の観点は以下の通りでした。

- 質問と回答の一貫性
- 質問と回答の関連性
- 質問と回答の難易度
- コードの存在
- コードの正確性
- コードの明確さ
- コードのコメント
- 学習のしやすさ

コード検証のための多言語サンドボックス（安全な実行環境）を構築しました。コードスニペットを抽象構文木に解析し、解析エラーのあるものを除外します。中でも、自己完結型のコード（アルゴリズムの問題など）のみがサンドボックスに送られます。

### 学習方針

まず「粗から密への微調整」という段階的な学習が行われます。第一段階では、品質はやや劣るものの幅広い種類の命令データを何千万も使ってベースモデルを改良します。その後の第二段階では、厳選された数百万の高品質な命令データを使って、さらなる改良を重ねます。この時、モデルに複数の選択肢を提示させ、その中から最も適切な回答を選んで学習に活用します。

次に「混合学習」を行います。通常の命令データは短い文章が多いのですが、モデルには長い文章も処理してほしいという課題があります。そこで、FIM形式（文章の途中を補完する形式）を活用して命令データを作ります。プログラミング言語の文法規則や、実際のプログラマーがどのようにコードを書くのかを参考にして、プログラムの重要な部分を空欄にしたデータを作成します。こうすることで、モデルは文脈を理解しながら適切なコードを補完する能力を身につけます。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_6-1024x128.png)

ファイルレベル事前学習→リポジトリレベル事前学習→アライメントの3段階を示す図

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_5-1024x165.png)

異なるデータ混合比率におけるモデルのパフォーマンス比較

最後に「直接選好最適化（DPO）」という方法で、モデルはさらに改良されます。本来なら人間が一つ一つ評価するのが理想ですが、それには膨大な時間と労力がかかります。そこで二つの方法を組み合わせています。

1. サンドボックスと呼ばれる安全な環境でコードを実際に動かし、その結果を評価として使う
2. 大規模言語モデル（LLM）を使って、人間ならどう評価するかを予測する

この結果の評価データとその他の一般的なデータを組み合わせて、オフラインでDPO学習が実施されます。すべては、モデルにより実用的で質の高いコードを生成させるためです。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_7-1-1024x149.png)

ファイルレベルでのコード補完学習に使用されるFIMフォーマットの具体例

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_8-1-1024x336.png)

リポジトリレベルでのコード補完学習に使用されるFIMフォーマットの具体例

## データの汚染除去

テストデータの漏洩（データの混入）によって、Qwen2.5-Coderの評価結果が不当に良くなることを防ぐため、事前学習と事後学習の両方のデータセットで汚染除去が行われました。

以下のような代表的なデータセットを学習データから完全に取り除きました。

- HumanEval（人間が作成した評価用のプログラミング課題）
- MBPP（プログラミングの問題と解答のデータセット）
- GSM8K（数学の問題解決データセット）
- MATH（数学の問題データセット）

汚染除去の方法としては、10-gramと呼ばれる手法が使用されました。連続する10個の単語のまとまりに注目し、テストデータと学習データの間でそのような単語のまとまりが一致する部分があれば、その学習データを除外するという方法です。テストで使う問題や似たような問題が学習データに含まれることを防ぎ、より公平な評価が可能になります。

## ベースモデルの評価

ベースモデルについて、以下の6つの重要な観点から総合的な評価が行われました。

1. コード生成能力
2. コード補完能力
3. コード推論能力
4. 数学的推論能力
5. 一般的な自然言語理解力
6. 長文脈処理能力

すべての評価結果が再現可能となるよう、評価用のコードは公開されています。比較対象として、StarCode [r2](https://ai-data-base.com/archives/26434 "決定決定係数（R2）") シリーズやDeepSeek-Coderシリーズなど、最も一般的で性能の高いオープンソースの言語モデルが選ばれました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_9-1024x513.png)

評価に使用された各モデル一覧

### コード生成能力の評価

コード生成は、より複雑なタスクを処理するための基本的な能力として位置づけられています。評価には、HumanEvalとMBPPという2つの代表的なベンチマーク（性能評価基準）が使用されました。

HumanEvalは164個の手作業で作成されたプログラミング課題で構成されており、各課題ではPython関数の定義と説明文がモデルに入力として与えられます。一方、MBPPは974個のプログラミング課題が含まれており、各課題には問題文、関数の定義、そして3つのテストケースが含まれています。

より厳密な評価のため、HumanEvalはHumanEval+へと拡張され、テストケースが80倍に増加し、オリジナルのHumanEvalに含まれていた不正確な解答も修正されています。同様に、MBPP+ではテストケースが35倍に増やされています。

また、MBPP 3-shotは、モデルの学習過程を監視するのに特に適していることが分かりました。学習の初期段階では性能が不安定になりがちですが、3つの簡単な例を示すことで、この不安定さが大きく改善されることが確認されています。

評価結果によると、Qwen2.5-Coderは基本的なコード生成において印象的な性能を示しました。同じサイズの他のオープンソースモデルを上回り、さらに大きなモデルをも凌ぐ結果が得られています。特に、Qwen2.5-Coder-7Bは、これまで最高性能とされていたDS-Coder-33Bを、すべての5つの評価指標で上回る結果を示しました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_10-1024x759.png)

各モデルのコード生成タスクでの性能比較表

### BigCodeBench-Completeによる評価

BigCodeBenchは、ツールの使用方法と複雑な指示への対応能力を評価するために作られた、より挑戦的な新しいベンチマークとして知られています。このベンチマークでは、ベースモデルに関数の定義と説明文が与えられ、期待されるコードが生成されます。これは「BigCodeBench-Complete」と呼ばれています。このベンチマークには完全版とハード版の2つのセットが用意されています。

HumanEvalやMBPPと比較して、BigCodeBenchは特に未知のデータに対する性能（OOD：Out-of-Distribution）の評価に適していることが指摘されています。評価の結果、Qwen2.5-Coderは、BigCodeBench-Completeにおいても高い性能を示し、モデルの汎用性の高さが実証されました。

### 複数プログラミング言語への対応評価

ここまでの評価は主にPython言語に焦点が当てられていましたが、優れたコードモデルには、ソフトウェア開発の複雑で変化する要求に応えるため、Python以外の言語でも高い性能が求められます。そこで、MultiPL-Eベンチマークが選択され、以下の（Pythonを含む）8つの主要言語について評価が実施されました。

- Python
- C++
- Java
- PHP
- TypeScript
- C#
- Bash
- JavaScript

評価の結果、Qwen2.5-Coderは複数プログラミング言語の評価でも最高水準の結果を達成し、各言語での能力がバランスよく備わっていることが確認されました。注目すべき点として、8つの言語のうち5つで60%以上のスコアが記録されています。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_11-1024x674.png)

8つのプログラミング言語における各モデルの性能比較

### コード補完能力の評価

多くの開発支援ツールでは、前後の文脈に基づいてコードを自動補完する能力が重要とされています。Qwen2.5-CoderではFill-In-the-Middle（FIM）と呼ばれる学習戦略が採用され、文脈に応じた適切なコードの生成が可能となっています。

コード補完能力の評価には、以下の5つのベンチマークが使用されました。

- HumanEval-FIM
- CrossCodeEval
- CrossCodeLongEval
- RepoEval
- SAFIM

これらの評価を通じて、Qwen2.5-Coder-32Bは特に優れた性能を示し、コード補完タスクにおける高い能力が実証されています。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_12-1024x379.png)

Humaneval-FIM、SAFIM、CrossCodeEval、RepoEval、CrossCodeLongEvalの5つのベンチマークにおける、Qwen2.5-Coder-32B-Base、DS-Coder-33B-Base、DS-Coder-V2-Lite-Baseの性能比較を示すバーグラフ

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_13-1024x955.png)

Python、Java、JavaScriptでのコード補完タスクの性能比較

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_14-1024x810.png)

4つのプログラミング言語でのクロスファイルコード補完性能の比較

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_15-1024x878.png)

チャンク補完と関数補完タスクでの性能比較

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_16-1024x898.png)

行、関数、API補完での各モデルの性能比較

### コード推論能力の評価

コードは高度に抽象化された論理言語であり、コードに基づく推論能力は、モデルがコードの背後にある論理の流れを真に理解しているかを判断する重要な指標とされています。

コード推論能力の評価には、CRUXEvalベンチマークが採用されました。800のPython関数が含まれており、それぞれに入出力例が付随しています。評価は2つのタスクで構成されています。

1. CRUXEval-I（与えられた入力に対して出力を予測する）タスク
2. CRUXEval-O（与えられた出力に基づいて入力を予測する）タスク

両タスクでは、思考の連鎖（Chain-of-Thought）アプローチが採用され、モデルは実行過程を段階的に出力することが求められます。評価結果から、Qwen2.5-Coderは、コードクリーニング過程での実行可能性重視の方針が功を奏し、非常に有望な結果を示しました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_17.png)

入力予測と出力予測タスクでの各モデルの性能比較

### 数学的推論能力の評価

数学とプログラミングには密接な関係があり、数学は基礎的な学問としてコーディングを支え、同時にコーディングは数学分野における重要なツールとなっています。そのため、オープンで強力なコードモデルには、数学的能力も求められます。

Qwen2.5-Coderの数学的能力を評価するため、以下の代表的なベンチマークが選択されました。

- MATH（数学問題解決の総合的な評価）
- GSM8K（数学的な文章題の解決能力評価）
- MMLU-STEM（科学技術・工学・数学分野の知識評価）
- TheoremQA（数学的定理の理解と応用の評価）

評価結果から、Qwen2.5-Coderは数学分野でも優れた性能を示すことが確認されました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_19-1024x798.png)

4つの数学ベンチマークでの性能比較

成功の要因としては、以下の2点が挙げられています。

1. Qwen2.5という強力な基盤モデルを土台としていること
2. 学習時にコードと数学のデータを慎重に組み合わせ、両分野でバランスの取れた性能を実現したこと

### 一般的な自然言語理解力の評価

数学的能力に加えて、ベースモデルが持つ一般的な言語処理能力をできるだけ維持することも目標とされています。一般的な自然言語理解力を評価するためには、以下のベンチマークが選択されました。

- MMLU（多岐にわたる知識を問う総合テスト）
- MMLU-Redux（MMLUの改訂版）
- ARC-Challenge（抽象的な推論能力の評価）
- TruthfulQA（事実に基づく回答能力の評価）
- WinoGrande（常識的な推論能力の評価）
- HellaSwag（文脈理解と予測能力の評価）

数学分野での結果と同様に、Qwen2.5-Coderは一般的な言語理解においても、他のコードモデルと比較して優位性を示しました。データ混合戦略の有効性を裏付ける結果となっています。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_20.png)

MMLU言語理解タスクでの性能比較

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_21-1024x702.png)

一般的な言語理解タスクでの性能比較

### 長文脈処理能力の評価

長文脈を処理する能力は、リポジトリレベルのコードを理解し、コードエージェントとして機能するための核となる技能とされています。しかし、現在の多くのコードモデルは、長さに関して非常に限定的なサポートしか提供できておらず、これが実用的な応用の妨げとなっています。

Qwen2.5-Coderでは、オープンソースのコードモデルにおける長文脈処理の進展を目指し、リポジトリレベルの長いコードデータを収集・構築して事前学習に使用しました。データの比率と構成を慎重に調整することで、最大128Kトークンまでの入力に対応できるようになっています。

評価には「Needle in the Code」と呼ばれる、シンプルながら基本的な合成タスクが作成されました。コードリポジトリ（評価にはMegatronが選択されました）の様々な位置にカスタム関数が挿入され、モデルがコードベース全体を理解した上でその関数を再現できるかがテストされます。

評価結果から、Qwen2.5-Coderは128Kの長さの範囲内でこのタスクを首尾よく完了できることが示されています。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_18-1024x320.png)

10kから128kまでの異なる文脈長での正確性を示すグラフ

以上の評価全体から、Qwen2.5-Coderが様々な側面で高い性能を持つことが実証されました。同じサイズの他のモデルと比較して、多くの評価基準で優れた結果を示していることが確認されています。

## Instructモデルの評価

Instructモデル（対話や指示に応答できるよう調整されたモデル）については、以下の6つの主要な分野で厳密な評価が実施されました。

1. コード生成能力
2. コード推論能力
3. コード編集能力
4. テキストからSQLへの変換能力
5. 数学的推論能力
6. 一般的な自然言語理解力

すべての評価結果が再現可能となるよう、評価用のコードは公開されています。比較対象として、DeepSeek-CoderシリーズやCodestralモデルなど、広く利用されているオープンソースの命令調整済みモデルが選択されました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_22-1024x581.jpg)

評価に使用された各指示チューニングモデル一覧

### コード生成能力の評価

基本モデルの性能向上を受け、Qwen2.5-CoderのInstructモデルシリーズもコード生成タスクにおいて優れた性能を示しています。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_23.png)

HumanEval、MBPP、BigCodeBench、LiveCodeBenchでの性能比較

まず、EvalPlusデータセットを用いた評価が行われました。その結果、Qwen2.5-Coder-7B-Instructは他の同規模のモデルを大きく上回る精度を達成しました。特筆すべきは、CodeStral-22BやDS-Coder-33B-Instructといった200億以上のパラメータを持つ大規模モデルをも上回る性能を示したことです。さらに、Qwen2.5-Coder-32B-Instructモデルは、EvalPlusにおいて最高性能を記録し、DS-Coder-V2-Instructをも上回る結果となりました。

BigCodeBench-Instructについても評価が行われました。このベンチマークは、命令ベースのモデルのコード生成能力を評価するために設計されています。評価の結果、Qwen2.5-Coder-7B-Instructは完全版で41.0%、ハード版で18.2%という高い精度を達成し、同規模の他のinstructモデルを上回りました。さらに、Qwen2.5-Coder-32B-Instructは完全版で49.6%、ハード版で27.0%を記録し、多くのクローズドソースAPIをも凌ぐ性能を示しています。

なおLiveCodeBenchは、LLMのコーディング能力を評価するための包括的で汚染のないベンチマークとして知られています。LeetCode、AtCoder、CodeForcesといった主要な競技プログラミングプラットフォームから継続的に新しい問題が収集され、2023年5月から2024年9月の間に公開された600以上の高品質なコーディング問題が含まれています。  
LiveCodeBench（2407-2409）での評価においては、Qwen2.5-Coder-7B-Instructは37.6%というPass@1精度（一回目の試行での成功率）を達成しました。これは同規模のモデルを大きく上回るだけでなく、CodeStral-22B-v0.1やDS-Coder-33B-Instructといった大規模モデルをも上回る結果となっています。また、Qwen2.5-Coder-32B-Instructは31.4%の精度を達成し、オープンソースのコード生成モデルの中で最高性能を示し、多くの商用APIに匹敵する水準に達しています。

### 多言語プログラミングへの対応評価

基本モデルの多言語処理能力は、instructモデルにも引き継がれています。能力の評価には、MultiPL-EとMcEvalという2つのベンチマークが使用されました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_25.png)

様々なプログラミング言語におけるQwen2.5-Coderと他のモデルの性能比較グラフ

MultiPL-Eでの評価結果によると、Qwen2.5-Coder-7B-InstructはDS-Coder-V2-Lite-Instructなどの同規模モデルを一貫して上回る性能を示しました。また、Qwen2.5-Coder-7Bと14Bの両モデルは、CodeStral-22BやDS-Coder-33B-Instructといった200億以上のパラメータを持つ大規模モデルをも上回りました。Qwen2.5-Coder-32B-Instructは、DS-Coder-V2-Instructと同等の性能を達成し、複数の商用APIに近い性能を示しています。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_24.png)

MultiPL-Eにおける8つの言語での性能比較

McEvalベンチマークでは、40のプログラミング言語にわたる16,000のテストケースを用いて、より広範な言語での評価が行われました。結果として、Qwen2.5-Coder-32B-Instructは他のオープンソースモデルと比較して、特に幅広いプログラミング言語において優れた性能を示しました。

MdEvalベンチマークでは、18の言語にわたるコードデバッグ（エラー修正）能力が評価されました。このベンチマークでは、バグのあるコードとテストケース（1,200サンプル）がモデルに提供され、正しいコードの生成が求められます。評価の結果、Qwen2.5-Coder-32B-Instructは、より大規模なモデルと比較しても同等以上の性能を示しました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_26-1024x494.png)

複数の言語でのコードデバッグ性能の比較グラフ

### 人間の好みとの一致性評価

Qwen2.5-Coder-32B-Instructと人間の好みとの一致度を評価するため、CodeArenaと呼ばれる内部の評価ベンチマークが使用されました。このベンチマークには、人間が作成した約400のサンプルが含まれています。Chatbot Arenaと同様に、CodeArenaは実際の環境でのコード関連の要求を模倣しています。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_27-1024x501.png)

各モデルの人間の選好との一致率を示すグラフ（Win/Tie/Lose）

評価にはGPT-4oが評価モデルとして使用され、「AとBの勝率」という評価方法が採用されました。これは、テストセット内でAのスコアがBのスコアを上回る割合を測定するものです。評価結果から、Qwen2.5-Coder-32B-Instructが人間の好みとの一致において優位性を持つことが示されました。

### コード推論能力の評価

コード推論能力の評価には、CRUXEvalデータセットが使用されました。評価結果から、Qwen2.5-Coder-7B-Instructは入力型推論（Input-CoT）で65.8%、出力型推論（Output-CoT）で65.9%という精度を達成しました。DS-Coder-V2-Lite-Instructと比較して、それぞれ12.8%と13.0%の大幅な改善となっています。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_29.png)

CRUXEvalでの入力予測と出力予測タスクの性能比較

また、Qwen2.5-Coder-7B-InstructはCodeStral-22BやDS-Coder-33B-Instructといった大規模モデルをも上回る性能を示し、より小さなパラメータ数でも高度なコード推論が可能であることが実証されました。特筆すべきは、Qwen2.5-Coder-32B-Instructが入力型推論で75.2%、出力型推論で83.4%という高精度を達成し、DS-Coder-V2-Instructを含む他のオープンソースコードモデルを大きく上回ったことです。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_28.png)

パラメータ数と推論性能の関係を示す散布図

### コード編集能力の評価

Aiderは、大規模言語モデルとの協調を定量的に測定するためのコード編集ベンチマークを作成しました。Exercismから取得した133のPython演習問題を基に、自然言語によるプログラミング要求を解釈し、ユニットテストに合格する実行可能なコードに変換する能力が評価されます。

この評価では、単なるコーディング能力だけでなく、既存のコードを編集し、その修正をAiderのシステムにシームレスに統合できる形式で提供する能力も試されます。つまり、ローカルのソースファイルが問題なく更新できることが確認されます。

評価結果によると、Qwen2.5-Coder-7B-Instructは特に優れたコード修復能力を示しました。70億というパラメータ数にもかかわらず、51.9%というPass@1精度を達成し、同規模のモデルを大きく上回りました。さらに、CodeStral-22BやDS-Coder-33B-Instructといった大規模モデルをも上回る性能を示しています。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_33-642x1024.png)

コード編集タスクでのPass@1とPass@2の性能比較

Qwen2.5-Coder-32B-Instructモデルはさらに高い精度を達成し、Pass@1で60.9%、Pass@2で73.7%を記録しました。要するに、モデルが高度なコード編集能力を持つことを実証しています。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_30.png)

デバッグ、翻訳、要件変更、改善の4分野での性能比較グラフ

### テキストからSQLへの変換能力の評価

SQLは日常的なソフトウェア開発や運用に不可欠なツールですが、プログラミングの知識がない人にとって習得の障壁が高いとされています。この課題に対処するため、自然言語の質問を構造化されたSQLクエリに自動的に変換する「テキストからSQL」というタスクが導入されました。

事前学習と微調整の両段階で、精密に作られた合成データを活用したことにより、Qwen2.5-CoderのテキストからSQLへの変換能力は大きく向上しました。評価には、SpiderとBIRDという2つの著名なベンチマークが選択されました。

公平な比較を行うため、ChangとFosler-Lussierの研究に従って統一された入力テンプレートが使用されました。このテンプレートには、データベースの説明に沿ったテーブル表現、テーブル内容の例、必要に応じて追加の知識、そして自然言語での質問が含まれています。このような標準化されたテンプレートにより、プロンプト（入力文）の違いによる偏りが最小限に抑えられています。評価の結果、Qwen2.5-Coderは同サイズの他のコードモデルを上回る性能を示しました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_31-1024x333.png)

BirdとSpiderデータセットでのSQL生成性能の比較グラフ

### テーブル理解能力の評価

構造化データの理解能力を評価するため、TableBenchという包括的で複雑なベンチマークが使用されました。このベンチマークには、テーブル質問応答（TableQA）能力に関する4つの主要カテゴリーにわたる18の分野が含まれています。テキストによる思考の連鎖（TCoT）設定のもと、他の言語モデルとの比較が行われ、Qwen2.5-Coder-32B-Instructが45.1という最高スコアを達成しました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_32.png)

事実確認、数値推論、データ分析、可視化の4つの側面での性能比較グラフ

### 数学的推論と一般的な自然言語処理の評価

最後に、Qwen2.5-Coderシリーズと DS-Coder-V2シリーズの性能比較が、数学的計算と一般的な [自然言語処理](https://ai-data-base.com/archives/26319 "自然言語処理（NLP）") の両面で行われました。評価結果から、Qwen2.5-Coderシリーズは複雑なコーディングタスクだけでなく、高度な一般的タスクでも優れた性能を示し、競合モデルとの差別化に成功していることが明らかになりました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_34-1024x348.png)

数学および一般言語タスクでの包括的な性能比較

以上から、Qwen2.5-Coderのinstructモデルは、コード関連のタスクから一般的な言語理解まで、幅広い能力を備えていることが実証されています。

## スケーリングの重要性についての考察

Qwen2.5-Coderの異なるサイズのモデルと、他のオープンソース言語モデルとの比較が、MBPP-3shotとLiveCodeBenchという2つの基準で行われました。

ベースモデルの評価には、MBPP-3shotが選択されました。広範な実験により、MBPP-3shotがベースモデルの評価に最も適しており、モデルの実際の性能と高い相関を示すことが明らかにされています。

一方、instructモデルの評価には、LiveCodeBenchの最新4ヶ月分（2024年7月から11月）の問題が選択されました。この期間の問題が選ばれた理由は、テストデータの汚染を厳密に避け、モデルの未知のデータに対する対応能力（OOD能力）を真に反映させるためです。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78609_35-1024x258.png)

MBPP-3shotとLiveCodeBenchでのモデルサイズと性能の関係を示すグラフ

評価結果から、以下の2つの重要な知見が得られました。

1. モデルのサイズとその性能の間には明確な正の相関関係が見られます。つまり、モデルが大きくなるほど、性能も向上する傾向にあります。
2. Qwen2.5-Coderは、すべてのサイズにおいて最高水準の性能を達成しています。

上記はより大規模なコード特化型言語モデルの探求を継続することの意義を示唆しています。つまり、モデルの規模を拡大することで、さらなる性能向上が期待できるという意味です。

計算資源とのバランスを取りながら、効率的にモデルの規模を拡大していく戦略の重要性が強調されています。

## まとめ

本記事では、Qwen2.5社が開発した新しいコード生成モデル「Qwen2.5-Coder」の研究を紹介しました。

Qwen2.5をベースに、0.5Bから32Bまでの6つのモデルサイズで展開され、5.5兆トークンという大規模なデータセットで学習されています。

特筆すべき点は、GitHubなどの公開コードデータに加え、ウェブから抽出した高品質なコード関連コンテンツを活用し、さらに数学や一般的な言語理解の能力も維持するよう設計されていることです。また、実用的なコードアシスタントとしての機能を重視し、綿密に設計された指示チューニングデータセットによる学習も行われています。

研究チームは今後、データサイズとモデルサイズの両面でのスケールアップを探求し、コードモデルの推論能力のさらなる向上を目指すとしています。

- 参照文献URL： [https://arxiv.org/abs/2409.12186](https://arxiv.org/abs/2409.12186)
- GitHub： [https://github.com/QwenLM/Qwen2.5-Coder](https://github.com/QwenLM/Qwen2.5-Coder)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[画像も文字も表も全部まとめて理解するRAGシステムの提案　Bloombergなど](https://ai-data-base.com/archives/78490)

[LLMプロジェクト開発に必要な新しい概念「AgentOps」とは](https://ai-data-base.com/archives/78733)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)