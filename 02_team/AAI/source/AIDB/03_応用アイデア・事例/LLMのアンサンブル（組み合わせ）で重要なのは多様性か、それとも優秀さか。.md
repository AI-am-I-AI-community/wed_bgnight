---
title: "LLMのアンサンブル（組み合わせ）で重要なのは多様性か、それとも優秀さか。"
source: "https://ai-data-base.com/archives/86165"
author:
  - "[[AIDB Research]]"
published: 2025-02-28
created: 2025-06-13
description: "本記事では、複数のLLMをどのように組み合わせるかを”再検討”した研究を紹介します。複数モデルの出力を束ねて答えを出す手法はこれまで注目され続けてきましたが、どのように組み合わせるとよいのかといった疑問は残ったままでした。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、複数のLLMをどのように組み合わせるかを”再検討”した研究を紹介します。

複数モデルの出力を束ねて答えを出す手法はこれまで注目され続けてきましたが、どのように組み合わせるとよいのかといった疑問は残ったままでした。

今回プリンストン大学の研究者らは、そもそも単一の強力なモデルを繰り返し用いるほうがよいのではないかと仮説を持ち、多様なモデルを混在させる戦略と比較して多角的に検証しました。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86165-1024x576.png)

参照論文情報は記事の下部に記載されています。

**本記事の関連研究**

- [オープンソースモデルでも力を合わせればGPT-4oに匹敵することを示す「Mixture-of-Agents（MoA）」アーキテクチャ](https://ai-data-base.com/archives/71419)
- [小さなLLMを多数組み合わせることで、単一の巨大モデルに匹敵する可能性](https://ai-data-base.com/archives/64708)

## 背景

LLMは、規模拡大と膨大なデータ投入によって性能向上が図られてきました。単一モデルの進化に注目されがちですが、複数のモデル出力を統合するアンサンブル手法も目が離せません。

アンサンブル手法の一種である「Mixture-of-Agents（MoA）」は、多様なモデルの知見を組み合わせることで、難易度の高いタスクにおいても性能向上が報告されています。しかしながら、モデル混合時に質の低いモデルが含まれるとどうなるのかといった疑問は未解決のままです。

単なる多様性だけではなく、各モデルの基本性能自体が重要ではないかという議論も生まれています。

また、同一モデルを複数回実行して異なる回答を取得する方法も検討されつつありますが、どのような条件下でどちらの手法が優位性を持つかについては、まだ十分に解明されていません。

このような背景を踏まえ、今回研究グループは「異種のLLMを組み合わせる本質的価値はどの程度あるのか」という根本的な問いに立ち返り、単一モデルから得られる複数出力を統合するアプローチと比較する研究に着手しました。

以下で詳しく紹介します。

## 「LLMを組み合わせること」の有効性

LLMを複数組み合わせる手法は、「モデルの多様性を活かして性能を高める」と期待されてきました。そのため複数のLLMを混ぜる際には、バリエーションを豊かにする方針がとられがちです。

しかし、モデル間の品質差が顕著な場合には質の低い解答も混入してしまう懸念があります。そのため、多様性と各LLMの品質をいかに両立させるかが重要な課題となっています。

このような課題に対応するため、今回研究者らは従来のアプローチを見直した新たな手法を提案しています。複数LLMを組み合わせる従来手法「Mixed-MoA」に対して、「Self-MoA」と呼ばれる新たなアプローチです。

Self-MoAは、一つのLLMを用いて複数の回答（ [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") ）を得たのち、まとめ役となるLLMがそれらを統合する構成がとられます。

Mixed-MoAが複数種類のLLMを混在させる方式だったのに対し、Self-MoAは単独の優秀なLLMを繰り返し走らせる点が特徴です。

さらに、多数のサンプルを効率的に処理するための拡張手法として「Self-MoA-Seq」も提案されています。これは、Self-MoAの基本概念を発展させ、多くのサンプルを段階的・順次的に統合することで、LLMの入力コンテキスト長の制限を克服する方法です。

Self-MoAを普通に使おうとすると一度にすべてのサンプルを統合することになりますが、Self-MoA-Seqではサンプルを小グループに分けて段階的に統合することで、より多くのサンプルを効率的に処理できます。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86165_1-1024x335.png)

MoA、Self-MoA、Self-MoA-Seqの構成を示した図。単一モデルや複数モデルからの出力をどのように合成するかを視覚化した概要図

LLMが複数ある場合と比較すると、Self-MoAのアプローチではいわゆる「多様性」はやや限定的になるかもしれません。しかし、最初から品質の高いLLMだけを集中的に利用できることから、合成結果の全体的な水準が向上しやすいのではないかと期待されました。

### AlpacaEval 2.0を用いた汎用モデルの実験

AlpacaEval 2.0とは、実際に出力された文章がどれほど指示に従えているかを比較するベンチマークとして活用されています。人間の採点ではなく、多数のサンプルへの応答をGPT-4ベースの評価者が見比べる方式となっています。

\*AlpacaEval 2.0： [https://github.com/tatsu-lab/alpaca\_eval](https://github.com/tatsu-lab/alpaca_eval)

LLMによっては出力が長大なほど優勢になる傾向が見られたため、回答の長さを揃えながら優劣を判定できる指標（LC win rate）が導入されました。

実験においては、従来手法であるMixed-MoA（性能にばらつきがある複数のLLMを用いる方法）が比較対象として設定されました。自分自身の出力を複数回 [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") してから一つにまとめるSelf-MoAとあわせて、それぞれの評価結果が検証されています。

結果として、Self-MoAがMixed-MoAより高いスコアを示す事例が多く、強力なLLM一つを繰り返し利用して回答を統合したほうが平均的には有利と見なせるデータが出ています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86165_2.png)

AlpacaEval 2.0を用いたMixed-MoAとSelf-MoAの比較結果。単一モデルと複数モデルを組み合わせた際の勝率をまとめた表

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86165_3.png)

AlpacaEval 2.0の上位モデルを使ったSelf-MoAの評価一覧。モデル単体とアンサンブル時のスコア差分を整理した表

同じLLM内で多様な出力を得る「自己アンサンブル」によって、いくつものLLMをまとめるより好成績になる傾向が見出されたといえます。LLMの品質が高いほど、その出力を複数採用してまとめる手法の効率が向上しやすいという見解です。

LLM間の品質差が顕著であれば、多様性を追求するより、優れたLLMを何度も走らせる戦略のほうが望ましい場合があるようです。

しかしAlpacaEval 2.0ベンチマークテストは単なる指示追従タスクなので、上記はあくまでも一つの側面での分析結果です。

### 専門モデルを用いた複数データセットでの実験

一方で、さまざまな専門領域を含む評価データを用意して検証が実施されました。研究者らは、MMLU・CRUX・MATHという三つのベンチマークを選定しました。

MMLUは幅広い教養科目を網羅する選択式問題で、LLMの知識カバレッジが試されます。CRUXは短いコードをどこまで正しく理解し出力推定できるかが要求され、MATHは競技数学レベルの難しい問題を解けるかどうかが焦点となります。

実験では三種類のLLMが登場しました。Qwen2-7B-Instructは一般的な読解や知識問題への対応が得意だとされ、DeepSeek-Coder-V2-Lite-Instructはコード関係の処理に向いた特性を持ち、Qwen2-Math-7B-Instructは数学問題への強みを備えていると想定されました。

研究者らは、これらのLLMをいくつかの組み合わせで同時に動作させるMixed-MoAと、ひとつのLLMから複数の回答を [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") してまとめるSelf-MoAを比較しました。LLMの組み合わせとしては「i」「d」「m」のように略称が振られ、それぞれの性能比較が行われています。

Qwen2-7B-Instructがまとめ役（アグリゲータ）として機能する設定では、単独で性能が高いLLMの回答だけを集めるSelf-MoAが、Mixed-MoAを上回る成績を示す傾向が確認されました。複数LLMが混在している場合に多様性は高まるものの、品質の低い回答も混入しがちで、最終的な合成結果がかえって劣る場面が生じたようです。

こうした状況から「強いLLM一つを繰り返し走らせたほうが有効なのでは」という見解がさらに裏付けられます。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86165_4.png)

MMLU、CRUX、MATHでMixed-MoAとSelf-MoAを検証した結果表。専門特化モデルの組み合わせと単独モデルの性能差を示す一覧

ただし、あらゆる場面でSelf-MoAが常に有利というわけではありません。各LLMが明確な専門性を有し、それぞれの得意分野だけが明らかに異なるなら、混合によるメリットが生まれる例も見出されます。

コードだけを対象とする場合や数学問題だけを解かせる場合といった単一領域タスクでは、品質に勝るLLMを集中的に活用したほうが統合結果が最も高い精度につながりやすいと見受けられます。何を解かせるかが多方面に及ぶ場合には、Mixed-MoAの強みが発揮される可能性が示唆されています。

実験全般を通じた結論としては、まずは品質の高いLLM一種類を用いた戦略（Self-MoA）が、あまり性能の良くないLLMを混ぜるより望ましいという傾向が読み取れました。一方、タスクが複数にわたる複雑な設定では、条件次第でMixed-MoAにも利点が生じる可能性があるという知見が得られました。

## 品質と多様性のトレードオフ

多数のLLMを組み合わせる際には「多様性」を高めるほど有利だと従来から指摘されてきました。しかしながら、品質に差があるLLMを無計画に加えると、かえって全体の性能が低下しかねない可能性も考慮する必要があります。

そこで実験を通じて、Self-MoAのような方法が高性能を示す理由を明らかにするために、LLM同士の「多様性」と各LLMの「品質」がどのように相互作用するかが検討されました。Vendi Scoreという数値指標が活用され、回答の似通い具合やばらつきを定量化する仕組みが導入されています。

さらに、LLMの [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") 温度を変更したり、異なる種類のLLMを取り入れたりして、どの程度多様性が増せるかが段階的に検証されました。その結果、多様性と品質の間に明確なトレードオフ構造が見出されたと報告されています。

多様性を増やす取り組みでは、幅広い回答の候補を得られる恩恵が見られた一方で、性能が低めのLLMが含まれるとアンサンブル全体にマイナスの影響が波及しやすい様子が示唆されました。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86165_5-1024x257.png)

多様性と品質のトレードオフを可視化した図。Mixed-MoAとSelf-MoAを比較しながら、それらの性能がどのように変動するかを示すグラフ

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86165_6.png)

MoAの性能と多様性・品質との相関関係を示す 線形回帰 の係数表。両方の要素がMoAに与える影響度を数値で示す結果

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86165_7.png)

品質の測定方法を変化させた際の回帰分析。Centered-1/K-Normなどを用いてR²値がどう動くかを比較した表

### 統計分析

最終的な答えの精度がどれほど多様性と各モデルの品質の影響を受けるかを調べるため、 [線形回帰](https://ai-data-base.com/archives/26362 "線形回帰") が適用されました。MoAが出す最終的な結果を **t** とし、質を示す値を **q** 、多様性を表す値を **d** とおいて、

t=α×q+β×d+γ

の関係式が当てはめられています。

結果として、p値がきわめて小さい水準で両変数との相関が示されるとともに、回帰モデルの決定係数 R² は全体的に 0.7 前後の値を示したため、一定の説明力があると評価されました。

そして、質に対応する係数が多様性に対応する係数を上回る事例が多く観測されました。つまり、MoAが最終的に導く回答の優秀さは、多様性の度合いにも助けられる一方で、個々のLLMがどの程度正確であるかという要素のほうが強く作用している可能性がうかがえました。

複数のLLMを混在させる方式が魅力的に映る側面がありながらも、質を満たすLLMを厳選し、そのうえで繰り返し回答を取る方法が有効となることが示唆されたといえます。

なお、質をどう計算するかについても試行錯誤が行われ、質の高いLLMが含まれるほどMoAの出力が底上げされる傾向がよりはっきりと数値で捉えられました。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86165_8.png)

MMLU、CRUX、MATHを混合したタスク設定でMixed-MoAとSelf-MoAを比較した結果表。複数のサブタスクをまとめた場合の平均精度を示す一覧

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86165_9.png)

Llama-3.1-8B-InstructとQwen2-7B-Instructを組み合わせたMoAのMMLU評価比較。類似性能のモデル同士を混在させた場合の精度差を示す表

### Mixed-MoAがSelf-MoAを上回る条件

ただし、もう一つ重要な観点があります。それは、優秀なモデルだけを集めて組み合わせる場合はどうなるのか？という検証課題です。

実験の結果、複数のLLMをまとめるうえで、品質が互いに近いLLMがそろっているならば、Mixed-MoAがSelf-MoAを上回る場合があると示唆されました。例えば、別々のLLMがそれぞれ得意とするサブタスクを含む複合的なタスクを設定し、どのように組み合わせると高い精度で解けるかを検証したところ、似た性能レベルを維持するLLM同士をうまく混在させることで、一つのLLMを繰り返し走らせる方式を若干上回る例が示されました。

ただし、その差はきわめて僅差にとどまった場面が大半でした。

とはいえ、LLMが多様な方面にそれぞれ強みを持ち、しかも大きく劣らない組み合わせをそろえられるならば、Mixed-MoAも有用であると見なせそうです。

## 推論計算リソースとSelf-MoAのスケーリング効果

Self-MoAが単一の優秀なLLMを基盤とする手法でありながら、より多くの計算資源を投下すればさらなる改善が期待できるのかが検討されました。LLMを多数回 [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") すれば、出力の幅が増える恩恵が見込まれます。

しかしながら、あまりに数を増やしすぎれば、多様性そのものは頭打ちになるおそれがあり、まとめ役のLLMが扱う文脈も膨大になりすぎて混乱が生じる可能性がありました。加えて、LLMには入力コンテキスト長の制限が存在することも考慮されました。

そこでSelf-MoA-Seqという拡張手法が提案され、より多くのサンプルを段階的に統合できる仕組みが導入されています。各ステップで一度に処理するサンプル数を抑えることで、コンテキスト長を超えない範囲で合成を繰り返す方式がとられました。この順序的な合成によって、サンプルをどれほど増やしても、一度にすべてを読み込む必要はなくなります。

実験として、MMLUやCRUXのベンチマークでサンプルを6から30程度に増やし、Self-MoAとSelf-MoA-Seqの双方で正答率の変化が観察されました。結果として、いずれの手法も単一LLMの単発推論を上回る性能向上が見られています。

ただし、サンプル数を増やすことでメリットが得られる場面がある一方、増やしすぎると出力内容が乱れてしまうこともあると報告されました。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86165_10.png)

Self-MoAとSelf-MoA-Seqをサンプル数拡大時に比較した図。単一モデルの単発推論（点線）との精度差を可視化したグラフ

Self-MoA-Seqは多数のサンプルを扱う場合でも比較的安定して成果を示したため、推論時に利用できる計算資源やコンテキスト長が限られる環境でも有効性が維持される可能性が示唆されました。

なお、その過程で別のLLMを加える実験も実施されました。類似の性能を持つLLMを追加すると精度が増すものの、明らかに性能が低いLLMでは逆効果になりうる傾向がみられています。

## まとめ

本記事では、LLMを複数組み合わせる手法を再考した研究を紹介しました。複数のLLMを混在させるよりも、性能の高いLLMを単独で何度も [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") する方式の有用性が明らかになりました。

一方、LLMの性能差が小さい場合や専門性の異なる課題を組み合わせる場合には、複数LLMの混合にメリットがあるとの示唆も得られました。

また、推論時にサンプル数を増やして合成する際の効率を高める試みとして、新たに提案されたSelf-MoA-Seqが検討され、限られたコンテキスト長でも拡張が可能と示されています。

研究者らは、適切なLLM選択と多様性の活用が今後のLLMの性能向上に重要な鍵となると見解を述べています。

**参照文献情報**

- タイトル：Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?
- URL： [https://doi.org/10.48550/arXiv.2502.00674](https://doi.org/10.48550/arXiv.2502.00674)
- 著者：Wenzhe Li, Yong Lin, Mengzhou Xia, Chi Jin
- 所属：Princeton University

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMにキャラクターの話し方だけでなく「キャラ独自の内面の思考プロセス」も模倣させる手法](https://ai-data-base.com/archives/86025)

[NTTの最新決算から読み取る技術投資動向と人材需要](https://ai-data-base.com/archives/86343)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)