---
title: "直感に頼るようなタスクだとLLMに「ステップバイステップで考えて」は逆効果"
source: "https://ai-data-base.com/archives/78145"
author:
  - "[[AIDB Research]]"
published: 2024-11-07
created: 2025-06-13
description: "本記事では、LLMの性能向上テクニック「Chain of Thought（CoT）」が逆効果になるケースについての研究を紹介します。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、LLMの性能向上テクニック「Chain of Thought（CoT）」が逆効果になるケースについての研究を紹介します。

これまでCoTは多くの課題でLLMの性能を向上させる手法として知られてきましたが、人間の認知研究から「考えすぎると失敗するケース」があることにヒントを得た研究者たちが、LLMでも同様の現象が起きるのではないかと考えて検証を行いました。

「ステップバイステップで考えて」があまり意味がないタスクもあるという話は以前にもありましたが、本件では逆効果まで確認されています。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78145-1024x576.jpg)

**参照論文情報**

- タイトル：Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse
- 著者：Ryan Liu, Jiayi Geng, Addison J. Wu, Ilia Sucholutsky, Tania Lombrozo, Thomas L. Griffiths
- 所属：Princeton University, New York University

**本記事の関連研究**

- [計画のステップが増えるほど、LLMは最初の目標を見失っていく傾向がある](https://ai-data-base.com/archives/77302)
- [CoTの推論ステップ数がLLMの推論能力に及ぼす影響を詳細に検証した結果](https://ai-data-base.com/archives/62364)
- [CoT（思考の連鎖）は数学や論理で劇的に性能を向上させる一方、常識や知識のタスクでほとんど効果がない](https://ai-data-base.com/archives/75942)

## 背景

LLMの性能を向上させる手法として、「思考の連鎖」(Chain of Thought、以下CoT)というテクニックが広く使われています。モデルに「ステップバイステップで考えて」と指示したり、回答の過程で考え方を説明してもらったりと段階的な推論を行わせる方法です。

CoTを使うと、いくつかの課題でモデルの性能が向上することが分かっています。多くの場合は数学的な問題や論理的な推論を必要とする課題で効果を発揮します。また、最新のLLMでは標準的な機能としてCoTが組み込まれている場合もあります。

しかし、今回研究者たちは「CoTが性能を低下させるケースもあるのではないか」という疑問を持ちました。というのも、人間の場合、「考えすぎること」が逆効果になる場面があることが心理学の研究などで分かっているからです。

例えば、以下のような場合に人間は「考えすぎると」むしろパフォーマンスが下がることが知られています。

- 暗黙のうちに学んだパターンを認識する時
- 顔を記憶して認識する時
- 例外を含むデータから規則性を見つける時

そこで研究チームは「人間が考えすぎると失敗するようなタスクで、LLMもCoTを使うと失敗するかどうか」という課題を立てました。

この課題を検証するため、心理学の研究で使われた様々なタスクを大規模に再現し、最新のLLMで実験を行いました。

実験アプローチや結果の詳細を以下にまとめます。

## 思考を”言語で行う”ことの影響

心理学の研究分野において、言語的思考（「熟考」とも呼ばれます）が記憶や学習、判断、意思決定にどのような影響を与えるかが調査されてきました。多くの場合、言語的思考はプラスの効果をもたらすことが確認されています。例えば、より時間をかけて熟考する人は、直感的だが誤った反応を引き起こすような質問に対して、より正確に答えられる傾向が示されています。

しかしながら、じっくり考えすぎることで、かえって成績が下がってしまう場合があることも分かってきました。具体例を見てみましょう。

### パフォーマンス低下の事例

#### 暗黙的な統計学習での影響

まず、暗黙的な統計学習の実験が挙げられます。例えば、ある規則に従って作られた文字や音の並びが被験者に示されるとします。被験者は、その規則に合っている並びと、合っていない並びを見分けることができます。しかし、「なぜそう判断したのか」を言葉で説明することはできません。さらに、「規則を言葉で見つけ出してください」と指示されると、かえって判断の正確さが落ちてしまうことが分かっています。

#### 言語による過剰な影響

「言語による過剰な影響」という現象も見つかっています。「見た人の顔を言葉で説明するように指示された」被験者と「説明を求められなかった」被験者を比較すると、顔を言葉で説明した人の方が後でその顔を正確に思い出すことができなくなるといった実験結果が報告されています。

#### パターン認識での影響

また、観察したことについて「なぜそうなるのか」という説明を求められた被験者は、広くて単純なパターンを見つけやすくなることも分かっています。しかし、そのパターンに例外が含まれるように実験が設計された場合、説明を求められた被験者の方が学習に時間がかかり、より多くの間違いを起こしてしまいました。これは、言葉で説明しようとすることで、かえって例外に気づきにくくなってしまうためだと考えられています。

以上のように、人間の言語には得意・不得意があることが分かっています。言語は、細かな見た目の違いを表現することが苦手です。一方で、論理的な関係や、シンプルで広く当てはまるパターンは表現しやすいです。ただし、複雑な規則や、例外を含むパターンを表現するのは不得意であることが示されています。

## 実験

心理学の研究から、人間の言語的思考がパフォーマンスを低下させる6つの代表的なタスクが選ばれ、それぞれ大規模な実験が実施されました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78145_1-1024x609.jpg)

CoTの性能低下を評価するための6つのタスク（暗黙的統計学習、顔認識、例外を含む分類、自然言語推論、空間直感、ワーキングメモリ）の概要図

### （１）「暗黙的な統計学習」実験

ある規則に従って作られた文字列を分類する課題が用意されました。4,400個の分類問題が作成され、各問題には15個の学習用例と、正しい例22個、間違った例22個が含まれていました。

実験の結果、CoTを使用したLLMは、使用しなかった場合と比べて大幅に性能が低下することが分かりました。例えば、OpenAI o1-previewでは、精度が36.3%も低下しました。他のモデルでも同様の傾向が見られ、CoTを使用すると性能が低下することが確認されました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78145_2.png)

GPT-4oからLlama 3.1までの9つのモデルで、CoTを使用すると最大36.3%の性能低下が見られることを示している

### （２）「顔認識」実験

次に、顔認識の実験が実施されました。500個の問題が作成され、それぞれに2,500個の異なる顔画像が使用されました。LLMには1枚の顔写真が示され、5枚の候補写真の中から同一人物の写真を選ぶよう求められました。

この実験でも、CoTを使用したLLMの性能は低下しました。例えば、GPT-4oでは精度が12.8%低下し、Claude 3 Opusでは14.4%の低下が見られました。他のモデルでも同様の傾向が確認されました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78145_3-1024x237.png)

GPT-4oからInternVL2までの6つのモデルで、CoTによって相対的に3%から34.78%の性能低下が見られることを示している

### （３）「例外を含むデータの分類」実験

3つ目の実験では、例外を含むデータの分類課題が用意されました。

LLMは、車両の特徴に基づいて分類を行うよう求められました。データには一見パターンがあるように見えますが、実際には例外が含まれています。

実験の結果、CoTを使用したモデルは、正しい分類を学習するのに多くの時間を要することが分かりました。GPT-4oでは、CoTを使用しない場合と比べて4倍以上の時間が必要でした。Claude 3.5 SonnetとClaude 3 Opusでも、2倍以上の時間を要しました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78145_4-1024x196.png)

GPT-4oやClaude 3.5 Sonnetなどで、CoTを使用すると学習に必要な試行回数が129%から331%増加することを示している

### （４）「論理的な矛盾を説明する」実験

次の実験では、2つの文が論理的に矛盾しているかどうかを判断する課題が用意されました。例えば「もしAならば必ずBである」という文と、「Aだが、Bではない」という文が矛盾しているかを判断します。

人間の場合、矛盾する文について「なぜこれらの文が共存できるか」と説明を求められると、かえって矛盾を見抜けなくなることが知られています。しかし、LLMの場合は少し状況が異なりました。CoTを使用しない場合、ほとんどのモデルは偶然の確率（50%）程度の正答率しか得られませんでした。つまり、そもそもの基本性能が低すぎたため、CoTによって性能が下がるかどうかを測ることができなかったのです。

実際、GPT-4oではCoTを使うことで正答率が40%以上も向上しました。これは、論理的な推論を必要とするタスクでは、CoTが役立つ可能性を示しています。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78145_5-1024x271.png)

様々なデータセットで、モデルごとのゼロショットとCoTの性能を比較。多くの場合、CoTによって性能が向上することを示している

### （５）「空間的な直感を必要とする」実験

この実験では、2つのグラスが用意されました。1つには水が入っており、もう1つは空です。両方のグラスを同じ角度で傾けたとき、水面が縁に同時に到達するためには、空のグラスにどれだけの水を入れるべきかを判断する課題です。

人間の場合、物を動かすイメージ（運動シミュレーション）を使う方が、言葉で考えるより正確な判断ができます。しかし、LLMには人間のような運動感覚の経験がありません。そのため、CoTを使用しても使用しなくても、性能に大きな違いは見られませんでした。LLMが人間とは異なる方法でこの問題に取り組んでいることを示唆しています。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78145_6-1024x230.png)

5つのモデルで、CoTの使用による有意な性能変化が見られないことを示している

### （６）「複数の特徴を集約して判断する」実験

最後に、4つの部屋の特徴について多くの情報（例：家賃、広さ、日当たり、騒音レベルなど）が与えられ、最も望ましい部屋を選ぶ課題が用意されました。

人間の場合、情報が多すぎると全てを記憶することができません。そのため、じっくり考えすぎると、かえって判断が悪くなることが知られています。一方、LLMには人間のような記憶の限界がありません。文脈の中にある全ての情報に同時にアクセスすることができます。

そのため、LLMはCoTを使うことで、むしろ性能が向上しました。これは、大量の情報を同時に処理できるAIモデルの特徴を活かした結果だと考えられています。例えば、Claude 3.5 Sonnetでは、CoTを使用することで正答率が最大で95%まで向上しました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78145_7.png)

4つのモデルで、選択の難しさ（Δ）の3つの範囲における性能を比較。多くの場合でCoTが性能を向上させることを示している

### 実験結果から見えてきたこと

#### CoTでパフォーマンスが低下する3つのケース

最初の3つの実験（暗黙的な統計学習、顔認識、例外を含むデータの分類）では、CoTを使用することで一貫してLLMの性能が低下することが確認されました。これらの実験に共通する特徴として以下が挙げられます。

1. 言語化することで本来の直感的な判断が妨げられる性質のタスク
2. 人間とLLMの両方に共通する制約（言語表現の限界など）が影響している
3. CoTを使用しない場合でも、ある程度の基本性能が出せるタスク

例えば、暗黙的な統計学習では最大で36.3%、顔認識では最大で14.4%もの性能低下が見られました。また、例外を含むデータの分類では、学習に必要な時間が最大で4倍以上に増加しました。

#### CoTでパフォーマンスが維持・向上した3つのケース

一方、後半の3つの実験（論理的矛盾の説明、空間的直感、複数特徴の集約）では、CoTの使用が必ずしも性能低下につながりませんでした。人間とLLMの能力の以下のような根本的な違いが結果に大きく影響しています。

1. そもそもの基本性能が低いときにはCoTで性能が向上する場合もある
2. 人間の身体的経験に基づく判断が必要なときはCoTの影響が判別不可
3. LLMには記憶の限界がないためCoTでさらに性能があるケースもある

### 重要な示唆

実験結果から、以下のような示唆が得られています。

LLMの性能を評価する際には、タスクの性質と人間との共通点・相違点を慎重に考慮する必要があります。

またCoTが有効かどうかは、タスクの性質によって大きく異なります。「直感的な判断が重要な場合」「言語化が困難な特徴を扱う場合」「例外的なパターンの学習が必要な場合」にCoTが逆効果となる可能性が高いことが分かりました。

さらに、LLMと人間の根本的な違い（記憶容量や身体的経験の有無など）が、タスクの性能に大きな影響を与えることが確認されました。

## まとめ

本記事では、LLMの推論方法の一種であるCoTが、どのような場合にパフォーマンスを低下させるのかを検証した研究を紹介しました。研究チームは、「言語的思考が人間のパフォーマンスを低下させる場合」という心理学の知見に着目し、それがLLMにも当てはまるかを検証しました。実験の結果、以下の3つのケースでは、CoTの使用によってLLMの性能が大きく低下することが確認されました。

- 規則性を暗黙的に学習する必要がある場合
- 顔のような視覚的な情報を認識する場合
- 例外を含むデータを学習する場合

一方で、論理的な矛盾の説明や、空間的な直感、複数の特徴を総合的に判断するような課題では、CoTの使用による性能低下は見られませんでした。それは記憶容量や身体的な経験といった点で、人間とLLMの能力に根本的な違いがあるためと考えられています。

今回の研究は、CoTがやはり必ずしも万能ではなく、タスクの性質によって使い分けが必要であることを示唆しています。また、心理学の知見をLLMの研究に活用できる可能性も示されました。

ただし、今回の方法では発見できなかったCoTの失敗ケースが存在する可能性があり、さらなる研究が必要とされています。

- 参照論文URL： [https://arxiv.org/abs/2410.21333](https://arxiv.org/abs/2410.21333)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMの「知っているのに嘘をつく」幻覚と「知らないから間違える」幻覚の違い](https://ai-data-base.com/archives/78047)

[LLMの機能別「領域」はまるで脳のようであるとの仮説](https://ai-data-base.com/archives/78200)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)