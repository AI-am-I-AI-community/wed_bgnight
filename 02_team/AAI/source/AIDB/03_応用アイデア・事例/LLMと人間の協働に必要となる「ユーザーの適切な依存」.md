---
title: "LLMと人間の協働に必要となる「ユーザーの適切な依存」"
source: "https://ai-data-base.com/archives/81239"
author:
  - "[[AIDB Research]]"
published: 2024-12-26
created: 2025-06-13
description: "本記事では、大規模言語モデル（LLM）と人間の適切な協働関係を探求している研究を紹介します。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、大規模言語モデル（LLM）と人間の適切な協働関係を探求している研究を紹介します。

LLMは膨大な知識を持ち、自然言語でユーザーとコミュニケーションを取る能力を備えていますが、ユーザーがその出力に過度に依存したり、逆に有益な助言を見逃したりするという課題が指摘されています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81239-1024x576.jpg)

実験手順と課題に関する図表。(a)実験全体の流れを示す図と(b)LSATの論理的推論問題の例と4つの条件でのアドバイス例を示す。

**発表者情報**

- 研究者：Jessica Y. Bo et al.
- 研究機関：To Rely or Not to Rely? Evaluating Interventions for Appropriate Reliance on Large Language Models

## 背景

LLMの発展により、人間の意思決定プロセスにおけるLLMの役割は日々重要度を増しています。LLMは膨大な知識を持ち、自然言語でユーザーとコミュニケーションを取る能力を備えています。意思決定の質と効率を向上させる可能性を秘めているものの、LLMが誤った情報を自信に満ちた口調で提示した場合、ユーザーは過度に依存してしまう危険性が指摘されています。一方で、LLMに対する不信感から有益な助言を見逃してしまうケースも報告されています。

「適切な依存」は、技術開発の長い歴史の中で常に議論されてきた課題でした。LLMに対する適切な依存とは、「LLMが正しい場合はその助言に従い、誤りがある場合は自身の判断に従う」状態を指します。依存度に影響を与える要因としては、ユーザーの専門知識レベル、タスクへの認知的関与度、LLMが提示する情報の性質などが挙げられます。

最近の研究では、ライティング、コミュニケーション、コーディング、教育など、様々な分野におけるLLMの活用可能性が探究されています。多くの場合、人間とLLMがチームとして協力することで、個別に作業する場合よりも優れた成果を上げられることが明らかになっています。

同時に、研究者らはLLMの透明性を高め、その能力や信頼性をユーザーに適切に伝える方法を模索してきました。例えば不確実性のハイライト表示や、対照的な説明の提示、信頼スコアの導入など、様々なアプローチが試みられています。しかし、複雑な手法は、ユーザーにとって認知的負担が大きすぎるという課題も存在します。

そのような状況の中、研究者らは以下の3つの異なるアプローチを用いて、LLMとの適切な関わりを促す研究に取り組みました。

## LLMへの依存度を評価する3つの介入手法

### 手法１：「免責事項」

LLMの出力に常に表示される固定の警告文です。

「この情報は必ず確認してください。文脈によっては完全な情報を持っていない可能性があります」といった注意書きが、ユーザーの判断を促すために付加されます。

チャットボットのインターフェースでよく見られる手法を模したものです。

### 手法２：「不確実性の強調」

こちらでは、より視覚的なアプローチを取ります。

LLMが出力する文章の中で、モデルの確信度が低い部分を赤色で、やや確信度が低い部分をピンク色でハイライトします。例えば「その装置は性能が優れている」という文において、「優れている」という評価部分の確信度が低ければ赤く表示されます。

こうすることで、ユーザーはLLMの回答のどの部分に注意を払うべきかを直感的に理解できます。

### 手法３：「暗黙的な回答」

最も間接的なアプローチです。

LLMは直接的な結論や答えを示さず、代わりにその結論に至る推論過程や計算手順を詳しく説明します。

数値計算であれば途中式まで示し、論理問題であれば各選択肢についての分析を提示しますが、最終的な判断はユーザーに委ねられます。

ユーザーは提示された情報を自分で解釈し、結論を導き出す必要があります。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81239_1-1024x304.png)

## 研究アプローチ

研究者らは、GPT-4oの出力に適用される3つの依存介入の有効性を評価するため、大規模な実験を設計しました。

### タスクの設計

実験に用いられた質問は、LSAT論理的推論と画像ベースの数値推定という2つのタスクから構成されました。タスクは一般の人々が取り組めるよう、専門知識を必要としない設計が採用されました。また、正答を得るために意味のある認知的努力が必要となるよう、十分な難易度が設定されました。さらに、検索エンジンなどの外部支援を使用した不正行為が困難になるよう工夫が施されました。

#### タスク１：LSAT論理的推論

研究者らによって事前に実施された予備実験では、論理的推論の質問が難易度とアクセスのバランスを最も適切に保っていることが明らかになりました。そのため、多肢選択式タスクとしてLSAT論理的推論の質問が採用されました。

LSAT（Law School Admission Test）とは、米国やカナダのロースクール入学希望者が受験する標準化テストの一部です。その中の論理的推論セクションは、文章を読んで正しい論理的な結論を導く問題です。

研究者らは「難しい」とされる2つの多肢選択式LSAT論理的推論の質問を用意し、各質問に5つの選択肢を設定しました。

#### タスク２：数値推定

研究者らは、数値推定タスクを導入することで新たな研究視点が得られると考えました。

採用されたのは、ガラス瓶の中の豆の量を推定する課題でした。研究者らは2種類のガラス瓶を用意しました。小さな瓶には403個のひよこ豆が、大きな瓶には856個のジェリービーンズが入れられました。

### 手順

実験参加者は4つの条件のいずれかにランダムに割り当てられ、LSATタスクから2問と数値推定タスクから2問に回答することが求められました。手順には2つの重要な要素が組み込まれました。まず、参加者は生成されたアドバイスを見る前に各質問に自分で回答する必要がありました。次に、実験の現実性を高めるため、正解報酬として参加報酬の最大20%に相当するボーナスが設定されました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81239_3-1024x303.png)

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81239_2-1024x749.png)

実験手順と課題に関する図表。(a)実験全体の流れを示す図と(b)LSATの論理的推論問題の例と4つの条件でのアドバイス例を示す。

### 測定

研究者らは依存行動と信頼のキャリブレーションに焦点を当てました。タスクのパフォーマンスと関与時間も測定されました。

#### 測定対象１：依存行動

参加者がアドバイスを受ける前と後に回答する2段階の質問応答形式が採用されました。研究者らは、参加者の最初の回答がアドバイスと異なるデータのみを分析対象としました。依存の結果は、Positive LLM Reliance、Positive Self-Reliance、Negative LLM Reliance、Negative Self-Relianceの4つのカテゴリに分類されました。

（１）Positive LLM Reliance  
LLMの助言を採用し、それが実際に正しい判断につながった場合。つまり、LLMを信頼して良い結果が得られた状態。

（２）Positive Self-Reliance  
LLMの助言を採用せず自分の判断を維持し、それが正しかった場合。つまり、LLMの誤った助言を適切に棄却できた状態。

（３）Negative LLM Reliance  
LLMの助言を採用したが、それが誤った判断につながった場合。つまり、LLMを過度に信頼してしまった状態。

（４）Negative Self-Reliance  
LLMの正しい助言があったにもかかわらず自分の判断を維持し、それが誤りだった場合。つまり、有用な助言を見逃してしまった状態。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81239_2_1-1024x590.png)

初期回答とLLMの助言に基づく依存度の経路を示す図。適切な依存、過度な依存、過小な依存の分類を含む。

#### 測定対象２：信頼と主観的認識

参加者の自信は、1（非常に不確か）から5（非常に確か）までのリッカート尺度で測定されました。研究者らは最初の段階から2番目の段階までの自信の変化を分析しました。実験の最後に、参加者は追加の主観的評価も行いました。

#### 測定対象３：タスクのパフォーマンス

研究者らは、参加者が独立して質問に回答した際の初期誤差と、アドバイスを受けて回答し直した後の誤差を比較しました。また、最良のパフォーマンス指標も計算されました。

#### 測定対象４：関与時間

質問応答の各段階に費やされた時間が測定され、アドバイスへの関与時間と最初の質問への回答時間の比率も算出されました。

## 実験と結果

LSATと数値推定という2つの質問回答タスクにおいて、3つの介入の効果を比較した実験結果です。

### 参加者

研究者らは、Prolificから400人のクラウドワーカーを募集しました。参加者の選抜条件として、アメリカ在住であること、英語に堪能であること、タスク成功率が98％以上であること、50以上のタスクを完了していることが設定されました。注意喚起チェックの結果により、Control条件から1人、Uncertainty Highlighting条件から2人が除外されました。実験の完了までにかかった時間は中央値で9分44秒でした。

### タスクパフォーマンス

研究者らは、良いアドバイスと悪いアドバイスを同率で [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") することで、人々の反応パターンを調査しました。

LSATタスクでは、参加者の最初の [正解率](https://ai-data-base.com/archives/25930 "正解率") は36.0％にとどまり、ランダムな推測での正解率である20％をわずかに上回る程度でした。生成されたアドバイスの正解率は50％に設定されました。アドバイスを受けた後の2段階目における正解率は46.1％まで向上しました。最適なパフォーマンスでは62.1％の正解率が期待されましたが、実際の成績はそれを下回る結果となりました。  
これは要するに、参加者は問題を最初に解いた時はほとんど正解できなかったということです。LLMの助言を受けた後は少し成績が良くなりましたが、理想的な成績からはかなり離れていました。つまり、LLMの助言をうまく活用できていなかったということです。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81239_2_2-1024x225.jpg)

LSAT課題での初期回答とLLM支援後の回答の遷移を条件ごとに示す性能結果。

数値推定タスクでは、正解に対する [平均絶対誤差](https://ai-data-base.com/archives/26526 "平均絶対誤差（MAE）") 率は全条件で45.7％となりました。アドバイスを受けた後も、参加者の成績に有意な改善は見られず、47.0％という同様の誤差率が観察されました。一方で、平均誤差率に着目すると、参加者は当初、数を-20.2％過小評価する傾向があり、アドバイスを受けた後は-5.6％まで改善されました。人間とLLMのチームが最適な状態で協働した場合、 [平均絶対誤差](https://ai-data-base.com/archives/26526 "平均絶対誤差（MAE）") は29.0％まで低下することが予測されました。  
ということで、豆の数を当てる問題では、LLMの助言を受けても全体的な正確さは改善されませんでした。ただし、最初は実際の数より少なく見積もる傾向があったのが、助言後はその偏りが減少しました。つまり、完全な正解には至らなくても、より現実的な推定ができるようになったということです。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81239_2_3-1024x493.jpg)

数値推定課題での性能結果。(a)初期推定とLLM支援後の推定の誤差分布、(b)助言の重み付け分布を示す。

### 依存行動

まず研究者らは、参加者の最初の回答がアドバイスと一致するデータを分析から除外しました。LSATタスクでは87.1％、数値推定では88.8％のデータが分析対象として保持されました。

その結果、LSATタスクにおいて、Reliance Disclaimerは過度の依存と過小な依存の両方を減少させ、適切な依存比率に有意な正の影響を与えました。Uncertainty HighlightingとImplicit Answerは自己依存を有意に向上させましたが、適切な依存機会においてLLMへの依存が減少するというトレードオフが生じました。  
単純な警告文（Reliance Disclaimer）が最も効果的だったということです。他の2つの手法は、参加者が自分の判断を信じるようになりましたが、その代わりLLMの正しい助言まで無視してしまう傾向が出てしまいました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81239_2_4-1024x365.jpg)

適切な依存度の測定結果の視覚化。(a)依存度の結果の参照図、(b)LSAT課題の結果、(c)数値推定課題の結果を示す。

数値推定タスクでは、アドバイスの重みに基づいて3段階の閾値（0.01、0.5、0.99）で依存行動が分析されました。LSATタスクとは異なり、適切な依存を効果的に向上させる介入は見られませんでした。Implicit Answerは適切な自己依存を最も増加させましたが、同時に過小な依存も増加させる結果となりました。  
要するに、どの介入方法も決定的な改善をもたらしませんでした。その中でも、答えを直接示さない方法（Implicit Answer）は、参加者が自分の判断を信じすぎる結果となり、LLMの有用な助言も活用されにくくなってしまいました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81239_2_5-1024x434.png)

### 信頼度のキャリブレーションと認識

LSATタスクにおいて、全条件で参加者は適切な依存決定を行うよりも、過度または過小な依存をした際に自信過剰になる傾向が観察されました。Reliance Disclaimerのみが、適切な依存と過度な依存の間で信頼度の差を適切に調整することに成功しました。  
要するに、参加者は間違った判断をしているときの方が、むしろ自信を持ってしまう傾向がありました。警告文（Reliance Disclaimer）だけが、この問題を改善できました。つまり、警告文があると、参加者は自分の判断の確かさを適切に評価できるようになったということです。

数値推定タスクでは、Implicit Answerだけが正しい依存結果において信頼度の変化を有意に高く調整することができました。実験後の主観的評価では、Control条件が他の全ての介入と比較して最高のスコアを獲得しました。  
これは、答えを直接示さない方法（Implicit Answer）だけが、参加者の自信の度合いを適切にコントロールできたことを意味します。ただし、参加者の主観的な評価では、何も介入しない通常の方法（Control）が一番好まれました。これは、介入方法が使いやすさを損なっていた可能性を示唆しています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81239_2_6-1024x436.png)

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81239_2_7-1024x428.png)

質問レベルでの確信度の変化を示すグラフ。正しい依存と誤った依存、LLMへの適切な依存と過度な依存を比較。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81239_2_8-1024x802.png)

実験後の知覚評価の評価結果を示すグラフ。3つの介入手法でのLLMの有用性等の評価を比較。

### 関与時間

研究者らは、アドバイスへの関与時間を1段階目の回答時間との比率で分析しました。LSATタスクと数値推定タスクの両方において、アドバイスに費やされた相対時間は、回答を抽出するために必要な労力とほぼ一致する結果となりました。Reliance DisclaimerはControl条件に最も近い時間比率を示し、Implicit Answerは数値推定で約2倍の時間を要しました。関与時間の長さは必ずしも適切な依存結果につながらず、むしろ理解しやすい簡潔な介入が最も効果的であることが明らかになりました。

つまり、参加者がLLMの助言を理解するのにかかる時間は、その助言の複雑さに比例していました。警告文は最も時間がかからず、答えを直接示さない方法は倍の時間がかかりました。時間をかければよい判断ができるわけではなく、むしろシンプルで分かりやすい介入の方が効果的だということが分かりました。これは実用化の観点からも重要な発見だと言えます。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81239_2_9-1024x444.png)

LLMの助言を受けた段階（2段階目）と初期回答段階（1段階目）の所要時間の比率を示す図。LSATタスクと数値推定タスクでは、y軸のスケールが異なることに注意が必要。

## 考察

研究から分かったことと、その意味について説明します。

### 主な発見

LLMの助言に対する人間の過度な依存は、いくつかの方法で減らすことができました。例えば、不確かな部分を目立たせたり、警告文を付けたり、直接的な答えを示さないようにしたりする方法です。しかし、LLMの助言を適切に活用してもらうのは難しかったことも分かりました。

LSATの問題では、単純な警告文が最も効果的でした。一方、数字を推定する問題では、答えを直接示さない方法は手間がかかりすぎて、参加者が助言を使わなくなってしまいました。また、不確かな部分を強調する方法は、参加者が助言を完全に信頼することをためらわせる結果となりました。

参加者の自信についても調べました。本来なら、間違った判断をした時は自信が下がるはずですが、実際にはむしろ自信が高まってしまうという予想外の結果が出ました。

### 広い視点から見えてきたこと

AIの技術は急速に発展していますが、それを使う人々の理解が追いついていない可能性があります。実験からは、人々がしばしばLLMの助言を誤って使用し、しかもその誤りに気付いていないことが分かりました。

この研究で使った評価方法は、プログラミングや文章作成、旅行の計画など、正解が明確な様々な場面で使えそうです。ただし、より自由な発想が必要な場面では、別の評価方法を考える必要があります。

また、LLMをどう使うかは、場面によって優先すべきことが変わります。医療や法律では誤りを防ぐことが最も重要です。創造的な作業では個性が失われないことが、教育では学習者の能力開発が妨げられないことが大切です。

### 限界

この研究にも限界があります。オンラインで募集した参加者による実験なので、現実の場面とは違いがあります。また、実験で使った選択式の問題は、実際の使用場面とは性質が異なります。参加者は追加の質問ができず、他の情報源も使えないなど、実際の使用環境とは異なる制限がありました。また、不確かさを示す方法など、実験で選んだ特定の方法が結果に影響を与えた可能性もあります。

## まとめ

本記事では、LLMの出力に対するユーザーの依存行動を改善するための3つの介入手法を検証した研究を紹介しました。研究では、LSATと数値推定という2つの質問応答タスクを通じて、永続的な免責事項の表示が最も効果的な手法であることが明らかになりました。一方で、ユーザーの信頼度は適切に調整されておらず、誤った判断をした際にむしろ自信が高まる傾向が観察されるなど、今後の課題も示されています。

**参照文献情報**

- タイトル：To Rely or Not to Rely? Evaluating Interventions for Appropriate Reliance on Large Language Models
- URL： [https://arxiv.org/abs/2412.15584](https://arxiv.org/abs/2412.15584)
- 著者： [Jessica Y. Bo](https://scholar.google.com/citations?user=fTKVUJsAAAAJ&hl=en), Sophia Wan, Ashton Anderson
- 所属：University of Toronto

## 理解度クイズ（β版）

1\. LLMと人間の適切な協働において最も重要な課題は何ですか？

研究ではLLMへの過度な依存と過小な依存の両方が問題視されました。適切な依存バランスの実現が人間とLLMの効果的な協働の鍵となります。

解説を見る

2\. 研究で最も効果的だった介入手法は何ですか？

永続的な免責事項の表示が、時間的コストを抑えつつ適切な依存を促進する最も効果的な手法でした。他の手法は過剰依存の抑制には効果があったものの、トレードオフが大きかったと報告されています。

解説を見る

3\. 実験で明らかになった信頼度に関する予想外の現象は何ですか？

理想的には不適切な依存時に信頼度が低下するはずでしたが、逆に上昇する現象が観察されました。この予想外の結果は、ユーザーの認知バイアスの存在を示唆しています。

解説を見る

4\. LSATタスクと数値推定タスクの実験結果の主な違いは何ですか？

LSATタスクでは免責事項が効果的に働きましたが、数値推定タスクではどの介入も適切な依存を効果的に向上させませんでした。タスクの性質の違いが介入効果に影響を与えたことが示されています。

解説を見る

5\. 研究者らが指摘した今後の課題は何ですか？

生成技術の急速な発展に対して消費者のリテラシーが追いついていない点が課題として指摘されました。誤った依存パターンの頻発と、ユーザーの自己認識の不適切さが問題視されています。

解説を見る

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[8つの質問で自分自身の答えを批評する哲学的手法を活用したLLMのプロンプティング技術](https://ai-data-base.com/archives/81166)

[ブラウザでLLMをローカル展開する手法](https://ai-data-base.com/archives/81302)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)