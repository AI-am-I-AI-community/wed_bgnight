---
title: "オープンソースモデルでも力を合わせれば先端モデルに匹敵することを示す「Mixture-of-Agents（MoA）」アーキテクチャ"
source: "https://ai-data-base.com/archives/71419"
author:
  - "[[AIDB Research]]"
published: 2024-06-21
created: 2025-06-13
description: "LLMが進化を続ける中、一つの疑問が浮かび上がってきました。「複数のLLMの長所を組み合わせることで、単一のモデルを超える性能を実現できるのではないか？」この問いに答えるべく、研究者たちは新たなアプローチを開発しました。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

LLMが進化を続ける中、一つの疑問が浮かび上がってきました。「複数のLLMの長所を組み合わせることで、単一のモデルを超える性能を実現できるのではないか？」この問いに答えるべく、研究者たちは新たなアプローチを開発しました。それが「Mixture-of-Agents（MoA）」です。なお、モデルマージとは別の概念です。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71419-1024x576.jpg)

**参照論文情報**

- タイトル：Mixture-of-Agents Enhances Large Language Model Capabilities
- 著者：Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, James Zou
- 所属：Together AI, Duke University, University of Chicago, Stanford University

## 背景

LLMは、人の好みに合わせて調整することで、より便利で一貫性のある文章を作れるようになってきました。しかし、モデルのサイズや学習データには限界があり、単純に大きくすることで性能を上げるには莫大な費用がかかります。

一方で、LLMは種類によって得意なことが違います。例えば、複雑な指示に従うのが得意なモデルや、コードを書くのが得意なモデルなどがあります。

そこで、研究者たちは新しいアイデアを思いつきました。それは、異なる得意分野を持つLLMを組み合わせることです。  
例えば、複雑な指示を理解するのが得意なモデルと、プログラミングコードを生成するのが上手なモデルを組み合わせれば、より高性能で柔軟になるのではないか、と考えたのです。

これまでにも、複数のLLMを組み合わせて使う方法はいくつか提案されてきました。例えば、出力された文章の順位を変えたり、どのモデルを使うかを選んだりする方法があります。また、複数のLLMを対話させて問題を解決する方法も考えられています。しかし、計算コストが増えたり、モデル同士の連携が難しいという問題があります。

そんな中、今回研究者たちは、LLMが持つ「協調性」という性質に着目しました。LLMが他のモデルの回答を参考にすることで、自分の回答の質を高められるという性質です。この特徴を活かすために考え出されたのが、「Mixture-of-Agents（MoA）」という新しい方法です。複数のLLMを段階的に組み合わせ、お互いの回答を参考にしながら、少しずつ回答を改善していくことで1つのモデルだけを使うよりも、はるかに優れた性能を引き出すことができるのです。

以下では本手法の詳細をお伝えします。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71419_2.png)

他のモデルの応答を提供された場合のAlpacaEval 2.0のLC勝率の改善を示す図。

## Mixture-of-Agentsの方法論

### LLMの協調性とは

協調性は、LLMが他のモデルの出力を参照することで、自身の出力の質を向上させることができる性質として定義されます。この特性を検証するために、様々なLLMを用いた実験が実施されました。その結果、多くのLLMに協調性が備わっていることが確認されました。また、他のモデルの出力を参照することで、自身の出力の質が向上するという現象が観察されました。

### MoAにおけるLLMの役割

MoAでは、LLMに2つの役割があります。

**（１）提案者(Proposer)  
**他のモデルによって参照される有用な出力の生成に長けたLLMとして定義されます。Proposerには必ずしも高スコアの出力生成は求められませんが、より豊富なコンテキストや多様な視点の提供を通じて、最終出力の質向上に貢献することが期待されます。

**（２）アグリゲーター(Aggregator)  
**他のモデルの出力を統合し、単一の高品質な出力を生成することに長けたLLMです。優れたアグリゲーターは、自身の出力よりも質の低い入力を統合する際にも、出力の質を維持または向上させる能力を有します。

本研究では、多くのLLMが提案者とアグリゲーターの両方の能力を備えていることが明らかにされました。一方で、特定の役割に特化したモデルの存在も確認されました。

なお、下記は、アグリゲーターが複数のLLMエージェントからの応答を統合するためのプロンプトです。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71419_3-1-1024x314.png)

```js
You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.

Responses from models:
1. [Model Response from Ai,1]
2. [Model Response from Ai,2]...n. [Model Response from Ai,n]
```

日本語版

```js
最新のユーザークエリに対する様々なオープンソースモデルからの応答セットが提供されました。あなたの役割は、これらの応答を単一の高品質な応答に合成することです。これらの応答に含まれる情報の中には、バイアスがかかっているものや不正確なものがある可能性があることを認識し、批判的に評価することが重要です。あなたの応答は、単に与えられた回答を複製するのではなく、指示に対して洗練され、正確で包括的な返答を提供する必要があります。あなたの応答が適切に構成され、一貫性があり、正確性と信頼性の最高基準に準拠していることを確認してください。

モデルからの応答:
1. [モデル Ai,1からの応答]
2. [モデル Ai,2からの応答]...n. [モデル Ai,nからの応答]
```

MoAでは、各層の複数のLLMが独立して応答を生成し、次の層のLLMがそれらの応答を参照して、より洗練された出力を生成します。上記のプロンプトは、次の層のLLMに対して、前の層の応答を批判的に評価し、バイアスや不正確な情報を取り除き、指示に対して正確で包括的な応答を提供するように指示しています。MoAの構造については以下でも説明します。

### Mixture-of-Agentsの構造

MoAは、複数のLLM層から構成される階層的な構造として設計されています。各層には複数のLLMエージェントが配置され、これらのエージェントは同一層内または異なる層間で再利用されることが可能です。

1. 入力が与えられます。
2. 最初の層のエージェントがそれぞれ独立して答えを出します。
3. 答えは次の層のエージェントに渡され、さらに改良されます。
4. プロセスを何度も繰り返し、より良い答えを作り出します。

MoAでは、各層でどのLLMを使うかが重要です。選ぶ際は、モデルの性能と多様性を考慮します。性能は各層のモデルの平均的な能力で判断し、多様性については、異なるモデルの答えを使うと、同じモデルの答えを使うよりも良い結果が得られることがわかっています。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71419_1-1024x341.png)

4層のMoA構造の例を示す図。各層に3つのエージェントがある。

### MoAとMixture-of-Expertsの類似点

MoAは、機械学習の有名な手法であるMixture-of-Experts(MoE)からヒントを得ています。

MoEは、異なる専門分野に特化した複数のネットワークがあり、ゲートネットワークで制御される手法です。MoAはこの考え方をモデルレベルに広げ、LLMをプロンプトで操作することでMoEの長所を活かしています。

一方でMoAでは、ゲートネットワークと専門家ネットワークの役割をLLMが担っています。LLMはプロンプトを理解し、一貫性のある答えを出せるので、外部の調整機能なしで入力を効果的に処理できます。

またMoAは既存のモデルのプロンプト機能だけを使うので、モデルの調整にかかるコストを抑えられます。また、モデルの大きさや構造に関係なく、最新のLLMにも適用できる柔軟性と拡張性があります。

## 評価の概要

MoA手法の有効性を検証するため、包括的な評価が実施されました。AlpacaEval 2.0、MT-Bench、FLASKという3つの主要なベンチマークが用いられ、MoAの出力の質が多角的に評価されました。

評価の結果をまず大まかにまとめると、MoAは以下の3点で優れた性能を示すことが明らかにされました。

1. AlpacaEval 2.0、MT-Bench、FLASKの全てで、顕著な改善が見られた。
2. オープンソースのモデルだけを使ったMoAが、AlpacaEval 2.0とFLASKでGPT-4oよりも良い性能を達成した。
3. 詳しい費用分析を通して、MoAのいくつかの実装がGPT-4 Turboに匹敵する性能を持ちながら、2倍のコスト効率を達成できることが分かりました。

### 評価の設定

**ベンチマーク**

主な評価にはAlpacaEval 2.0が採用されました。実際のユースケースを代表する805の指示が含まれています。各モデルの出力は、GPT-4の出力と直接比較され、GPT-4ベースの評価器によって判断されました。公平性を確保するため、長さ制御(LC)勝率が採用され、回答の長さによるバイアスが中和されました。

さらに、MT-BenchとFLASKも評価に使用されました。MT-BenchではGPT-4によってモデルの回答に点数が付けられ、FLASKでは12の異なるスキルについて詳細な評価が行われました。

**使用されたモデル**

最先端モデルに対する競争力を測りたいため、下記オープンソースのモデルのみを使用してMoAが構築されました。

- Qwen1.5-110B-Chat
- Qwen1.5-72B-Chat
- WizardLM-8x22B
- LLaMA-3-70B-Instruct
- Mixtral-8x22B-v0.1
- dbrx-instruct

MoAは3層構造で設計され、各層で同じモデルセットが使用され、最終層ではQwen1.5-110B-Chatがアグリゲーターとして機能しました。

また、高品質な出力を優先する「MoA w/ GPT-4o」と、コスト効率を重視する「MoA-Lite」という2つの変種も開発されました。MoA-Liteは2層構造で設計され、Qwen1.5-72B-Chatがアグリゲーターとして採用され、GPT-4oよりも高いコスト効率と品質向上が実現されました。

（全てのモデルのライセンス条項が厳密に遵守され、オープンソースモデルの推論はTogether Inference Endpointを通じて実行されました）

### ベンチマーク結果

AlpacaEval 2.0では、MoAによってリーダーボードで上位が獲得され、従来の最高モデルであるGPT-4oから8.2%の改善が示されました。またオープンソースモデルのみを使用したMoAによってGPT-4oが7.6%上回られたことは注目に値します。また、コスト効率を重視したMoA-Liteによっても、GPT-4oを1.8%上回る結果が達成されました。

一方、MT-Benchでは個々のモデルからの改善は比較的小さかったものの、MoAによって最高位が獲得されました。既に高度に最適化されたベンチマークにおいてもMoAによって性能の境界が押し広げられることが示されています。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71419_4-1024x413.png)

またFLASKでは、MoAの優れた性能が示されました。堅牢性、正確性、効率性、事実性、常識、洞察力、完全性において、単一モデルと比較して大幅な改善が観察されました。また、正確性、事実性、洞察力、完全性、メタ認知の点でGPT-4 Omniが上回られました。ただし、簡潔性においてはやや劣る結果となり、モデルの出力がやや冗長になる傾向が見られました。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71419_5.png)

FLASKにおける、GPT-4 Omni、GPT-3.5-turbo-0125、Qwen1.5-110B-Chat、MoAの比較結果を示す図。

## MoAの仕組みを深く理解する

研究チームは、MoAの内部メカニズムをより深く理解するために、広範な実験を行いました。主な発見は以下の通りです。

### 他の選択方法との比較

MoAは、LLMベースのランカー（最良の回答を選択するだけのモデル）と比較されました。その結果、下の図が示すように、MoAはLLMランカーを大幅に上回る性能を示しました。MoAのアグリゲーターが単に1つの回答を選ぶのではなく、提案された全ての回答を巧みに組み合わせて新しい回答を生成していることを示唆しています。

実験では、アグリゲーターの回答と提案者の回答が、BLEUスコア（文章の類似度を測る指標）を用いて比較されました。各サンプルで、提案された回答の類似度スコアと、GPT-4ベースの評価器による選好スコアの相関が計算されました。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71419_6-1024x372.png)

(a) 6モデルのMixture-of-Agents設定における、異なるアグリゲーターを用いたAlpacaEval 2.0でのLC勝率。(b) 提案された出力のBLEUスコアと勝率のSpearman相関。

上記の結果から、勝率（回答の良さ）とBLEUスコアの間に正の相関があることが確認されています。アグリゲーターが良い回答の特徴を効果的に取り入れていることを意味してます。

### 提案者の数と多様性の影響

提案者の数が最終的な出力の質にどう影響するかが分析されました。下の表に示すように、提案者の数が増えるほどスコアが上がり、より多くの情報が有益であることがわかりました。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71419_7.png)

AlpacaEval 2.0における提案者モデルの数がMoAの性能に与える影響を示す表。

また、同じモデルを複数回使う「単一提案者」設定と、異なるモデルを使う「複数提案者」設定が比較されました。結果、異なるモデルを使用する方が常に良い結果を得られました。これはつまりMoAの各層でより多様なモデルを使うことで性能が向上する可能性を示しています。

### モデルの専門化

特定の役割で優れた性能を発揮するモデルを特定する実験も行われました。表に示すように、GPT-4o、Qwen、LLaMA-3は提案と集約の両方で効果的な多才なモデルとして浮上しました。一方、WizardLMは提案者として優れていましたが、他のモデルの回答を集約する際には効果を維持するのに苦労しました。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71419_8.png)

異なるモデルが提案者および集約者として機能する際の影響を示す表。

### コストと性能の分析

コスト、計算量、性能の関係を理解するために詳細な分析が行われました。

図のaは、AlpacaEval 2.0ベンチマークでの平均推論コストと性能（LC勝率）の関係を示しています。この図から、コストと性能のバランスが最適なモデルが特定できます。例えば、MoAは最高の性能を示し、MoA-LiteはGPT-4oと同等のコストでより高い品質を達成しています。特筆すべきは、MoA-LiteがGPT-4 Turboを約4%上回りながら、2倍以上のコスト効率を実現していることです。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_71419_9-1024x417.png)

(a) コストに対する性能のトレードオフを示す図。(b) tflops（遅延の代理指標）に対する性能のトレードオフを示す図。

一方で図のbは、LC勝率とTFLOPS（1秒間に実行できる浮動小数点演算の数）の関係を示しています。TFLOPSは処理速度の指標として使用されています。グラフから、異なるモデルが性能を維持または向上させながら、どのように計算資源を管理しているかがわかります。ここでも、少ない計算量で高い性能を実現するモデルが特定されました。

## 実装に向けて

（編集部による脚注的セクション）

MoAの優れた性能は興味深いものです。と同時に、MoAの実際の実装プロセスに関しては、具体的な情報が不足しているように感じられます。

論文では、MoAの概念的な説明は詳細に行われていますが、実装の具体的なステップについては限定的な情報しか提供されていません。ただし、以下のような実装に関する情報が含まれています。

**モデル構成  
**研究チームは、Qwen1.5-110B-Chat、Qwen1.5-72B-Chat、WizardLM-8x22B、LLaMA-3-70B-Instruct、Mixtral-8x22B-v0.1、dbrx-instructなどのオープンソースモデルを使用しています。

**層構造  
**基本的なMoA構成は3層で構築されており、各層で同じモデルセットが使用されています。

**アグリゲーター  
**最終層では、Qwen1.5-110B-Chatがアグリゲーターとして機能しています。

**プロンプト  
**記事の「MoAにおけるLLMの役割」セクションに示されている「Aggregate-and-Synthesize Prompt」が、モデル間の統合のために使用されています。

**推論環境  
**オープンソースモデルの推論はTogether Inference Endpointを通じて実行されています。

しかし、以下のような具体的な実装詳細については、明確に記述されていません。

- 各層間でのデータの受け渡しの具体的な方法
- モデル選択の動的プロセス（もしあれば）
- 並列処理や効率化のための技術的な工夫

著者らは、GitHubリポジトリ（ [https://github.com/togethercomputer/moa](https://github.com/togethercomputer/moa) ）でコードを公開すると言及しています。現在は非公開状態になっていますが、こちらに追加の情報が共有されるかもしれません。

## まとめ

この記事では、複数のLLMを組み合わせて、段階的に文章を洗練していくMixture-of-Agents（MoA）という手法を紹介しました。LLMが持つ「協調性」という、他のモデルの出力を見て自分の出力を改善できる性質を利用し、様々なモデルの良いところを組み合わせることで、それぞれのモデル単体よりもはるかに良い文章を作るものです。

AlpacaEval 2.0、MT-Bench、FLASKを使った実験では、MoAが文章の質を大幅に改善し、最大で65%の勝率を達成しました。色々なモデルの視点を取り入れることで、1つのモデルだけを使うよりも良い結果が得られるという考えを裏付けるものです。

また、研究者らはMoAの仕組みを詳しく理解するための実験も行い、MoAの設計を改善するためのヒントを得ました。さらに、MoAのいくつかの実装が、GPT-4 Turboに匹敵する性能を持ちながら、2倍のコスト効率を達成できることが分かりました。

今回の方法論は、効率の良いLLMシステムを開発する際の参考になるかもしれませんね。

- 参照論文URL： [https://arxiv.org/abs/2406.04692](https://arxiv.org/abs/2406.04692)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[現実世界の確率分布における言語モデルの推定能力と改善方法　Googleが検証](https://ai-data-base.com/archives/71357)

[ロングコンテキストはRAGもText to SQLも解決するか　Googleがケーススタディを実施](https://ai-data-base.com/archives/71486)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)