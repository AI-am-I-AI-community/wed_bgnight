---
title: "LLMによるバグ全自動修正に成功するケースと失敗するケースの違い"
source: "https://ai-data-base.com/archives/78378"
author:
  - "[[AIDB Research]]"
published: 2024-11-25
created: 2025-06-13
description: "本記事では、LLMによるバグ修正に関する最新の研究を紹介します。LLMやLLMエージェントシステムは、プログラムのバグを自動的に発見・修正する能力に長けています。一方で、まだ課題もあります。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、LLMによるバグ修正に関する最新の研究を紹介します。

LLMやLLMエージェントシステムは、プログラムのバグを自動的に発見・修正する能力に長けています。一方で、まだ課題もあります。

今回、ByteDance社が開発したMarsCode Agentを始めとした7つの最先端システムの詳細な分析から、LLMによるバグ修正の現状と課題が明らかにされました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78378-1024x576.jpg)

**参照論文情報**

- タイトル：An Empirical Study on LLM-based Agents for Automated Bug Fixing
- 著者：Xiangxin Meng, Zexiong Ma, Pengfei Gao, Chao Peng
- 所属：ByteDance

## 背景

LLMやLLMエージェントシステムが、プログラムのバグを自動的に修正する分野で注目を集めています。  
LLMは言語を理解する能力が高いため、プログラムのソースコードや、ユーザーが報告したバグの説明文、コード内のコメントなどを効率よく理解・分析できます。さらに、エージェントシステムになるとパソコンの操作画面（ターミナル）を通じて、必要な情報を集めたり、コードを書き換えたり、動作を確認したりすることができます。こうしてバグの自動修正が以前よりもずっと現実的なものとなってきました。

さまざまな企業や大学の研究者たちが、コードの中からバグを見つけて直すLLMベースのエージェントシステムを開発しています。そして、性能を評価するために、「SWE-bench」という評価用のデータセットも作られました。SWE-benchは実際のプログラムから集めた本物のバグが収録されており、システムが提案した修正案が正しいかどうかをテストで確認できます。SWE-benchは現在、バグの自動修正技術を評価する上で最も重要な基準となっており、企業や研究者が定期的に性能を競い合っています。

こうした状況にもかかわらず、これまでバグの自動修正におけるLLMエージェントの性能や、異なるシステム間の性能の違いが体系的に調べられたことはありませんでした。実際には様々な要因によって自動修正しやすい場合と難しい場合があります。

そのため今回ByteDanceの研究者らによりLLMベースのエージェントがどのようなバグを修正できて、どのようなバグを修正できないのかが詳しく分析されることになりました。

## SWE-benchとSWE-bench Liteとは

[SWE-Bench（ソフトウェアエンジニアリングベンチマーク）](https://www.swebench.com/) は、LLMの”現実的な”ソフトウェアエンジニアリング能力を評価するために設計されたベンチマークです。12個の人気あるGitHubリポジトリの課題（issue）と対応するプルリクエストから作成されています。

SWE-Benchでは、複数の関数やファイルにまたがる大規模なコードベースでの変更を必要とするタスクが提示されます。ベンチマークには2,294のタスク事例が含まれており、モデルが実行環境とやり取りし、長いコンテキストを扱う必要性を強調しています。HumanEvalなど他のコーディングベンチマークの問題（現実に対応しきれていない問題）に対処した形です。

ただしSWE-bench計算要件が高く、課題の数もかなり多いため（2,000件以上）、今回研究者らは [SWE-bench Lite](https://www.swebench.com/lite.html) を導入しました。12のリポジトリのうち11個から選ばれた、300の厳選された課題が入っている圧縮版ベンチマークです。評価が容易であると同時に、SWE-benchの多様性は維持されています。課題の選択基準は以下の通りとされています。

1. 画像、外部リンク、コミットSHA、または参照を含む事例を削除
2. 40語未満の問題記述を除外
3. 複数ファイルの編集または3つ以上の編集箇所がある事例を除外
4. ファイルの作成/削除やエラーメッセージチェックを含む事例を除外

なお、300のテスト事例のほかに23の開発事例が [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") されています。  
各事例には「リポジトリ名-issue番号」という形式で識別子が付けられています。例えば「sympy-15346」は、数式処理ライブラリsympyのIssue #15346を指します。

## バグ修正タスクについて

バグ修正は、以下のような高度なリソースを必要とするソフトウェア開発タスクです。

- 課題レポートで報告されたバグの再現
- 大規模なコードリポジトリ内の欠陥のあるコード箇所の正確な特定
- エラーの原因の理解
- 修正の実装

バグ修正の自動化は、古くから学術界と産業界の双方から広く注目を集めてきました。最近ではLLMが示す強力な論理的推論とコーディング能力を踏まえ、現在多くの研究者がLLMベースの自動バグ修正ツールの開発を探求しています。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78378_1-1024x190.png)

SWE-benchで評価された主要なLLMベースのバグ修正システムの特徴比較表。商用/オープンソース、解決率、不具合特定手法、再現機能の有無などが整理されている。

## 研究課題

今回の研究課題は３つです。

### RQ1. LLMエージェントはコードリポジトリの自動バグ修正においてどのような性能を示すか？

SWE-bench Liteのリーダーボードでは、各システムの解決率に大きな差があり、それぞれのシステムが解決できる/できない事例にも大きな違いがあります。

この差異は、課題の説明の質とシステム自体の設計に起因します。課題の説明が十分に高品質であれば、LLMベースのエージェントがそれを解決できることが期待されます。

そのため、高品質な課題説明を持つ事例がエージェントによって修正されない理由や、低品質な課題説明の事例が解決される理由を分析する必要があります。また、エージェントベースのシステムと非エージェントシステムの実装には大きな違いがあり、その解決能力の違いを調査する価値があります。

そこで、以下のアプローチが取られました。

1. 様々なシステムが解決した事例の違いを分析し、全てのシステムが解決した事例の数と、どのシステムも解決できなかった事例の数を示します。
2. 次に、Agentless（エージェントを使用しないバグ修正手法）が提案した課題説明を評価する基準に基づいて、各事例の課題説明の品質をスコア化します（高スコアは高品質な説明を示します）。
3. その後、多くの高スコアの課題がどのシステムでも解決できない理由と、一部の低スコアの課題が全てのツールで解決できる理由を調査します。
4. さらに、全てのエージェントシステムで解決できるが非エージェントシステムでは解決できない事例の特徴、および全ての非エージェントシステムで解決できるがエージェントシステムでは解決できない事例の特徴を調査します。

参考：Agentlessの論文はこちら（ [https://arxiv.org/abs/2407.01489](https://arxiv.org/abs/2407.01489) ）、紹介記事はこちら（ [https://ai-data-base.com/archives/73060](https://ai-data-base.com/archives/73060) ）です。

### RQ2. 欠陥位置特定性能とその違いの理由は？

バグ修正の重要なステップに欠陥位置特定（Fault Localization）があります。欠陥の位置をより正確に特定できるほど、バグを成功裏に修正できる可能性が高くなります。そのため、異なるシステム間の欠陥位置特定の効果の違いを調査する必要があります。

この課題に対しては、正解（ground truth）に基づいて、各システムがSWE-bench Liteの各事例において、欠陥のあるファイルと欠陥のある行を成功裏に特定した割合を集計します。

### RQ3. バグ再現がバグ修正性能にどのように影響するか？

動的デバッグに不可欠な要素が「バグ再現」です。

バグ再現とは、報告されたバグを実際のプログラム環境で再度発生させることです。例えば、ユーザーから「ボタンをクリックするとエラーが出る」という報告を受けた場合、開発者は同じ条件でボタンをクリックし、本当にエラーが発生するか確認します。

これは人間の開発者が普段から行っている重要な作業で、基本的には以下のような目的があります。

1. バグの存在を確認する
2. エラーメッセージや動作ログからバグの原因を特定する
3. 修正後に本当に直ったかを確認する

LLMによるバグ修正システムでも、この「バグ再現」機能を実装することで、より正確にバグの位置を特定したり、修正の正しさを確認したりできます。

バグ再現スクリプトの品質が高いほど、エージェントにより正確な情報を提供でき、バグを成功裏に修正できる可能性が高くなります。

そのため今回は、各システムが各システムが生成した再現スクリプトの採用率を集計し、バグ再現のバグ修正への影響を比較します。さらに、再現スクリプトの関与によってのみ解決できる事例と、バグ再現がバグ修正に悪影響を与える事例を分析します。

### データ収集

RQ1では、Agentlessプロジェクトが提供する5つのメトリクスと対応する候補値に基づいて採点システムが設計され、異なる課題セットの品質を複数の次元で評価できるようにされました。

RQ2では、異なるツールが生成したパッチの逆分析が実施され、それにより各ツールの欠陥位置特定における性能が偏りなく評価されました。

RQ3では、異なるシステムの実行履歴から再現の使用を特定するために、まず”reproduce（再現）”のキーワードマッチングと手動分析に基づいて、Agentless、 [RepoGraph](https://github.com/ozyyshr/RepoGraph) +Agentless、および [Gru](https://gru.ai/) が再現をサポートしないシステムが特定されました。その後、残りの4つのシステムについて、異なるヒューリスティックルールを使用して再現スクリプトの構築が特定されました。

## 分析結果

### RQ1. LLMエージェントの自動バグ修正性能

SWE-bench Liteは、従来の [Defects4J](https://github.com/rjust/defects4j) ベンチマークと比べてより厳格な評価プロトコルが採用されています。失敗テストケースから生成された動的評価結果がパッチのフィルタリングに使用されることが禁止されており、実世界の開発シナリオにより近い環境が提供されました。

7つのツールの解決能力が分析された結果、 [MarsCode Agent](https://arxiv.org/abs/2409.00899) が最も高い性能を示し、SWE-bench Lite全300件中118件（39.3%）を解決しました。次いで [Honeycomb](https://www.honeycomb.io/) （115件）、Gru（107件）、 [Alibaba Lingma Agent](https://arxiv.org/abs/2406.01422) （99件）、 [AutoCodeRover](https://www.autocoderover.net/) （92件）、Agentless+ [RepoGraph](https://github.com/ozyyshr/RepoGraph) （89件）、Agentless（82件）の順となりました。

なお、300件の事例のうち、168件が少なくとも1つのツールで解決可能であると確認されましたが、132件はどのツールでも解決できないことが判明しました。また、36件が全てのツールによって解決可能であると示されました。

下記は、すべてのシステムが解決できなかったバグの一例です。LLMによる修正と人間による修正の違いを示いており、issueの中に誤解を招く解決策が提案された興味深いケースです。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78378_8-1024x527.jpg)

この結果を基に、「課題説明の品質が解決可能性に影響を与える」という仮説が立てられ、Agentlessによって提供された5つの品質メトリクスを用いた分析が実施されました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78378_2-1024x428.png)

バグ報告の質を評価する5つの指標とその評価基準、スコアリング方法の詳細。

なお、上の表で示されている “Quality of Bug Locations (File/Function/Line)” は実際には3つの独立した指標としてカウントされています。そのため5つの評価指標とは、以下の通りとなります。

1. Quality of Reproducible Examples（再現可能な例の質）
2. Quality of Resolve Solutions（解決策の質）
3. Quality of Bug Locations – File level（バグの場所の質 – ファイルレベル）
4. Quality of Bug Locations – Function level（バグの場所の質 – 関数レベル）
5. Quality of Bug Locations – Line level（バグの場所の質 – 行レベル）

分析の結果、全ての指標において「全て解決可能セット」のスコアが「誰も解決できないセット」を上回ることが明らかにされました。行レベルの位置情報の提供が最も大きな影響を与え（27.8倍の差）、次いで解決策の品質（2.13倍）、関数レベル（1.66倍）、ファイルレベル（1.47倍）の位置情報が影響を与えることが示されました。  
一方で、再現可能な例の品質は比較的小さな影響（1.1倍）しか示しませんでした。

以上の結果から、「LLMやエージェントの根本原因理解を強化する」ことが重要だと明確にされました。また、「パッチの完全性検証能力の向上」や「課題説明の品質の向上」が重要な改善点として特定されました。

下の図は7つのツールの性能比較を3つの観点（問題解決率、ファイルレベルの不具合特定、行レベルの不具合特定）から分析したグラフです。各ツールの関係性を示す行列図も含んでいます。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78378_5-1-1024x419.jpg)

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78378_6-1024x282.png)

全システムが解決できたケースと解決できなかったケースの品質指標スコアの比較分析結果。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78378_7-1024x443.png)

特異なケースとして、解決できなかった高品質な報告と解決できた低品質な報告の具体例とそのスコア。

さらに、エージェントシステムと非エージェントシステムの間で解決可能な事例には明確な違いが存在することがわかりました。

エージェントシステム（MarsCode Agentなど）は、複数の対話ラウンドとツール呼び出しを通じて、コードの依存関係などの追加情報を検出できます。例えば、django-12453の事例（Djangoフレームワークで必要なインポート文が欠落していたバグ）では、エージェントシステムは対話を重ねることで不足しているモジュールのインポートを検出し、正しく修正できました。

一方、非エージェントシステム（Agentlessなど）は、正しい修正位置を1箇所に絞り込むことに長けており、 [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") （複数の解決案の生成と選択）を活用できます。例えば、django-11422（Djangoのファイル監視機能で特定のファイルが監視対象から漏れていたバグ）では、Agentlessは正しい修正位置のみを選択したため修正に成功しました。

またそれぞれに限界もあります。

エージェントシステムは、複数の修正候補がある場合、正しい修正位置を選択できないことがあり、また、 [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") の欠如により、結果の多様性が低くなりがちです。

非エージェントシステムは、複数回の対話や追加ツールの利用ができないため、コードの依存関係の把握が不十分になりがちで、その結果、文法的に正しくないパッチを生成することがあります。

下の図は、バグ修正に対するエージェントシステムと非エージェントシステムのアプローチの違いを示した例です。同じバグに対して、MarsCode Agentは複数の修正候補を検討した結果、かえって正しい修正箇所を見落としてしまいました。一方、より単純な設計のAgentlessは、バグ報告の内容から直接的に正しい修正箇所を特定し、適切な修正を行うことができました。この例は、より高度な推論を行うエージェントシステムが、時として単純な解決策を見落としてしまう可能性があることを示しています。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78378_11-1024x575.png)

### RQ2：欠陥位置特定（FL）の有効性

この課題においては、MarsCode Agentが最も高い性能を示し、239件の事例で欠陥ファイルの特定に成功しました。次いでAlibaba Lingma Agent、Gru、Agentless+RepoGraph、AutoCodeRover、Honeycomb、Agentlessの順となり、Agentlessは206件の特定に成功しました。MarsCode AgentはAgentlessと比較して16.0%の性能向上を達成しました。

注目すべき点として、課題解決タスクで全体の43.9%という大きな差があったのに対し、ファイルレベルの位置特定では差が16.0%と比較的小さくなりました。

このことは、ファイルレベルの位置特定タスクが、その後のパッチ生成タスクよりも一般的に単純であることを示唆しています。

なお、課題解決タスクで2位のHoneycombが、ファイルレベルの位置特定では6位と低い性能を示しました。しかし、行レベルの位置特定ではMarsCodeに次ぐ2位となり、特定したファイル内での欠陥行の特定率が65.9%（137/208）と高いことが判明しました。ファイルレベルでは比較的高い性能が達成されていますが、行レベルの位置特定にはまだ改善の余地があります。より細かい粒度の行レベルでの位置特定結果が、エンド・ツー・エンドでの課題解決と強い相関を示しています。欠陥のある行を正確に特定できれば、正しいパッチの生成がより実現可能になります。

エージェントベースの修復手法（例：MarsCode Agent）は、パッチ生成の最適化により強みを発揮することがわかりました。一方、非エージェントシステム（例：Agentless+RepoGraph）は位置特定では良好な性能を示すものの、最終的な課題解決では劣る傾向にあります。

以上の結果から、バグ修正ツールの開発において、特に「行レベルの位置特定精度の向上」が重要であることがわかりました。ファイルレベルの位置特定は比較的確立されていますが、より細かい粒度での特定能力を向上させることで、バグ修正の成功率を高められる可能性があります。

### RQ3：バグ再現の有効性

バグ再現はどうであったかを確認します。

MarsCode Agentは解決成功118件中83件（70.3%）、Honeycombは解決成功115件全て（100%）、Alibaba Lingma Agentは解決成功99件中12件（12.1%）、AutoCodeRoverは解決成功92件中39件（42.4%）で再現を実施しました。一方、Gru、Agentless + RepoGraph、Agentlessは再現機能を利用していません。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78378_12-1024x196.png)

各システムのバグ再現機能の統計データ。解決したケース数と、その中でバグ再現を行ったケースの割合を示す。

システムの実行履歴分析から、再現が有効なケースと逆効果となるケースが確認されました。

再現が有効なケースとして、168件の解決可能な事例のうち24件は再現を使用した場合にのみ解決可能でした。簡潔なテキスト説明のみで位置特定が困難であり、そのうち20件にはバグ理解を助けるコードスニペットが課題説明に含まれていました。具体例として、sympy-15346（数式処理ライブラリsympyで特定の数式変換が正しく動作しないバグ）では、簡潔な説明と例のみからMarsCode Agentが再現スクリプトを実装し、実行中の中間変数を出力することで段階的に欠陥位置を特定しました。エージェントシステムの自律性の利点を明確に示す事例です。

しかし、再現が”逆効果”となるケースも存在します。例えば、django-11422（Djangoの開発サーバーで特定のファイルが自動リロードの対象から漏れていたバグ）では、課題説明に「Autoreloader」という直接的な手がかりが含まれており、Agentlessはこの情報から正確に修正箇所を特定し成功しました。一方、MarsCode AgentとHoneycombは再現情報に注意が逸れ、誤った位置特定につながりました。

したがって、再現の有用性は、課題説明の情報が不足している場合の追加情報源として、また生成された候補パッチの正確性検証に役立つ点にあります。  
しかし、課題説明が既に明確で正確な場合、再現はLLMの判断を誤らせる可能性があり、課題説明への集中を減少させ、解決の効率を下げる可能性があります。最適な使用方法として、システムは課題の性質に応じて再現の実施を選択的に判断し、バグ再現の正確性の判定を強化することが重要です。

つまり、バグ再現が強力なツールではあるものの、その使用は状況に応じて慎重に判断する必要があるということです。

### 考察

今回の調査を通じて、LLMとそれを活用したエージェントシステムによるバグの自動修正における課題が明らかになりました。

まず、LLMの性能向上についてです。モデルの推論能力を強化し、バグ報告の中から本当に重要な情報を正確に見分けられるようにする必要があります。中でも複数の修正候補がある場合に、どの箇所が最も重要かを適切に判断できる能力が求められます。

次に、エージェントシステムの設計面での改善点も見えてきました。バグ報告の品質を重視し、特にエラーの発生履歴（スタックトレース）に含まれる複数の疑わしい箇所に注意を払う必要があります。また、修正案が完全なものかをチェックする仕組みを設けることも重要です。さらに、個々の修正がシステム全体に与える影響を考慮できる設計にすることも必要です。

LLMの特性への対応も重要な課題です。LLMの出力にはランダム性が含まれるため、それを抑制するか、あるいは逆にその多様性を活用する仕組みを確立する必要があります。また、バグの位置を特定する際は、ファイルの特定だけでなく、具体的な行の特定まで正確に行える必要があります。これは探索範囲が広がるため、より高い精度が求められるためです。

最後に、バグの再現プロセスの改善も欠かせません。バグを正確に再現できているかどうかの判断を確実にする必要があります。なぜなら、不正確な再現は、その後の修正作業全体の失敗につながる可能性があるためです。

以上の課題に取り組むことで、バグ修正の自動化の精度と信頼性を高めることができると考えられます。

## まとめ

本記事では、SWE-benchにおいて上位の成績を収めた商用4システムとオープンソース3システムを比較分析した研究を紹介しました。研究チームは、これらのシステムのバグ修正能力、不具合位置の特定精度、バグ再現機能の有効性について詳細な分析を行いました。

分析の結果、今後のLLMベースのバグ修正システムには2つの重要な改善点があると示されました。1つ目はLLMの推論能力の向上です。バグ報告から関連情報を正確に抽出し、複数の修正候補地点から最適な箇所を選び出す能力が求められています。2つ目はエージェントの設計方針の改善です。バグ報告の質や複数の不具合箇所への対応、修正の完全性の検証、再現の正確性など、様々な要素を考慮した設計が必要とされています。

なお、同研究チームは、行レベルでの不具合位置の特定がファイルレベルよりも重要であることも指摘しています。これは、発見すべき範囲が広いため、より精緻な分析が求められるためです。

- 参照論文URL： [https://arxiv.org/abs/2411.10213](https://arxiv.org/abs/2411.10213)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMによる時系列データ分析に「ニュース情報」を混ぜるアプローチ　為替予測精度など大幅に向上](https://ai-data-base.com/archives/79028)

[Gemini-1.5-proやGPT-4o-miniなどの性能を上回るLLaVA-o1（11Bパラメータ）](https://ai-data-base.com/archives/79215)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)