---
title: "LLMにおける現状のリスクと対策に関するまとめ"
source: "https://ai-data-base.com/archives/74734"
author:
  - "[[AIDB Research]]"
published: 2024-08-23
created: 2025-06-13
description: "本記事では、LLMのリスクと対策に関する研究を紹介します。LLMの急速な普及に伴い、安全性と信頼性の確保が重要な課題となっています。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、LLMのリスクと対策に関する研究を紹介します。

LLMの急速な普及に伴い、安全性と信頼性の確保が重要な課題となっています。

今回研究者らは主なリスクをいくつかのカテゴリーに分け、それらリスクに対処するための技術、そして対策を行う上での課題についてまとめました。

なお記事の最後で、LLMの安全性向上のために開発されているオープンソースツールも紹介します。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_74734-1024x576.jpg)

**参照論文情報**

- タイトル：Current state of LLM Risks and AI Guardrails
- 著者：Suriya [Gan](https://ai-data-base.com/archives/26269 "敵対的生成ネットワーク（GAN）") esh Ayyamperumal, Limin Ge
- 所属：Carnegie Mellon University

## 背景

LLMは急速に高度化が進み、様々な分野で使用されるようになりつつあります。しかし、LLMの使用には本質的なリスクが伴うことも分かっています。そこでLLMの安全性と信頼性を確保するための「ガードレール」（安全装置）の開発が必要不可欠となっています。ガードレールとは、LLMの動作を望ましい方向に制御し、潜在的なリスクを軽減するための仕組みを指します。

LLMの主なリスクは、バイアス（偏見）、安全でない行動、データセットの汚染、説明可能性の欠如、ハルシネーション（幻覚とも呼ぶ。事実と異なる情報の生成）、再現性の欠如などが挙げられます。これらのリスクはLLMのアプリケーションを展開する際の懸念事項となっています。

そこで今回カーネギーメロン大学の研究者らは、LLMのリスクを改めて深掘りし、現在のガードレール技術を評価してまとめました。なお、調査の中で特に注目されたのは、実世界に影響を及ぼしうるLLMエージェントの安全性と信頼性です。

以下で詳しく紹介します。

## LLMのリスク

以下ではまず、LLMの主要なリスクについて見ていきます。

### リスク1. バイアス

LLMには、特定の人や集団を不公平に扱ってしまう傾向があります。これを「バイアス」と呼びます。

バイアスが起こる主な理由は二つあります。

一つ目は、LLMが学習に使うデータに、もともと社会の偏見が含まれている（場合がある）こと。そして二つ目は、意図的に特定の指示を与えることで、LLMに偏った考えを持たせることができるという事実です。

LLMにおけるバイアスを調べる方法は主に2つあります。

（１）言葉の使い方を直接調べる方法

例えば、「男性」や「女性」といった言葉が、「優秀」や「劣っている」などの言葉とどれくらい結びついているかを確認します。結びつきが強いほど、バイアスがある可能性が高いと考えられます。

（２）実際のタスクでの結果を調べる方法

LLMに特定の仕事をさせて、その結果にバイアスがないか確認します。例えば、文章中の「彼」や「彼女」といった言葉を正しく理解できているか調べたり、職業や宗教に関する偏見がないかチェックしたりします。

### リスク2. 公平性

AIが人種、性別、年齢などによって不公平な扱いをしないようにすることは、前提となっている道徳的なルールです。

公平なAIシステムを作るためには、以下の3つの段階で気をつけることがあります。

（１）データを集める段階

様々な背景の人々からバランスよくデータを集めます。また、偏ったデータだけを使わないように注意します。

（２）AIモデルを学習させる段階

公平性を重視した学習方法を選びます。そして、特定のグループに不利にならないよう、学習の過程をチェックします。

（３）AIの結果を使用する段階

結果が公平かどうかを確認します。また、不公平な結果が出ていないか、定期的にチェックします。

ただし、公平性を保つためには以下の課題があります。

1. 「完璧な」公平性を達成するのは難しい
2. 時には、公平性と他の目標（例：精度）がぶつかることがある
3. 社会の価値観によって、何が「公平」かの定義が変わることもある

それでも公平性を目指す理由は、差別のない社会を作るため、法律や規制に従うため、そしてAIシステムへの信頼を高めるためです。

なお公平性は、AIの開発者だけでなく、使う側も意識することが大切だという点も忘れない必要があります。

### リスク3. エージェントシステム

単なる文章生成を超えた能力を持つLLMのシステムをLLMエージェントと呼ぶことがあります。LLMエージェントは、デジタル世界と現実世界をつなぐ橋渡し役として機能することが期待されています。例えば、インターネット上で情報を探したり、スマート家電を操作したり、複雑なデータ分析の計画を立てたりすることができると考えられています。

デジタルの世界だけでなく私たちの現実の生活にも直接影響を与えるほど高度になるのであれば、AIの安全性はより重要になってきています。言い換えれば、LLMエージェントのリスクは、その影響力の大きさに比例して高まります。例えば、間違った情報に基づいて重要な決定を下したり、不適切なタイミングでデバイスを操作したりすることで、予期せぬ問題を引き起こす恐れがあります。

そのため、これらのシステムの開発と運用には、安全性のチェック、緊急時の対応策、そして倫理的な配慮など、多角的な視点から取り組むことが求められています。

### リスク4. データセットの汚染

LLMは、膨大な量の文章データを使って学習します。しかし、この学習データに悪意のある情報や間違った内容が混ざっていると（つまりデータセットが汚染されていると）大きな問題が起こる可能性があります。

データセットが汚染されると、どんな問題が起こるのでしょうか。主な現象は3つ考えられます。

（１）偏見や差別的な考えを広げてしまう  
LLMが偏った情報を学習すると、その偏見を含んだ文章を生成してしまいます。

（２）間違った情報を広めてしまう  
正しくない情報を学習すると、それを事実のように扱ってしまいます。

（３）特定の人々を傷つける可能性  
悪意のある内容を学習すると、特定の集団を攻撃するような文章を作り出してしまうかもしれません。

もし上記の問題があれば、LLMを使う企業の評判を傷つけたり、ユーザーに害を与えたりする恐れに繋がります。

さらに怖いのは、悪意のある人が意図的にデータセットを汚染し、LLMの安全機能を無効にするトリガーを仕込むことです。

このような問題を防ぐために、LLMの開発者たちは学習データの品質管理を徹底し、定期的にモデルの出力をチェックしています。また、新しい防御技術の研究も進められています。

### リスク5. 説明可能性と解釈可能性

AIの「なぜ」を理解する難しさです。

最新のLLMは非常に複雑で、私たちにはその内部がよく分かりません。これは「ブラックボックス問題」と呼ばれています。「ブラックボックス問題」は主に3つの更なる問題につながります。

（１）信頼性の問題  
LLMがなぜそのような答えを出したのか分からないと、その答えを信頼するのが難しくなります。医療や法律など、重要な決定を下す場面では特に大きな問題になります。

（２）改善が難しい  
LLMの動作の理由が分からないと、問題が起きたときに修正するのが難しくなります。例えば、過去にGPT-4が突然うまく動かなくなった時、専門家でも原因を特定できませんでした。

（３）責任の所在が不明確  
LLMの判断理由が分からないと、何か問題が起きたときに誰が責任を取るべきか決めるのが難しくなります。

「ブラックボックス問題」を解決するため、研究者たちはLLMの動きをより分かりやすく説明できるよう努力しています。例えば、以下のような取り組みを行っています。

- 判断過程を段階的に見える化する技術の開発
- 決定に最も影響を与えた要因を特定する方法の研究
- 判断を人間が理解しやすい言葉で説明させる技術の開発

これらの取り組みにより、将来的にはLLMの「なぜ」をより良く理解できるようになるかもしれません。そうすればLLMをより安全に、そしてより効果的に活用できるようになることが期待されています。

### リスク6. ハルシネーション

言うなればAIの「作り話」問題です。

LLMには、時々「ハルシネーション」と呼ばれる問題が起こります。実際には存在しない情報を、まるで本当のことのように話してしまうことです。

なぜこれが問題なのでしょうか？言わずもがなの理由もありますが、以下の4つが考えられます。

（１）とても本物らしく聞こえる  
LLMの作り話は、詳しい説明や正しい文法を使っているので、本当の情報と区別がつきにくいです。

（２）現実世界での影響  
例えば、弁護士がLLMの作り出した存在しない法律を引用して、実際に困ったことになったケースがあります。

（３）LLMへの信頼が損なわれる  
LLMが時々嘘をつくとわかると、ユーザーはLLMの情報を信じにくくなります。正確さが重要な分野では大きな問題になります。

（４）悪用の可能性  
悪意のあるユーザーが、この問題を利用して嘘の情報を広めたり、人々の考えを操作したりする恐れがあります。

ハルシネーションが起きる理由は、LLMは大量の文章を読んで学習しますが、その内容が本当かどうかを判断する能力はないためです。LLMは文章のパターンを覚えているだけなので、時々それらを組み合わせて、実際には存在しない情報を作り出してしまいます。

研究者たちは、この「作り話」問題を解決するために例えば以下のような様々な方法を試しています。

- 「自信がない」ときは正直に言わせる
- 回答を他の信頼できる情報源と照らし合わせる
- 事実と意見を区別させる訓練をする

### リスク7. 再現不可能性

LLMには、同じ質問をしても毎回少し違う答えを返すという特徴があります。これを「再現不可能性」と呼びます。簡単に言えば、AIの「気まぐれ」のようなものです。

より自然で多様な応答ができるようになることと、完全に同じ答えを繰り返すことが難しくなることはセットなのです。この「気まぐれ」がなぜ問題になるのか？その理由は3つあります。

（１）性能評価が難しい  
LLMの性能をテストしようとしても、結果が毎回変わってしまうと、本当に良くなったのかどうかわかりにくくなります。

（２）ユーザーからの信頼低下  
例えば、カスタマーサービスのチャットボットが同じ質問に対して違う答えを返すと、ユーザーは混乱し、信頼しにくくなります。

（３）重要な決定での使用が難しい  
医療や法律など、一貫した回答が必要な分野では、この「気まぐれ」は大きな問題になる可能性があります。

この再現不可能性の問題を解決するため、研究者たちは以下のようなアプローチを試しています。

- 回答をより安定させる新しい技術の開発
- 重要な部分では一貫性を保ちつつ、会話の自然さも失わない方法の探求
- 複数回の回答を比較して、最も信頼できる答えを選ぶシステムの構築

現状の最適解は、完全に同じ答えを返すLLMではなく、重要な情報は一貲して正確に、かつ会話の自然さも保てるLLMの開発を目指すことだと考えられます。

### リスク8. プライバシーと著作権

LLMには、プライバシーと著作権に関わる重要な問題があります。

（１）プライバシーの問題

LLMは学習したデータをよく覚えています。時々、個人情報や秘密の情報を思い出して話してしまうことがあります。これは、データ漏洩や間違って入ってしまった情報が原因かもしれません。悪意のある人が特別な方法を使って、LLMから秘密の情報を引き出そうとする可能性もあります。

（２）著作権の問題

LLMは本や記事など、著作権のある作品も学習しています。LLMIがこれらを基に新しい文章を作ると、誰に著作権があるのか分かりにくくなります。例えば、LLMが小説家の文体を真似て小説を書いたら、それは著作権侵害になるのでしょうか？

どちらも非常に複雑な問題ですが、これらに対する取り組みも進んでいます。

まずはプライバシー保護の技術開発です。個人情報を隠したままLLMを学習させる方法の研究や、LLMが個人情報を話さないようにする仕組みの開発などがされています。

次に著作権法の見直しがされています。LLMが作った作品の著作権をどう扱うか、法律の専門家が検討しています。またLLMの学習に使う作品の著作者の権利をどう守るか、議論が進んでいます。

さらに、LLMを開発・利用する企業は、個人情報や著作権を守るためのルールを厳しく設けています。また、データの扱いに関する訓練を従業員に行っています。

### より安全で信頼できるものにする方法

LLMが私たちの生活に深く関わるようになると、その安全性と信頼性がとても大切になります。将来的に、LLMがインターネットの世界を超えて、現実の世界に影響を与えるようになると、もっと注意が必要です。

そこで、LLM開発者たちは主に3つのことに力を入れています。

（１）テストを考え抜き多く試す

LLMをいろいろな状況で試すことが重要だと考えられています。普通の状況だけでなく、思いがけない難しい状況でも、LLMがどのように動作するかを確認します。LLMを実際に使用する前に問題を発見し、修正することが求められています。

（２）安全装置の用意

LLMが間違えたときのための、バックアッププランを用意します。例えば、人間がすぐに制御できるようにしたり、危ない状況を自動で避けるプログラムを入れたりします。何か問題が起きても大丈夫なように、安全網を張っておくということです。

（３）周りの状況をよく理解するようにする

LLMが正しく働くには、常に新しい情報が必要です。そこで、カメラやセンサーなどから得た最新の情報を、LLMに教える取り組みがされています。LLMに周りの状況をよく理解して、より良い判断をさせるためです。

## LLMを安全に使うための体系的な工夫

### LLMを安全に使うための工夫

LLMを守るのは簡単ではありません。LLMには特有の弱点があり、また重要な情報を扱うことが多いからです。そのため、いくつかの方法を組み合わせて使われています。以下はその例です。

- 勝手に使われないよう、しっかりとした使用制限をかける
- 悪意のある指示や危険な返答がないか、入力と出力をよく確認する
- 学習データに偏りがないか注意し、偏りの影響を減らすよう努める
- モデルの出力が正しいかどうかを確認する方法を用意する

なお下の表はLLMの安全性を保護する上で必要な3つのレイヤーでの工夫を各分野ごとにまとめたものです。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_74734_figure1-2-808x1024.png)

### 段階的な保護の仕組み

LLMに対しては、異なる段階で安全対策（ガードレール）が設けられています。最も制限が厳しい使い方は、OpenAIやAnthropicなどの会社が提供するネットワークAPIを通じたものです。この場合、悪意のある人や一般のユーザーは、以下のものを見たり触ったりすることができません。

- LLMの内部の仕組み
- 言葉の出やすさの情報
- LLMが学習に使ったデータ

ネットワークAPIにおいては、以下の異なる段階で安全対策が行われています。

#### ゲートキーパー層（入口での安全確認）

LLMを使う際、最初に「システムプロンプト」という指示が与えられます。「システムプロンプト」はLLMの行動を決める大切な役割を果たします。例えば、

- LLMの役割を決める
- 話し方（例えば丁寧か親しみやすいか）を指定する
- 背景情報を与える
- 回答の形式を決める
- 安全で適切な応答をするためのルールを設定する

開発者はこの指示を工夫して、LLMの動きをコントロールし、危険な出力を防ぎます。

また、LLMの外側にも保護の仕組みが設けられています。悪意のある指示を見つけ出し、データの漏洩などの問題を防ぎます。時には指示を言い換えたり、専門の小さなAIを使って入力をチェックしたりしています。

しかし、どんなに優れた保護策も完璧ではありません。そのため、新しい攻撃方法に対応できるよう、常に研究と改善が行われています。

#### 知識アンカー層（知識の確認と活用）

LLMが使う情報の選び方を決めるレイヤーです。主な目的は、LLMが現実とかけ離れた答えを出すのを防ぐことです。

本当らしく聞こえるが実際には間違った情報を作り出してしまう「ハルシネーション」の問題に対処するため、「RAG（検索拡張生成）」という方法が使われます。RAGでは、LLMが答えを作る時、ただ想像で答えるのではなく、確かな情報源を参照します。すると、より正確で信頼できる答えが得られやすくなります。そして誰かが意図的にLLMを使って間違った情報を広めるのを難しくします。

また、RAGには別の利点もあります。LLMが学習時に覚えてしまった個人情報を誤って漏らしてしまう危険性を減らせるのです。LLMが外部の文書から情報を得るようにすることで、学習データに含まれていた可能性のある機密情報を出力してしまう可能性が低くなります。

このように、RAGはLLMの正確性を高めつつ、プライバシー保護にも役立つ重要な仕組みです。

※編集部注：RAGに関する記事は本メディアに複数ありますので併せてご覧ください。

- [LLMのRAG（外部知識検索による強化）をまとめた調査報告](https://ai-data-base.com/archives/61367)
- [ファインチューニングとRAGを比較実験した結果　LLMに外部知識を取り入れる手法としての違い](https://ai-data-base.com/archives/63401)
- [RAGの失敗パターン7選と教訓9箇条](https://ai-data-base.com/archives/69154)
- [RAGシステムの最適な構築を探る](https://ai-data-base.com/archives/72121)

#### パラメトリック層（LLMの内部調整）

モデル自体を変更して様々な問題に対処する方法もあります。

例えば研究者たちは内部パラメータを調整してLLMが持つバイアスを減らす方法を探っています。既存のLLMを特別に選んだデータで微調整したり、学習の仕方自体を変えたりすることで、固定観念や文化的な無配慮、特定のグループに対する不公平な扱いなどを軽減しようとしています。

次に、プライバシーの保護も重要な課題です。LLMの学習データや出力する情報に含まれる個人情報を守るため、「差分プライバシー」という方法が広く採用されています。個人情報を直接使わずに、全体的な傾向だけを学ぶ手法です。

さらに、LLMの安全性を高めるため、わざと難しい例を学習データに加えることもあります。人間の評価を取り入れた学習方法や、危険な応答を取り除く仕組みなども研究されています。

ただし、LLMは非常に複雑なシステムなので、方法をそのまま適用するのは困難な場合があります。そのため、LLMの特性に合わせた新しい方法の開発が進められています。

## ガードレール実装における課題

LLMにガードレール（安全対策）を実装する際には、様々な課題が存在します。

### 柔軟性と安定性のバランス

LLMシステムには2つの大切な特徴があります。柔軟性と安定性です。

柔軟性とは、LLMが新しい情報や変わる状況に合わせて学び、変化できることです。現実世界で長く役立つために必要です。

そして安定性とは、LLMが予想通りに動き、思わぬ結果を避け、ルールを守ることです。これも大切です。

しかし、この2つのバランスを取るのは難しいことです。

安全対策が厳しすぎると、LLMの良さが活かせません。逆に緩すぎると、偏見や危険、悪用のリスクが高くなります。

### 複雑さの問題

LLMシステムは予想外の動きをすることがあります。そのため、何か問題が起きた時の対策をいくつも用意します。

しかし、これらの対策をつなぐ指示が曖昧で、うまくいくかどうか判断しにくい場合が多いのも確かです。

LLMの基本的な部分が予想外の動きをする中で、予想通りに動くシステムを作るのは難しい課題です。この複雑さのせいで、LLMの動きを予測したりコントロールしたりするのが難しくなります。

### 不明確な目標と指標

LLMシステムを作る時、システムに何を期待しているのか開発者にもはっきりしないことがよくあります。

例えば、よく使われる「コサイン類似度」という評価方法は、実はとても曖昧で、LLMの次の動きを予測できません。

目標や評価方法がはっきりしないと、LLMがうまく動いているかを判断したり、改善したりするのが難しくなります。また、期待していた動きと実際の動きに差が出てしまうかもしれません。

### テストの難しさ

LLMを使ったシステムを作る時、そのシステムが思い通りに動くかを確かめるのはとても難しい作業です。多くの場合、システムを作る人は、情報を処理する中心部分を自分で管理できていません。そのため、システムがどれだけ信頼できるか、どう動くか予想するのが難しくなります。

また、時間が経つにつれて、システムの働きが良くなっているのか、それとも悪くなっているのかを判断するのも容易ではありません。システムを長く使い続けられるようにするには、これらの問題を解決する必要があります。

### コストの問題

悪意のある入力や偏った答えを見つけて防ぐために、「判断役」のLLMを使って答えをチェックすることがあります。しかし、この方法はコストがかなりかかります。

面白いことに、大規模なLLMよりも、小さくて特定の目的に特化した「判断役」モデルの方が、LLMの答えを評価するのが上手だとわかってきました。このような取り組みの末に、効率的で効果的な安全対策を作る新しい方法が見つかるかもしれません。

## オープンソースツール

より安定したLLMベースのアプリケーションを構築するために、ガードレールの実装に特化したオープンソースツールが急増しています。それぞれ少しずつ異なるアプローチでこれらの問題の解決を試みています。

### Nemo-Guardrails

Nemo-Guardrailsというツールは、Colangという特定の目的のために作られた特別な言語を使います。

このツールは、新しい安全対策を作ったり、すでにある安全対策を使い回したりできます。どの安全対策を使うかは、Colangでの設定によって決まります。

情報が入ってきてから出ていくまでの間、いろいろな段階で別々の処理を設定できます。例えば、検索で見つけた情報を使って新しい内容を作るシステムの出力を、変更することもできます。

このツールでは、次に何をすべきかを判断するのにもLLMを使用します。ただし、最近の研究では、LLMは自分の出した答えに引っ張られやすいことがわかっており、これはリスクとなりえます。

注意すべきことに、Nemoには、処理にかかるお金をコントロールする機能がありません。そのため、簡単な処理でも、思ったより多くのお金がかかってしまう可能性があります。

### LLamaGuard

LLamaGuardは、Metaが作ったツールです。llama2-7bというモデルを元に作られています。

このツールは、入力された情報と、それに対する応答を見て、その内容がどのくらい慎重に扱うべきものかなどを判断します。

LLamaGuardは、言葉や文章の意味を理解する力があります。そのため、これまでの分類方法よりも優れた働きをすると考えられています。LLamaGuardを作るとき、Metaは理想的な入力と応答の組み合わせを数多く用意し、llama-7Bという基本のモデルを特別に訓練しました。なお、訓練用データを作ったのは、Metaの「レッドチーム」と呼ばれる特別なグループです。

### Guardrails AI

Guardrails AIというツールも、Nemo-Guardrailsと同じようにRAILSと呼ばれる特別な言語を作りました。XML形式を少し変えて作られています。

そしてGuardrails AIも、他のLLMを使って、入力と出力を判断し、決定を下すことができます。XMLは、LLMが理解しやすく、処理しやすく、そして指示に従いやすいことがわかっています。また、出力の意味が正しいかどうかを、ある程度柔軟に確認する機能もあります。

## まとめ

本記事では、LLMのリスクとガードレールに関する研究を紹介しました。LLMは産業変革ほどの可能性を秘めていると言われていますが、バイアスや安全性の懸念といったリスクも伴います。これらに対処するため、研究者たちはバイアス評価方法や公平性指標の検討、LLMエージェントの安全性確保策の探求を行っています。

リスクへの技術的戦略としては、階層型保護モデルやRAG [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") などが提案されていますが、実装には様々な課題が存在します。そしてそれらの課題に対処するため、複数のオープンソースツールが開発されています。

ただしLLMは急速に進化する分野であり、今の戦略が将来的に陳腐化する可能性もあります。そのため、継続的な研究開発と分野全体での協力が重要です。

- 参照論文URL： [https://arxiv.org/abs/2406.12934](https://arxiv.org/abs/2406.12934)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMのサイバーセキュリティタスク性能評価フレームワーク「Cybench」](https://ai-data-base.com/archives/74630)

[モデルとデータの大規模化で変化するLLMのハルシネーション　Google DeepMindの研究](https://ai-data-base.com/archives/74778)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)