---
title: "心の理論をLLMエージェントに実装することの効果"
source: "https://ai-data-base.com/archives/72954"
author:
  - "[[AIDB Research]]"
published: 2024-07-17
created: 2025-06-13
description: "本記事では、LLMマルチエージェントにおける「心の理論」実装の研究を紹介します。本手法はさまざまな複雑な環境で評価され、意思決定に新しい可能性をもたらすことが示されました。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、LLMマルチエージェントにおける「心の理論」実装の研究を紹介します。

本手法はさまざまな複雑な環境で評価され、意思決定に新しい可能性をもたらすことが示されました。従来手法をどう改善し、どのような成果を上げたのか、詳しく見ていきましょう。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72954-1024x576.jpg)

**参照論文情報**

- タイトル：Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models
- 著者：Lo [gan](https://ai-data-base.com/archives/26269 "敵対的生成ネットワーク（GAN）") Cross, Violet Xiang, Agam Bhatia, Daniel LK Yamins, Nick Haber
- 所属：Stanford University

**本記事の関連研究**

- [LLMの「心の理論」能力を詳しく調べるためのベンチマーク『OpenToM』 GPT-4など複数モデルの評価結果](https://ai-data-base.com/archives/64820)
- [ポーカーなど不完全情報ゲームを「心の理論」で上手にプレイするGPT-4ベースの『Suspicion（疑心）-Agent』松尾研など開発](https://ai-data-base.com/archives/56141)
- [競争環境でのLLMエージェントが自発的に協力し始める現象を観測](https://ai-data-base.com/archives/72854)
- [人間のような内省メカニズムをLLMに導入することの効果 Google DeepMindなどが検証](https://ai-data-base.com/archives/72194)

## 背景

LLMを活用したマルチエージェントの能力向上が注目されています。

今回研究者らは、LLMが他者の意図を推測する「心の理論」の能力にも長けていることに着目しました。そして、認知科学に基づく設計で、他のエージェントの状態を推測して行動するエージェントの開発を行いました。

評価においては、多様なケースで実験を行い、モデルの汎用性と適応能力が厳密に検証されました。

## 方法論

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72954_1-1024x498.jpg)

Hypothetical Mindsの アーキテクチャ とモデルのワークフロー

今回の提案手法は、「部分観測マルコフゲーム」と呼ばれる枠組みに基づいています。

複数のプレイヤーが限られた情報のもとで意思決定を行い、各プレイヤーは環境の全体像ではなく自身の視点からの部分的な観測のみに基づいて行動を選択する枠組みです。

そして認知科学に基づき、力を発揮するエージェントが設計されました。

### 『Hypothetical Minds（仮説的思考モデル）』

研究者らは、複数の認知モジュールから構成される『Hypothetical Minds（仮説的思考モデル）』というアプローチを考案しました。各モジュールが協調して働くことで、LLMを中心とした具現化されたエージェントが形成される仕組みです。

主なモジュールは以下の通りです。

**知覚モジュール  
**環境からの観測が、テキストベースの地図/状態表現に変換される

**記憶システム  
**過去の観測や重要な状態情報が保存される

**心の理論（ToM）モジュール  
**他のエージェントの戦略や目標に関する仮説が生成・評価される

**サブゴールモジュール  
**高レベルの目標を達成するための具体的な行動計画が生成される

**行動プランナー  
**サブゴールを実現するための具体的な行動系列が作成される

上記の中で「心の理論（ToM）モジュール」が重要視されています。

仮説的思考モデルは、上記のモジュールの働きによって他のエージェントの戦略に関する仮説を継続的に更新し、それに基づいて自身の行動を適応させます。そうすることで、環境の変化や新しいエージェントの登場にも柔軟に対応できるとされます。

### 心の理論（ToM）モジュールの詳細

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72954_2-1024x617.jpg)

Running With Scissorsゲームにおける心の理論（ToM）モジュール。相手の戦略に関する仮説の生成、評価、改善のプロセスを5つのステップとする

本アプローチの中核を担っているToMモジュールでは、以下のプロセスが実行されます。  
正確には、下記プロセスのためのプロンプトでLLMを実行します。

（１）仮説の生成  
他のエージェントの行動履歴に基づいて、その戦略や目標に関する仮説が自然言語で生成されます。

（２）仮説の評価  
生成された仮説の妥当性が、他のエージェントの将来の行動の予測精度に基づいて評価されます。

（３）仮説の改善  
評価結果に基づいて、仮説が逐次的に改善されます。

（４）高レベル計画の条件付け  
選択された最良の仮説に基づいて、エージェント自身の高レベルの行動計画が調整されます。

### サブゴールモジュールと行動プランナーについて

サブゴールモジュールは、ToMモジュールで生成された高レベルの計画を具体的なサブゴールの系列に分解します。サブゴールは、LLMを用いて生成されます。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72954_8.png)

サブゴールモジュールのシステムプロンプト

行動プランナーは、サブゴールを実現するための具体的な行動系列を生成します。例えば、ある地点への移動や、他のエージェントとの相互作用などが含まれます。

### 仮説評価メカニズム

仮説の評価には、認知モデリングのアプローチが採用されました。レスコラ・ワグナー学習則に基づく価値更新メカニズムにより、仮説の予測精度に基づいて各仮説に価値が割り当てられ、最も説明力の高い仮説が選択されます。

## 実験

仮説的思考モデルの性能評価のため、「 [Melting Pot](https://github.com/google-deepmind/meltingpot) 」ベンチマークの4つの環境が使用されました。競争的、協調的、混合動機の合計30の異なるシナリオで評価されました。

競争的シナリオとは各エージェントが自身の利益を最大化しようと競い合う状況、協調的シナリオはエージェント同士が協力して共通の目標を達成しようとする状況、そして混合動機シナリオは協力と競争の要素が混在し、エージェントが個人と集団の利益のバランスを取る必要がある状況です。

（重要なポイントとして）エージェントには事前に相手の戦略に関する情報が与えられず、エピソード内でのオンライン学習が求められました。

### ベースラインモデル

仮説的思考モデルの性能を比較するため、以下のベースラインモデルが導入されました。

**[ReAct  
](https://arxiv.org/abs/2210.03629)**LLMを用いて推論と行動生成を交互に行うモデルです。サブゴールモジュールのみを持つ仮説的思考モデルの簡略版と見なすことができます。

**[Reflexion  
](https://arxiv.org/abs/2303.11366)**行動生成、評価、自己反省の3つのモジュールから構成されるモデルです。評価結果を基に建設的なフィードバックを生成し、後続の行動に活用します。

**PlanReAct  
**階層的な構造を持つモデルで、高レベルの計画を言語で生成し、それに基づいてサブゴールを設定します。仮説的思考モデルとの主な違いは、心の理論モジュールの有無です。

**PPO（近位方策最適化）  
**モデルフリーの [強化学習](https://ai-data-base.com/archives/26125 "強化学習") アルゴリズムです。同じパラメータを持つモデルの集団で学習が行われました。

### 競争的環境での実験

「Running With Scissors in the Matrix Repeated (RWS)」という環境で実験が行われました。

2人のプレイヤーが競争するゲームで、じゃんけんのルールを基にしています。プレイヤーは3色（黄、紫、青）のリソースを集めながら移動し、勝敗を決めます。

シナリオは9つ、各シナリオで単純な戦略から複雑な戦略まで様々な対戦相手が用意されました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72954_7.jpg)

「RWS Arena」におけるシステムプロンプト

仮説的思考モデルは、全てのシナリオで最高の成績を収めました。特に、変化しない戦略の相手に対して強く、一度相手の作戦を見抜くと、それをうまく利用し続けることができました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72954_4-1024x542.png)

Running With Scissorsゲームにおいて、仮説が検証閾値を超える前後のHypothetical Mindsの報酬を示し ている

さらに、8人で競争する「RWS Arena」という難しい環境でも実験が行われました。様々な戦略を持つプレイヤーが同時に存在するため、より複雑な判断が必要です。この環境でも、仮説的思考モデルは他のモデルより優れた結果を示しました。特に、似た戦略を持つグループに対して強さを発揮しました。

これらの結果から、仮説的思考モデルが競争的な環境で相手の戦略を上手く推測し、適切に対応できることが分かりました。特に、全体的な計画と具体的な行動のバランス、そして過去の経験を活かす能力が重要だと分かりました。

### 協調的料理環境での実験

「Collaborative Cooking: Asymmetric」という環境で実験が行われました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72954_14.png)

「Collaborative Cooking: Asymmetric」のシステムプロンプト

2人のプレイヤーが協力してトマトスープを作ります。キッチンは二つに分かれており、各プレイヤーは異なる利点を持っています。高得点を取るには、自分の位置に合わせて役割分担をする必要があります。

実験では、主となるエージェントに次の2つの適応が求められました。

1. キッチン内での自分の位置に合わせた役割の選択
2. パートナーの料理の腕前に合わせた対応

結果として、仮説的思考モデルが全てのシナリオで最高得点を獲得しました。中でも、有能なパートナーがいる場合に高い成績を示しました。ということは、パートナーの能力を見極め、それに合わせて行動を調整できる能力がある可能性が示されています。

他のLLMベースのモデルは、パートナーが不器用な場合に比較的良い成績を出しました。パートナーを無視して単純な作業を繰り返すだけでも上手くいく場合があるためです。なお強化学習モデル（PPO）は、熟練したパートナーとは上手く協力できましたが、不器用なパートナーへの対応が苦手でした。

### 混合動機環境での実験

「Prisoner’s Dilemma in the Matrix Repeated」という環境で実験が行われました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72954_13.png)

「Prisoner’s Dilemma in the Matrix Repeated」のシステムプロンプト

繰り返し行われる囚人のジレンマゲームです。プレイヤーは「協力」か「裏切り」のリソースを集め、その組み合わせで結果が決まります。1回のゲームでは裏切るのが得策ですが、長期的には協力し合うのが最も良い結果をもたらします。

仮説的思考モデルは、全体的に最高の成績を収め、10種類のシナリオのうち半分で最高の性能を示しました。中でも、状況に応じて戦略を変えるパートナーに対して、他のモデルより上手く対応できました。

例えば、「目には目を」戦略（Tit-for-Tat）のパートナーに対しては、一貫して協力しつつも適度に許す姿勢を見せ、お互いの裏切りが続く悪循環を避けることができました。

また、自分の利益ばかりを追求するパートナーに対しては、最初は裏切ることで相手の行動を変えさせ、その後は互いに利益を得られる協力関係を築く戦略を取りました。

上記の結果から、仮説的思考モデルが様々な状況に柔軟に対応し、長期的に良い結果を生み出す能力を持っていると推察されます。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72954_5-1024x302.png)

ベースとなるLLMの比較

### アブレーション実験

モデルの各部分がどれほど重要かを調べるため、いくつかの実験が行われました。中でも「心の理論」に注目し、繰り返しじゃんけんの環境で詳しく調べました。

主に以下の4つのバージョンが比較されました。

（１）最も単純なバージョン（VMP）  
相手の心を一度に推測しようとする単純な方法

（２）少し複雑になったバージョン（MMP）  
相手の心の推測を複数の段階に分けて行う方法

（３）別の方法で複雑にしたバージョン（HE+HR）  
相手の行動から直接仮説を立てて評価・改善する方法

（４）完全な仮説的思考モデル（MMP + HE + HR）  
モジュール化された心の推測と仮説評価・改善を組み合わせた総合的な方法

結果として、モジュール化された心の推測が基本的な心の推測より多くの場面で優れていることが分かりました。また、仮説評価を加えたバージョンと完全版は全ての場面で安定して高い得点を獲得し、特に難しい相手に対して強さを発揮しました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72954_6-1024x483.png)

Hypothetical Mindsの異なるバージョンの比較

上記の結果から、モデルを細かい部分に分けることと、仮説を評価することが、安定した推論と良い戦略を生み出すのに重要だということが分かりました。

一方で、心の推測を省いた仮説評価と改善モデルは、戦略を変える相手には強かったものの、戦略を変えない相手には弱いという特徴がありました。

実験を通じて、仮説的思考モデルにおいてはすべてのモジュールが重要であり、それらが組み合わさることで全体的な性能が向上することが明らかになりました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72954_3-1024x538.jpg)

全モデルの結果。各環境とシナリオにおける1エピソードあたりの平均報酬を表す

## まとめ

本記事では、マルチエージェント環境で適応的な意思決定を実現する『仮説的思考モデル』の研究を紹介しました。

LLMを活用し、「心の理論」モジュールを中核とする本手法は、多様なシナリオで優れた性能を示しました。中でも、動的な戦略への適応能力が際立ちました。

ただし人間の介入や事前知識の必要性など制限もあります。今後はより自律的な学習能力の開発が期待され、複雑な社会的相互作用を要する実世界の問題解決にも応用できる可能性があります。

- 参照論文URL： [https://www.arxiv.org/abs/2407.07086](https://www.arxiv.org/abs/2407.07086)
- コード： [https://github.com/locross93/Hypothetical-Minds/](https://github.com/locross93/Hypothetical-Minds/)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[競争環境でのLLMエージェントが自発的に協力し始める現象を観測](https://ai-data-base.com/archives/72854)

[エージェントなしで行うLLMによるソフトウェアのバグ修正手法](https://ai-data-base.com/archives/73060)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)