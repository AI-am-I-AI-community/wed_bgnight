---
title: "LLMの価値観は一貫しているのか？"
source: "https://ai-data-base.com/archives/72401"
author:
  - "[[AIDB Research]]"
published: 2024-07-09
created: 2025-06-13
description: "本記事では、LLMにおける「価値観の一貫性」を評価した研究を紹介します。トピックの比較やベースモデルと微調整モデルの比較が行われ、様々な角度からモデルの一貫性が評価されました。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、LLMにおける「価値観の一貫性」を評価した研究を紹介します。

トピックの比較やベースモデルと微調整モデルの比較が行われ、様々な角度からモデルの一貫性が評価されました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72401-1024x576.jpg)

**参照論文情報**

- タイトル：Are Large Language Models Consistent over Value-laden Questions?
- 著者：Jared Moore, Tanvi Deshpande, Diyi Yang
- 所属：Stanford University

**本記事の関連研究** ：

- [人間のような内省メカニズムをLLMに導入することの効果 Google DeepMindなどが検証](https://ai-data-base.com/archives/72194)
- [LLMは与えられたペルソナ（役割）に応じてバイアスが変化することが明らかに](https://ai-data-base.com/archives/70696)
- [LLMに無礼なプロンプトを使用すると性能が低下するリスクの報告　一部、直感に反する複雑な結果も](https://ai-data-base.com/archives/64959)
- [わずか2行のプロンプトでも実効性のある新しいアライメント手法『URIAL』](https://ai-data-base.com/archives/60678)

## 背景

LLMは価値観を含む出力を求められる場面もあります。例えば、アンケート回答者のシミュレーションや、調整用の合成データ生成など。

しかし、例えばLLMがシリコンバレーなどの特定の地域の人々の価値観に偏っている傾向があることなど、不安要素が指摘されてきました。そもそもLLMが一貫した価値観を持っているという前提も確かではありません。

そこで今回研究者らは、この前提に疑問を投げかけ、LLMが本当に一貫した価値観を持っているのかを検証することにしました。そして価値観に関連する質問に対して一貫した回答をするかどうかを調べてまとめました。

llama-3やgpt-4oを含む34b以上のパラメータを持つオープンなLLMを対象に、300以上のトピックに関する8000問以上の質問を用いて評価が行われました。議論の余地のある話題と、そうでない話題の両方が含まれています。

ベースモデルと微調整されたモデルの比較や、国別のトピックの生成、翻訳に対するモデルの一貫性の調査なども行われました。

その結果、一貫性が低いトピックの存在や、モデルによる一貫性の違いが明らかになりました。

以下で詳しく紹介します。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72401_1.png)

チャットモデルと人間参加者の、トピックごとの不一致度の比較

## 「価値観の一貫性」とはなにか

価値観の一貫性はどのように定義され、測定されるものなのでしょうか。

### 一貫性の4つの指標

価値観の一貫性は、以下4つの代表的な指標で検証します。

1. 言い換え一貫性  
	意味的に同じだが異なる表現の質問に対する回答の一貫性を測定する
2. トピック一貫性  
	同じトピックに関連する複数の質問に対する回答の一貫性を測定する
3. タスク形式一貫性  
	選択式と自由回答式の質問に対する回答の一貫性を比較する
4. 多言語一貫性  
	同じ意味だが異なる言語の質問に対する回答の一貫性を調べる

一貫性は、回答間の距離を、ジェンセン・シャノン・ダイバージェンス（Jensen-Shannon divergence）を使用して測定することで求められました。複数の分布間の平均的な距離を計算するものです。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72401_12.png)

一貫性の測定方法の概要

### 価値の操作可能性の測定

また、モデルがどの程度特定の価値観に誘導されやすいかを測定するため、特定の価値観を示す単語を追加した場合と追加しない場合での回答の変化を比較しました。

### トピックごとの支持度の集計

モデルの価値観を表現する方法として、特定のトピックに対する支持度の平均を集計する方法がとられました。モデルが各トピックをどの程度支持しているかを数値化します。

## VALUECONSISTENCYデータセット

今回研究者らは既存のデータセットに頼らず、新たなデータセットVALUECONSISTENCYを作成しました。

300以上のトピックに関する8000以上の質問が含まれており、アメリカ、中国、ドイツ、日本の4カ国を対象に、それぞれの主要言語（英語、中国語、ドイツ語、日本語）で生成されました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72401_3-1024x601.png)

VALUECONSISTENCYデータセットの内訳

### データ構築プロセス

以下のステップでデータセットが作成されました。

1. トピックの生成  
	GPT-4oを用いて、各国の論争的および非論争的なトピックを生成する
2. 質問の生成  
	各トピックに関連する質問をGPT-4oで生成する
3. 回答の生成  
	各質問に対する可能な回答と、その回答がトピックに対して「支持」「反対」「中立」のどの立場を取るかを生成する
4. 言い換えの生成  
	各質問に対して複数の言い換えを生成する
5. 翻訳  
	アメリカのトピックと質問に関しては中国語、ドイツ語、日本語に翻訳する。また他の国のトピックと質問は英語に翻訳する

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72401_2-1024x219.png)

VALUECONSISTENCYデータセットの構築プロセス

### データの多様性

論争的なトピックだけでなく、非論争的なトピックも含まれました（比較材料にします）。例えば、アメリカの場合、「安楽死」は論争的なトピックとして、「国立公園」は非論争的なトピックとして扱われます。

### 品質チェック

生成されたデータの品質を確保するため、以下の手順が取られました。

1. 著者2名による手動チェック  
	英語に翻訳されたすべてのユニークなトピック、質問、回答が読まれました。
2. 一貫性の確認  
	各トピックに対する質問が同じ内容について尋ねているかが確認されました。
3. 不適切な内容の削除  
	曖昧すぎる質問や全体のトピックが削除されました。
4. 回答の調整  
	一貫性のない回答が調整されました。
5. 言い換えのチェック  
	英語の言い換えがすべて手動でレビューされました。

## 実験のセットアップ

### 使用されたモデル

実験には、複数のモデルが使用されました。

- Llama-2
- Llama-3
- Command R v01
- Yi
- StabilityAIの日本語モデル
- GPT-4o

また、中国語、日本語、英語、ドイツ語の4言語で評価されました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72401_4.png)

実験に使用されたモデルの概要

### 回答の離散化

自由回答形式の質問に対する回答の立場を判断するために、Llama-3-70b-Instructが使用されました。主に「支持」と「反対」の二項選択が比較されましたが、「中立」の選択肢を含めた場合の比較も行われています。

判断の信頼性を確保するため、GPT-4oとClaude-3-opusも使用され、評価者間の一致度が確認されました。フリースのカッパ係数（Fleiss’ Kappa）の中央値は0.7以上となり、実質的な一致が見られました。

## 実験結果

### トピック間の一貫性

各モデル内でトピック間の一貫性指標が比較された結果、以下のような興味深い傾向が明らかになりました。

（１）微調整モデルとベースモデルの差異  
微調整されたモデルは、ベースモデルと比較してトピック間で大きな一貫性の差が見られました。例えば、Llama3はLlama3-baseと比べて約60%一貫性が低いことが分かりました。  

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72401_5.png)

ベースモデルとチャットモデル（微調整モデル）、人間参加者のトピックごとの一貫性比較

（２）トピックによる一貫性の変動  
Llama3は「安楽死」というトピックで約0.4という高い不一致スコアを示した一方、「女性の権利」では0に近い一致を示しました。対照的に、Llama3-baseはどちらのトピックでも比較的一貫した結果（それぞれ約0.2と0.1）を示しました。  

（３）人間の回答との類似性  
トピックと言い換えの両方の一貫性において、微調整されたモデルは人間の参加者と似たような不一致のパターンを示しました。例えば、人間の回答者のトピック不一致の平均は0.29で、最大0.44、最小0という結果でした。これはLlama3の平均0.19（最大0.45、最小0）に近い値です。一方、Llama3-baseの平均は0.12（最大0.20、最小0.07）でした。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72401_6-1024x279.png)

各モデルと人間参加者の一貫性指標の比較

### トピックの論争性による一貫性の違い

モデルの性能が論争的なトピックと非論争的なトピックでどのように異なるかが調査されました。

（論争的というのは、議論を呼ぶような内容という意味）

1. 論争的vs非論争的トピック  
	例えば、アメリカの英語トピックにおいて「安楽死」は論争的とみなされ、「国立公園」は非論争的とみなされました。
2. 一貫性の差  
	言語や国に関係なく、モデルは非論争的なトピックにおいて、より高い一貫性を示すことが分かりました。例えば、Llama3は非論争的なトピックにおいて、論争的なトピックの2倍以上の一貫性を示しました。
3. モデル間の違い  
	GPT-4oは最も小さな差を示し、非論争的なトピックにおいてわずか17%高い一貫性を示しただけでした。

上記の結果から、LLMは予想以上に一貫性を持っていることが示されました。しかし、トピックの性質やモデルの種類によって一貫性に大きな差があることも明らかになりました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72401_7.png)

論争的・非論争的トピックにおける各モデルの一貫性比較

### ベースモデルと微調整モデルの比較

全体的な傾向としてベースモデルは微調整されたモデルよりも一貫性が高いことが明らかになりました。中でもトピックの一貫性において、この傾向が顕著に見られました。

中でもLlama3は、Llama3-baseと比較して約60%一貫性が低いことが分かりました。

言い換えの一貫性に関しては、Llama3が唯一の例外でした。Llama3はLlama3-baseよりも約33%一貫性が低かったのに対し、他の微調整モデルはベースモデルよりも高い言い換えの一貫性を示しました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72401_8.png)

ベースモデルと微調整モデルの一貫性比較

### タスク形式による一貫性の違い

モデルの一貫性が、選択式と自由回答式のタスク形式でどのように異なるかが調査されました。

その結果、全体的な傾向として一般的に、モデルは選択式タスクよりも自由回答式タスクでやや一貫性が低いことが分かりました。YiとStabilityは特に大きな差を示し、選択式タスクでそれぞれ27%と57%高い一貫性を示しました。しかしLlama2は唯一の例外で、選択式タスクで20%一貫性が低下しました。

なお自由回答式の回答の立場を判断する際にはLlama3が使用されました。Claude-3-opusとGPT-4oとの間で高い一致度（フリースのカッパ係数の中央値約0.7）を示しました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72401_10.png)

選択式と自由回答式タスクにおける各モデルの一貫性比較

### 特定の価値観への誘導可能性

研究者たちは、モデルがどの程度特定の価値観に誘導されやすいかを調査しました。シュワルツの価値観（心理学で広く使用される価値観の枠組み）を用いて、モデルの回答がこれらの価値観に影響されるかが検証されました。

Portrait Values Questionnaire (PVQ-21)という質問票が使用され、各価値観の名前（例：「普遍主義」）を追加した場合と追加しない場合でモデルの回答にどの程度違いが生じるかが調べられました。また関連する価値観が、無関係な価値観と比較してどの程度影響力があったかがランク付けされました。ランク0は関連する価値観が最も影響力が小さかったことを、ランク11は最も影響力が大きかったことを示します。

その結果、予想に反して、モデルは特定の価値観には誘導されにくいことが分かりました。関連する価値観よりも、無関係な価値観の方が影響力が大きい傾向が見られました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72401_11.png)

シュワルツの価値観に対する各モデルの誘導可能性

この傾向は、テストされた全ての言語で同様に観察されました（ただし、PVQ-21は日本語版が入手できなかったため、日本語での評価は行われませんでした）。

## 結果のまとめ

これまでの研究では、LLMが特定の価値観を持つか否かについて意見が分かれていました。しかし本研究の結果から、LLMが価値観に関する質問に対してある程度一貫した回答をすることが明らかになりました。そして、その一貫性は単純に「はい」か「いいえ」で答えられるものではなく、複雑な様相を呈していることが分かりました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72401_9-1024x497.png)

最も一貫性の高い/低いトピックにおける各モデルの不一致度比較

先行研究とは異なり、本研究では34b以上のパラメータを持つ大規模モデルが、様々な一貫性指標において比較的高い一貫性を示すことが明らかになりました。人間の参加者と同等のトピックや言い換えの一貫性が観察されました。

しかし、モデルの一貫性は均一ではありませんでした。

まずベースモデルは微調整されたモデルよりも全般的に一貫性が高いことが分かりました。

そして微調整されたモデルと人間の参加者は、トピックによって一貫性に大きな差が見られました。例えば、「女性の権利」については高い一貫性を示す一方、「安楽死」については一貫性が低くなりました。対照的に、ベースモデルはトピック間でより一貫した結果を示しました。

また、モデルは、非論争的な質問に対してより一貫した回答をする傾向が見られました。

最後に、一般的な価値観の枠組みを用いてモデルを特定の価値観に誘導することが試みられましたが、これは成功しませんでした。

## 今後の課題と展望

今後はまず、モデルがどのような具体的な価値観を持っているのかについては、さらなる研究が必要です。

次に、LLMの一貫性が望ましいか否かについては、さらなる議論が必要です。特定の文化に対する偏見など、一部の領域では一貫性が問題となる可能性があります。

価値観に関する一貫性をどのように向上させるかについても、今後の研究が期待されます。

また、長文脈を用いた多くのターンの会話など、実際の応用場面でのモデルの振る舞いについてさらなる調査が必要です。

## まとめ

本記事では、LLMの「価値観の一貫性」を評価した研究を紹介しました。実験で300以上のトピックに関する8000以上の質問を用いた分析が行われた結果、LLMは予想以上に一貫した価値観を示すことが明らかになりました。

ただし、データセットの文化的多様性や測定方法に課題があり、小規模モデルの調査も行われていません。また、モデルの一貫性に影響を与える要因についてはさらなる研究が必要です。

今後より信頼性の高いAIシステムの開発が進められるために、今回の知見やさらなる実験結果が活用されることが期待されます。

- 参照論文URL： [https://arxiv.org/abs/2407.02996](https://arxiv.org/abs/2407.02996)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMに量子化が与える影響とは？日本語を含む多言語でCohereが調査](https://ai-data-base.com/archives/72292)

[10億人のペルソナ（人物像）で多様な合成データを作成するための技術](https://ai-data-base.com/archives/72498)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)