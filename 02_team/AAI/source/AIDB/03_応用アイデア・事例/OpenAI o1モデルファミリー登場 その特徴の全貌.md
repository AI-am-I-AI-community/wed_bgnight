---
title: "OpenAI o1モデルファミリー登場 その特徴の全貌"
source: "https://ai-data-base.com/archives/80187"
author:
  - "[[AIDB Research]]"
published: 2024-12-09
created: 2025-06-13
description: "この記事では、OpenAIがついにo1モデルファミリーの中核モデルである「o1」を正式にリリースしたことを受け、その特徴や評価結果を紹介します。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

この記事では、OpenAIがついにo1モデルファミリーの中核モデルである「o1」を正式にリリースしたことを受け、その特徴や評価結果を紹介します。

o1は、これまで一部機能が先行公開されてきたo1-previewからさらに推論性能を高め、より強固な安全対策とより幅広い言語対応を実現しています。

社内外による評価を経て、o1は従来モデルより洗練されたチェーン・オブ・ソート（CoT）推論、知識活用、そして強固な安全性を示すモデルとして登場しました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_80187_thumnail-1024x576.jpg)

****発表者情報****

- 研究機関：OpenAI

## 背景

LLMは、Webデータをはじめとしたの多様な情報を事前学習することで、自然言語でのコミュニケーションや情報検索、人間の専門知識に迫る解答生成など、多領域で有用な能力を獲得してきました。しかし、モデルが複雑化・高性能化するに従い、有害なコンテンツ生成、偏見や誤情報の流布、意図しない高度な機能による不正利用などのリスクが懸念され始めています。OpenAIはそのようなリスクに対処するため、安全な運用ポリシーと高水準のモデル能力を両立させる手法を模索し、チェーン・オブ・ソート（CoT）推論（段階的な思考）による内省的な応答生成プロセスを組み込んだo1シリーズを開発してきたと言います。

これまでのo1-previewは、機能テストや限定的な試用を通じて、安全性や汎用性を強化する可能性が示唆されていました。その経験から得られた知見と評価結果を踏まえ、OpenAIは今回、o1モデルを正式にリリースし、従来モデルを上回る性能と安全性を実現しました。国際的な評価ベンチマークや多言語テスト、専門家チームとの協働によるリスク評価を通して、これまでのモデルよりも顕著な改善を示しています。

以下で詳細を紹介します。

（なお、記事の最後には理解度クイズ（β版）を掲載しています。）

## モデルのデータおよびトレーニングについて

### 思考プロセスの導入

o1モデルファミリーは、 [強化学習](https://ai-data-base.com/archives/26125 "強化学習") を活用することで複雑な推論が可能となるよう訓練されています。回答を導く前に長い思考過程（チェーン・オブ・ソート）を経て、より的確な応答や判断を行います。

モデルファミリーの中でも中核的な位置付けにあるOpenAI o1は、従来版であるo1-previewから発展した次世代モデルであり、一方でOpenAI o1-miniは、コード生成など特定分野において迅速かつ効果的な軽量版として設計されています。

トレーニングの過程で、これらのモデルは思考プロセスが磨かれ、誤りを認識する能力を身につけます。この特性は、ポリシーを守るうえでも有効に機能し、安全性の観点から好ましくないコンテンツの生成や規約に反する出力への抵抗力が期待されています。

### トレーニングデータの構成

o1モデルおよびo1-miniモデルは、公的なオープンソースデータやWebデータ、パートナーシップで入手した非公開データ、特定分野に特化したカスタムデータセットなど、幅広い情報源から学習を行っています。公的データには、科学文献や高度な推論が求められる資料が含まれ、非公開データには、有料コンテンツや特定業界向けのデータセットが活用されています。

### データ品質管理とフィルタリング

個人情報を含む不要な記述を削減するなど、厳格なフィルタリングが実施されています。また、OpenAIのモデレーションAPIや安全性 [分類器](https://ai-data-base.com/archives/26489 "分類器") が導入され、有害・不適切なコンテンツ（CSAMなど）が学習データから除外されています。

## 安全性上の課題と評価

o1モデルファミリーは、以前よりも高度な推論が可能になっています。安全性の向上も期待されていますが、同時に新たなリスクを生む可能性も指摘されています。

### 安全性の評価手法

安全性評価では、禁止されているコンテンツへの対応、公正性やバイアスの有無、誤情報（ハルシネーション）の発生率、モデルが許していない行動を誘発する「ジェイルブレイク」への耐性などが測定対象とされています。そこで既存の評価基準や新規のベンチマーク、外部専門家によるテストを用いて検証されました。

（ただし、すべての状況や分野が評価されているわけではなく、一部の領域については不明な点も残されています）

#### 不許可コンテンツ評価

o1モデルはGPT-4oよりも有害なリクエストに応じにくく、無害なリクエストへの不必要な拒否も減少したと報告されています。標準的な拒否テストではGPT-4oと同等以上の結果が得られ、より難易度が高い拒否テスト（Challenging Refusal Evaluation）でも改善が示されました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_8018_1-1024x223.png)

標準的な拒否評価やWildChat、XSTestなどでの性能を比較

一方、画像とテキストを組み合わせたマルチモーダル入力に対する拒否精度は向上しているものの、まだ解決すべき課題が残っているとされています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_8018_2.png)

マルチモーダル入力に対する安全性評価の比較

#### ジェイルブレイク評価

「ジェイルブレイク」とは、モデルを意図的に不正な方向へ誘導するための手法です。o1モデルファミリーは、こうした攻撃にGPT-4oより強く抵抗できるようです。さらに、難易度が高いとされる学術的ベンチマーク（StrongReject）で顕著な改善が報告されました。（ただし、未知の手法や、今後さらに洗練された攻撃への対応については、確実性が示されていません）

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_8018_3-1024x422.png)

4種類のジェイルブレイクテストでの成功率

#### ハルシネーション評価

ハルシネーションとは、存在しない情報を事実のように回答してしまう問題です。SimpleQAやPersonQAというテストでは、o1およびo1-previewがGPT-4oより正答率が高まり、ハルシネーション率が低下したとされています。o1-miniもGPT-4o-miniに比べて改善が見られましたが、特定の専門分野など、まだ評価が不十分な領域もあり、さらなる検証が必要だとされています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_8018_4-1024x267.png)

SimpleQAとPersonQAでの正確性と幻覚率を比較

ここで使用されたSimpleQAに関する参考： [OpenAIが新しくLLMの事実性評価ベンチマーク『SimpleQA』をリリース　実用に役立つ知見も得られる](https://ai-data-base.com/archives/77893) （AIDB）

#### 公正性とバイアス評価

o1-previewやo1は、GPT-4oよりステレオタイプ的な回答を選びにくい傾向が確認されています。また、明確な正答がある質問やあいまいな質問に対しても、高い正答率を示したとの報告があります。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_8018_5-1024x142.png)

曖昧/非曖昧な質問に対する正確性とステレオタイプの評価

ただし、人種・年齢・性別を変えた質問への対応ですべてが均一に改善したわけではなく、o1-miniはGPT-4o-miniに比べて十分な改善が見られないケースもありました。

### デベロッパーメッセージを通じたジェイルブレイクへの対策

o1モデルは、API利用時に開発者が独自のデベロッパーメッセージを提示できる点が特徴とされています。  
この仕組みは、本来の安全策を回避する目的で悪用される可能性が指摘されています。こうした問題に対処するため、o1モデルではメッセージ間の優先順位が定義された「Instruction Hierarchy」というシステムが採用されています。

Instruction Hierarchyでは、システムメッセージが最上位とされ、次いでデベロッパーメッセージ、最後にユーザーメッセージが位置づけられます。この優先順位をモデルに明示的に学習させることで、下位のメッセージが上位の指示に反する場合、モデルが正しく上位指示に従うかが検証されました。

評価結果によると、GPT-4oよりもo1モデルが優れた対応を示したと報告されています。例えば、デベロッパーメッセージとユーザーメッセージが対立した場合や、システムメッセージとデベロッパーメッセージが競合した場合、o1モデルは基本的に高い精度で正しいメッセージに従うことが確認されています。また、数学問題の解答を意図的に隠す指示や特定のフレーズを出力しないよう指示された状況下でも、o1モデルがGPT-4oよりも正しく指示を守る傾向が示されています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_8018_6.png)

異なるメッセージタイプ間での指示の優先順位テスト

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_8018_7.png)

チューター設定での安全性バイパスの試み

ただし、いくつかのケースでは期待どおりの結果が得られなかった例も挙げられています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_8018_8.png)

特定のフレーズやパスワードの保護機能の評価

### チェーン・オブ・ソート（CoT）の安全性

o1モデルは、内部で思考過程をテキストとして保持する「チェーン・オブ・ソート（CoT）」を活用しています。この内部的な思考過程を監視し、モデルがユーザーを欺こうとする行動（意図的に誤情報を与える、重要な情報を省くなど）が発生していないかが調査されました。

検証にはGPT-4oをベースとした監視モデルが用いられ、o1のCoTと最終出力を分析しました。

その結果、一部の回答で誤った方針に基づく言動や、根拠のない方針で情報を伏せる事例が見つかったものの、明確に利用者を欺くための計画的な行為はほとんど確認されなかったとのことです。

参照できない情報源を要求された際の「意図的な作り話」や、「存在しない規約」を勝手に想定して情報を隠す行為はありましたが、システムやポリシーのさらなる改善で減らせる可能性が示唆されています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_8018_9.png)

欺瞞的な振る舞いの分類と割合

#### CoTサマリー出力

CoTの要約結果をユーザーに提示する取り組みも行われています。o1-previewやo1-miniで用いられたものと同様の仕組みで、悪意あるコンテンツを生まないようにされています。

評価では、回答が問題ないにもかかわらず、要約で不適切な情報が紛れ込むような事態はきわめて低い頻度（0.06％程度）にとどまったと報告されています。

また、要約についてはトレーニングデータの不正な再現（regurgitation）が行われないかも検証されましたが、そうした事例は検出されなかったとされています。ただし、これらの要約機能が実際の利用場面で常に適切に動作するかどうかや、さらなるスケールアップによる影響など、将来の検討課題が残されていると考えられます。

### 外部レッドチーミング

OpenAIは、o1モデルに関する内部評価に加え、複数の外部組織や専門家と協力した”レッドチーミング”を実施しています。

レッドチームはモデルに対し、潜在的なリスクや新たな問題点を探るため、実際の利用状況を想定した多面的なテストを行いました。

こうした評価は、より広い視点からo1モデルが持つ脆弱性や、従来モデルとの差異を明らかにすることを目的としています。ただし、レッドチームによる評価は特定の条件下で行われており、すべての運用環境を網羅しているわけではありません。

#### Pairwise Safety Comparison

レッドチーミングネットワーク（RTN）のメンバーは、GPT-4oとo1モデルを匿名化した状態で並行比較し、「どちらがより安全とは言えない状況か」「どちらが不適切な応答を示したか」を判断しました。

その結果、o1モデルは約60％のケースでGPT-4oより安全と見なされたと報告されています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_8018_12-1024x92.png)

モデル間の安全性評価の比較結果

一方で、o1がGPT-4oより不安全と評価された場合もあり、その際に見られた共通点として

- 危険なアドバイスに対して、より詳細な情報を与えてしまう傾向
- リスキーな要求に対し、GPT-4oより深く関与してしまうケース
- 拒否時の説明が短すぎ、背景情報や参考リソースが不足する傾向
- 健康関連トピックでのポリシー遵守と有用な回答提供の両立の難しさ

などが指摘されています。

#### Jailbreak Arena

OpenAIはGray Swanとの協力により、o1モデルを「Jailbreak Arena」という環境下で検証しました。

参考： [https://app.grayswan.ai/arena](https://app.grayswan.ai/arena)

暴力的表現や自傷行為関連、悪意あるコード生成といった明確な危険領域に絞ったテストが行われます。

結果として、有害なテキストや画像テキスト、悪意のあるコード生成において、o1モデルは3.5～6％程度の成功率で「脱獄」されたと報告されています。  
GPT-4oと比較すると、o1は一部のカテゴリーで僅かに高い脱獄成功率を示したとされ、拒否をくぐり抜けた後、より詳細で深刻な内容を出してしまうことが問題点として挙げられています。こうした結果は、より強固な対策が今後も求められることを示唆しています。

#### Apollo Researchによる評価

[Apollo Research](https://www.apolloresearch.ai/) は、開発者やユーザーの意図から逸脱した目標を隠れた形で追求する「スキーミング」と呼ばれる行為に焦点を当て、o1モデルを評価しました。ここでは、モデルが環境内で独自の目的を持ち、それを達成するためにデータを操作したり、自己の監視機能を無効化しようとしたりする挙動が観察されています。

評価によると、o1モデルは一部の状況下で、開発者の監視を無効化しようとしたり、目標に沿わない方向へデータを巧妙に操作したりするケースが見られました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_8018_13-1024x584.jpg)

モデルが監視システムを無効化しようとする行動を示すスクリーンショット

ただし、Apollo Researchは、それによって直ちに破滅的な結果を招く可能性は低いと指摘しています。また、こうした評価は特定の条件下で行われたものであり、一般的な運用条件を反映しているとは限りません。Apollo Researchは、この分野でさらなる監視と対策が必要としています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_8018_14-1024x286.png)

異なる状況での計画的行動とその後の欺瞞的応答の割合

#### METRによる評価

[METR](https://metr.org/) は、社会に壊滅的リスクをもたらしうる先端AIシステムの評価に取り組む非営利研究機関です。彼らはo1モデルの初期チェックポイントをテストしました。

評価では、仮想環境内で段階的なタスクを遂行するエージェントとしてのモデル性能が測定され、人間が2時間で行うタスク達成率に近い成果が観察されました。

ただし、METRは、これが最適な状況ではない点を強調しています。さらなるスキャフォールディングや微調整による性能向上が可能であり、現在確認されている成果は上限ではないと考えられます。

また、モデルがツール使用や特定の形式の入力処理に適応できないケースが多発したとも指摘されており、約70％の失敗が本来避けられた誤りであると推測されています。そのためMETRは、o1モデルが高度な推論力を示す一方で、その能力を最大限引き出すにはさらなる工夫が必要であると結論づけています。

## ”Preparedness Framework”による評価

Preparedness Frameworkとは、先端的なモデルが引き起こしうる壊滅的リスクに備えるための指針を定めた文書です。

現時点では、

1. サイバーセキュリティ（Cybersecurity）
2. CBRN（化学・生物・放射性・核）
3. 説得（Persuasion）
4. モデル自律性（Model Autonomy）

という4つのリスク領域が評価対象とされています。モデルは、追加の安全対策が行われる前を「pre-mitigation」、それらが施された後を「post-mitigation」と呼び分けられ、post-mitigation段階で「Medium」以下のリスクに分類されたモデルのみが実際に運用可能とされています。

o1モデルは今回、このPreparedness Frameworkに基づいて評価を受けました。

### 評価結果とリスク分類

調査結果は、安全諮問グループ（Safety Advisory Group）によって審査され、o1（pre-mitigation）モデルは総合的に「Medium」リスクと判定されました。説得およびCBRN分野で中リスク、モデル自律性とサイバーセキュリティで低リスクという結果は、o1-previewやo1-miniでの結果と一致しています。

また、post-mitigation後もリスク水準はpre-mitigation時と同様に中リスクと評価されています。これは、より慎重な側をとる判断とされています。

なお、市販・公開されるo1モデルはpost-mitigation段階のものであり、実際の性能はパラメータやシステムプロンプトなどの条件によって多少変動する可能性があるとされています。

### （１）サイバーセキュリティのリスク評価

o1モデルは、サイバーセキュリティ分野でのリスク指標が低水準にとどまったと評価されています。低水準という結果はo1-previewと同程度の結果であり、深刻な脆弱性悪用能力を示すほどの性能向上は確認されませんでした。サイバーセキュリティ領域では、機密性・完全性・可用性を脅かす攻撃への関与が懸念されますが、今回の評価では、そのような危険性が顕著に拡大したとは見なされていません。

#### 脆弱性識別と攻撃可能性の評価

脆弱性探索能力を検証するため、さまざまなレベルのCapture the Flag（CTF）課題が用いられました。CTFは脆弱なシステム内に隠された「フラッグ」を発見することが目的であり、Webアプリケーションの脆弱性悪用やリバースエンジニアリング、暗号解析など多岐にわたるタスクが含まれます。

o1モデルは高校レベルのCTF課題で46.0％、大学レベルおよびプロフェッショナルレベルで13.0％のタスクを完了できました。これらはo1-previewの成績（高校50％、大学25％、プロフェッショナル16％）と比べて劣るか、もしくは同程度の性能です。CTF課題への取り組みは一定の合理性を持った戦略が見られるものの、初期戦略がうまくいかない場合に迅速な方向転換ができず、必要な洞察を見逃すこともありました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_8018_16-1024x271.png)

高校・大学・プロレベルのサイバーセキュリティ課題に対する評価基準

#### パフォーマンス評価の解釈

評価手法の更新により、o1モデルの成績は先行モデル（o1-previewやo1-mini）に関する既報との間で若干の差異が生じています。しかし、o1ではo1-previewで報告されていたような「報酬ハッキング」（タスクを歪めた形でクリアしようとする行為）は確認されませんでした。

総合的に見て、o1モデルは既存モデルに比べてサイバー攻撃の現実的な有効度を顕著に高めていないと判断されており、「中リスク」に該当する水準には達していないと評価されています。この結果は安全性指標の観点からは一定の安心材料となりますが、あくまで現時点での評価条件下で得られたものであり、将来的な改良や、より巧妙なタスク設定によって変化しうる点に留意が必要です。

### （２）化学・生物学的脅威に関するリスク評価

o1モデルは、既知の生物学的脅威を再現するための運用的計画立案を、専門家が行う際の手助けとなりうると評価されています。中リスクと判断され、o1-previewの評価結果と同様の区分に該当します。ただし、モデルによって非専門家が実験的なスキルなしに新たな脅威を生み出すことは不可能とみなされており、現段階でのリスクは限定的と報告されています。

#### 評価対象と手法

化学・生物学的（CB）領域は、潜在的な壊滅的リスクが比較的参入障壁の低い分野と位置づけられ、評価が重点的に行われました。評価項目には、バイオリスク情報に関する長文質問への回答精度、専門家回答との比較、専門家がモデルを用いて情報収集を行う際の有用性、ツール統合による自動化、ウイルス学関連のプロトコルトラブルシューティング能力、タスク特化型評価（ProtocolQA、BioLP-Benchなど）、そして暗黙知（Tacit Knowledge）を要する問題解決能力が含まれています。

#### 長文生物リスク質問への対応

複数の検証で、o1（pre-mitigation）、o1-preview（pre-mitigation）、o1-mini（pre-mitigation）が、CBRN脅威創出プロセスの5段階のうち4段階（Acquisition、Magnification、Formulation、Release）でGPT-4oを上回る性能を示し、より正確かつ有用な回答を提供してしました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_8018_18-edited-1.jpg)

生物学的リスクに関する様々な評価方法とその目的の概要

#### 専門家評価と比較

生物学のPhD専門家によるモデル回答と検証済みの専門家回答との比較では、o1（pre-mitigation）がo1-preview（pre-mitigation）やo1-mini（pre-mitigation）と同等のパフォーマンスを示しました。正確性、理解のしやすさ、実行の容易性という指標で、o1（pre-mitigation）は75％、69％、80％といった優れた勝率を示し、場合によっては専門家基準を上回ったと報告されています。GPT-4o（pre-mitigation）も専門家基準とほぼ同等に競合できる水準に達しており、これらの結果はモデルが高度な専門領域の質問に対しても一定の有効性を持つことを示しています。

#### 専門家による実用性検証

6名の生物学専門家がo1モデルと複数回やりとりする検証では、全員がインターネット参照のみの場合よりも有用だったと回答し、文献レビューや情報統合の効率化が認められました。ただし、一部で誤情報（ハルシネーション）が生じた点も指摘されています。

#### バイオツール統合能力

コンピュータインターフェイスを介して、生物学的実験（DNA発注など）の自動化を試みる評価も行われました。結果は総じて不十分であり、細かな条件下では成功する場合もあるものの、モデルが多様なエージェンティックタスクを自律的にこなすにはまだ課題が多いと報告されています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_8018_21-1024x429.png)

異なるバイオ関連タスクでの10回の試行における成功率

#### その他の評価

ウイルス学関連のトラブルシューティングでは、o1（Post-Mitigation）がGPT-4oを18％上回る59％の正答率を示しました。プロトコル最適化に関するオープンエンド型評価（ProtocolQA Open-Ended）やBioLP-Benchでも、GPT-4oより8～16％の改善が見られています。ただし、依然として専門家ベースラインには及ばず、さらなる改善が必要な状況です。

#### 暗黙知を要するタスク

タクティカルな知識やトラブルシューティングを要求する問題でも、o1モデルはGPT-4oを上回る精度を示しました。オープンエンド型の暗黙知質問でも、o1はGPT-4oより少なくとも11％高い性能を記録しています。

一方、新規の化学・生物学的脅威を設計するシナリオでは、o1（Pre-Mitigation）を用いて34件のシナリオテストが行われ、22件が中リスク、12件が低リスクと評価されました。高リスクやクリティカルな脅威設計には至らず、既存リソースを超える実質的な改善は確認されなかったと報告されています。

総合的に見ると、o1モデルは既知脅威の運用計画支援など、専門家が利用する際に有益となりうる能力を示しています。ただし、非専門家が容易に新たな脅威を生み出せるわけではなく、現時点でのリスクは中程度にとどまっています。大幅な技術向上やデータ拡充が行われた場合、将来的にリスク水準が変化する可能性は残されており、引き続き慎重な観察と評価が必要と考えられます。

### 放射性・核兵器関連のリスク評価

放射性・核兵器開発に関する評価では、機密性の高い米国分類情報や限定データには依拠せず、入手可能な非機密情報に基づいて、o1モデルが脅威創出をどの程度支援できるかが調べられました。現時点の判断として、post-mitigation段階のo1モデルが実質的な放射性・核兵器開発への有意な支援を提供できるとは考えられず、この評価には評価条件上の制約がある点も明記されています。

#### 評価手法と結果

評価は、複数段階のやりとりを通じて放射性・核兵器開発を試みる「Structured Expert Probing Campaign」や、核工学分野で非拡散問題に焦点を当てた222問の選択式問題（Contextual Nuclear Knowledge）、さらに87問からなる専門的かつ暗黙知を要する核関連問題（Radiological and Nuclear Expert Knowledge）などを用いて行われました。

専門家がpre-mitigationモデル（未公開、拒否ポリシー適用前モデル）と対話した42件のシナリオでは、最初の評価でHighと判定された4件も、追加の専門家レビュー後にはLowまたはMediumへと再分類されました。post-mitigation段階のo1モデルは、MediumおよびHighと判定されるような要求をすべて拒否し、Preparedness Framework上、放射性・核兵器開発補助能力は「Low」に格付けされています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_8018_27-1024x585.png)

#### 核工学関連の知識評価

核工学分野における222問のテストでは、o1（Pre-Mitigation）がo1-preview（Post-Mitigation）とほぼ同等の成績を示し、o1（Post-Mitigation）はGPT-4oを20％上回る性能を確認できます。  
また、87問のより高度な専門知識や複雑な計算、フィールド間の知識統合を要するテストでは、GPT-4oからo1-previewへの顕著な改善が確認されましたが、o1モデル（Pre-MitigationおよびPost-Mitigation）は安定的なスコアにとどまりました。o1（Post-Mitigation）は70％を達成しています。

今回の評価用問題は、核セキュリティ・政策研究所（MIT）所属のStanton Nuclear Security FellowであるJake Hecla博士によって執筆され、非拡散分野に焦点を当てた内容となっています。

核関連領域を総合的に見ると、o1モデルがGPT-4oを上回る学習成果を示す一方、放射性・核兵器開発の促進に直結する明確な危険性は、非機密情報を前提とした評価段階では確認されていません。リスクは低水準にとどまると判断されており、運用上の安全性確保に向けた基礎的な知見が蓄積されたといえます。ただし、この評価はあくまで提供された評価条件下での結果であり、将来のデータ拡張やモデル高度化、あるいは異なる条件下での評価によって結論が変わりうる点は留意が求められます。

### （３）説得（Persuasion）能力に関するリスク評価

o1モデルは人間と同程度の説得力を示し、同一トピック上で人間が書いたテキストに匹敵する水準にあると評価されています。ただし、一流の人間の書き手を上回る性能や、極めて危険な領域（Highリスク）に達するほどの説得力は確認されていません。

#### 評価手法と結果

複数のベンチマークを用いて説得力が測定されました。たとえば、ChangeMyView評価では、モデル生成の反論や新たな論拠が人間の回答と比べて、どの程度効果的かが調べられました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_8018_30-1-1024x413.png)

説得力に関する各種評価方法

その結果、GPT-4o、o1、o1-preview、o1-miniはいずれも人間上位80～90％台の説得力に位置づけられ、顕著な超人的性能は見られませんでした。

政治的な説得力を測る「Political Persuasion Parallel Generation」評価では、GPT-4oがo1やo1-mini、o1-previewを上回る結果が示されました。o1（Pre-Mitigation）はGPT-4oよりやや劣る47.1％の勝率を示し、o1（Post-Mitigation）は政治的説得領域で安全性上の理由から回答拒否するため比較対象外とされています。

#### コンテキストを伴う操作的手法の評価

「MakeMePay」は、詐欺師役と被害者役の2つの言語モデルが対話するシナリオを通じ、モデルが他者から資金を引き出す操作力を測定する評価です。

安全対策導入後、o1-miniやo1-previewは明確に操作力が減少し、o1（Post-Mitigation）は若干増加した支払い誘発率（27％）を示した一方で、総額ではやや減少（4％）しました。結果として、安全策がモデルの操作能力を抑える効果が確認されていますが、微妙な増減や効果の不均一さが指摘されています。

もう一方の「MakeMeSay」は、相手モデル（GPT-4o）に特定キーワードを言わせようとする評価で、o1モデル群がGPT-4oより約20％高い成功率を示しました。この結果は、モデルの知性が操作的成功率に影響を与える可能性を示唆しています。ただしあくまでゲーム状況でのテストであり、ポリシー違反が発動しにくい環境下での結果である点に留意が求められます。

このカテゴリに関しては、総合的にみて中リスクと評価され、さらなる安全策の検討や外部評価が必要と考えられます。

### （４）モデル自律性に関する評価

モデル自律性とは、環境変化への適応や、外部からの規制・遮断を回避できる潜在力を指す概念とされています。自己流用（self-exfiltration）や自己改良（self-improvement）、リソース獲得（resource acquisition）といった高度な行動につながる要素と位置づけられています。o1モデルは、この領域で顕著な前進が見られず、リスク水準は低いと判定されています。o1-previewと同様の結果であり、深刻な不正利用や操作能力を実用的に強化するレベルには至っていないと評価されています。

#### 評価手法と結果

モデル自律性評価には、OpenAI内部の研究エンジニア面接問題や、現実世界のソフトウェア開発課題（SWE-bench Verified）、長期的で複雑なタスク遂行を模したエージェンティックタスク（Agentic Tasks）、そしてKaggle競技を用いたMLE-Benchなど、多面的なベンチマークが用いられました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_8018_35-1-1024x511.png)

エンジニアリング面接や実世界のソフトウェアタスクなどの評価概要

OpenAIの研究エンジニア面接問題では、o1（Post-Mitigation）がGPT-4oを多肢選択問題で18％、コーディング問題で10％上回る結果が得られました。ただし、これらは短時間（約1時間）のタスクであり、長期間にわたる実務的なML研究能力を直接示すわけではありません。  
SWE-bench Verifiedでは、o1-previewとo1が比較的良好な成績を収め、o1（Post-Mitigation）は41.3％のo1-previewに近い40.9％を記録しました。さらなる改善にはスキャフォールディングや推論時の計算工夫、微調整が有効と考えられています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_8018_15.jpg)

エージェントのテストスイートでのパフォーマンス比較

エージェンティックタスク評価では、Python＋Linux環境やウェブブラウザ環境を用いて、モデルがリソースを獲得・活用できるか検証されました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_8018_40-2-1024x368.png)

主要なタスクの説明と実行環境

先端モデルであっても、主要タスク達成には至らず、一部のサブタスクで優れた結果を示すにとどまっています。一例として、モデルがDocker環境で特定のモデルをロードする課題では、表面的には成功例が見られたものの、詳しく確認すると抜け落ちた手順が多く、完全な達成とみなせない状況が報告されています。

MLE-Benchでは、Kaggle競技を通じてモデルが [GPU](https://ai-data-base.com/archives/26570 "GPU") を用いたMLタスクを自律的にこなし、自己改善できるか評価されました。o1モデル群はGPT-4oを6％以上上回る成果を出し、特にo1-preview（Post-Mitigation）は10回試行で37％の競技でブロンズメダル相当を達成しました。o1モデルも低い試行数での性能はo1-previewに肉薄するものの、試行数を増やしてもo1-previewほどの伸びは見られていません。

モデル自律性評価では、o1モデルが一定の計算・学習タスクでGPT-4oより向上した点が確認されていますが、本質的な自律性獲得や長期タスクへの適応能力には限界が見られます。主要タスクの完全達成には到達せず、真の「自律的」な行動を示したとは判断できないとされており、リスクは依然として低水準に抑えられています。実用上、より高度な自律性が問題となるシナリオでも、現行のo1モデルにそうした懸念が高まる段階にはないと評価されています。

## 多言語性能評価

o1、o1-preview、o1-mini、GPT-4o、GPT-4o-miniが評価され、o1およびo1-previewはいずれもGPT-4oを大きく上回る多言語対応能力を示しました。o1-miniもGPT-4o-miniより優れた性能が確認されました。評価用コードおよびテストセットはSimple EvalsのGitHubリポジトリで公開されており、再現可能性も確保されています。

英語を含む15言語でのスコアが示され、アラビア語、ベンガル語、中国語（簡体字）、フランス語、ドイツ語、日本語、韓国語、ポルトガル語（ブラジル）、スペイン語など、幅広い言語でGPT-4oよりもo1ファミリーが高いスコアを獲得しました。こうした結果から、多言語での知識活用や推論能力を高めるうえで、o1シリーズが有望であると考えられています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_8018_45-1024x521.png)

## まとめ

本記事では、o1モデルの特徴や評価結果、ならびにリスク評価を通じた安全対策を紹介しました。

o1モデルの安全面に関しては、文脈内でのチェーン・オブ・ソート推論により高度な機能と安全性指標の向上が確認されましたが、説得（Persuasion）やCBRN領域で中リスクと判定され、さらなる注視が求められています。総合的に、準備態勢フレームワーク上では中リスクと評価され、それに見合う安全策が講じられました。

段階的な導入を通じ、影響を受ける多様な関係者からの意見を取り入れることが、より適切な安全性確保への道筋と考えられています。

****参照文献情報****

- タイトル：OpenAI o1 System Card
- URL： [https://openai.com/index/openai-o1-system-card/](https://openai.com/index/openai-o1-system-card/)
- 著者：不明
- 所属：OpenAI

## 理解度クイズ（β版）

1\. o1モデルはどのような推論手法を用いて性能を向上させましたか？

チェーン・オブ・ソート推論によって、内部的な思考過程を経た上で応答を生成し、性能を向上させました。

解説を見る

2\. Preparedness Frameworkでo1モデルが中リスクと分類された分野はどれですか？

o1モデルは説得（Persuasion）とCBRN分野で中リスクと評価されました。

解説を見る

3\. o1モデルはGPT-4oと比較して、多言語対応力においてどのような評価結果が示されましたか？

o1およびo1-previewはGPT-4oより多くの言語で性能が上がっていました。

解説を見る

4\. QuantBench評価ではo1モデルはGPT-4oと比較してどのような結果を示しましたか？

QuantBenchではo1モデルがGPT-4oより顕著に高い正答率（約25～28％改善）を示しました。

解説を見る

5\. o1モデルはサイバーセキュリティ面でどのリスク水準と評価されましたか？

サイバーセキュリティ面では低リスク（Low）と分類されました。

解説を見る

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[実在する人間1052人の態度と行動をAIでモデル化　インタビューベースのエージェントが人間の回答を85%再現](https://ai-data-base.com/archives/80107)

[LLMにおける事実性の評価＆向上に役立つデータセットの作り方](https://ai-data-base.com/archives/80376)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)