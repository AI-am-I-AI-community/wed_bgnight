---
title: "RAGの検索精度を実務レベルに高めるには、「情報ごとに ”質問文” を作りデータベースに入れる」のが効果的との報告"
source: "https://ai-data-base.com/archives/75110"
author:
  - "[[AIDB Research]]"
published: 2024-09-04
created: 2025-06-13
description: "企業がRAGを行う上での参考になるテクニックを研究している事例を紹介します。企業が独自のデータをRAGで参照したい場合、検索精度を向上させるのがネックになります。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

企業がRAGを行う上での参考になるテクニックを研究している事例を紹介します。

企業が独自のデータをRAGで参照したい場合、検索精度を向上させるのがネックになります。そこで文書を小さな「原子」単位に分け、それをもとにした質問でベクトルデータベースを作成する方法が考案されました。この方法で、ユーザーの探したい内容と文書の中身がより正確に合うようになるとのことです。

本記事では、手法の詳しい内容や実験の結果、そして会社の知識管理や顧客サポートにどのような影響があるかについて紹介します。

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_75110-1024x576.jpg)

**参照論文情報**

- タイトル：Question-Based Retrieval using Atomic Units for Enterprise RAG
- 著者：Vatsal Raina, Mark Gales
- 所属：ALTA Institute, University of Cambridge

**本記事の関連研究**

- [LLMのRAGシステムで「検索能力を高める」方法　AWSの研究](https://ai-data-base.com/archives/74922)
- [RAGとLong-Contextの比較、そしてハイブリッドで活用する新しい方法](https://ai-data-base.com/archives/73468)
- [RAGにおいて長文を検索することで一気に効率を上げるアプローチ『LongRAG』](https://ai-data-base.com/archives/71774)
- [NVIDIAが教えるRAGチャットボット実装の重要ポイント](https://ai-data-base.com/archives/72680)

## 背景

最近は、多くの企業が自社の内部文書を活用するためにLLMを使う取り組みを始めています。いわゆる企業向けRAGシステムの開発です。RAGは、検索対象の文書を小さく（チャンクに）分け、ユーザーが欲しい内容に関係するチャンクを探し出し、それをLLMに渡して回答を作る技術です。

ところが、RAGには問題点があります。適切なチャンクを選べないとLLMが間違った答えを出してしまうことがあるという点です。この問題を解決するためのアプローチが探求されています。

一般的な方法では、ユーザーの質問と文書のチャンクを直接比べています。しかし、1つのチャンクにはたくさんの情報が含まれているため、うまく合う情報を見つけるのが難しい場合があります。

そこで今回、チャンクをさらに小さな「原子」という単位に分けることが提案されています。そして、その原子をもとにした質問を作成することもポイントです。検索する際には、ユーザーのクエリに最も近い質問と、それに関連するチャンクを見つけ出すという流れになります。

この方法は、会社のみが持っている情報にも使えるのが大切なポイントです。LLMのチューニングなど特殊な工夫は必要ありません。

RAGを改善する研究は今までもありましたが、多くはユーザーの質問の書き方を工夫するものでした。一方、この研究では文書の表し方を工夫することで、より効果的な検索ができるようにしています。

実験の結果、本手法は一般的なRAGよりも良い検索性能を示すことがわかりました。

以下で手法の詳細や実験結果を詳しく紹介します。

## RAGの検索方法

企業向けに限らず、RAGシステムの主な流れは、次の3つの段階からなっています。一般的な内容のおさらいです。

1. まず、多くの文書を小さな部分（チャンク）に分ける
2. ユーザーが知りたいことに関係するチャンクを見つける
3. ユーザーの質問とみつけたチャンクを使って、答えを作る

ここでは、2番目の「探す」段階をより良くする方法について考えていきます。

### よく使われる方法

普通のRAGでは、ユーザーの質問に最も関係あるチャンクを見つけるのに、文章の意味を数字の並び（ベクトル）で表す仕組みを使います。

質問とすべてのチャンクを数字の並びに変え、質問に最も近い数字の並びを持つチャンクを選びます。

しかし、質問の意味と、本当に探すべきチャンクの意味が必ずしも同じではないため、間違ったチャンクを選んでしまうことがあります。

### 質問を書き換える方法

別の方法として、質問を答えの形に書き換えてから探す方法があります。例えば、「インドの首都は何ですか？」を「インドの首都はロンドンです」と書き換えます。答えが間違っていても構いません。大事なのは、答えの形になっていることです。

この書き換えた文を使って探すと、正しいチャンクを見つけやすくなることがあります。

### 細かく分ける方法

ユーザーの質問は、普通はチャンクの中の特定の情報を探しています。しかし、チャンクの中にはさまざまな情報があるので、質問の意味とチャンク全体の意味が離れてしまうことがあります。

そこで今回、チャンクをさらに小さな「原子」という単位に分ける方法が考えられました。質問の意味を、この原子の意味と比べます。そして、最も近い意味の原子がある元のチャンクを選びます。

この研究では、2種類の原子を考えています。

1. チャンクの中の1つ1つの文を原子とする場合
2. チャンクの中の重要な情報だけを含む短い文を作る場合

さらに、各原子の情報に基づいて、いくつかの質問を自動的に作ります。

こうすることで、各チャンクがさまざまな角度から質問できるようになります。要するに、文書を細かい意味ごとに区切ること、そしてそれらをユーザーの質問文と似通う形（質問文形式）に変えることが両方大事というテクニックです。

ユーザーの質問と文書の内容の間にあるギャップを埋める新しい試みだと言えるでしょう。

下の図は、本提案手法のプロセスを示しています。以下のような流れが表現されています。

```js
ドキュメント：
"Company Board Meeting Notes" という会議議事録が元データとして示されています。
チャンク：
文書が複数のチャンクに分割されています。例えば、「We will close the crude oil project by Q3 2024...」や「The installation of the new wind farm is projected to generate approximately 150 megawatts in ...」などのチャンクが見られます。
原子（Atoms）：
各チャンクがさらに小さな原子単位に分解されています。例えば、「The crude oil project closed by Q3 2024」や「Wind farm output is 150MW」などの原子が生成されています。
質問（Questions）：
各原子に基づいて質問が生成されています。例えば、「What is the wind farm output」や「Which source outputs 150MW」などの質問が作成されています。
クエリ：
ユーザーからの質問「How much is the wind farm output?」が示されています。
シンセサイザーLLM：
最終的に、関連する情報と元のクエリがシンセサイザーLLMに渡され、「The new wind farm generates approximately 150 megawatts.」という回答が生成されています。
```
![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_75110_1-1024x423.jpg)

なお、ユーザーの質問文を「質問形でない文」に変換してチャンクと類似度検索する方が効率的ではないか？という疑問が湧くかもしれません。

しかし上の図を見て分かる通り、本手法は「文書の各部分に対して”複数の”質問を生成する」アプローチをとることで、より細かい情報検索を可能としています。単一のクエリを変換するだけでは、文書の細かい部分との対応が難しい場合があります。

また、質問文をあらかじめ変換するアプローチでは、その時点でドメイン知識に伴った変換が求められますが、企業内データの場合は難しい可能性があります。

## 実験

### データ

実験では、主に2つの種類のデータセットを使いました。SQuADとBiPaRです。

### SQuAD

SQuADは、文章を読んで答えを見つける問題のデータです。文章、質問、そして文章から抜き出した答えがセットになっています。文章はWikipediaのさまざまな記事から取られています。

実験では、SQuADのデータを少し変えて使用されました。「すべての質問に答えられる」という前提で、ある質問の答えは「その質問に対応する文章にだけある」と仮定しました。

このデータセットには、2,067の文章の塊（チャンク）と10,570の質問があります。1つのチャンクに対して約5つの質問がある計算です。各チャンクには平均6.6個の文があります。

### BiPaR

BiPaRは、小説のような文体で書かれた2か国語の文章データセットです。1つの言語での読解や、2つの言語をまたいだ読解の研究によく使われます。

今回の研究では、英語の文章だけを使いました。BiPaRも、SQuADと同じように使われています。すべての質問の文章をバラバラにして、新しい知識ベースを作りました。

BiPaRは、企業向けRAGの研究にとても役立ちます。なぜなら、小説の一部を使っているからです。小説はフィクションなので、言語モデルが学習済みの知識では答えられません。そのため、BiPaRは企業独自の知識ベースに似ており、質問に関係する情報を見つけるには検索が必要になります。

### 使用されたモデル

文章の意味を数字で表すのに、Huggingfaceの [All-MPNet-Base-V2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) というモデルを使いました。このモデルは企業向けRAGでよく使われていて、性能も良いです。また、比較的小さなサイズ（1億1000万のパラメータ）も特徴です。

他にも、 [E5-base-v2](https://huggingface.co/intfloat/e5-base-v2) というモデルが使われました。このモデルは、同じくらいの大きさのモデルの中で最も性能が良いと考えられています。

また、質問を書き換えたり、文章を小さく分けたり、新しい質問を作ったりする作業には、ChatGPT 3.5 Turboを使いました。なお、それぞれの作業に合わせた指示を用意しました。

### 評価

情報を探す能力を評価するには、さまざまな方法があります。この研究では、「R@K」という方法を使いました。正しい文章の塊（チャンク）が上位K個の中に含まれている質問の割合を計算します。

具体的には、R@1、R@2、R@5を見ました。R@1は完全に一致するかを調べ、R@2とR@5はもう少し緩めの評価になります。RAGでは、複数のチャンクを選び、その中から正しい答えを見つける作業を言語モデルに任せるのも面白いアイデアです。Kを決定する際には言語モデルが一度に扱える文章の量を考える場合が多いです。例えば、ChatGPT 3.5は16,000単語分まで扱えます。そのため、R@Kの「K」はあまり大きすぎない数を考えます。

## 結果

### SQuADデータセットの結果

まず、all-mpnet-base-v2モデルを使ったSQuADデータセットの結果を見てみましょう。

普通のRAG方式では、正しい答えが1番目に来る確率（R@1）は65.5%、上位5つの中に入る確率（R@5）は89.3%でした。HyDE（仮の文書を作る方法）を使っても、あまり良くなりませんでした。

文章を文ごとに分ける方法（今回の提案手法における一つ目のポイント）では、R@1が70.2%、R@5が90.6%まで良くなりました。さらに、HyDEをこの方法と組み合わせると、もっと良くなりました。

質問を自動で作る方法（二つ目のポイント）を使うと、R@1が最大73.8%まで上がりました。

文章を重要な部分だけに分ける方法を使うと、さらに良くなりました。最終的に、この方法で自動的に作った質問を使う方法が、すべての指標で一番良い結果を出しました。

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_75110_5-1.png)

e5-base-v2モデルを使った場合は、最初から性能が高かったので、違いがあまりはっきりしませんでした。それでも、R@1では重要な部分だけに分けて質問を作る方法が一番良かったです。

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_75110_6-1.png)

### BiPaRデータセットの結果

BiPaRデータセットでは、どちらのモデルでもよく似た傾向が見られました。SQuADと違って、BiPaRではHyDE（クエリの書き換え）がどの方法でも普通の方法より良くなりませんでした。

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_75110_7.png)

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_75110_8.png)

この違いは、BiPaRが架空の物語を使っているのに対し、SQuADが本当のWikipedia記事を使っているためだと考えられます。架空の情報では、HyDEで作った仮の答えがあまり役に立たないのかもしれません。

一方で、文章を分けて質問を作る方法は、BiPaRでも普通の方法よりずっと良い結果を出しました。例えば、e5-base-v2を使った場合、R@1が約14%も良くなりました。つまり本提案手法がヒットしている結果です。

### 効率的な質問作り

実験では、作る質問の数を減らす方法も試しました。ランダムに選んだ質問と、うまく選んだ質問を比べました。

結果、質問の半分以上を減らしても（つまり、保存するデータ量を半分にしても）、最高の性能を保てることがわかりました。極端な場合、質問の20%だけを使っても、うまく選べば性能はあまり下がりませんでした。

文章を分けて質問を作る方法は普通のRAGよりデータ量が多くなる可能性がありますが、この結果から質問をうまく選ぶことで効率的に性能向上が果たせることが示唆されました。

### HyDEの限界

なお、さらなる分析で、HyDEの性能が本当の情報に基づいているかどうかに影響されることがわかりました。 [CLAPNQ](https://github.com/primeqa/clapnq) というデータセットでの実験では、HyDEが長い答えに対してとても良い結果を出しました。これは、仮の答えが本当の答えに近く、目的の文章とよく似ている可能性が高いためです。

しかし前述した通り、BiPaRのような架空の情報を使ったデータセットでは、HyDEの効果があまりないことが今回わかりました。この結果は、企業の内部情報にHyDEを使う際には注意が必要だということを示しています。

## まとめ

この記事では、会社がRAGシステムを開発する際に性能をより良くする方法の研究について紹介しました。

研究者たちが提案した方法は、文書を小さな（「原子」）単位に分け、それぞれの原子から質問を作るというものです。実験の結果、この方法が今まで使われていた方法よりも上手に必要な情報を探せることがわかりました。

さらに、文書から重要な部分だけを取り出して原子を作り、それを基に質問を生み出す方法が一番効果的でした。しかも、作る質問の数を減らしても、高い性能を保てることもわかりました。

今回の考案手法の優れたところは、会社が持っている独自の情報にも使えるという点と、モデル自体を変える必要がないと言う点です。

ただし、注意点もあります。情報の種類によって、この方法の効果が変わることがあるかもしれません。例えば、事実に基づく情報と架空の情報では、効果が異なる可能性があります。

今回の研究結果は、企業におけるRAGシステム開発の参考になる可能性があります。自社の情報の特性を考慮しながら活用を検討してみてください。

- 参照論文URL： [https://arxiv.org/abs/2405.12363](https://arxiv.org/abs/2405.12363)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[プロンプトに5つほど”価値観の例”を示すだけで、LLMは特定の文化に適応した回答ができるようになるとの報告](https://ai-data-base.com/archives/75111)

[RAGの検索データにおける「ノイズ（事実とは異なる情報など）」には有益なノイズと有害なノイズがある](https://ai-data-base.com/archives/75220)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)