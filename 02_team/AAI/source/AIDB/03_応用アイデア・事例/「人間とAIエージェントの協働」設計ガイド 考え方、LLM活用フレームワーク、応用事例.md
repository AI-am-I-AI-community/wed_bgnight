---
title: "「人間とAIエージェントの協働」設計ガイド 考え方、LLM活用フレームワーク、応用事例"
source: "https://ai-data-base.com/archives/89432"
author:
  - "[[AIDB Research]]"
published: 2025-05-13
created: 2025-06-13
description: "本記事では、人間とLLMベースのエージェントが協働する仕組みを体系的に整理した研究を紹介します。AIを組み込んだエージェントの活用が進む中で、人間との相互作用をどう設計するかが重要な課題となっています。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、人間とLLMベースのエージェントが協働する仕組みを体系的に整理した研究を紹介します。

AIを組み込んだエージェントの活用が進む中で、人間との相互作用をどう設計するかが重要な課題となっています。今回は、協働のあり方を支える5つの設計軸と、具体的なフレームワークや応用事例を整理していきます。

AIを道具としてだけでなく、共に作業を進めるパートナーととらえる視点を持つうえで参考になる内容です。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89432-1024x576.png)

## 背景

生成AIが注目される中で、「AIにまるごと仕事を任せられたら便利なのに」と考える人は少なくありません。実際、LLMを使って環境を読み取り、必要な行動を考え、ツールも使いながら目標に向かって動いていく、そんな“自律型エージェント”のアイデアに期待が集まっています。

自律型エージェントは、記憶を活用したり、長い作業をいくつかのステップに分けたりして、人の手をほとんど借りずに処理を進める存在です。業務の自動化が進む中で、次のステップとして自然な流れかもしれません。

ただ、ここでいくつかの壁にぶつかります。一つは、出力の信頼性です。もっともらしく見えても、実際には間違った情報が出てくることがあり、これが後続の処理に大きな影響を及ぼすことがあります。もう一つは、タスクの複雑さです。専門的な知識が求められたり、何段階にもわたる判断が必要だったりする場面では、AIだけでは手に余ることもあります。そして、安全性や倫理の問題も見過ごせません。意図せず有害な行動をとってしまったり、データに含まれる偏りをそのまま引き継いでしまったりするケースも想定されます。

こうした現状を見ると、「完全に自律したAI」にすべてを任せるのは現実的ではないという見方も出てきます。むしろ、人がうまく関わることで、精度や安全性を高めていくという考え方のほうが、実践的です。人が必要な場面で判断を助けたり、足りない情報を補ったりすることが、結果としてより信頼できるシステムにつながっていきます。

そこで本記事では、そうした“人とAIが一緒に働く”という前提に立って、新しいアプローチを整理していきます。どんな場面で人が関わるべきか、その関わり方によってどんな効果が得られるかといった視点から、全体の構造を明らかにする論文を参照してまとめていきます。

以下で見ていきましょう。

## 人とエージェントが協力するという考え方

すべてをAIに任せきるのではなく、人がうまく関与しながら力を合わせていく。その発想に基づいて提案されているのが、人とLLMベースのエージェントが協働するシステムです。

エージェントが自動で動くだけでなく、人が状況に応じて追加の情報を渡したり、出力に対してフィードバックを返したり、ときには行動を制御する場面も含まれます。人の柔軟な判断や経験をうまく取り込みながら、システム全体の精度や安全性を高めることを目指しています。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89432_1-1024x463.png)

「LLMエージェントと人間の協働」システムの大枠

ポイントとなるのは、互いの強みを活かすことです。人は、直感や創造力、倫理観や専門知識といった、機械では代替しづらい部分を担います。一方でエージェントは、大量の情報を素早く処理し、高度な言語表現でタスクを進めることが得意です。この両者を組み合わせることで、それぞれ単独では難しい課題にも取り組めるようになります。

人とエージェントの関わり方はいくつかの形で整理できます。

### 状況を共有しやすくするための情報提供

タスクに必要な背景知識や細かな条件など、人がエージェントに補足することで、より適切な判断ができるようになります。たとえば専門用語の意味を説明したり、文脈に合った方針を伝えたりするような場面が該当します。

### 出力へのフィードバックと修正

エージェントの答えに対して、人が「これは少しずれている」と指摘したり、「こうするともっと良くなる」と具体的に修正を加えたりすることで、次第に精度が高まっていきます。このやり取りが繰り返されることで、より信頼できる動作が可能になります。

### 必要なときに手を止める制御

医療や金融など、間違いが大きな影響をもたらす分野では、人が最終的な判断を担ったり、途中で処理を止めたりする仕組みが重要になります。たとえば診断の提案はAIが行い、最終判断は医師が下すといった形です。

## 人とエージェントの協働を支える設計

人とエージェントがうまく協力するには、どのような環境で、どんな役割を持ち、どんなやり取りをするのかを明確にしておく必要があります。

### 協働の舞台と役割をどう設けるか

人とエージェントがやり取りする「場」は、物理的な現実空間であることもあれば、仮想的な環境の中であることもあります。たとえば、会議室で人とAIが共同作業をするケースもあれば、オンライン上のチャット環境でエージェントと対話しながらタスクを進めることもあります。どのような空間でやり取りが行われるかによって、必要な設計や制約も変わってきます。

関わる人数や立場もさまざまです。たとえば、1人の人間と1体のエージェントという組み合わせもあれば、複数人のチームが複数のエージェントと同時に関わるような状況もあります。人が複数いれば、意見の対立や役割の重複も起こりやすくなりますし、エージェントが複数であれば、動作の調整や情報の統合が課題になることもあります。

また、人の関わり方にも幅があります。あまり関与せず最小限の指示だけを与える人もいれば、エージェントの出力に対して細かく修正を加えたり、自ら積極的に手を動かしたりする人もいます。どちらが正しいというわけではなく、状況に応じた関わり方の幅があるということです。

エージェント側も、役割や能力によって振る舞いが異なります。汎用的なアシスタントとして動くものもあれば、特定の職能を持った「専門家」のような形で設計されることもあります。エンジニア、医師、秘書、掃除ロボットのように、それぞれの役割に応じてどの程度の判断を委ねるかも変わってきます。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89432_2-910x1024.png)

システムの体系

### フィードバックの設計が協働の質を決める

人とエージェントが関わるうえで避けて通れないのが、フィードバックのあり方です。人からの指摘や修正がどのように行われるかによって、エージェントの動き方や学習の進み方が大きく変わります。

フィードバックには、いくつかの種類があります。

単純に「良い」「悪い」と評価するものもあれば、出力に手を加えて直接修正するもの、より良い方針を示すような指導型のものもあります。また、明示的に言葉として伝えるだけでなく、人の振る舞いからエージェントが推測するような、暗黙的なフィードバックも存在します。

フィードバックの粒度も重要です。出力全体に対して「これはOK」と判断するだけでなく、文章の一部やコードの一行など、細かな単位での修正が求められることもあります。複雑なタスクほど、こうしたセグメント単位での指摘が欠かせません。

さらに、いつフィードバックを与えるかも大切な観点です。最初のタスク設計時に目標を明確にしておくこともあれば、タスクの途中でその都度修正を加えていく場合もあります。タスクが終わった後に、結果を振り返りながら改善点を伝える形もあります。協働のタイミングやスタイルに合わせて、適切なフェーズを選ぶことが求められます。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89432_3-1024x894.png)

人間からのフィードバックを分類する3つの視点

### 関わり方のかたちはさまざま

人とエージェントがどう関わるかには、いくつかのパターンがあります。タスクの内容や目的によって、求められる関係性は変わってきます。

たとえば、「協働」のかたちは基本的なスタイルのひとつです。人とエージェントが共通の目標に向かって、それぞれの役割を担いながら作業を進めます。人が方向性や文脈を提供し、エージェントが作業の実行を担うような構図が一般的です。

協働の中にも、さらに細かなタイプ「委任」「監督」「協力」「調整」などがあります。

「委任」では、人がエージェントに明確な指示を出して動かします。人が主導し、エージェントは補助役に徹するような形です。

「監督」は、エージェントの出力を人がチェックしながら、必要なときにだけ介入するスタイルです。うまくいっている限りは任せ、誤りがあれば修正するというバランスのとれた関係です。

「協力」や「調整」は、複数の人と複数のエージェントが関わるような複雑な場面でよく使われます。互いの動きを合わせながら、対立やズレを減らし、全体として成果を出していくかたちです。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89432_6.png)

「委任」「監督」「協力」「調整」

一方で、「競争」や「協力しつつ競争」というスタイルもあります。それぞれが別の目的を持って動きながら、部分的には同じ目標にも協力して取り組むような関係です。情報の共有や意見のぶつかり合いを通じて、新たな発見が生まれることもあります。

### タスクの進め方をどう調整するか

人とエージェントが共同作業を行う際には、「誰がどの順番で動くか」「どのタイミングでやり取りするか」といった進行のルールを決めておくことが欠かせません。この調整のしくみは、協働の滑らかさや効率に直接関わってきます。

調整のしかたには、大きく分けて2つの視点があります。ひとつはタスクの順番をどう設計するか。もうひとつは、やり取りのタイミングをどこで合わせるかという点です。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89432_4.png)

人間とAIエージェントの協働における調整のしかた

#### タスクの順番をどう組み立てるか

作業の進め方としては、「一つずつ順番にやる方法」と「同時並行で進める方法」の2つが基本になります。

順番に進める方法では、まず人がタスクの意図や背景を伝え、そのうえでエージェントが作業を実行し、人が結果を確認して修正を加えるという流れがよく見られます。このように交互に進めるスタイルは、タスクの流れを明確にし、ミスが起きたときに原因をたどりやすいという利点があります。

一方で、同時に進める方法では、人とエージェントが並行して動きながら、必要に応じて調整していきます。たとえば、エージェントが文書を生成している間に、人がその構成をリアルタイムで調整したり、途中で追加の指示を出したりするような場面です。この方法は柔軟性が高く、反応の早さも期待できますが、タスクが錯綜しやすいため、役割分担や同期のしくみを丁寧に設計しておく必要があります。

#### やり取りのタイミングをどう合わせるか

タイミングの面でも、協働には2つの方法があります。ひとつは「リアルタイム型」、もうひとつは「時間差型」です。

リアルタイム型は、まさにその場でやり取りをしながら進めるスタイルです。音声対話やチャットベースのやり取りなど、即時応答が求められるタスクでよく使われます。たとえば、会議中にエージェントがその場で要点をまとめてくれるような場面では、この形式が効果的です。

時間差型では、やり取りに時間的な余裕を持たせます。エージェントがまず案を出し、それに対して人が後からコメントを返すといった流れです。この方法は、一度に全員が参加できない場面や、時間をかけて検討する必要があるタスクで有効です。非同期的に進むため、やり取りの自由度が高く、負荷を分散しやすいというメリットがあります。

### 情報のやり取りの設計をどう考えるか

人とエージェントが協働する場面では、ただ会話が成立するだけでは不十分です。情報をどう伝え合い、どう共有し、どこに集約するかといった設計が整っていてはじめて、信頼できるやり取りが可能になります。

ここでは、「構造」と「手段」という2つの観点から、情報の流れ方を整理していきます。

#### 情報が流れる構造をどう組み立てるか

やり取りの構造にはいくつかのパターンがあります。どの形が適しているかは、タスクの性質や関わる人数、目的によって変わってきます。

もっともわかりやすいのが中央集権型です。ここでは、ある1つのエージェントや人が情報のハブとなり、すべてのやり取りを取りまとめます。会議の進行役や、集約されたインターフェースとしてのチャットボットなどがこれにあたります。調整がしやすく、意思決定も速くなりますが、中央に過度な負担がかかるリスクもあります。

次に、分散型では、各エージェントや人がそれぞれ独立して動き、必要に応じて直接やり取りを行います。指示系統がなくても自律的に情報をやり取りできるため、柔軟性があり、規模の大きいシステムでも適応しやすい構造です。ただし、全体の統一感を保つには追加の工夫が必要になります。

階層型は、全体をいくつかのレベルに分け、上位が方向性やガイドラインを決め、下位が具体的な作業を担う形式です。組織的なタスクや、役割が明確なプロジェクトではよく使われる構造で、マネジメントのしやすさや責任の明確さが利点です。

#### どんな方法で情報をやり取りするか

情報のやり取りそのものには、いくつかの手段があります。どの手段を使うかによって、やり取りの質や効率も変わってきます。

まず中心になるのが会話ベースのやり取りです。人とエージェントが自然言語を使って対話し、質問と応答を重ねながらタスクを進めていきます。多くのAIアシスタントが採用している方法で、直感的に扱いやすいのが特徴です。ただし、あいまいな表現や文脈のずれによって誤解が生じるリスクもあるため、対話の精度を支える仕組みが求められます。

次に挙げられるのが観察ベースのやり取りです。ここでは、人の行動や選択をエージェントが観察し、そこから意図を読み取ります。たとえば、操作の順番や滞在時間、クリックパターンなどから、何に関心があるのかを推測する仕組みです。言語によらず反応できる柔軟さがありますが、誤読のリスクもあるため、補助的な仕組みと併用することが多くなります。

もうひとつの手段として、共有メッセージプールがあります。これは、人やエージェントが共通のスペースにメッセージやデータを記録し、必要なときにそれを参照する仕組みです。掲示板、タスクリスト、共通のログなどがこれにあたります。誰がどの情報にアクセスしたかを明示できるため、進捗の可視化や履歴管理に向いています。ただし、情報が増えすぎると混乱しやすくなるため、フィルタリングやアクセス制御の工夫が欠かせません。

## 協働システムはどこで使われているか

人とLLMエージェントが協力して動く仕組みが、実際にどのような場面で役立っているのかを見ていきます。実体を持つロボットの操作から、ソフトウェア開発、会話システム、ゲーム、そして金融分野まで、応用の広がりを紹介します。

### 実世界で動くAIとの協働

ロボットなど物理的な世界で活動するAIとの協働では、人の判断や補助が欠かせない場面が多くあります。たとえば、組み立て作業のように複雑で微妙な調整が必要なタスクでは、人が状況に応じてフィードバックを返すことで、ロボットの動きをスムーズに補完できます。

人の発話タイミングを見ながら、必要なときだけ話しかけるように設計された支援エージェントも提案されています。将来的には人とロボットが同じ空間で無理なく協働できるようになると考えられています。

### ソフトウェア開発での活用

コーディング作業も、実は人とエージェントの協働が活かされやすい分野です。人が問題の意図や修正の方向性を伝え、エージェントがコードの生成やデバッグを担うという分担が可能になります。

最近は、どのタイミングで人が介入するのが効果的かを学習させる試みや、複数回のやり取りを通じてフィードバックを最適化する仕組みも登場しています。開発プロセスの中で、やり取りの設計自体が成果に大きく影響することが示されています。

### 会話システムをより自然にする

AIとの会話をより自然で使いやすくするためにも、人の関与は重要です。文脈の把握が難しい場面や、あいまいな指示に対する対応など、人の補助があることでAIの性能が安定します。

ユーザーの意図を先回りして予測する仕組みや、複数のエージェントを連携させて応答を強化する取り組みなどが行われています。

### ゲーム環境での協働

ゲームは、動きが速く、戦略性も求められる複雑な環境です。そのため人とエージェントの連携が成果を大きく左右します。人の指示を受けて柔軟に動くAI、混乱が起きたときに質問で状況を整理するAIなど、さまざまな役割分担が試みられています。

やり取りの遅延を抑えつつ、エージェントが一貫性を持って動けるよう、階層的な言語設計を導入した事例もあります。プレイヤーの体験を損なわず、戦略的にも効果的な動きが求められます。

### 金融の意思決定を支える

市場の予測や投資判断のように、複雑で変化の激しい分野では、人の直感とAIの分析を組み合わせることが大きな強みになります。

たとえば、投資家の戦略とAIの提案を組み合わせるシステムでは、実際にリターンやリスクのバランスが改善されたという報告もあります。人が状況を読み取り、AIが大量のデータを分析するという役割分担が、実用的な成果につながっています。

## 実装を支えるツールとリソース

以上のように、人とエージェントが協働するしくみは、さまざまな分野で活用され始めています。では、こうしたシステムを実際に構築したり評価したりするには、どんな道具や仕組みが必要になるのでしょうか。以下では、開発や検証に使われている主なフレームワークやベンチマークを紹介します。

### 協働の設計に使えるフレームワーク

人とAIが協力して作業を進める際の枠組みを支えるフレームワークはいくつか存在します。それぞれに特徴があり、目的や環境に応じて使い分けられています。

たとえば [Collaborative Gym](https://github.com/SALT-NLP/collaborative-gym) は、旅行の計画やデータ分析、論文の執筆など、幅広いタスクを想定した実験環境を提供しています。人とエージェントのやり取りを非同期で進められるのが特徴で、タスクの達成度だけでなく、協働の質そのものも評価できます。

[COWPILOT](https://oaishi.github.io/cowpilot.html) は、ブラウザ上で人とエージェントが一緒にウェブ操作を進める仕組みです。エージェントが提案を出し、人が承認するという流れでタスクが進むため、人の判断をうまく取り入れながら、作業効率を高める設計になっています。

さらに [DPT-Agent](https://github.com/sjtu-marl/DPT-Agent) では、人の即時的な反応と熟慮的な判断の両方に対応するかたちで、エージェントが反応を切り替える構造が取り入れられています。人の動きに合わせて柔軟に振る舞いを調整するしくみが特徴です。

そのほか、サイバーセキュリティや金融、ロボティクスなど特定領域に特化した設計も提案されています。こうした多様なフレームワークがそろっていることで、目的に応じた実装が可能になっています。

### 協働の力を測るためのベンチマーク

実装が進む中で、協働の質や効果を客観的に評価するためのベンチマークも整備されつつあります。これらは、人とエージェントがどれだけうまく連携できるかを測るための共通の指標として機能します。

ロボティクスや実世界タスク向けには [TaPA](https://github.com/Gary3410/TaPA) 、EmboInteract （データセット公開あり）、 [IGLU](https://github.com/microsoft/iglu-datasets) 、 [PARTNR](https://github.com/facebookresearch/partnr-planner) などが用いられ、複雑な操作や即時対応での協働を検証します。

会話系の評価には [WEBLINX](https://github.com/McGill-NLP/weblinx) 、 [Ask-before-Plan](https://github.com/magicgh/Ask-before-Plan) 、 [HOTPOTQA](https://hotpotqa.github.io/) 、StrategyQAなどが広く使われています。

コーディング支援では [MINT](https://github.com/xingyaoww/mint-bench) 、 [InterCode](https://github.com/princeton-nlp/intercode) 、 [ConvCodeWorld](https://github.com/stovecat/convcodeworld) が採用され、修正精度や対話の滑らかさが評価されます。

ゲーム領域では CuisineWorldや [MineWorld](https://github.com/microsoft/mineworld) が用いられ、人とAIが協力して戦略を立てる際の連携を測定します。

金融では [FinArena-Low-Cost](https://arxiv.org/abs/2503.02692) が、人の判断とAIの分析がどの程度補完し合って成果を上げられるかを検証するベンチマークとして使われています。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89432_5.png)

これらのツールや評価軸は、異なるアプローチを比較したり、改善点を探したりするうえで欠かせません。ただし、現在のベンチマークはまだ個別性が強く、分野横断的な標準が不足しているという課題も指摘されています。

## 人とAIの協働における課題とこれから

人間とAIが連携することで、日常的な作業から高度な推論を要するタスクまで、幅広い課題に対してより賢く、効率的に取り組むことが可能になります。ただし、そうした仕組みの導入には、新たな可能性だけでなく、慎重な検討が求められる課題も含まれています。

最近では、自然言語の理解や生成、推論の精度が高まったことで、AIを組み込んだシステムがより高度な協働を担えるようになってきました。一方で、研究と実装の両面において、いくつかの本質的な壁も指摘されています。

以下では、注目すべき5つの観点に分けて、現在の限界と今後の展望を見ていきます。

### 課題１：AI主導に偏りがちな設計

多くの現行システムでは、人間がAIの出力を評価したり、フィードバックを返したりする一方向のやり取りが中心です。つまり、AIは助言を受ける立場であり、能動的に人間を導く役割は担っていません。

しかし、AIが人の行動を観察し、判断ミスや非効率を察知して適切なタイミングで提案を行うことができれば、協働のスタイルは一変します。進行中のタスクに対して別の選択肢を提示したり、見落とされたリスクを指摘したりできるAIが加わることで、両者にとってより有益な関係が築けます。

真の意味での協働は、人間とAIが対等なパートナーとして相互の知見を尊重することで実現します。現状の多くは、あくまで人間中心の設計にとどまっており、AIの視点からのフィードバックが十分に活かされていません。今後は役割のバランスを見直し、より対等な連携を目指す必要があります。

### 課題２：人間側の多様性とその扱いの難しさ

人によってタスクへの関与の仕方は大きく異なります。フィードバックの頻度、タイミング、語調なども一定ではありません。こうした多様性は、人間ならではの柔軟性の表れでもありますが、AIとの連携を設計するうえでは難題にもなります。

加えて、システム上で「人」は特別な存在として扱われることが多く、AIに比べて制約や評価基準が緩やかです。しかし、結果としてのエラーや非効率の原因が人間側にある場合、その問題が見過ごされがちになります。人とAIの両方を等しく評価し、改善の対象とする姿勢が必要です。

また、実験の多くは実際の人間の代わりにシミュレーションされた「擬似的な人間」を使っています。そのため、現実の人間が持つ予測不可能性や多様性が反映されにくく、実運用とのギャップが大きくなる懸念もあります。

### 課題３：評価の基準がAI寄りに偏っている

現在広く使われている評価指標は、主にAIの出力の正確さやパフォーマンスに焦点を当てています。一方で、人間の側にどれだけ負担がかかっているか、または協働によって本当に作業効率が向上しているのか、といった観点は十分に測られていません。

たとえば、どれくらいの時間を要したか、どれほど集中力を要したか、どれだけ明確な指示が必要だったか、といった点も含めて、協働の質を定量的に把握する仕組みが求められています。

人とAIが組み合わさることでより高い成果が出せるとしても、その過程にかかるコストが見過ごされては意味がありません。今後は、双方の貢献と負荷をバランスよく測る包括的な評価方法の開発が鍵になります。

### 課題４：セキュリティと信頼性の課題が未解決のまま

多くの取り組みでは、パフォーマンスの向上に重点が置かれる一方で、安全性や堅牢性、プライバシーへの配慮は後回しになっています。とくに、人間とAIがリアルタイムで関わる場面では、誤作動や機密情報の漏洩など、リスクの管理が不可欠です。

たとえば、AIが不適切な振る舞いをしたときの対応策、データ共有における透明性、会話の途中での逸脱への備えなどが整っていなければ、実際の利用にはつながりません。設計の初期段階から人間の立場を重視し、安全性とプライバシーに対する明確な方針を組み込む必要があります。

### 課題５：協働スタイルの整理が不十分

「協働」や「補完」などの言葉は使われるものの、それぞれがどのようなパターンを含んでいるのかは明確でないことが多くあります。たとえば「委任」と「調整」では、やり取りの性質も、責任の所在も大きく異なります。

委任では、人が方針だけを示し、AIが具体的な実行を担います。調整では、両者が互いの動きを見ながら細かく役割を分担していきます。こうした細分化がなされていなければ、実験結果を比較することも、設計に一貫性を持たせることも困難になります。

とくに、リアルタイムでの連携や高い信頼性が求められる場面では、役割ややり取りのかたちが曖昧であることが、大きなリスクにつながりかねません。精緻な分類と定義の整備が進めば、より滑らかで効果的な連携が実現しやすくなります。

## まとめ

本記事では、人とAIの協働を扱った枠組みとその設計に関する研究を紹介しました。

役割分担や情報のやり取りといった要素を軸に、協働の構成を整理しています。実装を支えるツールやベンチマークも紹介されており、評価や比較の参考になることが期待されます。

また、設計上の偏りや評価手法の限界、安全性の課題なども指摘されています。

本稿で例を示したように、LLMと人間が協調するエージェントシステムの導入を検討する際は、活用する文脈やタスクの特性に応じて協働の形を見極める視点が求められます。

**参照文献情報**

- タイトル：A Survey on Large Language Model based Human-Agent Systems
- URL： [https://doi.org/10.48550/arXiv.2505.00753](https://doi.org/10.48550/arXiv.2505.00753)
- Github： [https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers](https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers)
- 著者：Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Yankai Chen, Chunyu Miao, Hoang Nguyen, Yue Zhou, Weizhi Zhang, Liancheng Fang, Langzhou He, Yangning Li, Yuwei Cao, Dongyuan Li, Renhe Jiang, Philip S. Yu
- 所属：University of Illinois Chicago, The University of Tokyo, Tsinghua University, Google DeepMind

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[プロンプトによるLLM応答のパーソナライゼーション　仮説を活用して文体を調整](https://ai-data-base.com/archives/89384)

[LLM活用アプリ開発事例　ユーザーの目標達成を支援する習慣化システムの作り方](https://ai-data-base.com/archives/89474)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)