---
title: "LLMには正解例だけでなく、「よくある間違い例」と理由も一緒に教えるのが有効"
source: "https://ai-data-base.com/archives/77507"
author:
  - "[[AIDB Research]]"
published: 2024-10-25
created: 2025-06-13
description: "本記事では、LLMの思考プロセスに関する研究を紹介します。LLMの思考ステップを示す「Chain-of-Thought（CoT）」の仕組みは今でも謎に包まれています。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、LLMの思考プロセスに関する研究を紹介します。

LLMの思考ステップを示す「Chain-of-Thought（CoT）」の仕組みは今でも謎に包まれています。そこで研究チームは、LLMが前のステップを記憶して次のステップを考えるという「思考の連続性」に着目し、理論的な解明に挑戦しました。そこから導かれる実用的なテクニックも注目に値します。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_77507_top2-1024x576.jpg)

**参照論文情報**

- タイトル：A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and Error-Aware Demonstration
- 著者：Yingqian Cui, Pengfei He, Xianfeng Tang, Qi He, Chen Luo, Jiliang Tang, Yue Xing
- 所属：Michi [gan](https://ai-data-base.com/archives/26269 "敵対的生成ネットワーク（GAN）") State University, Amazon

**本記事の関連研究**

- [CoT（思考の連鎖）は数学や論理で劇的に性能を向上させる一方、常識や知識のタスクでほとんど効果がない](https://ai-data-base.com/archives/75942)
- [CoTの推論ステップ数がLLMの推論能力に及ぼす影響を詳細に検証した結果](https://ai-data-base.com/archives/62364)
- [計画のステップが増えるほど、LLMは最初の目標を見失っていく傾向がある](https://ai-data-base.com/archives/77302)
- [LLMの推論能力を戦略的に向上させる新しいプロンプト手法『SCoT』](https://ai-data-base.com/archives/75505)

## 背景

LLMに「考えるステップを示す」というテクニック（Chain-of-Thought、以下CoT）が注目されています。例えば数学の問題を解く時に、「まずこう考えて、次にこうして…」というように、考え方の手順を示すものです。

このCoTが効果的だということは実験で分かってきましたが、なぜ上手くいくのか、その仕組みはよく分かっていませんでした。

これまでの研究では、考えるステップを「バラバラに切り離して」分析していました。つまり、「ステップ1→ステップ2→ステップ3」という流れを、それぞれ独立した部分として扱っていたのです。  
しかし、実際のLLMは前のステップの内容を覚えていて、それを踏まえて次のステップを考えています。そこで「各ステップのつながりを大切にした分析が必要なのでは？」という問題意識が生まれました。

また、CoTを使う時に「途中のステップで間違えると最終的な答えにどれくらい影響するのか？」という疑問もあります。例えば、数学の問題で途中の計算を間違えた場合と、最後の答えだけ間違えた場合では、どちらが深刻な影響を及ぼすのでしょうか。

このような疑問があったので、研究チームは以下の2つを明らかにしようと考えました。

1. ステップ同士のつながりを考慮した方が、バラバラに分析するよりも良い結果が得られるのか
2. 途中のステップでの間違いと、最後の答えでの間違いでは、どちらがより大きな影響を与えるのか

つまり、「考えるプロセスの一貫性」と「間違いの影響度」を理論的に解明しようとしたのです。その結果、興味深い結論がいくつか得られました。

以下で紹介します。

## Chain-of-Thought（CoT）の捉え方は二つある

まず本研究では、CoTには、Coherent CoT（一貫性のある推論）とStepwise ICL（段階的な学習）の2種類の捉え方があることを前提としています。

どちらもCoTの動作を理解するための考え方です。

Stepwise ICLはこれまで理論研究で用いられてきた単純化された考え方です。推論を独立した段階に分割し、それぞれ別で処理しているといったものです。

一方で、Coherent CoTは実際のLLMのCoT動作により近い、より現実的な考え方です。推論の全過程で文脈を保持しながら、一貫した推論を行っているというものです。今回新たに提唱されました。

これまでCoTを研究する際には推論ステップを独立して分析するアプローチ（Stepwise ICL）が一般的でした。しかし実際のLLMは、文脈全体を考慮しながら推論を行っています。そこでCoherent CoTが考案され、Stepwise ICLと比較される運びとなりました。

結果から先に述べると、本研究では理論的な分析を通じて、Coherent CoTの方がより優れた推論性能を示すことが証明されました。

### 検証の条件

研究者らはデータの生成プロセスに関して特定の仮定を設けました。

各プロンプトにおいて、例示データと質問データは同一の確率分布から独立に生成されます。

つまり、独立変数xが正規分布に従い、中間応答zがxの線形変換として生成され、最終応答yがzにノイズを加えた形で生成されます。

### Coherent CoTとStepwise ICLの比較結果

上記の条件に基づくと、Coherent CoTがStepwise ICLよりも優れた予測性能を示すことが分かりました。  
この優位性は、データの次元数やノイズの大きさに依存せず、常に成立します。

Coherent CoTの特徴は、推論全体を通じて文脈を保持し続け、適切なパラメータバランスを維持することにあります。中間予測でエラーが発生した場合でも、元の入力情報を活用して修正を行います。

この予想の妥当性は、シミュレーション実験によっても確認されました。データ数を増やすにつれて、Coherent CoTの優位性がより顕著になることが示されています。

この結果は要するに、実際のLLMの設計において、推論過程全体を通じた文脈の保持が重要であることを示唆しています。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_77507_1.png)

Coherent CoTとStepwise ICLの比較実験結果を示すグラフ

### 推論ステップの途中で誤りが発生する場合について

Coherent CoTでは推論の途中で間違いが起きた時に何が起こるのかが調べられました。

調べた結果、最後の答え（y）に誤りが混じっても、それほど大きな問題にはならないことがわかりました。また、データの量を増やすと、この影響はさらに小さくなります。しかし、最初の入力（x）や途中の推論結果（z）に誤りが混じると、モデルの予測が大きく外れてしまうことがわかりました。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_77507_2.png)

CoTの各ステップにおける感度の違いを示すグラフ（緑線と赤線が重なっているのは、yへのノイズ追加とノイズなしの場合の結果が非常に近いため）

注目すべきは、推論の途中での間違いが、最終的な結果を大きく狂わせてしまうという点です。このことから、LLMのために推論手順を設計する際には、途中の推論ステップを正確に行うことが非常に重要だとわかりました。  
実際にコンピュータ上で試してみた結果も、この考えが正しいことを示しています。

## 理論に基づくCoTの改善

前章で説明した通り、推論の途中での間違いが最終結果に大きく影響することがわかりました。この発見を活かして、CoTの性能を向上させる新しい方法が提案されています。

### 提案手法

これまでのCoTでは、正しい推論例だけをLLMに示していました。しかし今回研究者らの提案は、間違った推論例とその修正過程も一緒に示すことです。

まず問題を示し、次に誤った推論過程を「間違いの例」として明確に示します。そしてなぜその推論が間違っているのかを丁寧に説明し、最後に正しい推論過程を示します。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_77507_5-1024x418.jpg)

標準的なCoTプロンプティングと、正しい推論パス・誤った推論パスの両方を含む提案手法の比較

### 実験による検証

この手法の効果を確かめるため、複数のLLMを使って実験を行いました。

使用したモデル

- GPT-3.5-Turbo
- GPT-4o-mini
- Gemini Pro
- DeepSeek 67B

評価用データセット

1. BBHベンチマークから選んだ4つのタスク
- 曖昧さの解消に関する質問応答
- 物体の追跡（7つの対象）
- 日付の理解
- ペンギンに関する表データの分析
1. GSM8k
- 算術計算や学校レベルの数学問題

### 実験の主な結果

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_77507_3.png)

異なるデータセットとモデルにおける、手作りの誤った推論（IR）を含むCoTの性能比較

実験の結果、ほとんどのケースで提案手法が従来の方法より良い成績を示しました。特に印象的な改善が見られたのは次の二つのケースです。

一つ目は物体追跡のタスクで、Gemini Proの [正解率](https://ai-data-base.com/archives/25930 "正解率") が58.20%から64.80%へと6.60%も向上しました。

二つ目はペンギンデータの分析タスクで、DeepSeek 67Bの正解率が73.97%から80.14%へと6.17%改善しました。

### 追加実験

提案手法の重要な要素を明らかにするため、エラーの説明を含まない場合の効果も調査されました。

間違った推論の道筋が示されるものの、その推論がなぜ間違っているのかの説明が省かれたケースを検証したのです。この実験はGPT-3.5-Turboを用いて行われました。

実験の結果、エラーの説明が省かれると、提案手法の効果が大幅に減少することが確認されました。特に日付理解のデータセットでは、エラーの説明がない場合の性能（82.2%）は、従来手法（82.27%）よりもわずかに低くなりました。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_77507_4.png)

GPT-3.5-Turboを使用した、モデルのエラー説明の有無による性能比較

つまり間違った推論を提示するだけでは不十分で、その間違いの理由を説明することが極めて重要であることが示されています。説明がない場合、モデルが正しい推論と誤った推論を適切に区別できなくなる可能性があります。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_77507_6.png)

ペンギンのテーブルデータセットにおける、手作り/モデル生成の誤った推論を含むCoTの性能比較

### モデルが生成した間違い例の活用

提案手法のさらなる発展として、モデル自身によって生成された間違い例が活用されました。LLMによって生成された誤答が収集され、それらが例示として使用されたのです。手間がかかりますが、モデル自身の誤りパターンを活用することで、より効果的な改善が期待されます。（ただし、間違いの説明は手動で作成する必要があります）

この手法の効果を確認するために、ペンギンの表データセットを用いて実験が行われました。

その結果、ほとんどのモデルにおいて、モデルが生成した間違い例を使用する方が、人の手で作成された間違い例よりも高い性能が示されました。例えばDeepSeek 67Bでは、性能が82.88%から88.36%へと大幅に向上しました。

この結果は、モデルが自身の誤りパターンから効果的に学習できることを示唆しています。  
なお、従来手法の性能が先の実験結果と異なる点については、デモンストレーションに使用される例題の選択が重要な要因であることが示されています。つまり、間違い例の活用に加え、適切な例題の選択がCoTの性能向上に重要な役割を果たすということです。

## まとめ

本記事では、Chain-of-Thought （CoT）を理論面から探求した研究を紹介しました。

研究チームは、思考ステップ間の関連性を保持することでより正確な予測が可能になること、そして中間推論での誤りが最終的な答えの誤りよりも重要であることを発見しました。

この知見に基づき、正しい推論と誤った推論の両方をデモンストレーションに含める新手法を開発し、その有効性を実証しています。

- 参照論文URL： [https://arxiv.org/abs/2410.16540](https://arxiv.org/abs/2410.16540)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[o1-previewが人間のように6つの思考パターンを使い分けているとの実験結果](https://ai-data-base.com/archives/77445)

[コンテキスト内で重要な情報同士が離れすぎるとLLMの性能は大幅に下がる](https://ai-data-base.com/archives/77563)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)