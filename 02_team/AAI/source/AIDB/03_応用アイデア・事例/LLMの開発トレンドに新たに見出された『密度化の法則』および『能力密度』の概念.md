---
title: "LLMの開発トレンドに新たに見出された『密度化の法則』および『能力密度』の概念"
source: "https://ai-data-base.com/archives/80454"
author:
  - "[[AIDB Research]]"
published: 2024-12-11
created: 2025-06-13
description: "本記事では、LLMにおける新しい評価指標「能力密度」について紹介します。LLMの性能向上には巨大なパラメータ数が必要とされてきましたが、その規模拡大に伴う課題が深刻化しています。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、LLMにおける新しい評価指標「能力密度」について紹介します。

LLMの性能向上には巨大なパラメータ数が必要とされてきましたが、その規模拡大に伴う課題が深刻化しています。

このような背景から、研究者たちは高性能化と効率化を両立させる新たな指標として能力密度に注目し、その測定方法と進化の法則性を明らかにしました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_80454-1024x576.jpg)

**発表者情報**

- 研究者：Chaojun Xiao et al.
- 研究機関：Tsinghua University, ModelBest Inc.

## 背景

LLMは、その規模が大きくなるほど性能が向上することが知られています。この性能向上は、LLMの「スケーリング則」と呼ばれており、モデルのパラメータ数と学習データ量の増加が性能向上につながることが明らかになっています。この発見を背景に膨大なパラメータ数を持つLLMが開発され、様々な分野で目覚ましい成果を上げてきました。

しかし、より高性能なLLMを追求する一方で、その巨大な規模が課題として浮上してきました。 また、LLMのトレーニングや推論にかかるコストが膨大になり、その運用が困難になるという問題が発生しています。

さらに、LLMの活用範囲が拡大するにつれて、推論の効率化が喫緊の課題となっています。もはや推論にかかるコストがトレーニングコストを上回り、実用的なアプリケーションにおけるボトルネックとなっています。  
さらに最近では、スマートフォンなどの処理能力の限られたデバイスにLLMを搭載してパーソナルアシスタントとして活用する動きが活発化しており、よりコンパクトで効率的なモデルが求められています。  
また、複雑な推論タスクにおいて、LLMが推論段階でより多くの思考時間を必要とすることが明らかになっており、効率的な推論の重要性がさらに高まっています。

以上の課題を解決するために、パラメータ数を抑えながらも効率的なLLMの開発が進められています。

このように、LLM開発は、高性能化と効率化という、一見相反する方向へと進んでいます。この状況を受けて、異なる規模のLLMの品質をどのように評価すべきか、そしてLLMの効率性向上に関する法則性はあるのかという疑問が持ち上がっています。

これらの疑問に対する答えを探るべく、研究者たちは、LLMの”能力密度”という新しい概念に着目し始めました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_80454_1-1024x492.png)

2023年3月から2024年9月までの様々なLLMの能力密度を示すグラフ。密度の指数関数的な増加傾向が示されている。

## LLMの「能力密度」そして「密度化の法則」の概要

さまざまな規模のLLMが開発・公開されてきましたが、どの程度のパラメータ数でどれほどの性能が得られているかを客観的に評価する方法は明確ではありませんでした。そこで今回、研究者らはLLMの「能力密度」という新たな指標を提案しています。

能力密度とは、モデルの「性能」を、モデルがもつ「パラメータ数」で割った値とイメージすればわかりやすいでしょう。つまり、少ないパラメータ数で高い性能を引き出せるモデルほど「密度が高い」と言えます。

研究チームは、2023年以降に公開された29種類のオープンソース・ベースモデルを対象に、この能力密度を調べました。その結果、LLMの能力密度が時間の経過とともに「指数関数的に」上昇していることを突き止め、この現象を「密度化の法則（Densing Law）」と名付けました。

具体的な数字を見てみると、MMLU、BBH、MATH、HumanEval、MBPPといった5つの一般的なベンチマークを用いた評価から、最高水準の能力密度は約3ヶ月ごとに2倍になることが示唆されました。たとえば、2024年2月にリリースされたMiniCPM-1-2.4Bは、2023年9月末にリリースされたMistral-7Bと同等以上の性能を、わずか35%のパラメータ数で達成しています。これは、数ヶ月先には、同等の性能を半分以下のパラメータで実現するモデルが登場する可能性を示しています。

ただし、この「倍増ペース」は、使用するベンチマークによって多少のブレがあることもわかっています。より精密な測定のためには、LLMの能力を多面的に評価できる包括的なベンチマークが必要です。研究者らは、こうした新たなベンチマークの開発を強く促しています。

能力密度の概念と密度化の法則は、「性能を上げるにはパラメータ数を増やせばいい」という従来の常識に一石を投じ、より小さなモデルでも同等の性能を目指せる可能性を示しています。計算資源コストや環境負荷を抑えながら、高品質なモデルを社会に広く普及させられる希望とも言えます。

## LLMの「能力密度」について詳しく

### 能力密度の定義

能力密度を理解するために、まずは具体例を考えてみましょう。

スマートフォンの性能を比較する際、単に「メモリ容量が大きい方が良い」とは限りません。同じ4 [GB](https://ai-data-base.com/archives/26343 "勾配ブースティング") のメモリでも、効率的なソフトウェア設計により、8 [GB](https://ai-data-base.com/archives/26343 "勾配ブースティング") のメモリを搭載した他機種と同等の性能を発揮することがあります。この場合、4 [GB](https://ai-data-base.com/archives/26343 "勾配ブースティング") のモデルの方が「メモリ効率が良い」と言えます。

LLMの能力密度も、これと同じような考え方に基づいています。

### 能力密度の計算方法

#### Step 1：モデルの性能曲線を作る

まず、基準となるモデルのシリーズ（例：1B、2B、4B、8Bパラメータなど）を用意し、それぞれの性能を測定します。

例えば、

- 1Bモデル：精度60%
- 2Bモデル：精度70%
- 4Bモデル：精度80%
- 8Bモデル：精度85%

となります。

この関係から、「パラメータ数と性能の関係」を示すグラフ（関数）を作ります。

#### Step 2：評価したいLLMの実効パラメータを推定する

例えば、2Bパラメータの新しいLLMが80%の精度を達成したとします。

この性能は、参照モデルでは4Bパラメータが必要な水準でした。したがって、この新しいLLMの「実効パラメータサイズ」は4Bとなります。

#### Step 3：能力密度を計算する

```js
能力密度 = 実効パラメータサイズ ÷ 実際のパラメータサイズ
        = 4B ÷ 2B
        = 2.0
```

能力密度が2.0というのは、このLLMは「パラメータあたりの効率が2倍良い」と言い換えることができます。

### 能力密度の”推定”方法

研究チームは、LLMの能力を正確に評価するため、独自の2段階推定方法を開発しました。

#### 第1段階：損失推定による基礎評価

まず、言語モデルとしての基本的な性能として、テキスト予測タスクにおける精度をスケーリング則に基づいて評価します。この段階では、モデルのパラメータ数と予測精度の関係性を数理的に分析し、基礎的な性能指標を確立します。

#### 第2段階：実用タスクでの性能評価

次に、実際の応用場面での性能を推定します。質問応答や文章要約など、様々な実用的タスクでの性能データを収集し、先ほどの基礎評価との相関関係を分析します。この過程では、既存のオープンソースLLMのデータを参照点として活用します。

### 「密度化の法則」を発見

研究チームは、29種類のオープンソースLLMを詳細に分析した結果、興味深い法則性を発見しました。それは「密度化の法則」です。

これが何かというと、LLMの能力密度が時間とともに指数関数的に向上するという法則です。実際に、約3ヶ月で能力密度が2倍になるという驚くべき成長率が観察されました。半導体産業におけるムーアの法則のようなとらえ方もできるかもしれません。

この傾向（法則）は、単純なモデルの大規模化ではなく、より効率的な設計と実装が重要視されるようになっていることを意味しています。限られた計算資源でより高い性能を実現するという方向性を具体化したものとも言えます。

### 今後の課題

しかしLLMの能力密度を測定する上では、以下の課題があるともされています。

現在の技術では、LLMの能力を絶対的な数値として測定することが難しい状況にあります。そのため、現状では相対的な能力密度の算出に留まっていますが、将来的にはLLMの知能レベルをより正確に測定できる手法の開発が期待されています。

また、能力密度の測定結果は、評価に使用するベンチマークの質に大きく依存します。LLMの能力は日々進化しており、その進化する能力を適切に反映できる包括的な評価データセットの開発が求められています。様々な種類の言語理解能力や推論能力を公平に評価できるベンチマークの整備が重要となっています。

## 密度化の法則を実際に測定する実験

### 能力密度の評価設定

「密度化の法則」を検証し、LLMの能力密度の変化を時系列で分析するため、研究者らは、Llama-1のリリース以降に広く使用されているLLMを選択し、その能力密度を測定しました。Llama-1以前のオープンソースモデルは、選択されたデータセットでは意味のある性能を達成できなかったため、対象外となっています。

**データセット**

評価には、広く使用されているデータセットが採用されました。タスクとともに以下に示します。

- MMLU：知識集約型タスク
- BBH：難しい論理推論タスク
- MATH：数学的推論タスク
- HumanEval：コーディングタスク
- MBPP：コーディングタスク

評価は、オープンソースツール [OpenCompass](https://github.com/open-compass/opencompass) を使用し、数ショットのインコンテキスト学習方式で行われました。モデルは、与えられた例題とテストインスタンスの入力に基づいて、最終的な回答ラベルを生成することが求められました。

**損失推定モデル**

研究チームは、能力密度を正確に測定するため、基準となるモデル（参照モデル）の開発に取り組みました。この過程で、以下のような実験設計を行いました。

まず、基盤となるモデルとして、実用的な規模で広く利用されているMiniCPM-3-4Bの学習データを採用しました。このデータを用いて、様々な規模の小型モデルを作成し、学習を進めました。

モデルの [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") には、効率的な注意機構である「Grouped-query attention」を実装し、 [活性化関数](https://ai-data-base.com/archives/27011 "活性化関数") にはSiLUを採用しました。また、学習の安定性を確保するため、段階的に学習率を調整するWarmup-Stable-Decayスケジューラを使用しています。

学習データ量についても、慎重に設計を行いました。各モデルのパラメータ数（N）に対して、10倍、15倍、20倍、30倍、40倍、60倍のトークン数で学習を行い、データ量と性能の関係を詳細に分析しました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_80454_2.png)

0.005Bから0.8Bまでの様々なサイズのモデルのパラメータ設定を示す表

**性能推定モデル**

性能推定の段階では、MiniCPM-3の様々なバージョンとその学習過程で得られた中間モデルを活用しました。5億から数十億のパラメータを持ち、基準となるモデルと同じ語彙を使用しながらも、異なる規模とデータセットで学習されています。

**評価対象モデル**

能力密度の時間的な変化を調査するため、2023年にリリースされたLlama-1以降の主要なオープンソースモデルシリーズを評価対象としました。使用されたモデルシリーズは以下の通りです。

- Llama
- Falcon
- MPT
- Phi
- Mistral
- StableLM
- TinyLlama
- MiniCPM

評価にあたっては、各モデルの公式な技術レポートのデータを優先的に使用しています。

ただし、評価の正確性を確保するため、命令チューニングを行っていない事前学習モデルのみを対象としました。命令チューニングのデータに評価用テストデータと類似したデータが含まれている可能性があり、結果が歪む懸念があるためです。また、事前学習の段階で教師ありデータを使用しているモデルも多く、テストデータの汚染という課題も残されています。この問題については、今後の研究でより適切な評価方法を確立する必要があります。

### 「密度化の法則」

研究チームは、損失と性能の関係を分析した後、2023年のLlama-1以降にリリースされた主要なオープンソースモデルの能力密度を詳細に調査しました。その結果、注目すべき傾向が明らかになりました。

まず、LLMの能力密度は時間とともに著しい向上を示しています。例えば、2023年2月にリリースされたLlama-1の密度は0.1未満でしたが、最新のGemma-2-9BやMiniCPM-3-4Bでは3という高い値を達成しています。この進歩は、主に学習データの質と量の改善によるものです。Llama-1が1.4兆トークンの学習データを使用していたのに対し、Llama-3では15兆トークンの厳選されたデータを活用しています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_80454_3-edited.png)

(a)損失推定と(b)パフォーマンス推定の結果を示す2つのグラフセット

しかし興味深いことに、モデルの性能向上が必ずしも高い密度につながるわけではありません。たとえば、Llama-3.1-405Bは圧倒的な性能を誇りますが、能力密度の観点では最適とは言えません。大規模なモデルほど、計算資源や学習データの制約により、完全な最適化が困難になるためです。

この傾向をより正確に理解するため、研究チームは能力密度の増加を数学的にモデル化しました。最大密度の対数値と時間の関係を分析したところ、以下の法則性が発見されました。

ln(ρmax) = A · t + B

ここでtはLlama-1リリースからの経過日数を、ρは時刻tでの最大密度を表します。分析の結果、A ≈ 0.0073という値が得られ、これは約95日で密度が2倍になることを示しています。この関係性は高い精度（ [R2](https://ai-data-base.com/archives/26434 "決定決定係数（R2）") = 0.912）で確認されました。

### 密度化の法則がもたらしてくれる示唆

研究者らは、密度化の法則と評価結果から得られた重要な示唆について議論を行いました。

**推論コストの指数関数的な減少**

研究者らは、LLMの密度が約3ヶ月ごとに倍増する指数関数的な成長を観察しました。つまり、現在のモデルと同等の性能を達成するために必要なパラメータ数が、3ヶ月後には半分で済むようになる可能性があるということです。

パラメータ数の削減は、計算コストの直接的な低減につながります。同じ性能を実現するためのコストが、時間とともに大幅に下がっていくのです。この傾向を具体的に示すため、研究者らはGPT-3.5以降に登場した高性能モデルのAPI価格を分析しました。その結果、2022年12月時点でGPT-3.5が100万トークンあたり20ドルだったのに対し、2024年8月にはGemini-1.5-Flashが0.075ドルまで低下し、実に266.7分の1になったことが判明しました。概算すると、推論コストは2.6ヶ月ごとに半減していることになります。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_80454_4.png)

2023年から2024年にかけてのLLMの価格推移グラフ

さらに興味深いことに、推論コストの減少速度は、モデルの密度向上の速度を上回っています。コストがパラメータ数だけでなく、推論システムの効率化にも大きく依存しているためです。自己注意層のメモリアクセス最適化や、フィードフォワードネットワークの効率的な計算など、様々な技術が開発されてきた成果とも言えます。

**密度化の法則とムーアの法則の相乗効果**

密度化の法則は、モデルの効率が時間とともに指数関数的に向上することを示しています。ここで、ムーアの法則との関連についても考えてみましょう。ムーアの法則は計算機ハードウェアの性能向上を予測しています。研究者らは、これら2つの法則が組み合わさることで、近い将来、高性能なLLMがスマートフォンやPCでも省電力で動作するようになると予測しています。

具体的な数値を見てみましょう。同じ価格のチップの計算能力は約2.1年で倍増し、モデルの密度は3ヶ月で倍増します。この相乗効果により、一定価格のチップで実行できるモデルの実効的な規模は、約88日で倍増すると推定されています。これは、アルゴリズムとハードウェアの両面での進歩が、予想を上回るスピードで高性能なAIの実用化を可能にすることを示唆しています。

**ChatGPTがもたらした密度向上の加速**

2022年のChatGPTの登場は大きな転換点となりました。様々なタスクでの優れた性能とゼロショットでの汎化能力は、産業界と学術界の双方に大きな影響を与えました。研究チームは、この影響をより詳しく理解するため、GPT-3以降に登場した主要なLLMの密度を分析しました。分析にはMMLUベンチマークが用いられ、興味深い結果が得られています。

データが示すところによると、ChatGPTの登場を境にモデルの密度向上が大幅に加速しました。ChatGPT登場前の傾向線の傾きはA ≈ 0.0048でしたが、登場後にはA ≈ 0.0073まで上昇し、密度の向上速度が50%も向上したことが明らかになりました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_80454_5-1024x503.png)

ChatGPTリリース前後での密度の成長率の変化を示すグラフ

この加速の背景には、主に二つの要因があると考えられます。

第一に、ChatGPTの成功によってLLMの潜在的な可能性が広く認識され、開発への投資が大幅に増加しました。

第二に、高品質なオープンソースモデルが増加したことで、研究開発の敷居が大きく下がりました。数十億パラメータ規模の比較的小型ながら高性能なLLMが多数登場し、限られた [GPU](https://ai-data-base.com/archives/26570 "GPU") リソースでも研究が可能になりました。

このような状況を踏まえ、研究コミュニティには最先端の技術やモデルのオープンソース化が推奨されています。

**モデル圧縮の課題**

LLMの実用化において、高い推論コストは大きな課題となっており、一般消費者向けデバイスでの運用は困難を極めます。この問題に対し、多くの開発者がプルーニングや蒸留といった圧縮技術を採用していますが、その効果は必ずしも期待通りではありません。

研究チームは、複数の圧縮モデルを分析しました。例えば、Llama-3.1-8Bから派生したLlama-3.2-3B/1BやLlama-3.1-minitron-4B、またGemma-2-27Bから派生したGemma-2-9B/2Bなどです。分析の結果、興味深いことに、Gemma-2-9B以外のすべての圧縮モデルで、元のモデルよりも密度が低下していることが判明しました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_80454_6-1024x800.png)

圧縮前後のモデルの密度を比較するグラフ

このことは、一見直感に反するように見えます。プルーニングは重要度の低いニューロンを削除する手法であり、理論的には密度の向上につながるはずだからです。この予想外の結果について、研究チームは圧縮プロセスにおける学習の不十分さを指摘しています。今後の研究では、圧縮モデルの学習プロセスの改善に重点を置く必要があると提言されています。

※上記で言及されている圧縮は、量子化を意味していません。プルーニングと蒸留による圧縮は量子化とは異なる技術のためです。

**密度最適化を目指した学習手法への転換**

これまでのLLM開発は、GPT-3の登場とスケーリング則の発見以来、モデルの大規模化による性能向上を追求してきました。その結果として、PaLM-540BやGopher-280Bなどの巨大モデルが登場し、 [自然言語処理](https://ai-data-base.com/archives/26319 "自然言語処理（NLP）") の様々な課題で大きな進歩を遂げてきました。

しかしながら、事前学習に必要な計算資源には常に制約があります。そのため、開発者たちは限られた計算資源を最大限に活用し、学習効率の最適化に注力してきました。さらに近年では、推論時の計算コストが学習時のコストを上回る懸念が強まっており、より小規模なモデルを大規模なデータで学習させる方向へと、開発の軸足が移りつつあります。

このような背景の中、今回研究チームは密度化の法則の発見を踏まえ、新たな開発アプローチ「密度最適な事前学習」への転換を提言しています。世界中で激しい開発競争が繰り広げられる中、モデルの密度は急速に向上し、個々のモデルの有効寿命は短縮の一途をたどっています。

従来のように単純に事前学習データを増やし続けるアプローチのままでは、開発期間の長期化とコストの高騰を招くリスクがあります。注目すべきなのは、新しいモデルがリリースされてからわずか3ヶ月後には、同等の性能でより効率的な新モデルが登場する可能性が高いという点です。

このような状況を考慮すると、いま開発者には新たな視点が求められています。モデル密度の向上傾向を踏まえた上で、より効率的で汎用性の高い学習技術を取り入れる必要に迫られているかもしれません。

## 今後の展望

この論文で提案された「能力密度」という新しい概念であり評価指標には、主に3つの重要な課題と展望があります。ここまでのセクションでも述べられていたことですが、改めて整理します。

**より正確な評価手法の必要性**

現在の技術では、LLMの絶対的な能力を正確に測定することは困難です。研究チームは相対的な密度値を測定する方法を考案しましたが、より精密な評価手法の開発が今後の課題となっています。

**ベンチマークの限界**

現在使用されているベンチマークには数の制限があり、また一部のモデルがベンチマークに過度に最適化されている可能性も指摘されています。このため、より包括的で公平な評価基準の確立が求められています。

**新たな研究の方向性**

研究チームは、以下の3つの重要な研究分野を特定しています。

1. マルチモーダルモデルの密度評価手法の開発
2. より効率的な推論プロセスの実現
3. 新しい評価データセットの継続的な開発

## まとめ

本記事では、LLMの効率性を評価する新しい指標として「能力密度」を紹介しました。

この指標を用いて2023年以降にリリースされたオープンソースのLLMを分析した結果、モデルの能力密度が約3ヶ月ごとに倍増するという「密度化の法則」が発見されました。

この法則は、より少ないパラメータ数で同等の性能を達成できるようになることを示唆しており、LLMの急速な技術進歩と効率化の方向性を明確に示しています。

**参照文献情報**

- タイトル：Densing Law of LLMs
- URL： [https://arxiv.org/abs/2412.04315](https://arxiv.org/abs/2412.04315)
- 著者：Chaojun Xiao, Jie Cai, Weilin Zhao, Guoyang Zeng, Biyuan Lin, Jie Zhou, Zhi Zheng, Xu Han, Zhiyuan Liu, Maosong Sun
- 所属：Tsinghua University, ModelBest Inc.

## 理解度クイズ（β版）

1\. 「密度化の法則」における能力密度の倍増周期はどのくらいですか？

研究チームの分析によると、LLMの能力密度は約95日（およそ3ヶ月）で2倍になることが示されました。数式では ln(ρmax) = A · t + B で表され、A ≈ 0.0073という値が得られました。

解説を見る

2\. 研究チームが評価に使用したベンチマークに含まれていないものはどれですか？

研究チームは評価に5つの主要なベンチマーク（MMLU、BBH、MATH、HumanEval、MBPP）を使用しました。GLUEベンチマークは本研究の評価対象には含まれていませんでした。

解説を見る

3\. MiniCPM-1-2.4Bが達成した成果として正しいものはどれですか？

2024年2月にリリースされたMiniCPM-1-2.4Bは、2023年9月末にリリースされたMistral-7Bと同等以上の性能を、わずか35%のパラメータ数で達成しました。

解説を見る

4\. 密度化の法則とムーアの法則の組み合わせにより、一定価格のチップで実行できるモデルの実効的な規模は何日で倍増すると推定されていますか？

同じ価格のチップの計算能力は約2.1年で倍増し、モデルの密度は3ヶ月で倍増します。この相乗効果により、一定価格のチップで実行できるモデルの実効的な規模は約88日で倍増すると推定されています。

解説を見る

5\. ChatGPTの登場後、密度向上の速度はどのように変化しましたか？

ChatGPT登場前の傾向線の傾きはA ≈ 0.0048でしたが、登場後にはA ≈ 0.0073まで上昇し、密度の向上速度が50%向上したことが研究で明らかになりました。

解説を見る

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMにおける事実性の評価＆向上に役立つデータセットの作り方](https://ai-data-base.com/archives/80376)

[科学者はLLMをどう使っているのか、何を好むのか](https://ai-data-base.com/archives/80509)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)