---
title: "RAGとLong-Contextの比較、そしてハイブリッドで活用する新しい方法"
source: "https://ai-data-base.com/archives/73468"
author:
  - "[[AIDB Research]]"
published: 2024-07-29
created: 2025-06-13
description: "本記事では、RAG（検索拡張生成）と、ロングコンテキストLLM（長文を処理できる大規模言語モデル）を比較した最新研究を紹介します。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、RAG（検索拡張生成）と、ロングコンテキストLLM（長文を処理できる大規模言語モデル）を比較した最新研究を紹介します。

Googleの研究者らは、複数のベンチマークデータセットを用いてRAGとロングコンテキストLLMの性能を評価し、さらに両者の長所を組み合わせたアプローチ「Self-Route」を提案しています。

またRAGの失敗原因の詳細な分析なども含めた多角的な考察が行われています。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73468-1024x576.jpg)

**参照論文情報**

- タイトル：Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach
- 著者：Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Michael Bendersky
- 所属：Google DeepMind, University of Michi [gan](https://ai-data-base.com/archives/26269 "敵対的生成ネットワーク（GAN）")

**本記事の関連研究** ：

- [ロングコンテキストはRAGもText to SQLも解決するか　Googleがケーススタディを実施](https://ai-data-base.com/archives/71486)
- [RAGにおいて長文を検索することで一気に効率を上げるアプローチ『LongRAG』](https://ai-data-base.com/archives/71774)
- [多くの「長いコンテキストを要するタスク」を、短いコンテキストウィンドウのLLMで解決する手法](https://ai-data-base.com/archives/69938)
- [LLMのプロンプトに数百から数千の例を含める超長尺のコンテキスト内学習（In-context learning）とファインチューニングの性能比較](https://ai-data-base.com/archives/68564)

## 背景

最近、LLMに長文を理解させることが注目を集めています。LLMの長文理解に対しては、RAGとロングコンテキストLLMの2つがアプローチとして該当します。

RAGは、入力されたクエリに関連する情報を外部のデータベースから検索し、その情報をもとにLLMが回答を生成します。外部知識にアクセスすることで、効率的に長文を処理することが可能となります。

ロングコンテキストLLMについては、最新のLLMモデルであるGeminiなどは、直接数百万トークンもの長い文脈を理解する能力を示しています。

RAGとロングコンテキストLLMにはそれぞれ長所と短所があります。RAGは計算コストが低いという長所がありますが、複雑な推論を要する質問に対しては性能が劣る場合があります。一方、ロングコンテキストLLMは高い性能を示す一方で、計算コストが高くなるという問題があります。

今回研究者らは、RAGとロングコンテキストLLMの性能と効率性を様々な公開データセットで徹底的に比較しました。さらに、両者の長所を活かすハイブリッドアプローチも提案しています。

以下は本研究を通して示されるRAGとLCの性能とコストの比較、およびSELF-ROUTEの位置づけを表す図です。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73468_1-1024x731.png)

## RAGとロングコンテキストLLMのベンチマークテスト

### データセットと評価指標

本研究では、 [LongBench](https://github.com/THUDM/LongBench) と [∞Bench](https://arxiv.org/abs/2402.13718) という二つのベンチマークから選ばれたデータセットが使用されました。長文理解能力を評価するために最近開発された包括的なベンチマークです。

評価指標としては、オープンエンド（自由回答）型質問応答タスクには [F1スコア](https://ai-data-base.com/archives/26112 "F1スコア（F値）") 、多肢選択式質問応答タスクには正確性、要約タスクにはROUGEスコアが採用されました。

### モデルと検索システム

実験には、最新の3つのLLMが使用されました。

1. **Gemini-1.5-Pro  
	**Googleが開発した100万トークンまで処理可能な長文対応LLM
2. **GPT-4o  
	**OpenAIの最新軽量モデルで、12.8万トークンまで処理可能
3. **GPT-3.5-Turbo  
	**1.6万トークンまで処理可能なモデル

また、検索システムには、ContrieverとDragonという2種類が採用されました。

1. Contriever  
	対照学習で訓練された高性能な密ベクトル検索システム
2. Dragon  
	汎用性の高い検索システムで、教師あり学習と零発学習の両方で高い性能を示す

長文は300語ごとにチャンクに分割され、クエリとの類似度に基づいて上位k個のチャンク（デフォルトはk=5）が選択されます。選択されたチャンクは類似度順に並べられ、各チャンクの先頭にはインデックスが付与されます。

### ベンチマーク結果

実験結果は下記の表にまとめられています。3つのLLMそれぞれについて、RAGとロングコンテキストの性能が比較されています。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73468_3-1024x444.png)

Contriever検索システムを使用した、Gemini-1.5-Pro、GPT-3.5-Turbo、GPT-4Oの結果比較表

各データセットの名称と説明を一覧で示します。

1. **Narr: NarrativeQA  
	**小説や映画脚本などの長い物語に基づく質問応答データセット
2. **Qasp: Qasper  
	**[NLP](https://ai-data-base.com/archives/26319 "自然言語処理（NLP）") 分野の学術論文に関する質問応答データセット
3. **Mult: MultiFieldQA  
	**法律文書、政府報告書、百科事典、学術論文など多分野の文書に基づく質問応答データセット
4. **Hotp: HotpotQA  
	**Wikipedia記事に基づく2ホップの推論を必要とする質問応答データセット
5. **2Wiki: 2WikiMultihopQA  
	**Wikipedia記事に基づく最大5ホップの推論を必要とする質問応答データセット
6. **Musi: MuSiQue  
	**最大4ホップの推論を必要とする複雑な質問応答データセット
7. **Sum: QMSum  
	**会議の議事録に基づくクエリベースの要約データセット
8. **En.QA  
	**長い小説に基づく質問応答データセット（∞Benchから）
9. **En.MC  
	**長い小説に基づく多肢選択式質問応答データセット（∞Benchから）

なお、表の中にあるSELF-ROUTEについては後ほど説明します。

結果から、ロングコンテキストがRAGを一貫して上回っていることが明らかになりました。平均して、Gemini-1.5-Proでは7.6%、GPT-4Oでは13.1%、GPT-3.5-Turboでは3.6%の性能差が観察されました。

ただし、∞Benchの2つの長文データセット（En.QAとEn.MC）においては例外が見られました。GPT-3.5-TurboにおいてRAGがLCを上回る結果となりました。データセットの平均文脈長（14.7万語）がGPT-3.5-Turboのコンテキストウィンドウ（1.6万トークン）を大幅に超えていることに起因する例外と考えられます。

## ハイブリッドアプローチ

下の図は、RAGとロングコンテキストでの予測スコアの差の分布を示しています。多くのクエリに対してRAGとロングコンテキストのスコアが非常に近いことが明らかになっています。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73468_2.png)

63%のクエリで両者の予測が完全に一致し、70%のクエリでスコアの差が10未満（絶対値）でした。さらに興味深いことに、この一致は正解の場合だけでなく、誤りの場合にも見られました。

このことから、以下のような戦略が有効であると期待できます。

1. 大半のクエリに対してRAGを使用する
2. ロングコンテキストの優位性が顕著な一部のクエリにのみロングコンテキストを適用する

うまくいけば、全体的な性能を維持しつつ、計算コストを大幅に削減できる可能性があります。

### 手法「Self-Route」

上記の発見に基づき、RAGとロングコンテキストを組み合わせた「Self-Route」という新しい手法が提案されました。LLM自身の自己反省能力を活用してクエリの振り分けを行う手法です。

Self-Routeには以下の2つのステップがあります。

（１）RAG-and-Routeステップ

1. クエリと検索されたチャンクがLLMに提供される
2. LLMは、与えられた情報でクエリに回答可能かどうかを判断する
3. 回答可能と判断された場合、RAGによる予測が最終回答として採用される
4. 回答不可能と判断された場合、次のステップに進む

（２）長文コンテキスト予測ステップ

1. 回答不可能と判断されたクエリに対しては、全文脈が長文コンテキストLLMに提供される
2. LLMはこの全文脈を基に最終予測を行う

この手法の良いところは、大多数のクエリがRAG-and-Routeステップで処理されることです。例えば、Gemini-1.5-Proでは82%のクエリがこのステップで処理されました。RAG-and-Routeステップでは、入力として検索されたチャンク（約1,500トークン）のみが必要であり、すべてのコンテキスト（1万から10万トークン）よりも大幅に短いため、計算コストを大きく削減することができます。

### 結果

下の表にSelf-Routeの結果が示されています。（表は再掲）

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73468_3-1024x444.png)

Contriever検索システムを使用した、Gemini-1.5-Pro、GPT-3.5-Turbo、GPT-4Oの結果比較表

性能面では、Self-RouteはRAGを大きく上回り、ロングコンテキストとほぼ同等の結果を達成しました。3つのモデル全てにおいて、Self-RouteはRAGを5%以上上回りました。ロングコンテキストと比較すると、GPT-4o（-0.2%）とGemini-1.5-Pro（-2.2%）でわずかな性能低下が見られましたが、GPT-3.5-Turboでは逆に性能向上（+1.7%）が観察されました。

効率性の面では、Self-Routeは大幅なトークン数の削減を実現しました。例えば、GPT-4oではロングコンテキストの61%のトークン数で同等の性能を達成し、Gemini-1.5-Proでは38.6%のトークン数で済みました。LLMのAPI料金がトークン数に基づいて課金される点を考慮すると、この削減は大きなコスト削減につながります。

長文データセットにおいては、Self-Routeの優位性がより顕著に現れました。中でもGPT-4oではEN.QAとEN.MCでそれぞれ2.3%と7.4%の性能向上が見られました。GPT-3.5-Turboではさらに大きな向上が観察されました。

一方、Gemini-1.5-Proでは長文データセットにおいてロングコンテキストより低い性能を示しました。この違いは、モデルのアラインメントの差によるものと推測されています。OpenAIのモデルはRAGによる回答をより慎重に拒否する傾向があり、これが低い回答可能率と高い精度につながっているようです。

## 結果に対する分析

### kの影響

RAGとSelf-Routeの性能は、検索される上位kチャンクの数に大きく依存します。この影響を調査するため、異なるk値に対する性能とコスト（入力トークンの割合）の関係がプロットされています。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73468_4.png)

性能面では、RAGとSelf-Routeの両方において、kが大きくなるほど性能が向上する傾向が見られました。より多くのチャンクがLLMに提供されることで、徐々にロングコンテキストの性能に近づくためです。また小さいkの値では、Self-RouteのRAGに対する優位性が顕著に現れました。例えば、k=1の場合、RAGの精度が20.24%であるのに対し、Self-Routeは37.9%を達成しています。

一方、コスト面では、Self-Routeにおいて興味深い傾向が観察されました。k=5の時にコストが最小となっています。kが増加するとRAG（およびルーティング）のコストは増加しますが、同時により多くのクエリがRAGで処理されるようになり、全体のコストが減少する可能性があるためです。

ただし最適なkの値はデータセットによって異なるでしょう。例えば、NarrativeQAやQMSumのような抽出型の質問を含むデータセットではk=1が最もコストが低くなっています。タスクの性質や要求される性能によって最適なkが変わるのだと思われます。

### RAGの失敗理由

RAGがロングコンテキストに劣る原因を理解するため、RAGで「回答不可能」と判断された例の失敗理由が分析されました。まず手動での確認によって、以下4つの典型的な失敗理由が特定されました。

A. 複数ステップの推論が必要な質問  
B. 一般的すぎる質問  
C. 長くて複雑な質問  
D. 暗示的でコンテキスト全体の理解が必要な質問

そしてGemini-1.5-Proに全ての「回答不可能」な例を上記4つに分類させました。その結果が図に示されています。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73468_6.png)

データセットの性質に応じて、失敗理由の分布には一貫性が見られました。例えば、Wikipedia基づく3つのマルチホップ推論データセット（HotpotQA、2WikiMQA、MuSiQue）では、複数ステップの検索が必要なため、Aの理由（複数ステップの推論が必要な質問）による失敗が多く見られました。NarrativeQAのような長い物語や対話を含むデータセットでは、Dの理由（暗示的でコンテキスト全体の理解が必要な質問）による失敗が多く見られました。

この分析結果からRAGの改善における方向性を考察できます。例えば、Chain-of-Thoughtプロンプティングを取り入れることで複数ステップの質問に対応できる可能性が示唆されています。また、クエリ拡張などのクエリ理解技術を再検討することで、一般的な質問や複雑な質問への対応が改善される可能性があります。

### 異なる検索システムの比較

ここまでのデータはContrieverという検索システムによる実験結果ですが、Dragonという別の検索システムを用いた結果もまとめられています。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73468_5-1024x202.png)

Contrieverを使用した結果と一貫性があることが確認され、本研究の知見が検索システムの違いを超えて一般化できることが示されました。

### 合成データセットに関する考察

本研究では主に実際のデータセットに焦点が当てられました。合成データセット（人工的に作成されたデータセット）は、意図せず結果に大きく影響を与える恐れがあるためです。

例として、∞Benchの「PassKey」データセットとその変種が挙げられています。このデータセットでは、パスキーを含む文が無関係なテキストの中に隠されており、モデルは「パスキーは何か」という質問に答えるよう求められます。このタスクでは強力な検索能力が要求されます。

興味深いことに、元のPassKeyデータセットではRAGがロングコンテキストを上回る性能を示しました（RAG: 80.34%、ロングコンテキスト: 65.25%）。しかし、質問を「テキスト内に隠された特別なトークンは何か」と少し変更すると、RAGの精度は4.58%に急落する一方、ロングコンテキストはほぼ同じ性能（69.32%）を維持しました。つまり合成データに変わった途端、結果が変わってしまったのです。

このような結果は、実験において合成データを使用することに対して慎重になるべきと示唆しています。

## まとめ

本記事では、RAGとロングコンテキスト LLMの比較研究を紹介しました。ロングコンテキストLLMは長文理解で優れた性能を示す一方、RAGは低コストが強みでした。

そして研究チームが提案した「Self-Route」手法により、ロングコンテキストLLMと同等の性能を維持しつつ計算コストを削減できることが示されました。

これまでRAGとロングコンテキストの比較のみが議論されることが多かったため、ハイブリッド手法の可能性が示されたのは非常に有意義と言えるでしょう。

- 参照論文URL： [https://arxiv.org/abs/2407.16833](https://arxiv.org/abs/2407.16833)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[Among UsのようなゲームでLLMエージェントはどれほど活躍できるか](https://ai-data-base.com/archives/71915)

[LLMでASDを含む人間同士のコミュニケーションを支援するアプリケーション開発事例](https://ai-data-base.com/archives/73534)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)