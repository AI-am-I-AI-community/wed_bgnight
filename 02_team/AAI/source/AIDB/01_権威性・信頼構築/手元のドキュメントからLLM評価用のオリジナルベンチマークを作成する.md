---
title: "手元のドキュメントからLLM評価用のオリジナルベンチマークを作成する"
source: "https://ai-data-base.com/archives/87773"
author:
  - "[[AIDB Research]]"
published: 2025-04-09
created: 2025-06-13
description: "本記事では、手元にある文書をもとに自動的に評価ベンチマークを作成する仕組みを紹介します。LLMの活用が広がる中、モデル単体だけでなく、自分の業務やシステムに統合した際の性能を適切に評価したいというニーズが高まっています。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、手元にある文書をもとに自動的に評価ベンチマークを作成する仕組みを紹介します。LLMの活用が広がる中、モデル単体だけでなく、自分の業務やシステムに統合した際の性能を適切に評価したいというニーズが高まっています。しかし、一般的なベンチマークでは、用途に特化した細かな性能評価が難しく、独自の評価セット作成には専門的な知識や手間がかかります。この課題に対して低コストかつ手軽な評価セット作成を可能にする新しい方法が提案されています。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_87773-1024x576.png)

## 背景

LLMは、さまざまな仕事に活用されるようになってきています。ただ、実際にLLMを業務やサービスに組み込んで使おうとすると、単にモデルの基本的な性能だけでなく、「自分たちのシステムに統合したときの性能はどうか」「設計したプロンプトや検索機能（RAG）との組み合わせで十分な精度が出ているか」といった、より実践的な評価が必要になってきます。

現在、モデルの性能を評価する一般的な方法として「ベンチマーク」と呼ばれる質問セットが広く使われています。ベンチマークとは、モデルやシステムが質問にどれだけ正しく答えられるかを試すための標準的なテストセットです。ただ、既存のベンチマークはモデル単体の性能を評価するものが中心で、実際のアプリケーションや具体的な業務シナリオでの性能を十分に評価するのは難しい場合があります。

たとえば、企業が独自に設計したシステムプロンプト（LLMに与える命令や設定文）や、特定の情報を検索して回答する仕組み（RAG）などを活用している場合、既存の一般的なベンチマークでは現実の性能を評価しきれないという問題があります。特定の業務分野に特化したエージェント型システムや、複雑なワークフローを組み合わせている場合にはなおさら、独自に設計された評価方法が求められます。一方で、基盤モデル自体の性能をシンプルに評価したい場合にも、もちろん独自の評価セットは有効です。

しかし、独自の評価ベンチマークを作るには、専門的な知識や作業が必要で、時間やコストがかかるため、これまで一部の専門家や大企業に限られてきました。

この課題を解決するために、今回研究者らは、手元にある文書を入力するだけで、その内容に基づいて、自分の用途に合った独自の評価ベンチマークを自動で作成できる仕組みを考案しました。PDFやHTMLなどの文書を用意してアップロードすれば、自動的に質問セット（評価用のQ&Aペア）が生成されます。

技術的な仕組みそのものは複雑ですが、ユーザーに求められる操作は簡単で、専門的な知識もほとんど必要ありません。短時間かつ低コストで、自社独自のプロンプトや検索機能、ワークフローなども含めたアプリケーション全体の性能を評価できる設計となっています。

評価作業を、専門家や大企業だけでなく、誰もが手軽に取り組めるものに変えることを目指した取り組みです。

以下で詳しく紹介します。

## 自分の文書から評価ベンチマークを作る仕組み「YourBench」

上述したように、一般的なベンチマーク（評価のための質問セット）の課題を解決するために提案されたのが「YourBench」です。利用者が持つ文書を入力するだけで、その内容に基づき、用途に合った評価ベンチマークを自動で作成する仕組みです。

YourBenchで作成された質問と回答のセットは、LLMやアプリケーションの性能を評価するために使用できます。一般的な方法として、複数の回答を比較する形式の評価が使われます。たとえば、「どちらの回答がより優れているか」を複数のLLMを使って判断し、その結果を総合してモデルの性能を評価します。また、質問を選択式に設定し、シンプルに [正解率](https://ai-data-base.com/archives/25930 "正解率") を測定する評価方法も使えます。

このツールは下記のレポジトリから使用できるようになっています。

[https://github.com/huggingface/yourbench](https://github.com/huggingface/yourbench)

そんなYourBenchの仕組みは、大きく次のステップに分かれています。

### ステップ１：文書の前処理

最初に行うのが、文書の前処理です。評価ベンチマークを作る際には、PDFやWord、HTMLなど多様な文書形式をそのまま扱うのは困難です。そのため、まずすべての文書を統一的な形式（Markdown）に変換します。また、文書内の画像などには自動的に説明文が追加され、画像の情報もテキスト化されます。

さらに、文書が長い場合には内容を小さく分割（チャンク）します。文書を分割すると全体の文脈が分かりづらくなるため、文書全体の概要（要約）も同時に作成し、後のステップで活用します。

### ステップ２：質問・回答ペアの自動生成

文書の前処理が終わったら、次にその文書から質問と回答のセット（評価用Q&Aペア）を自動で生成します。このプロセスをYourBenchでは「Document-to-Evaluation Generation（D2EG）」と呼んでいます。

このプロセスでは、以下の3つを満たす質問セットを目指します。

**網羅性**  
文書に含まれる情報を幅広くカバーすること。

**多様性**  
質問の難易度やタイプを多様にすること。

**回答可能性と品質**  
質問は元の文書を見れば明確に答えられるものであること。

これらを実現するため、次の具体的な方法を使用します。

#### （１）文脈の提示

文書を細かく分割したチャンク（局所的な詳細）と、文書全体の概要（グローバルな文脈）を組み合わせてLLMに提示します。文書内の情報を的確に反映した質問生成を行いやすくします。

#### （２）誘導型の質問生成

単に自由に質問を生成するのではなく、質問の種類（事実確認型、多段階推論型、数値回答型など）や難易度（簡単・高度）を指定して、目的に合った質問が生成されるようLLMを誘導します。

#### （３）複数モデルのアンサンブル利用

1種類のLLMだけでなく、複数のLLMを同時に使って質問を生成します。それぞれのモデルが得意とする質問のタイプが異なるため、複数モデルを利用することでより広範囲かつ多様な質問を効率的に作り出します。

#### （４）品質フィルタリング

最後に、生成された質問を自動的にチェックします。不明確だったり、文書の内容から答えられない質問は除外され、必要に応じて人間が修正を加えます。

### ステップ３：品質フィルタリングと重複除去

質問と回答が生成されると、次はその品質をさらに高め、重複した質問を取り除く作業を行います。このステップは2段階で行われます。

#### （１）根拠文の検証

生成された回答が本当に元の文書に書かれているか、根拠（引用箇所）の正確さを自動的に検証します。この検証では、引用された文と元の文書を比較することで、回答の根拠がどの程度確かかを数値的に評価します。

#### （２）意味的な重複除去と重み付け

複数のモデルから質問を生成すると、内容が似た質問が多数生じます。そこでここでは、質問を意味的にグループ化し、同じ内容を聞いている質問を自動でまとめます。そして、各グループから代表的な質問だけを残し、重複を削除します。

また、グループの大きさ（類似質問が多かったか少なかったか）に応じて質問に重みをつけます。よく出てきた重要な質問ほど、評価の際に高く評価されるようにします。

## YourBenchの性能検証結果

ここまで、YourBenchがどのように評価ベンチマークを作成するかを説明してきました。実際に役に立つかどうかを確かめるためには、YourBenchが作った評価セットの品質や信頼性を検証する必要があります。このセクションでは、YourBenchの評価ベンチマークがどれほど信頼できるかを検証するために行った実験の内容と、その結果を詳しく紹介します。

### 実験のセットアップ

#### 使用したデータセット「Tempora-0325」

LLMを評価する際の難しい問題のひとつが、「モデルが元々知っている情報で答えているのか、それとも与えられた文書の情報を使って答えているのかが分からない」ということです。そこで、本研究ではYourBenchを使用して「Tempora-0325」という新しいデータセットを作りました。このデータセットには、2025年3月1日以降に公開されたばかりの新しい文書のみを収録しています。こうすることで、モデルが過去に覚えた情報ではなく、「与えられた文書をきちんと使っているか」を明確に評価できます。

「Tempora-0325」には、政府、企業、法律、医療、スポーツ、ニュース、ブログなど、幅広い分野の文書を合計7,368個集めました。さらに、この全体データの中から、各分野が均等に含まれるバランスのよいサブセット「Tempora-0325B」を作成し、主な実験に使用しました。データセットは誰でも利用できるよう下記に公開されています。

[https://huggingface.co/datasets/sumuks/tempora](https://huggingface.co/datasets/sumuks/tempora)

#### 評価に使ったモデルの選定方法

YourBenchの質問生成性能を公平に検証するため、今回の実験では幅広いタイプの最新LLMを選びました。

実験で評価対象となったLLMは、具体的には以下の通りです。

**DeepSeekシリーズ（6モデル）**

- DeepSeek V3（671B）
- DeepSeek R1（671B）
- DeepSeek R1-Distill-Llama（70B）
- DeepSeek R1-Distill-Qwen（32B）
- DeepSeek R1-Distill-Qwen（14B）
- DeepSeek R1-Distill-Qwen（7B）

**Qwenシリーズ（5モデル）**

- Qwen2.5（72B）
- Qwen2.5（32B）
- Qwen2.5（14B）
- Qwen2.5（7B）
- Qwen QwQ（32B、推論特化型モデル）

**Mistralシリーズ（2モデル）**

- Mistral Large 2411（132B）
- Mistral 3.1 Small（24B）

**Llamaシリーズ（3モデル）**

- Llama 3.1（405B）
- Llama 3.1（8B）
- Llama 3.3（70B）

**Googleシリーズ（3モデル）**

- Gemini 2.0 Flash（モデルサイズ非公開）
- Gemini 2.0 Flash Lite（モデルサイズ非公開）
- Gemma 3（27B）

**OpenAIシリーズ（3モデル）**

- GPT-4o（モデルサイズ非公開）
- GPT-4o mini（モデルサイズ非公開）
- o3 mini（モデルサイズ非公開）

**Anthropicシリーズ（2モデル）**

- Claude 3.7 Sonnet（モデルサイズ非公開）
- Claude 3.5 Haiku（モデルサイズ非公開）

モデル群は以下のような特徴を持っています。

- 開発元が多様（OpenAI、Google、Anthropic、Mistral、Qwenなど）
- モデルサイズ（規模）が多様（小規模な7Bから最大規模の671Bまで）
- 得意な作業（特に複雑な推論に特化したモデルなど）が多様

公平性を保つため、全モデルに対して全く同じ設定で質問生成を実施しました。また、誰でも検証できるよう、生成されたデータも公開されています。

### YourBenchが生成する質問の品質評価

YourBenchが本当に役立つ評価ベンチマークを作れるかを確かめるため、生成された質問セットについて、「質問の妥当性」と「質問の多様性」という2つの観点から評価を実施しました。

#### 「質問の妥当性」の評価方法と結果

評価ベンチマークが役立つためには、質問自体が明確で、文書を見ればはっきりと答えられるものであること（妥当性）が必須です。これを検証するため、今回の実験では約2,000個の質問を対象に、訓練された評価者が実際に人手で質問を評価しました。

具体的には、質問が明確か、文書の情報だけで答えられるか、内容が論理的で適切かなどの基準で評価しました。この評価作業は複数の評価者が独立して行い、高い信頼性が確認されています。その結果、生成された質問の平均85％が十分に妥当な質問であると認められました。

#### 「質問の多様性」の評価方法と結果

質問が役に立つためには、単調でなく幅広いテーマやタイプをカバーしていることも重要です。この「多様性」を測るため、質問を数値的に分析しました。具体的には、質問文を数値化し、それぞれの質問がテーマ的にどのくらい離れているか（ばらつき）を測定しています。

結果として、妥当性と多様性の間には一定のトレードオフ（バランス）があることが判明しました。たとえば、一部のモデル（例：o3 mini）は非常に妥当性の高い質問を作りましたが、質問の内容が単調になる傾向がありました。一方で別のモデル（例：Qwen2.5 32B）は多様性が非常に高かったものの、妥当性が若干低下しました。この中でDeepSeek V3などの一部のモデルは、妥当性と多様性の両方を高いレベルで両立していました。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_87773_3-1024x1024.png)

### 回答の根拠の正確さを検証

評価の信頼性を高めるためには、回答が元の文書を忠実に引用していること（引用の正確さ）が重要です。YourBenchでは、回答の根拠となる文書の一部（引用箇所）を自動で提示する仕組みを持っています。この引用がどれくらい正確かを数値的に検証しました。

検証の結果、一部の高性能モデル（Claude 3.7 SonnetやQwenシリーズなど）は引用が非常に正確で、信頼できることが示されました。また、引用の精度と質問を生成するためにかかる計算コスト（処理にかかる時間や費用）との間にはトレードオフがあることも分かりました。たとえばQwen2.5 32Bは、比較的低いコストで高い引用精度を達成できました。このことは、YourBenchのフレームワークが効率的かつ信頼性の高い評価セット作成を可能にしていることを示しています。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_87773_4-1024x1024.png)

### 既存の評価手法（MMLU）との比較検証

最後に、既存の代表的な評価ベンチマーク「MMLU」と比較した検証を行いました。MMLUで使われている特定の分野の質問セットを、Wikipedia記事を元にYourBenchで再現し、両者の評価結果を比較しました。

この比較実験の結果、YourBenchで新たに作成した評価セットと、元のMMLUでの評価結果との間には、非常に高い相関があることが明らかになりました。特にモデルごとの平均性能を比較すると、 [相関係数](https://ai-data-base.com/archives/26481 "相関係数") がほぼ完璧（r=0.96〜1.0）であり、モデルの相対的なランキングが正確に再現されていました。この結果から、YourBenchが既存の評価手法に匹敵する品質のベンチマークを新たに作成できることが確認されました。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_87773_1-1024x682.png)

YourBenchで生成したMMLU型ベンチマークと元のMMLUの比較（モデルの性能順位を保持しつつ難易度が高い評価セットを自動生成）

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_87773_6-1024x478.png)

YourBenchで生成したMMLU型質問セットの分野別比較例

## YourBenchの始め方

公開されているリポジトリの内容をもとに、まずはプロジェクトを始めるための初めのステップを紹介します。

参照： [https://github.com/huggingface/yourbench](https://github.com/huggingface/yourbench)

**GitHubからリポジトリをクローン**

```js
git clone https://github.com/huggingface/yourbench.git
```

**リポジトリに移動**

```js
cd yourbench
```

**環境構築用ツールuvを使って仮想環境を作成し、依存パッケージをインストール**

```js
# uvをインストールしていない場合のみ
pip install uv

uv venv
source .venv/bin/activate
uv sync
uv pip install -e .
```

**APIキーを取得して環境変数として設定**

（OpenRouterからキーを取得 (https://openrouter.ai/)）

```js
touch .env
echo "HF_TOKEN=" >> .env
echo "HF_ORGANIZATION=" >> .env
```

**サンプル設定ファイルを使ってYourBenchを実行**

```js
yourbench run --config configs/example.yaml
```

独自ファイルをもとに実行する際はレポジトリを参照して設定していただけると幸いです。

## まとめ

本記事では、手元の文書から評価ベンチマークを自動作成する仕組み「YourBench」を提案した研究を紹介しました。

従来の固定的なベンチマークや手作業による評価の限界に対し、効率的で信頼性の高い評価セットを自動生成できることが検証されています。実際に、MMLUなどの既存ベンチマークの結果を再現できることも示されました。

また、農業分野での専門知識評価、教育分野での個別評価ツール開発、RAG（検索連携型生成）システムの高度な訓練データ作成にもすでに活用されているとのことです。

皆さんのビジネスや業務でも、自分の目的や分野に合わせた評価方法として役立つ可能性があります。

**参照文献情報**

- タイトル：YourBench: Easy Custom Evaluation Sets for Everyone
- URL： [https://doi.org/10.48550/arXiv.2504.01833](https://doi.org/10.48550/arXiv.2504.01833)
- 著者：Sumuk Shashidhar, Clémentine Fourrier, Alina Lozovskia, Thomas Wolf, Gokhan Tur, Dilek Hakkani-Tür
- 所属：Huggingface, UIUC

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMアプリケーション約1,500事例から学ぶプロンプトテンプレート](https://ai-data-base.com/archives/87853)

[会話メモやマニュアルをワークフロー化するLLMマルチエージェントシステムの仕組み](https://ai-data-base.com/archives/87661)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)