---
title: "LLM評価の盲点とそれを解消する手法"
source: "https://ai-data-base.com/archives/83704"
author:
  - "[[AIDB Research]]"
published: 2025-03-05
created: 2025-06-13
description: "本記事では、LLMの性能評価に関する最新の研究動向を紹介します。LLMは様々な分野で驚くべき成果を示していますが、実用化に向けては「信頼性の確保」が重要な課題となっています。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、LLMの性能評価に関する最新の研究動向を紹介します。LLMは様々な分野で驚くべき成果を示していますが、実用化に向けては「信頼性の確保」が重要な課題となっています。そのため、研究者たちは従来のベンチマークテストを根本から見直し、より正確な性能評価方法の確立に取り組んでいます。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83704-1024x576.png)

**発表者情報**

- 研究者：Joshua Vendrowほか
- 研究機関：MIT

論文情報詳細は記事の下部に記載されています。

## 背景

LLMは大学院レベルの複雑な問題を解いたり、プログラミングコードを生成したり、膨大な知識の中から必要な情報を探し出したりと目覚ましい発展を遂げています。

そのため、多くの企業がLLMの実用化に強い関心を寄せています。とはいえ、実際の業務への導入には慎重な検討が必要とされています。医療現場での診断支援や、金融機関での投資判断、法律事務所での契約書作成など、わずかな間違いが取り返しのつかない結果を招く可能性がある場面では特に導入の是非が議論されています。実際に、LLMの誤った判断が法的な問題に発展したケースも報告されています。

現在、LLMの性能を測定するために「ベンチマークテスト」と呼ばれる評価手法が広く採用されています。しかしベンチマークテストの捉え方には課題があります。あるベンチマークでLLMの正答率が90-95%に達すると、そのテストは「簡単すぎる」とみなされ、より難しい新しいベンチマークが作られる傾向にあります。しかし、実際には、残りの5-10%の誤りについては十分な検証がなされていません。

つまり、これらのエラーがベンチマーク自体の不完全さによるものなのか、それともLLMの本質的な限界を示しているのか、明確な結論が出ていない状況です。

このような「ほとんど正解しているが少し間違えることがある」ベンチマークは非常に多く存在し、利用者がどう解釈していいのか分からない状況にあります。

このような課題に対応するため、MITの研究者たちは既存の15種類の代表的なベンチマークを徹底的に見直す取り組みを始めました。曖昧な問題や不正確な評価基準を排除し、より信頼性の高いベンチマークの開発を目指しています。以下で詳しく紹介します。

## LLMの性能評価基準を見直す取り組み

現在、LLMの性能を測るためのベンチマークテストには、問題文とその正解が用意されています。ただし、データ量が膨大なため、中には不適切な問題や誤った正解が含まれている可能性が指摘されていました。そのため、研究チームはより信頼性の高い評価方法を確立すべく、既存のベンチマークの見直しに着手しました。

研究チームが選んだのは、15種類の代表的なベンチマークでした。たとえば数学の問題解決能力を測るテスト、文章の読解力を評価するテスト、写真を理解する能力を確認するテストなど、幅広い分野が含まれています。なお、データの質を厳密に確認するため、一部のベンチマークからはランダムに少数の問題が選び出されました。

以下が今回選ばれた15種類のベンチマークのリストです。

#### 数学

**SingleOp**

単一の算術演算（加算、減算、乗算、除算）の問題を含むベンチマーク。

**SingleEq**

単純な一次方程式の解法を評価するベンチマーク。

**MultiArith**

複数の演算を必要とする算数問題を含むベンチマーク。

**SVAMP**

小学生レベルの数学文章問題のセット。

**GSM8K**

小学生レベルの数学文章問題（Grade School Math 8K）を含むデータセット。

**MMLU High School Math**

高校数学の問題を対象としたベンチマーク。

#### 論理

**BIG-bench Logical Deduction**

複数の論理的推論問題を含むベンチマーク。

**BIG-bench Object Counting**

物体の数を正確にカウントする能力を測る問題を含むベンチマーク。

**BIG-bench Navigate**

仮想的なナビゲーションタスクを扱うベンチマーク。

#### 表データ理解

**TabFact**

表データに基づく事実検証の能力を評価するベンチマーク。

#### 文章読解 (Reading Comprehension)

**SQuAD2.0**

文章を読解し、質問に正しく答える能力を測るベンチマーク。

**HotPotQA**

複数の文書にまたがる情報を統合し、質問に答える能力を測るベンチマーク。

**DROP**

数値的推論を伴う読解問題を評価するベンチマーク。

#### 常識推論 (Commonsense Reasoning)

**Winograd WSC**

代名詞の指示対象を正しく推測する能力を測るWinograd Schema Challenge。

#### 視覚的理解 (Visual Understanding)

**VQA v2.0**

画像に関する質問に正しく答える能力を測る視覚質問応答（Visual Question Answering）ベンチマーク。

### 評価に使用されたLLM

実験では、2025年1月時点で利用可能な最新のLLMが使用されました。企業が開発した商用モデルだけでなく、誰でも利用できるオープンソースのモデルも含まれています。さらに、「なぜそう考えたのか」という思考プロセスを示させる手法（Chain-of-Thought）が導入されました。ただし、推論に特化したモデルについては、すでにその機能が備わっているため除外されています。

実験で使用されたLLMは以下の通りです。

1. **o1-2024-12-17 (high)**
2. **Claude 3.5 Sonnet (Oct)**
3. **o1-2024-12-17 (med)**
4. **DeepSeek-R1**
5. **Claude 3.5 Sonnet (June)**
6. **Llama 3.1 405B Inst**
7. **GPT-4o (Aug)**
8. **GPT-4o (Nov)**
9. **o1-preview**
10. **DeepSeek-V3**
11. **o1-mini**
12. **Gemini Thinking (12/19)**
13. **Qwen 2.5 72B Inst**
14. **o3-mini-2025-01-31 (high)**
15. **Grok 2**
16. **Mistral Large**
17. **Gemini 2.0 Flash**
18. **Llama 3.3 70B Inst**
19. **Llama 3.1 70B Inst**
20. **Gemini 1.5 Pro**
21. **GPT-4o mini**
22. **Claude 3.5 Haiku**
23. **Gemini 1.5 Flash**
24. **Mistral Small**

### ベンチマークの問題点を見つける方法

信頼できる評価を行うためには、テスト問題自体に曖昧さがあってはいけません。そこで研究チームは、問題点を「正解の間違い」と「問題文自体の不備」という2つのタイプに分けて調査しました。

#### 具体的な問題事例

たとえば、ある数学のテストでは「袋の数は14個」という問題に対して「2個」という誤った答えが正解とされていました。このような場合は、正解を修正すれば問題自体は使えます。

一方で、問題文自体に以下のような不備がある場合は、修正が難しいとされました。

まず、問題文の中で数字が食い違うなど、論理的に矛盾している場合。次に、複数の解釈が可能な曖昧な表現が含まれている場合。最後に、問題を解くために必要な情報が不足している場合です。

たとえば「写真に写っている赤ちゃんは靴下を履いているでしょうか？」という質問で、赤ちゃんの足元が写真に写っていないケースがありました。このような問題は、より信頼性の高い「プラチナベンチマーク」を作る過程で、除外されることになりました。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83704_1.png)

既存のベンチマークにおけるエラーの例

結果として、このような体系的な見直しにより、LLMの真の実力を正確に測れる評価基準の確立が目指されています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83704_2.png)

15の主要なベンチマークのサブセットを修正し、誤りや曖昧さを除去した結果。

## プラチナベンチマークによる信頼性評価

研究チームは、ノイズを取り除いた新しい評価基準「プラチナベンチマーク」を作成しました。さらに、異なる難易度のテストを組み合わせることで、LLMの信頼性をより正確に分析することに成功しました。

### LLMが確実に解ける問題の範囲を探る

研究チームは難易度の異なる6種類の数学テストを用意しました。そのため、LLMには、小学生レベルの計算から大学院レベルの複雑な問題まで、様々な課題が与えられます。

たとえば、1桁の掛け算はほぼすべてのLLMが完璧に解けることが分かっています。そこで、正答率が下がり始める難易度を特定することで、各モデルの「信頼できる限界」が見えてきました。

従来の評価では、正答率が90-95%に達すれば「十分な性能」とされていました。ただし、残りの5-10%のミスの原因を理解することが、実用化への重要な鍵となっています。

### 評価から得られた4つの重要な発見

#### 発見①　基本的な問題でも発生するエラー

驚くべきことに、最新のLLMでさえ、単純な計算以外のほぼすべての分野でミスを犯すことが判明しました。博士課程レベルの難問を解けるモデルが、基礎的な問題で失敗するという事実は、実用化に向けた大きな課題となっています。

#### 発見②　従来の評価方法の問題点

プラチナベンチマークと従来の評価方法を比べたところ、興味深い事実が明らかになりました。たとえば、あるテストでは「LLMのミス」とされていた事例の約75%が、実は問題文自体の不備によるものでした。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83704_4.png)

各ベンチマークにおけるモデルの平均誤答数を修正前後で比較。 大半のベンチマークで誤答数が大幅に減少し、多くは50%以上の改善。 元のベンチマークの誤りの多くがラベルノイズによるものであり、モデルの真の失敗ではない可能性を示唆。

#### 発見③　性能と信頼性の関係

一般的に、より高性能なモデルは信頼性も高いことが確認されました。たとえば、GPT-4oはGPT-4o miniと比べて、より確実に正しい答えを出せています。また、上位モデルの中には、複数のテストで100%の正答率を達成したものもありました。

#### 発見④　得意分野による違い

モデルごとに得意分野が異なることも分かりました。たとえば、o1シリーズとDeepSeek-R1は数学が得意で、Claude 3.5 Sonnet（10月版）は一般常識に関する問題で優れた成績を収めています。用途に応じて適切なモデルを選ぶ必要があるでしょう。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_83704_3.png)

各モデルの誤答数をプラチナベンチマークで集計。最先端の言語モデルは単純な課題でも信頼性に欠ける。スコアは各カテゴリごとの誤答率の平均（視覚カテゴリを除外）。 誤答なし、誤答率2%以下、誤答率5%以下を色分け表示。 \*RC：読解、Tab：表データ理解、CR：常識推論、Vis：視覚認識

### 新たに発見された2つの特徴的なミス

プラチナベンチマークを使った詳しい分析により、LLMに共通する興味深いミスのパターンが見つかりました。

#### ミスの特徴①　時系列の理解における「最初の出来事」への固執

「2番目に起きたのはどちらですか？」という質問に対して、複数のモデルが特徴的なミスを示しました。たとえば、Gemini 1.5 Flash、Gemini 1.5 Pro、Mistral Smallは、85%以上の確率で「最初の出来事」を答えてしまいます。さらに興味深いことに、「これが最初に起きました」と自ら書きながらも、誤った答えを出し続けるのです。

#### ミスの特徴②　素数に関連する不適切な四捨五入

Claude 3.5 Sonnet（6月版）は、割り算を含む問題で特徴的なミスを見せました。答えが整数になるはずの計算でも、不適切に数値を切り上げてしまうことがあったのです。研究チームが詳しく調べたところ、答えが素数に近いほど、このミスが起きやすいことが分かりました。実際、答えが素数となる問題では、約20%の確率でこの種のミスが発生しています。

結果として、高度な能力を持つLLMでも、基本的な処理で思いがけないミスを起こすことが明らかになりました。実用化に向けては、このような特定のパターンへの対策が求められそうです。

## 考察

プラチナベンチマークの開発により、LLMの信頼性評価は大きく前進しました。とはいえ、研究チームは現時点での限界と今後の課題についても詳しく検討しています。

### まだ評価できていない能力の存在

現在のプラチナベンチマークでは、数学分野は細かい難易度設定ができていますが、プログラミングやツールの使用など、重要な能力の一部が評価対象から外れています。さらに、数学以外の分野では、きめ細かい難易度分けができていない状況です。

### データ数の不足による制約

一部のテストでは、もとのデータ数の制約により、100例程度しか評価できていません。そのため、「まったく間違えない（エラー率0%）」のか「わずかに間違える（エラー率1%）」のか、統計的な判断が難しい状況となっています。

### 見直しが必要な問題の見落とし

研究チームは、LLMが間違えた問題だけを見直しました。そのため、すべてのLLMが正解した問題の中にも、実は不適切な問題文や曖昧な表現が残されている可能性があります。

### 評価できる難易度の上限

現状では、高校数学レベルまでの問題しか評価できていません。なぜなら、それ以上難しい問題になると、評価する人間側により専門的な知識が必要になるためです。将来、LLMの性能が向上した際には、各分野の専門家による評価が不可欠となるでしょう。

### 実用化に向けた重要な示唆

研究チームは、LLMへの指示（プロンプト）の影響についても注目しています。現在のLLMは、指示の言い回しのわずかな違いで性能が大きく変わることがあります。

ただし、本当に信頼できるLLMであれば、指示が明確である限り、表現の違いに左右されず、正しい結果を出せるはずです。そのため、研究チームは「指示の書き方を工夫する（プロンプトエンジニアリング）」よりも、LLM自体の信頼性を高めることが重要だと指摘しています。

## まとめ

本記事では、LLMの信頼性を評価する新しい手法「プラチナベンチマーク」の研究を紹介しました。研究チームは、既存の15種類のベンチマークから曖昧さやエラーを排除し、より厳密な評価基準の確立を目指しました。

評価の結果、最新のLLMであっても基本的なタスクで予想外の失敗を起こすことが明らかになり、実用化に向けてはさらなる改善が必要とされています。

一方で、プラチナベンチマークにも評価対象の範囲やサンプル数などの制約があることが指摘されており、今後の改善が期待されます。

[https://github.com/MadryLab/platinum-benchmarks](https://github.com/MadryLab/platinum-benchmarks)

**参照文献情報**

- タイトル：Do Large Language Model Benchmarks Test Reliability?
- URL： [https://doi.org/10.48550/arXiv.2502.03461](https://doi.org/10.48550/arXiv.2502.03461)
- 著者：Joshua Vendrow, Edward Vendrow, Sara Beery, Aleksander Madry
- 所属：MIT

## 理解度クイズ

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMはシステムプロンプトをどれほど守れるか](https://ai-data-base.com/archives/86276)

[LLMアプリケーション（LLMを利用したシステム）の安全評価方法・レッドチーミングの進め方](https://ai-data-base.com/archives/86443)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)