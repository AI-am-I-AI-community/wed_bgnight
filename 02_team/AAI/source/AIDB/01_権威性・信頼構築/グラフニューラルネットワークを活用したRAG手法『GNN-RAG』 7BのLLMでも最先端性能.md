---
title: "グラフニューラルネットワークを活用したRAG手法『GNN-RAG』 7BのLLMでも最先端性能"
source: "https://ai-data-base.com/archives/70237"
author:
  - "[[AIDB Research]]"
published: 2024-06-04
created: 2025-06-13
description: "RAGの新しい手法『GNN-RAG』が提案されています。LLMの自然言語理解能力とGNNの推論能力を組み合わせた手法で、ナレッジグラフを使って自然言語の質問に答えるタスクで非常に高い性能を示しています。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

RAGの新しい手法『GNN-RAG』が提案されています。LLMの自然言語理解能力とGNNの推論能力を組み合わせた手法で、ナレッジグラフを使って自然言語の質問に答えるタスクで非常に高い性能を示しています。

なおナレッジグラフは、事実を「主体（head）」、「関係（relation）」、「対象（tail）」の三つ組（トリプレット）で表現し、集めたグラフです。例えば、「ジャマイカ → 話される言語 → 英語」といった具合です。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70237-1024x576.jpg)

**参照論文情報**

- タイトル：GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning
- 著者：Costas Mavromatis, George Karypis
- 所属：University of Minnesota

## 背景

LLMによる出力の事実性を高めるため、検索拡張生成（RAG）が活用されるようになってきました。LLMの入力に外部の情報源から関連するコンテキストを追加するものです。

RAGにおける情報源には、ナレッジグラフを活用することも多く研究されるようになってきました。ナレッジグラフは人間が作成した事実知識を体系化したものです。エンティティ間の関係をグラフ構造としてまとめることで、複雑な知識のやり取りを効率的に扱うことができ、質問応答などの知識集約型タスクにおいて役立つと考えられています。

そして、RAGの性能は、取得する情報の質に大きく依存します。ナレッジグラフは複雑なグラフ情報を持つため、適切な情報を取得するには効果的なグラフ処理が必要です。そこで、グラフ [ニューラルネットワーク](https://ai-data-base.com/archives/26117 "ニューラルネットワーク") （GNN）に注目されています。GNNは、グラフ構造を考慮した強力な特徴表現学習能力を持っており、ナレッジグラフ内の複雑な関係性を的確に捉えると期待されているのです。

今回研究者らは、LLMの言語理解能力とGNNの推論能力を組み合わせた新しい手法『GNN-RAG』を提案しています。さらに、GNN-RAGの検索性能を向上させるために、LLMベースの検索器を組み合わせる手法も開発されました。

実験結果から、GNN-RAGを使用するとわずか7Bの軽量なLLMを使用した場合でも最先端モデルに匹敵する性能を達成できることが明らかになっています。  
以下では手法や実験結果の詳細を紹介します。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70237_1-1024x636.png)

多段階・多エンティティのKGQAにおける検索の効果。GNN-RAGは既存のKG-RAG手法を8.9～15.5ポイント上回る。

## 前提

本手法は新しい技術であるLLM+RAGと、従来のニューラルネットワークの応用です。やや難解な方法論の紹介論文となっているため、問題設定や背景から見ていきます。

### ナレッジグラフを用いた質問応答（KGQA）

KGQAは、ナレッジグラフを用いて自然言語の質問に答えるタスクです。与えられた質問に対して、ナレッジグラフ内のエンティティの集合を正解として抽出することが目的となります。一般的に、質問応答ペアのみが学習データとして与えられ、正解に至る推論パスは与えられません。

今回、システムの性能はこのKGQAタスクで測定されており、どれだけ正解できるかが問題となっています。逆に言えば、KGQAタスク以外のタスクは今回あまり関係がありません。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70237_2-1024x344.png)

既存のKGQA手法の全体像。GNNベースの手法は密なサブグラフ上で推論を行い、LLMベースの手法は検索と推論の両方にLLMを用いる。

### 検索と推論

ナレッジグラフは通常、数百万のファクトと [ノード](https://ai-data-base.com/archives/26470 "ノード") を含む大規模なグラフです。そのため、質問に関連する部分グラフを抽出し、推論に用いるのが一般的です。理想的には、抽出された部分グラフに質問の正解が含まれています。

抽出された部分グラフと質問は、推論モデルの入力として使用されます。推論モデルは、入力に基づいて正解を出力します。KGQAで主に用いられる推論モデルは、GNNとLLMの2種類です。

#### グラフニューラルネットワーク（GNN）

KGQAは、ノード分類問題として捉えることができます。ナレッジグラフのエンティティを、与えられた質問に対する答えか否かに分類するタスクです。そこで、ノード分類などのタスクに適したグラフ表現学習モデルGNNが浮上します。

GNNは、グラフ構造を持つデータを扱うために設計されたニューラルネットワークの一種です。ノード（頂点）とエッジ（辺）から構成されるグラフの情報を用いて、各ノードの特徴を学習し、ノード間の関係性をモデル化します。また各ノードの隣接ノードからメッセージを集約することで、ノードの表現を更新します。

KGQAでは、メッセージの伝播は与えられた質問にも依存します。

簡略化のため、次のようなGNNの更新式を考えます。  
ノードvの新しい表現 ＝ 前の層のノードvの表現と、ノードvの隣接ノードからのメッセージの和を組み合わせたもの

ここで、メッセージの和を計算する際、各メッセージに対して、そのメッセージが伝播されるファクト（隣接ノード間の関係）が質問にどれだけ関連しているかを測る重みを掛けます。

### LLMとナレッジグラフ

LLMがナレッジグラフの情報を用いてRAGを行う場合、まず、抽出された部分グラフを自然言語に変換します。するとLLMが処理できる形式になります。

LLMへの入力は、ナレッジグラフの事実情報＋質問＋プロンプトとなります。例えば、「Knowledge: ジャマイカ → 話される言語 → 英語 \\n Question: ジャマイカの人々はどの言語を話しますか?」のような入力が与えられ、LLMはナレッジグラフの情報を含めて質問を考えることができます。

### KGQA手法の全体像

既存のKGQA手法は、ナレッジグラフの検索と推論の観点から分類できます。GNNベースの手法は、複雑で多段階のグラフ情報を扱うために、密なサブグラフ上で推論を行います。

一方、最近のLLMベースの手法は、自然言語を理解する能力を活かして、検索と推論の両方にLLMを用います。ToGとRoGが存在します。

- ToG：LLMを用いて関連するファクトをホップごとに検索する手法
- RoG：LLMを用いて妥当な関係パスを生成し、そのパスをナレッジグラフに写像して関連情報を検索する手法

### LLMベースの検索器の例

RoGについて詳しく説明します。

まずは質問応答ペアから、質問エンティティから答えへの最短パスを抽出します

抽出されたパスに基づいて、LLMを微調整し、質問が与えられたときに推論パスを生成します。式で表すと次のようになります。  
LLM（プロンプト, 質問）＝⇒ 関係1 → 関係2 → … → 関係t（k個の推論パス）

プロンプトは、

```js
次の質問に答えるのに役立つ有効な関係パスを生成してください: {質問}
```

です。ビーム探索デコーディングを用いて、答えの網羅性を高めるためにk個の多様な推論パス集合を生成します。

生成されたパスは、質問エンティティから始まるナレッジグラフに写像され、RAGのための中間エンティティを検索します。例えば、「ジャマイカの人々はどの言語を話しますか?」という質問に対して、ジャマイカ → 話される言語 → 英語 というパスが得られます。

以上の前提部分を下記にまとめます。

1. **KGQAは、ナレッジグラフを用いて自然言語の質問に答えるタスクで、質問に対する正解のエンティティ集合を抽出することが目的である。**
2. **ナレッジグラフは大規模なため、質問に関連する部分グラフを抽出し、GNNやLLMを用いて推論を行う。**
3. **GNNは、グラフ構造を考慮した強力な特徴表現学習能力を持ち、隣接ノードからのメッセージを集約してノード表現を更新する。**
4. **LLMは、ナレッジグラフの情報を自然言語に変換し、検索による生成（RAG）を行うことで、質問応答を行う。**

## GNN-RAGとは

さて、いよいよ本題です。LLMの言語理解能力とGNNの推論能力を組み合わせる手法である「GNN-RAG」次のような特徴を持ちます。

1. まず、GNNが密なナレッジグラフサブグラフ上で推論を行い、与えられた質問に対する答えの候補を検索します。
2. 次に、質問のエンティティとGNNが検索した答えの候補をつなぐナレッジグラフ上の最短パスを抽出し、有用な推論パスを表現します。
3. 抽出されたパスは自然言語に変換され、RAGによるLLMの推論の入力として与えられます。

つまり、GNNが密なサブグラフの推論器として機能し、有用なグラフ情報を抽出する一方、LLMは [自然言語処理](https://ai-data-base.com/archives/26319 "自然言語処理（NLP）") 能力を活かして最終的なKGQAを行います。

### GNNの役割

GNNは、複雑なグラフの相互作用を扱う能力と、多段階の質問に答える能力を持つため、他の手法よりも好ましいと考えられています。多様な推論パスを探索する構造的な利点を持ち、高い [再現率](https://ai-data-base.com/archives/26095 "再現率") を実現します。

GNNの推論が完了すると、サブグラフ内のすべてのノードは、最終的なGNN表現に基づいて、答えか答えでないかのスコアが付けられ、確率に変換されます。GNNのパラメータは、学習時の質問応答ペアを用いて、ノード分類（答えか答えでないか）によって最適化されます。

推論時には、確率スコアが最も高いノードが候補の答えとして返されます。同時に、質問のエンティティと候補の答えを結ぶ最短パス（推論パス）も返されます。検索された推論パスは、LLMベースのRAGの入力として使用されます。

異なるGNNは、RAGのために異なる推論パスを取得する可能性があります。GNNの推論は、質問と関係のマッチング操作に依存します。このマッチング操作は、質問表現と関係表現の要素ごとの積を取り、それをニューラルネットワークに入力することで実現されます。質問表現とナレッジグラフの関係表現は、共有の言語モデルによってエンコードされます。

質問表現は、異なる質問トークンに注目するように設計された、注意機構ベースのプーリングニューラルネットワークを用いて、言語モデルの出力から計算されます。一方で関係表現は、言語モデルの出力の特殊なトークン(\[CLS\]トークン)をプーリングすることで得られます。

言語モデルの選択は、どの答えノードが検索されるかに重要な役割を果たします。異なるGNN [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") を試すのではなく、事前学習済みの汎用言語モデルとナレッジグラフ上の質問-関係マッチングのために事前学習された専用言語モデルを用いて、2つの異なるGNNモデルを学習します。

### LLMの役割

GNN-RAGによって推論パスが得られた後、それらを自然言語に変換し、ChatGPTやLLaMAなどの下流のLLMへの入力として与えます。ただし、LLMはプロンプトテンプレートとグラフ情報の言語化の方法に敏感なため、その点に注意が必要です。

そのため重みが公開されているLLMに対しては、RAGプロンプトチューニングを採用します。LLaMAモデルは、学習時の質問応答ペアに基づいて微調整され、正解のリストを生成します。プロンプトは以下の通りです。

```js
"推論パスに基づいて、与えられた質問に答えてください。答えはできるだけ簡潔に、可能な答えをすべてリストで返してください。\n 推論パス: {推論パス} \n 質問: {質問}"
```

推論パスは、”{質問エンティティ} → {関係} → {エンティティ} → · · · → {関係} → {答えエンティティ}”のように言語化されます。  
なお、学習時と推論時それぞれにおける推論パスは以下のとおりです。

- 学習時には、推論パスは質問エンティティから答えエンティティへの最短パスです。
- 推論時には、推論パスはGNN-RAGによって得られます。

### GNNを使う理由

GNNは、グラフ構造を活用して、ナレッジグラフの関連部分を検索します。すると多段階の情報を含む部分が検索されます。

実験により、多段階のKGQAにおけるGNNの検索能力の高さが示されています。深いGNN（3層）と浅いGNN（1層）を学習し、それらの検索能力を測定しました。

今回は、検索器が少なくとも1つの正解を取得できるかどうかを評価する「答えの網羅率」という指標が使用されています。下流のKGQA性能ではなく、検索器が関連するKG情報を取得できるかどうかを測定するものです。なお「入力トークン数」は、検索されたナレッジグラフパスの入力トークン数の中央値を表します。

WebQSPデータセットの単一ホップ質問と多段ホップ質問について、LLMベースの検索器（RoG）と比較したGNNの検索結果を示します。結果は、深いGNN（3層）が複雑なグラフ構造を扱い、LLMや浅いGNNよりも効果的（答えの網羅率）かつ効率的（入力トークン数）に有用な多段階情報を検索できることを示しています。

一方、GNNの限界は、正確な質問-関係のマッチングが重要な単純な質問（1ホップ）にあります。このような場合、LLMの方が自然言語を理解する能力が高いため、適切なナレッジグラフ情報を選択することができます。

### 検索拡張（RA）

今回、GNNを用いて情報を取得するだけでなく、LLMを使った情報の取得も組み合わせる手法をRAと呼んでいます。異なるアプローチから検索されたナレッジグラフ情報を組み合わせることで、多様性と答えの再現率が高くなるという考え方に基づくものです。

多段階質問と単純な質問におけるそれぞれの長所を組み合わせるために、GNN検索器をLLMベースの検索器で補完するもので、RoGの検索器を用います。推論時には、2つの検索器から得られた推論パスの和集合を取ります。

なおLLMベースの検索の欠点は、多様なパスを検索するために複数の生成が必要なことです。そのため、効率と効果のトレードオフになります。より安価な代替手案として、マッチング操作に異なるLMを用いた複数のGNNの出力を組み合わせる方法GNN-RAG+Ensembleがあります。2つの異なるGNN（汎用LMと専用LMを用いたもの）から得られた推論パスの和集合をRAGの入力として取る方法です。

以上で述べたGNN-RAGシステム概要を下記にまとめます。

1. **GNN-RAGは、LLMの言語理解能力とGNNの推論能力を組み合わせた新しい手法。**
2. **GNNは、密なサブグラフ上で推論を行い、答えの候補を検索し、質問エンティティと候補をつなぐ最短パスを抽出して推論パスを表現する。**
3. **LLMは、推論パスを自然言語に変換し、RAGによる最終的な質問応答を行う。**
4. **検索拡張（RA）は、GNNとLLMベースの検索器から得られた推論パスを組み合わせることで、GNN-RAGの性能を向上させる。**

## 実験設計

### データセット

実験では、広く使われている2つのKGQAベンチマークであるWebQuestionsSP (WebQSP)とComplex WebQuestions 1.1 (CWQ)が使用されました。

WebQSPには、Freebaseのサブセットを使って答えられる4,737の自然言語の質問が含まれています。1億6,460万のファクトと2,490万のエンティティが含まれています。また質問の回答には、最大2ホップの推論が必要です。30%の質問では2つのファクトを集約する必要があり、7%の質問では制約条件を考慮する必要があり、残りの質問では単一のファクトを使用します。

CWQは、WebQSPを拡張して、質問エンティティを拡張したり、答えに制約を追加したりすることで、より複雑な多段階の質問（合計34,689問）を構築したものです。質問には、composition（複数の条件やステップを組み合わせて質問に答える必要があるタイプの質問）45%、conjunction（複数の条件が同時に満たされることを要求する質問）45%、comparative（複数の項目を比較して答える必要がある質問）5%、superlative（最大や最小などの最上級の概念を含む質問）5%の4種類があります。質問には、WebQSPと同じナレッジグラフを使って、最大4ホップの推論が必要です。

### 実装

サブグラフの検索では、WebQSPではYihらによって提供されたエンティティとナレッジグラフのリンク、CWQではTalmorとBerantによって提供されたエンティティとナレッジグラフのリンクを使用しました。リンクされたエンティティから始めてPageRank Nibbleアルゴリズムを実行し、上位m個（m = 2,000）のエンティティを選択してサブグラフに含めることで、密なサブグラフを取得しました。

GNNの推論には、深いナレッジグラフ推論を目指したReaRevを採用しました。デフォルトの実装では、ReaRevとSBERTを組み合わせています。また、ReaRevとLMSRを組み合わせたものも用意しました

RAGベースのプロンプトチューニングには、RoGを採用し、公式の実装コードに従いました。学習と推論には、ハイパーパラメータの探索を行わずに、提案されたハイパーパラメータを使用しました。モデルの選択は、検証データに基づいて行われました。

GNNを使った実験は、128 [GB](https://ai-data-base.com/archives/26343 "勾配ブースティング") のRAMを搭載したNvidia Geforce RTX-3090 [GPU](https://ai-data-base.com/archives/26570 "GPU") で行われました。LLMを使った実験は、NVLinkで接続された4つのA100 [GPU](https://ai-data-base.com/archives/26570 "GPU") と512 [GB](https://ai-data-base.com/archives/26343 "勾配ブースティング") のメモリで行われました。実験は [PyTorch](https://ai-data-base.com/archives/26256 "PyTorch") で実装されています。

検索時のLLMプロンプトには、以下のプロンプトが使用されています。

```js
"次の質問に答えるのに役立つ有効な関係パスを生成してください:
{質問}"
```

推論時のLLMプロンプトには、以下のプロンプトが使用されています。

```js
"推論パスに基づいて、与えられた質問に答えてください。答えはできるだけ簡潔に、可能な答えをすべてリストで返してください。\n
推論パス: {推論パス} \n
質問: {質問}"
```

GNNの推論時には、サブグラフ内の各ノードに、正解である確率が [ソフトマックス関数](https://ai-data-base.com/archives/26443 "ソフトマックス関数") で [正規化](https://ai-data-base.com/archives/26401 "正規化") されて割り当てられます。答えの候補を検索するために、ノードを確率スコアに基づいてソートし、累積確率スコアが閾値（0.95に設定）を下回る上位のノードを選択します。質問エンティティと答えの候補をつなぐ最短パスをRAGのために検索するには、NetworkXライブラリを使用します。

### 比較手法

以下のカテゴリの手法を評価しました。

1. Embedding（埋め込み）
2. GNN（グラフニューラルネットワーク）
3. LLM（大規模言語モデル）
4. KG+LMM（ナレッジグラフと大規模言語モデルの組み合わせ）
5. GNN+LLM（グラフニューラルネットワークと大規模言語モデルの組み合わせ）

主な手法は以下の通りです。

EmbedKGQAは、事前学習済みのKG埋め込みを利用して、多段階推論を改善します。

NSMはGNNをKGQAに適用したものです。SQALERは、GNNの推論中にどの関係（ファクト）を取得するかを学習します。同様に、SRは関係パスの検索を提案しています。ReaRevは、多段階の方法で多様な推論パスを探索します。

モデルにが、Alpaca、LLaMA2-Chat、ChatGPTなどの命令調整済みLLMが使用されました。

KD-CoTは、KGからの関連知識を使ってLLMのCoTプロンプティングを強化しています。StructGPTは、RAGのためにKGファクトを検索します。ToGは、強力なLLMを使って、関連するファクトをホップごとに選択します。RoGは、LLMを使って、より良い計画のために関係パスを生成します。

G-RetrieverはGNNベースのプロンプトチューニングでLLMを拡張しています。

以上が実験設計です。以下にまとめます。

1. **実験では、WebQuestionsSP（WebQSP）とComplex WebQuestions 1.1（CWQ）の2つのKGQAベンチマークを使用した。**
2. **比較手法として、埋め込み、GNN、LLM、KG+LMM、GNN+LLMの5つのカテゴリーの手法を評価した。**
3. **GNNの推論にはReaRevを、RAGベースのプロンプトチューニングにはRoGを採用し、公式の実装コードに従った。**
4. **Nvidia Geforce RTX-3090 [GPU](https://ai-data-base.com/archives/26570 "GPU") とNVLinkで接続された4つのA100 [GPU](https://ai-data-base.com/archives/26570 "GPU") を使用し、 [PyTorch](https://ai-data-base.com/archives/26256 "PyTorch") で実装された。**

## 実験結果

下記の表は、様々なKGQA手法の性能結果を示しています。GNN-RAGがほぼすべての指標で2つのKGQAベンチマークにおいて最高の性能を達成し、全体的に最も優れた手法であることが示されました。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70237_5.png)

結果は、LLMにGNNベースの検索を組み合わせることで、推論能力が大幅に向上することを示しています（GNN+LLM vs. KG+LLM）。具体的には、GNN-RAG+RAは、Hitで5.0～6.1ポイント、RoGを上回りました。また、わずか7Bのパラメータと少ないLLM呼び出し回数で、ToG+GPT-4の性能を上回るか、同等の性能を達成しました。

GNN-RAG+RAは、Hitで最大14.5ポイント、ToG+ChatGPTを上回り、Hits@1ではSOTAのGNNを5.3～9.5ポイント、F1では0.7～10.7ポイント上回りました。

### 多段階・多エンティティKGQA

次に下の表は、答えが質問エンティティから複数ホップ離れている多段階質問と、複数の質問エンティティを持つ多エンティティ質問の性能結果を比較しています。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70237_6-1024x296.png)

GNN-RAGは、複雑なグラフ情報を扱うためにGNNを活用し、WebQSPではF1で6.5～17.2ポイント、CWQではF1で8.5～8.9ポイント、RoG（LLMベースの検索）を上回りました。さらに、GNN-RAG+RAは、F1で最大6.5ポイントの追加の改善を提供しました。

上記の結果は、GNN-RAGが深いグラフ探索が成功したKGQAに重要な複雑な質問に対して、効果的な検索手法であることを示しています。

### LLMへの検索の影響

下の表は、GNN-RAGやLLMベースの検索器（RoGやToG）を使用した様々なLLMの性能結果を示しており、Hitメトリックを報告しています。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70237_8.png)

GNN-RAG（+RA）は、RAGで最大の改善を達成する検索アプローチです。例えば、GNN-RAG+RAは、RoGとToGに比べて、Hitで最大6.5ポイント、ChatGPTを改善しました。

さらに、Alpaca-7BやFlan-T5-xlなどの弱いLLMのKGQA性能を大幅に向上させました。RoGに比べて、Hitで最大13.2ポイントの改善が見られました。また、軽量な7B LLaMA2モデルを使用して、LLaMA2-Chat-70B+ToGの性能を上回りました。

この結果は、GNN-RAGが再学習なしに他のLLMと併用可能で、KGQA推論を改善できることを示しています。

### 忠実性に関するケーススタディ

また下の図はCWQデータセットからの2つのケーススタディで、GNN-RAGがLLMの忠実性（LLMが質問の指示にどれだけ従い、KGから適切な情報を使用しているか）を改善する方法を示しています。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70237_9-1024x487.jpg)

どちらのケースでも、GNN-RAGは質問に正しく答えるために必要な多段階の情報を検索しています。1つ目のケースでは、GNN-RAGはベースラインのKG-RAG（RoG）とは異なり、質問に答えるために必要な重要なファクト<Gilfoyle → characters\_that\_have\_lived\_here → Toronto>と<Toronto → province.capital → Ontario>の両方を検索しています。

2つ目のケースでは、KG-RAGベースラインは、質問が言及している<Erin Brockovich → film\_character>ではなく、<Erin Brockovich → person>に関する情報を誤って検索しています。 GNN-RAGは、GNNを使用して、<Erin Brockovich>と<Michael Renault Mageau>エンティティがKGでどのように関連しているかを探索し、<Erin Brockovich → film\_character>に関するファクトを検索した結果、<films\_with\_this\_crew\_job → Consultant>という重要な情報を含むファクトを検索しました。

次に下の図は、WebQSPデータセットからの1つのケーススタディを示しており、RA（検索拡張）がGNN-RAGをどのように改善するかを示しています。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70237_10-1024x142.png)

最初、GNNは自然言語を理解する能力が限られているため、有用な情報を検索できません。つまり、<jurisdiction.bodies>が通常「法律を作る」ことを理解していません。一方でGNN-RAG+RAは正しい情報を検索し、LLMが質問に正しく答えるのを助けることに成功しています。

以上が実験結果です。下記にまとめます。

1. **GNN-RAGは、ほぼすべての指標で2つのKGQAベンチマークにおいて最高の性能を達成し、SOTAを達成した。**
2. **複数ホップの推論を必要とする複雑な質問に対して、大きな性能向上を示した。**
3. **様々なLLMを使用した場合でも、KGQAの性能を大幅に改善することができた。**
4. **ケーススタディでは、GNN-RAGがLLMの忠実性を向上させ、複雑な質問に答えるために必要な多段階情報を検索できることが示された。**

## まとめ

本記事では、LLMとグラフニューラルネットワーク（GNN）を組み合わせた、ナレッジグラフを用いた質問応答（KGQA）の新手法GNN-RAGを紹介しました。GNNをKGQA検索に再利用し、検索分析に基づいて性能を高める検索拡張（RA）手法を設計することで、LLMの推論能力を向上させる手法です

実験の結果、GNN-RAGは、2つのKGQAベンチマークでSOTAを達成し、複雑な質問に対するLLMの忠実な推論に必要な多段階情報を検索できることが示されました。また、追加のLLM呼び出しを行わずにLLMのKGQA性能を向上させ、7BのファインチューニングされたLLMを使用して、GPT-4に匹敵する性能を達成しました。

以上の結果から、実用的な質問応答システムの構築に役立つと期待されています。

読解の難易度が高い研究でしたが、今後ナレッジグラフベースのRAGを考える際には読み返してみてください。

- 参照論文URL： [https://arxiv.org/abs/2405.20139](https://arxiv.org/abs/2405.20139)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMで因果推論を行うためのプロンプト手法](https://ai-data-base.com/archives/70145)

[MMLUをアップデートしたベンチマーク『MMLU-Pro』Phi-3やLlama 3、Claude 3、GPT-4oなどの評価結果](https://ai-data-base.com/archives/70358)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)