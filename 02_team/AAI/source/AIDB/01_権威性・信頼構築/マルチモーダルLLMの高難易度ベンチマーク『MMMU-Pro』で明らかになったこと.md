---
title: "マルチモーダルLLMの高難易度ベンチマーク『MMMU-Pro』で明らかになったこと"
source: "https://ai-data-base.com/archives/75326"
author:
  - "[[AIDB Research]]"
published: 2024-09-09
created: 2025-06-13
description: "この記事では、MMMU-Proという新しいベンチマークについて説明します。LLMが文章だけでなく画像を含めて問題どれだけ理解できるかをより正確にそして厳しく測るベンチマークです。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

この記事では、MMMU-Proという新しいベンチマークについて説明します。LLMが文章だけでなく画像を含めて問題どれだけ理解できるかをより正確にそして厳しく測るベンチマークです。

実際に最新のLLMでMMMU-Proを試したところ、以前のベンチマーク（MMMU）より少ないスコアが出ました。MMMUの時点で「AGIを目指したベンチマーク」とされていたため、MMMU-Proの格段の難しさを示しています。

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_75326-1024x576.jpg)

**参照論文情報**

- タイトル：MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark
- 著者：Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shen [gb](https://ai-data-base.com/archives/26343 "勾配ブースティング") ang Tong, Yuxuan Sun, Ming Yin, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, Graham Neubig
- 所属：MMMU Team

## 背景

LLMは画像と文章の組み合わせで難しい問題を解く力が大きく向上してきました。例えば、GPT-4oは [MMMU](https://ai-data-base.com/archives/61463) というベンチマークで69.1%という高い [正解率](https://ai-data-base.com/archives/25930 "正解率") を出しました。MMMUとは、大学レベルのマルチモーダル問題で構成されたベンチマークで、どんどん優秀になるLLMが今後AGIに近づくことを見据えて難しい基準を設けたものです。

しかし、LLMは本当に深く理解して答えを出しているのでしょうか？それとも単に表面的な手がかりを使って正解を当てているだけなのでしょうか。

この疑問はとても重要です。なぜなら、もしLLMが本当の理解ではなく表面的な手がかりに頼っているだけなら、新しい状況では思わぬ間違いをする可能性があるからです。

そこで今回研究チームは、LLMの能力をより厳しくチェックする『MMMU-Pro』という新しいベンチマークを作りました。次の3つの特徴があります。

1. 文章だけで答えられる問題は使わない
2. 選択肢の数を増やして、当てずっぽうで正解する可能性を減らす
3. 質問が画像の中に書かれていて、それを読み取る必要がある問題も含める

この中で、特に3つ目の特徴が大切とのことです。人間が自然に行っている「見ること」と「読むこと」を同時に行う能力をLLMがどれだけ持っているかを調べる必要があるためです。

実験では、最先端のLLMがこのような難しいベンチマークでどれほどの性能を見せるのかという点と、プロンプトのテクニックでどれほどスコアを伸ばせるのかという点が確かめられました。

以下で詳しく紹介します。

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_75326_1-1024x600.png)

MMMUとMMMU-Proでの各種LLMのスコアの違い

## MMMU-Proの作り方

### まずMMMUベンチマークの見直し

以前開発された「 [MMMUベンチマーク](https://ai-data-base.com/archives/61463) 」は、大学レベルの問題を使ってLLMの画像と文章を理解する能力を評価するベンチマークです。11,500問もの（慎重に選ばれた）問題で構成され、6つの分野、30の科目、183の細かい分野をカバーしています。各問題は大学の試験、クイズ、教科書から集められたものです。

MMMUの各問題は画像と文章のセットで、4つの選択肢から答えを選びます。使われている画像はグラフ、図、地図、化学の構造などさまざまです。そんなMMMUは、LLMを評価するための標準的なツールになりました。

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_75326_4-1024x254.jpg)

テキストのみのLLM（Llama-3-70B Instruct）が正確に答えられるMMMU問題の2つの例を示す図

しかし、MMMUには2つの問題点があることも分かっています。

1つ目の問題は、一部の問題が画像なしでも答えられてしまうこと。問題が対応する画像とあまり関係がない場合があるためです。

2つ目の問題は、LLMが簡単な方法で答えを出せてしまうこと。人間なら画像が必要な問題でも、LLMは選択肢の中から手がかりを見つけたり、既に持っている知識で答えられてしまいます。

よって、より優れたベンチマークの登場が待たれていました。

### 新しいアプローチの考案

研究チームは下記の目的をもって新しいベンチマークを作ることにしました。

- 現実の世界に近づける
- 人間の認知方法に近づける
- 画像と文章を同時に理解する能力を評価する

そして、問題を解決するために3つのステップで取り組みました。

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_75326_2-1024x236.png)

MMMU-Proの構築プロセスの概要

まず、問題を選び直しました。  
4つの強力なオープンソースLLM（Llama3-70B-Instruct、Qwen2-72B-Instruct、Yi-1.5-34B-Chat、Mixtral-8×22B-Instruct）に画像なしで問題に答えてもらい、3つ以上のLLMが半分以上正解した問題を除きました。そして、残った問題から1,800問を選び、30の科目に均等に60問ずつ分配しました。

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_75326_5.png)

テキストのみの問題に対するLLMの精度を示すグラフ

次に、選択肢を増やしました。  
一部の問題は文章だけで答えられたため、選択肢を4つから10個に増やしてLLMが当てずっぽうで答えるのを難しくしました。また専門家が問題を見直し、テキストと画像の関連性が低いものを除いて最終的に1,730問になりました。

最後に、質問を画像の中に含めました。  
LLMの画像と文章を理解する能力をさらに厳しく評価するためです。質問をスクリーンショットや写真の中に入れ、文章は別に与えず、画像の中の文章だけで答える必要があるようにしました。実際の画面のようなスクリーンショットや写真を手動で撮り、背景、文字の種類、大きさを変えて現実世界の多様性を表現しました。

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_75326_3-1024x863.jpg)

画像からテキストを読み取る必要のある問題の例

最終的に3,460問（普通の形式1,730問、質問が画像の中にある形式1,730問）ができました。

## 実験

### 実験設定

MMMU-Proがどれくらい難しいか（そして今後の方針）を示すため、さまざまなLLMが評価されました。評価されたLLMは以下の2つのグループに分けられます。

**商用モデル（最高レベルのマルチモーダルモデル）**

- GPT-4o (0513)とGPT-4o mini
- Claude 3.5 Sonnet
- Gemini 1.5 Pro (0801と0523バージョン)

**オープンソースのマルチモーダルモデル**

- InternVL2 (8B, 40B, Llama3-76Bバージョン)
- LLaVA (OneVision-7B, OneVision-72B, さまざまなNeXTバージョン)
- VILA-1.5-40B
- MiniCPM-V2.6
- Phi-3.5-Vision
- Idefics3-8B-Llama3

上記のLLMは、3つの設定で評価されました。

1. 普通の設定（選択肢が4つ）
2. 普通の設定（選択肢が10個）
3. 質問が画像の中にある設定

なおスコアは、設定2と3の成績の平均として計算されました。設定1と元のMMMUの成績は、MMMU-Proがどれだけ難しくなったかを示すために比較用に使用されます。

#### 質問の仕方

LLMには、直接的な質問方法と「考えるプロセスを示す」思考の連鎖アプローチ（CoT）の両方で質問し、高い方の成績を全体の結果として報告しています。

## 実験結果

さまざまなLLMの、MMUM-Proにおける全体的な結果は以下のとおりです。

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_75326_6-1024x641.jpg)

LLMの全体的な結果を示

主な発見は以下の通りです。

**選択肢を増やした影響（Δ1）**

選択肢を4つから10個に増やすと、すべてのLLMで成績が大きく下がりました。例えば、GPT-4o (0513)は64.7%から54.0%へと10.7%下がりました。選択肢を増やすことで、LLMが正解を当てる確率が減り、画像と文章をより深く理解する必要が出てきたことを示しています。

**質問が画像の中にある設定の影響（Δ2）**

質問が画像の中にある設定を導入すると、LLMはさらに解決が難しくなりました。例えば、GPT-4o (0513)はこの設定で4.3%さらに成績が下がり、LLaVA-OneVision-72Bは14.0%もの大きな低下を示しました。画像と文章の情報を組み合わせるのは、文章で情報が明確に与えられる場合と比べて難易度が高いということを示しています。

**MMMUからの進化（Δ3）  
**MMMU-Proと元のMMMUの差を表すΔ3は、すべてのLLMでスコアが大きく低下していることを示しています。例えば、Gemini 1.5 Pro (0801)とClaude 3.5 Sonnetはそれぞれ18.9%と16.8%の低下を示し、VILA-1.5-40Bのような一部のLLMでは26.9%もの大幅な低下が見られました。

以上から、MMMU-ProはLLMによる簡単な推測を効果的に防ぎ、より厳密な評価を行えることを示しています。そして全体的にスコアが大幅に低下していることから、現在のLLMの限界が明らかになっています。

### OCRプロンプトの効果

次に、MMMU-Proへの入力において、 [光学文字認識](https://ai-data-base.com/archives/26261 "光学文字認識（OCR）") （ [OCR](https://ai-data-base.com/archives/26261 "光学文字認識（OCR）") ）プロンプトが性能向上に役立つかどうかが調べられました。 [OCR](https://ai-data-base.com/archives/26261 "光学文字認識（OCR）") プロンプトとは、モデルに画像内のテキストを書き出すことを明示的に要求するアプローチを意味します。以下の図で示すプロンプト文（”Write out the multiple-choice question in the image and then solve it.”）が使用されました。

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_75326_ocrprompt.png)

結果は下の図に示される通りで、多くのモデルにおいて、 [OCR](https://ai-data-base.com/archives/26261 "光学文字認識（OCR）") プロンプトの導入は性能に大きな変化をもたらしませんでした。現在のモデルは画像からテキスト情報を抽出し理解する能力を既に備えていることを示唆しています。

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_75326_7-1024x583.png)

ビジョン設定での OCR の影響を示すグラフ

### Chain of Thought（CoT）は役立つか

次に、Chain of Thought（CoT）プロンプティングがMMMU-Proベンチマークでのモデル性能向上に効果的かどうかが検証されました。標準設定とビジョン入力設定の両方で検証し、スコアが以下にまとめられています。

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_75326_8-1024x360.png)

CoTプロンプティングの影響を示すグラフ

全体的な傾向として、両設定において、CoTプロンプトの導入は性能向上につながりました。

ただし性能向上の程度はモデル間で大きく異なりました。例えば、Claude 3.5 Sonnetは標準設定で42.7%から55.0%へと大幅な向上を示しました。一方で、LLaVA-OneVision-72Bのような一部のモデルでは最小限の改善しか見られませんでした。

VILA1.5-40Bのような一部のモデルでは、顕著な性能低下が観察されました。この現象の解釈としては、指示に従う能力に依存した結果ということです。モデルが指示を正確に理解し従うことが難しい場合は、CoTに対しても効果が出ません。また、「応答形式の崩壊」という問題につながる可能性があります。

### 定性的分析

より深い洞察を得るため、MMMU-Proの結果について徹底的な定性的分析も行われました。分析は主に以下の2つに焦点が当てられました。

1. 4つの選択肢では正解だが、10個の選択肢では失敗する標準設定のケース
2. 標準的な10個の選択肢設定では成功するが、ビジョン入力設定（画像で問題文を与える）では失敗するケース

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_75326_9-1024x669.jpg)

選択肢の拡大の有無によるGPT-4oの回答の比較を示す図

モデルの性能に影響を与えるいくつかの重要な要因が明らかになりました。

**選択肢が増加することによる課題**

- モデルは決定的な選択をするのではなく、最も近い答えを選ぶ傾向があり、選択肢が増えるとエラーが増加する
- 正解に概念的に近い選択肢の追加は、微妙な質問において混乱を引き起こす可能性がある
- 概念的な質問では、モデルは微妙に異なる選択肢を区別するのに苦労する

**画像とテキストを統合することの課題**

- テキストと画像の統合は、情報処理と取り扱いの複雑さを大幅に増加させる
- 視覚情報とテキスト情報を同時に処理する際、モデルはより幻覚を起こしやすく、誤った推論チェーンを持つ傾向が観察された
- 複雑な視覚入力やユニークなレイアウトは、論理的判断を混乱させ、認知負荷を増加させる可能性がある
- テキストと画像間の急激な遷移は、処理バイアスを引き起こす可能性がある

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_75326_10-1024x706.jpg)

標準設定とビジョン入力設定間のGPT-4oの回答の比較を示す図

以上の結果は、MMMU-Proが現在のマルチモーダルLLMの限界をはっきりと示しています。研究者らは、以下の方向性でLLMを改善すべきだと結論づけました。

1. 選択肢が多い問題でも正確に答えられるようにする
2. 画像の情報と文章の情報を、より上手く組み合わせて理解できるようにする
3. 複雑で混ざり合った形の問題でも、安定して答えられるようにする

## まとめ

この記事では、MMMU-Proという新しいベンチマークについて紹介しました。MMMU-Proは、以前作られたベンチマークMMMUを改良したものです。

MMMU-Proで最新のLLMを評価したところ、LLMにはまだ多くの課題があることがわかりました。そして、今後マルチモーダルLLMがどのように改善されるべきかが示されました。

リーダーボードは以下に公開されています。以前のMMMUがそうであったように、新しいモデルがリリースされた際にはこのMMMU-Proでの評価結果が一つの参考になることが期待されます。

- 参照論文URL： [https://arxiv.org/abs/2409.02813](https://arxiv.org/abs/2409.02813)
- プロジェクトページ： [https://mmmu-benchmark.github.io/#leaderboard](https://mmmu-benchmark.github.io/#leaderboard)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[ロングコンテキストLLM台頭の今もRAGを使用する理由](https://ai-data-base.com/archives/75289)

[AIコーディング補助ツール（GitHub Copilot）で開発者の生産性が26%向上 Microsoft・アクセンチュアなど3社の大規模調査結果](https://ai-data-base.com/archives/75407)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)