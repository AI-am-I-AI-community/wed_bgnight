---
title: "マルチモーダルLLMにおける幻覚（ハルシネーション）の原因と対策 クリエイティブでの活用も推奨 AWSなどが網羅的に調査"
source: "https://ai-data-base.com/archives/68720"
author:
  - "[[AIDB Research]]"
published: 2024-05-09
created: 2025-06-13
description: "テキストだけでなく画像や動画などの視覚情報も理解し、それらを組み合わせて高度なタスクを遂行するマルチモーダルの大規模言語モデル（マルチモーダルLLM）が注目を集めています。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

テキストだけでなく画像や動画などの視覚情報も理解し、それらを組み合わせて高度なタスクを遂行するマルチモーダルの大規模言語モデル（マルチモーダルLLM）が注目を集めています。

そんな中、モデルが生成した内容が入力された情報と矛盾したり、事実とかけ離れたりする現象である「ハルシネーション」の問題が、マルチモーダルLLMにおいても問題となっています。

そこで今回Amazon Prime Videoなどの研究者らは、マルチモーダルLLMにおけるハルシネーションの原因や評価方法、対策などについて詳しく調査しています。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_68720_thum2-1024x576.jpg)

**参照論文情報**

- タイトル：Hallucination of Multimodal Large Language Models: A Survey
- 著者：Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zon [gb](https://ai-data-base.com/archives/26343 "勾配ブースティング") o Han, Zheng Zhang, Mike Zheng Shou
- 所属：National University of Singapore, Amazon Prime Video, AWS Shanghai AI Lab

## 背景

マルチモーダルLLMが注目を集めるようになりました。マルチモーダルLLMは主に、画像や動画などの視覚情報を理解し、それに基づいて言語生成を行うことができるモデルを指します。今後も、さらに異なるモダリティ（情報の種類）に対応できるようになることが期待されています。

しかし、マルチモーダルLLMにおいてもハルシネーションの問題が顕在化しています。生成されたテキストと入力された視覚情報が不整合するといった現象です。存在しないオブジェクトを言及したり、オブジェクトの属性や関係性を誤って説明したりするなどのケースがあります。

ハルシネーションに起因する信頼性や安全性への懸念から、マルチモーダルLLMの社会実装が阻まれる恐れがあります。そのため、解決に向けた取り組みが求められています。

今回研究者らは、マルチモーダルLLMのハルシネーション問題に関する最新の動向を整理し、課題と将来の方向性を示しています。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_68720_1-1-1024x320.png)

本論文の主要な内容の流れと分類を示した図

本論文の主要な内容の流れと分類を示した図

## 前提となる概念

### 大規模言語モデル（LLM）について

LLMは、膨大なテキストデータを用いて訓練されたトランスフォーマーベースのモデルの一種です。データ量とモデルの規模を拡大することで、コンテキスト内学習、思考連鎖（Chain of Thought）プロンプティングなど、様々な新しい能力を獲得しました。

LLMの訓練には通常、事前学習、教師あり微調整（ファインチューニング）、人間のフィードバックからの [強化学習](https://ai-data-base.com/archives/26125 "強化学習") （RLHF）の3つの主要なフェーズが存在します。

**（１）事前学習**

LLMは自己回帰的予測によって、シーケンス内の次のトークンを予測します。この原理に基づき、膨大なテキストデータを用いた自己教師あり学習による事前学習を通じて、言語の構文を理解し、世界の知識にアクセスし、推論能力を向上させます。

**（２）教師あり微調整（ファインチューニング）**

事前学習だけでは、LLMは次の単語を予測することに特化しすぎてしまいます。そこで、指示と応答のペアからなる、 [アノテーション](https://ai-data-base.com/archives/26297 "アノテーション") 済みデータセットを用いて追加の訓練を行い、LLMの能力と制御性を向上させます。

****（３）** 人間のフィードバックからの強化学習（RLHF）**

人間の好みに合わせるために、報酬モデルを用いた強化学習が行われます。報酬モデルは、プロンプトと人間がラベル付けした応答に基づいて好みのランキングを予測するように訓練されます。LLMは、報酬モデルが提供する報酬を最大化するように最適化されます。

### マルチモーダルLLM

マルチモーダルLLMは、様々なモダリティのデータを理解する大規模モデルです。中でも、ビジョン+LLM（視覚言語モデル）が特に注目されています。マルチモーダルLLMの目標は、画像や動画を通じて「世界を見る」ことができるようにすることです。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_68720_8-1024x411.png)

マルチモーダル大規模言語モデルの一般的な アーキテクチャ を示した図

マルチモーダルLLMの訓練に関しては、主に2つのアプローチがあります。1つ目は、事前学習済みのモダリティごとのモデルを利用する方法です。視覚エンコーダとLLMの間に学習可能なインターフェイスを設けます。2つ目は、スクラッチからエンドツーエンドで訓練する方法です。

事前学習済みのモダリティごとのモデルを利用するマルチモーダルLLMの訓練は、通常2つの段階で行われます。

**（１）事前学習**

各モダリティのモデルは、それぞれのデータで事前学習されているため、この段階ではクロスモーダルな特徴のアライメントを実現することが目的となります。

**（２）指示チューニング**

事前学習後のモデルには、マルチモーダルな文脈での指示に従う能力がまだ備わっていません。機械生成されたデータセットと人間が [アノテーション](https://ai-data-base.com/archives/26297 "アノテーション") したQAデータセットを用いて、モデルのマルチモーダルな指示理解能力を向上させます。

### マルチモーダルLLMにおけるハルシネーション

生成されたテキストと入力された視覚情報が一致しない現象を指します。最新の研究では、主にオブジェクトのハルシネーションに焦点が当てられています。前述の通り、画像に存在しないオブジェクトを言及したり、オブジェクトを誤った属性で説明したりする現象が観察されています。

オブジェクトのハルシネーションは、以下3つのタイプに分類されます。

**（１）カテゴリータイプ**

画像に存在しないオブジェクトのカテゴリーや誤ったカテゴリーを同定してしまう。

**（２）属性タイプ**

オブジェクトのカテゴリーは正しく認識されているが、色、形状、素材、内容、数、動作などの属性の説明が誤っている。

****（３）** 関係性タイプ**

オブジェクトとその属性は正しく説明されているが、オブジェクト間の関係性（人間とオブジェクトのインタラクションや相対的な位置関係など）が実際の画像の内容と一致していない。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_68720_4-1024x311.jpg)

3種類の典型的なハルシネーションの例を示した図

## ハルシネーション（幻覚）の原因

マルチモーダルLLMのハルシネーションにはさまざまな根本的原因があります。下記では、主にデータ、モデル、訓練、推論の4つの側面から掘り下げて分析しています。

### データに起因するハルシネーション

マルチモーダルLLMにとって、データは土台となる存在です。しかし同時にハルシネーションの原因にもなり得ます。データに起因するハルシネーションは、主に量、質、統計的バイアスの3つの観点から分析されます。

**（１）データ量**

深層学習モデル全般、特にマルチモーダルLLMのような大規模モデルは、大量のデータを必要とします。現在、画像とテキストのペアからなるデータセットや視覚的QAデータが利用されていますが、テキストのみのデータと比べるとまだ十分な量があるとは言えません。データ不足は、クロスモーダルなアライメントの問題を引き起こし、ハルシネーションにつながる恐れがあります。

**（２）データの質**

大規模な訓練データへの需要の高まりから、効率的にデータを収集するためのヒューリスティックな手法が用いられています。しかし、必ずしも質の保証はありません。ノイズが多い、多様性の欠如、詳細な記述の不足などが品質の低いデータにあたります。

**（３）統計的バイアス**

LLMを含む [ニューラルネットワーク](https://ai-data-base.com/archives/26117 "ニューラルネットワーク") 全般は、訓練データを記憶する傾向があります。訓練データセットにおけるオブジェクトの分布は、モデルの振る舞いに大きな影響を与えます。つまり頻繁に登場するオブジェクトとそうでないオブジェクトには差が生じます。

### モデルに起因するハルシネーション

現在における主流のマルチモーダルLLMの [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") は、事前学習済みの視覚モデル、事前学習済みのLLM、アライメントモジュールで構成されています。互いに連結されているため、各モジュールのエラーが蓄積されるリスクがあります。各原因について下記に述べます。

**（１）視覚モデルの弱さ**

マルチモーダルLLMの視覚部分を担う視覚モデルの性能が低いと、入力画像の理解に失敗し、誤った情報が言語モデルに伝わってしまいます。例えば、画像中の「リンゴ」を「トマト」と誤認識した場合、言語モデルは「トマト」に関する記述を生成してしまい、実際の画像内容と矛盾が生じます。また、物体検出や属性認識の精度が低いと、画像に存在しない物体を検出したり、誤った属性を割り当てたりする可能性があります。

**（２）言語モデルの事前知識**

マルチモーダルLLMでは、大規模言語モデル（LLM）の規模や能力が視覚モデルを大きく上回ることが多く、LLMの事前知識が視覚情報よりも優先される傾向にあります。LLMの事前知識に誤りや偏りがある場合、それが入力画像の内容よりも優先されてしまい、ハルシネーションを引き起こす可能性があります。

**（３）弱いアライメントインタフェース**

アライメントインタフェースは、視覚モデルとLLMを接続する重要な役割を果たします。このインタフェースが適切に設計されていないと、視覚情報がLLMに正しく伝達されず、ハルシネーションの原因となります。アライメントインタフェースの設計は、視覚情報の伝達とLLMとのインタラクションに直接影響を与えるため、ハルシネーションの発生と密接に関係しています。

### 訓練に起因するハルシネーション

マルチモーダルLLMの訓練目的は、基本的にLLMと同じで、自己回帰的な次のトークン予測損失です。この [損失関数](https://ai-data-base.com/archives/27017 "損失関数") は、トークンレベルで最適化を行いますが、シーケンスレベルでの監督は欠けています。また、LLMの訓練手順とは異なり、マルチモーダルLLMの訓練手順にはRLHFの段階が欠けていることが、ハルシネーションの原因となる可能性があります。

### 推論に起因するハルシネーション

推論に関しては、自己回帰的な生成にも問題があると指摘されています。生成の過程でシーケンスの長さが長くなるにつれ、自己注意機構がこれまでに生成されたテキストトークンに集中するようになり、視覚的な内容への注意が薄れていきます。この「注意の喪失」の問題も、モデルの出力応答が視覚的な内容と無関係になる原因となり得ます。

## ベンチマーク

マルチモーダルLLMにおけるハルシネーション（の度合い）を評価するために設計された、既存の評価指標とベンチマークについて概説します。現在、ベンチマークは主にモデルが生成したコンテンツにおけるオブジェクトのハルシネーションの評価に焦点が当たっています。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_68720_5-1024x537.jpg)

オブジェクトのハルシネーションに関連する主要なベンチマークとメトリクスをまとめた表

### CHAIR

初期の研究の1つで、画像キャプション生成タスクにおけるオブジェクトのハルシネーションを評価するベンチマークです。正解のキャプションとオブジェクトの [セグメンテーション](https://ai-data-base.com/archives/26353 "セグメンテーション") に基づいて、生成された単語のうち実際に画像内に存在するものの割合を計算します。インスタンスごとに計算するCHAIR\_iと、文ごとに計算するCHAIR\_sの2つの評価指標があります。

### POPE

ハルシネーションの評価を2値分類タスクに変換するベンチマークです。対象となるオブジェクトについて簡単な「はい」か「いいえ」で答えられる質問をマルチモーダルLLMに与えます。MSCOCOデータセットから500枚の画像を抽出して構築されており、正例と負例の質問で構成されています。

### MME

知覚能力と認知能力の検査を含む14のサブタスクをカバーしています。オブジェクトのハルシネーションに関しては、オブジェクトの存在、数、位置、色の4つのサブタスクが知覚評価に含まれています。POPEと同様に、これらのタスクは「はい」か「いいえ」のタスクとして定式化されています。

### CIEM

人間が [アノテーション](https://ai-data-base.com/archives/26297 "アノテーション") したオブジェクトを利用する研究とは異なり、CIEMは自動パイプラインを使って生成されます。LLMベースのデータ生成パイプラインは完全には信頼できませんが、生成されたデータのエラー率は約5%と低いことが経験的に示されています。

### MMHal-Bench

8つの質問カテゴリと12のオブジェクトトピックからなる96の画像-質問ペアで構成されています。評価の際には、GPT-4モデルが応答を分析し、評価します。

### GAVIE

GAVIEはGPT4-Assisted Visual Instruction Evaluationの略です。2つの側面からマルチモーダルLLMの出力を評価することを目的としています。（１）関連性（Relevancy）は指示に従う性能を評価し、（２）正確性（Accuracy）は視覚的なハルシネーションを測定します。1,000のサンプルからなるベンチマークと評価アプローチで構成されています。マルチモーダルLLMの出力をオープンエンドな方法で評価し、人間が [アノテーション](https://ai-data-base.com/archives/26297 "アノテーション") した正解は必要ありません。

### NOPE

NOPEは、Negative Object Presence Evaluationの略です。オブジェクトのハルシネーションと不正確さを区別することが提案されています。オブジェクトのハルシネーションとは、正解が否定の不定代名詞（例えば、”none”、”no one”など）である場合に、マルチモーダルLLMの応答に存在しないオブジェクトが含まれる現象を指します。不正確さとは、正解がNegP以外の場合に、マルチモーダルLLMが質問に正確に答えられないことを指します。既存のVQAデータセットはNegPデータが極端に少ないため提案されました。

### FaithScore

オープンエンドの質問に対する自由形式の応答を評価することを目的としたベンチマークです。自動パイプラインを設計して、応答を分解、評価、詳細に分析します。評価指標には、エンティティ、数、色、関係、その他の属性など、きめ細かいオブジェクトのハルシネーションのカテゴリが含まれています。

### RAH-Bench

RAH-Bench（Relation-Associated Hallucination Benchmark）は、POPEのアップグレード版と見なすことができ、対応する画像を含む3,000の「はい」または「いいえ」の質問が含まれています。POPEとは異なり、RAH-Benchはネガティブな質問を3つのサブセットに分類しています。各サブセットには、カテゴリーのハルシネーション、属性のハルシネーション、関係性のハルシネーションなど、異なる側面で誤解を招くような記述を含む500の質問が含まれています。

### HallusionBench

マルチモーダルLLMの潜在的な障害モードを診断・分析するために、ハルシネーションを異なる視点から評価するベンチマークです。視覚依存型と視覚補完型の2つのカテゴリに分類された質問で構成されています。視覚依存型の質問は、視覚的文脈がなければ肯定的な答えができない質問と定義されています。視覚補完型の質問は、視覚的入力がなくても答えることができますが、視覚的要素は補足的な情報や修正を提供するだけです。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_68720_6-1024x425.png)

生成タスクにおける主要なマルチモーダル大規模言語モデルの性能比較表

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_68720_7-1024x538.jpg)

識別タスクにおける主要なマルチモーダル大規模言語モデルの性能比較表。

## ハルシネーションを軽減するには

マルチモーダルLLMにおけるハルシネーションを軽減するための最新の手法を紹介します。データ由来、モデル由来、訓練由来、推論由来の4つのグループに分類しています。

### データ由来ハルシネーションの軽減

ネガティブデータの導入、反実データの導入、既存データセットのノイズや誤りの削減などの試みがなされています。

### （１）LRV-Instruction

既存の指示チューニングデータが主にポジティブな指示サンプルに焦点を当てているため、モデルが一貫して「はい」と答える問題に対処するために提案されました。ポジティブな指示とネガティブな指示の両方を含むように設計されており、より堅牢な視覚指示チューニングを目指しています。

### （２）HalluciDoctor

指示チューニングデータセットを調整することで、マルチモーダルLLMのオブジェクトの幻覚問題に取り組んでいます。複数のマルチモーダルLLMの一貫性のクロスチェックによるハルシネーション検出パイプラインを開発し、ハルシネーションコンテンツを排除します。また、訓練データの長尾分布とオブジェクトの共起がハルシネーションの主要な要因であることから、反実的な視覚指示生成戦略を提案し、データセットを拡張しています。

### （３）ReCaption

既存の画像-テキストペアのテキストキャプションを書き直すフレームワークです。キーワード抽出とキャプション生成の2つのステップで構成されています。最終的には高品質な画像キャプションペアのセットが生成されます。

### （４）EOS Decision

マルチモーダルLLMの知覚限界に達したときに生成プロセスを適切に終了させる能力に着目し、シーケンスの終了（EOS）決定の観点から詳細に検討されています。モデルのシーケンス終了能力を損なう可能性のある有害な訓練データを排除するデータフィルタリング戦略を提案しています。

## モデル由来ハルシネーションの軽減

マルチモーダルLLMの知覚能力を向上させることは、全体的な性能を改善し、ハルシネーションを軽減することが示されています。解像度のスケールアップ、汎用視覚エンコーダの使用、専用モジュールの導入などのアプローチが検討されています。

### （１）解像度のスケールアップ

LLaVAからLLaVA-1.5へのアップデートでは、CLIP ViT視覚エンコーダの解像度をCLIP-ViT-L-224からCLIP-ViT-L-336にスケールアップしたことが大きな変更点の1つであり、かなりの性能向上が見られました。解像度のスケールアップは単純ながら効果的な解決策であることが示唆されています。

### （２）汎用視覚エンコーダ

視覚的詳細の損失を補うために、他の視覚エンコーダからの視覚的特徴を組み込むことが提案されています。CLIP ViTとDINO ViTの特徴を混合する研究や、視覚ツールモデルを利用してマルチモーダルLLMの知覚を向上させる研究が行われています。

### （３）専用モジュール

HallE-Switchは、詳細なキャプション内のパラメトリック知識の程度を制御する専用の「スイッチ」モジュールを訓練することを提案しています。具体的には、文脈的（視覚コンテンツ関連）データセットとパラメトリックデータセットの両方から対照的な訓練データを用いてスイッチモジュールを訓練します。

## 訓練由来ハルシネーションの軽減

### （１）補助的な教師データ

マルチモーダルLLMの訓練では、通常、言語モデルの [損失関数](https://ai-data-base.com/archives/27017 "損失関数") （言語の予測がどれだけ正しいかを測る指標）が主に使われます。しかし、これだけでは視覚情報を十分に扱えない可能性があります。そこで、視覚に関する追加の教師データを利用して、モデルを指導する方法が提案されています。

例えば、画像に写っている物体の詳細な情報（どこに何が写っているかなど）を用意し、それを使ってマルチモーダルLLMが正しく物体を認識できるように訓練するという研究があります。このように、言語だけでなく視覚に関する教師データを活用することで、ハルシネーションを減らせる可能性があります。

### （２）強化学習

強化学習は、モデルが出力した結果の良し悪しを評価し、それに基づいてモデルを改善する手法です。マルチモーダルLLMのハルシネーションを減らすために、強化学習を様々な形で利用する研究が行われています。

例えば、自動的に計算できる評価指標を用いてモデルの出力を評価し、その指標が高くなるようにモデルを訓練する方法があります。また、AIや人間が出力の良し悪しを判断し、そのフィードバックに基づいてモデルを改善する方法もあります。

## 推論由来ハルシネーションの軽減

### （１）生成プロセスへの介入

1つ目は、マルチモーダルLLMが出力を生成する過程に介入する方法です。

例えば、「対照デコーディング」という手法では、元の画像と少し変化させた画像の両方を使って出力を生成し、その違いを比較します。モデルが画像の細部ではなく全体的な傾向に基づいて出力していることが分かれば、それを修正できます。

もう1つの例は、「ガイドデコーディング」という手法です。物体検出のような視覚的な処理を追加で行い、その結果をモデルの出力生成に利用するというものです。画像に写っている物体を明示的に認識させることで、モデルが画像の内容により忠実な出力を生成できるようにします。

### （２）出力の事後修正

2つ目は、いったんモデルに出力を生成させた後、その中からハルシネーションと思われる部分を見つけて取り除く方法です。出力された文章の内容を画像と照らし合わせて、矛盾がある部分を特定します。そして、その部分を修正するか削除します。

この方法は、画像に基づいて出力をチェックする、専用のモデルを使って出力を修正する、モデル自身に出力を修正させるなど、様々なバリエーションがあります。例えば、「Woodpecker」と呼ばれる手法では、出力された文章から重要なキーワードを抜き出し、それが画像内に存在するかどうかを確認することで、ハルシネーションを見つけて修正しています。

## 課題

マルチモーダルLLMにおけるハルシネーションの研究はまだ始まったばかりで、まだまだ多くの課題が残されています。分野の重要な課題と将来の方向性について詳しく見ていきましょう。

### データの質と量の改善

マルチモーダルLLMは大量のデータを必要とするため、データの質や多様性、偏りが大きな課題となっています。ハルシネーションを減らすためには、高品質で多様な訓練データが不可欠です。

今後の研究では、以下のような点に注目すべきと考えられています。

- 初期段階で十分なデータを集めること
- データ拡張技術を使ってデータ量を増やすこと
- 既存のデータから偏りを取り除き、多様性を高めること

### モダリティ間の整合性の向上

ハルシネーションの主な原因の1つは、生成された内容と入力された情報との不一致です。これを解決するには、モダリティ間の関係性をうまくモデル化する必要があります。

そこで以下のような研究の方向性が考えられてます。

- モダリティ間の表現を上手く対応づける方法の探求
- より高度なモデル [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") の設計
- 多様な教師信号の活用

また、モダリティ間の整合性をチェックする評価手法の改善も重要です。

### モデルアーキテクチャの改善

ハルシネーションに特化した効果的なモデル [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") はまだ確立されていません。今後は、以下のような点に注目した研究が望まれてます。

- より強力な視覚認識モデルの開発
- モダリティ間の情報伝達を円滑にするモジュールの考案
- 入力情報に忠実な言語モデルの設計

### 標準的なベンチマークの確立

ハルシネーションの評価には、標準的なベンチマークや評価指標が不可欠です。しかし、現状では統一された基準が存在しません。

今後は、理論的に妥当で使いやすいベンチマークの開発が求められています。そうしないとハルシネーション対策の研究が正しい方向に進まない恐れがあります。

### ハルシネーションを特徴として捉える

ハルシネーションは、マルチモーダルLLMの欠点ではなく、特徴の1つと捉えることもできます。ユーザーの入力に応じて、モデルが「夢」を見るようなものだと考えるのです。すると、ハルシネーションを活用してユーザー体験を向上させたり、創造性を高めたりする可能性も考えられます。今後の研究では、ハルシネーションとユーザー体験の関係性にも注目すべきでしょう。

### 説明可能性と信頼性の向上

既存のハルシネーション対策の多くは、特定のパターンに基づいたものです。しかし、根本的なメカニズムはまだよく分かっていません。

そのため、モデルの内部を可視化したり、生成プロセスを追跡したりする技術の開発が求められています。

### 倫理的な課題への取り組み

マルチモーダルLLMが高度になるにつれ、生成される内容の倫理的な問題はより重要になってきます。ハルシネーションは、深刻な倫理的懸念を引き起こすリスクも含んでいます。今後は、データ収集から評価に至るまで、倫理的な点を優先すべきと考えられています。

## まとめ

本記事では、マルチモーダルLLMのハルシネーションに関する最新の研究を紹介しました。ハルシネーションはマルチモーダルLLMの実用化における大きな課題であり、信頼性や安全性への懸念から社会実装が阻まれる可能性があります。

そんな中、今回研究者らはハルシネーションの原因、評価指標、ベンチマーク、軽減手法などを詳細に調査し、今後の研究の方向性を示唆しました。

標準化された評価指標やベンチマークの確立、倫理的な課題への対処など、継続的な研究が求められています。

マルチモーダルLLMの実用を進める上では見逃すことのできない観点ですね。

- 参照論文URL： [https://doi.org/10.48550/arXiv.2404.18930](https://doi.org/10.48550/arXiv.2404.18930)
- GitHub： [https://github.com/showlab/Awesome-MLLM-Hallucination](https://github.com/showlab/Awesome-MLLM-Hallucination)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMのプロンプトに数百から数千の例を含める超長尺のコンテキスト内学習（In-context learning）とファインチューニングの性能比較](https://ai-data-base.com/archives/68564)

[Googleが開発した「LLMに長文を高精度で読解させる方法論」と実行プロンプト](https://ai-data-base.com/archives/68821)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)