---
title: "LLMのプロンプトに数百から数千の例を含める超長尺のコンテキスト内学習（In-context learning）とファインチューニングの性能比較"
source: "https://ai-data-base.com/archives/68564"
author:
  - "[[AIDB Research]]"
published: 2024-05-08
created: 2025-06-13
description: "LLMを利用する中でIn-context learning（ICL、コンテキスト内学習）が注目されています。モデルに例示を与えるだけで、追加の学習なしに様々なタスクをこなせるようにする手法です。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

LLMを利用する中でIn-context learning（ICL、コンテキスト内学習）が注目されています。モデルに例示を与えるだけで、追加の学習なしに様々なタスクをこなせるようにする手法です。

これまでコンテキスト内学習は、短いコンテキスト（入力の長さ）が前提となっており、利用できる例示の数が限られていました。ところが最近、超長尺のコンテキストを扱えるモデルが次々と開発され、状況は一変し、性能向上の可能性が広がりました。

そこで研究チームは、大規模なコンテキスト内学習の性能を評価し、特性を探ることに挑戦しています。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_68564-1024x576.jpg)

**参照論文情報**

- タイトル：In-Context Learning with Long-Context Models: An In-Depth Exploration
- 著者：Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew R. Gormley, Graham Neubig
- 所属：Carnegie Mellon University, Tel Aviv University

**本記事の関連研究** ：

- [プロンプトに例を多く載せるほど、どんなタスクでも性能が上がるのか？DeepMindによる『Many-shot Learning』の実験結果](https://ai-data-base.com/archives/67883)
- [LLMにおける、長いコンテキストから欲しい情報を見つけ出す「needle-in-a-haystack（干し草の中の針）」テスト結果とプロンプト例](https://ai-data-base.com/archives/68016)
- [Microsoftなどのプロンプト圧縮技術『LLMLingua-“2″』タスクの精度を維持したまま圧縮率2-5倍](https://ai-data-base.com/archives/66170)
- [GPT-4やGeminiなどさまざまなLLMで、プロンプトの入力が長くなるにつれて推論性能に顕著な低下が見られる](https://ai-data-base.com/archives/64873)

## 背景

コンテキスト内学習とは、LLMに対して、タスクの例示をプロンプトとして与えることで、タスクを学習させる手法のことです。実装が簡単で計算コストが低く、汎用的なモデルを様々なタスクに適用できるため、近年注目を集めています。

しかし冒頭で述べた通り、これまでのコンテキスト内学習に関する研究の多くは、短いコンテキスト長（モデルが一度に処理できる入力の長さ）のモデルに焦点を当てており、例示の数が限られていました。そのため、コンテキスト内学習では大規模なデータを利用することができず、性能に限界がありました。

ところが最近では、極端に長いコンテキストウィンドウに適応できる言語モデルが次々と開発されています。大量のデータをコンテキスト内学習で直接利用できるようになり、ファインチューニング（モデルを追加学習させること）の代替手段としても期待されるようになりました。

そこで研究者らは、超長尺コンテキストでのコンテキスト内学習の性質や、大量のデータを用いた場合のコンテキスト内学習とファインチューニングの性能を比較することにしました。これまでほとんど理解されていなかった、大規模コンテキスト内学習の特性を明らかにしたいと考えています。

なお、これまでにもコンテキスト内学習の性質を調べた先行研究はいくつかあり、例示の選択戦略が重要であることや、コンテキスト内学習がタスク認識とタスク学習の2つのモードで機能しているという指摘などがされています。  
その中で、コンテキスト内学習とファインチューニングの比較は（実用上重要な意義があるにもかかわらず）あまり行われてきませんでした。

## 実験内容

研究チームは、複数のデータセットとモデルを用いて体系的な実験を行いました。実験の設定について詳しく見ていきます。

実験に使用されたデータセットは、下記5つの分類タスク用データセットです。ドメインやラベル数が異なり、多様性のあるベンチマークとなっています。

- TREC（質問の分類を6つのカテゴリーで行う）
- TREC-fine（TRECの質問を50の細かいカテゴリーに分類する）
- NLU（対話システムにおける発話の意図を68のカテゴリーで分類する）
- Banking-77（銀行業務に関する顧客の発話を77の意図に分類する）
- Clinic-150（医療分野の10のドメインにおける151の意図分類と、それ以外の発話を判定する）

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_68564_2-1024x414.png)

使用したデータセットの概要

### モデル

言語モデルとしては、主に Llama-2-7b（非対話型）の変化系が使用されました。下記3つです。

1. オリジナルのLlama2モデル。4096トークンのコンテキスト長で学習されています。
2. TogetherAI によって 32k トークンのコンテキスト長にファインチューニングされたモデルLlama2-32k。
3. Fu et al. (2024) によって 80k トークンのコンテキスト長にファインチューニングされたモデル [Llama2-80k](https://huggingface.co/yaofu/llama-2-7b-80k) 。

なお、Llamaシリーズ以外のモデルとして、Mistral-7b-v0.2（対話型）も使用され、結果の一般性が確認されました。

### 制約

全ての実験において、有効なラベルのみを出力するように制約付きデコーディング（モデルが生成する出力に制約を設ける手法）が適用されました。  
通常、言語モデルは、与えられた入力に対して、考えられる全ての出力を生成し、その中から最も確率の高いものを選びます。しかしタスクに適さない無効な出力が生成されてしまう恐れがあります。  
そこで制約付きデコーディングでは以下のような処理によって、有効な出力のみを生成するように制約が設けられます。

1. 出力の候補となるトークン（単語や記号）のリストを予め用意しておく。
2. 言語モデルが出力を生成する際、候補リストに含まれるトークンのみを考慮する。
3. 候補リストにないトークンの確率を0にするか、非常に小さな値に設定する。

要するに本研究では、制約付きデコーディングを用いることで、例え少数の例示しか与えられない場合でも、無効なラベルが出力されるのを防ぐことができると考えられました。またファインチューニングの際は、無効なラベルが出力されないように分類ヘッドが使用されました。

なお、ラベルとは、機械学習、特に教師あり学習において、データに付与される正解の情報のことを指します。例えば、メールのスパム分類タスクでは、各メールに対して「スパム」または「非スパム」というラベルが付けられます。画像分類タスクでは、各画像に「犬」「猫」などのラベルが付けられます。  
今回の研究においては、各データセットには予め定義された一連のラベル（例: 質問の種類、ユーザの意図など）が存在し、モデルはこれらの有効なラベルの中から最も適切なものを選ぶ必要があります。制約付きデコーディングは、モデルが有効なラベル以外の無効な出力を生成してしまうことを防ぐための手法です。

### 評価方法

各データセットのテストセットから250個のサンプルが無作為に抽出され、評価に使用されました。評価指標としては、全体的な性能を表すAccuracyと、マイノリティクラスの性能を表すMacro-F1を用いました。（ただし結果的にトレンドが似ていたため、主にAccuracyが報告されています）

### コンテキスト内学習の設定

コンテキスト内学習では、下記の二つが比較されました。

（1）ランダムに例示を選択する方法  
（2） 各テストサンプルに関連する例示を検索する方法

なお（1）ランダム選択の場合は、例示のエンコーディングが1回だけ行われ、全てのテストサンプルで再利用できます。一方、（2）検索ベースの場合は、各テストサンプルごとに検索とエンコーディングが必要となります。

### ファインチューニングの設定

ファインチューニングでは、Llama2-7bに分類ヘッドが追加され、各データセットの様々な量のデータで学習されました。初期化には、事前学習済みの言語モデルの重みが利用され、ランダム初期化よりも良い結果が得られるようにされました。

## 長尺でのコンテキスト内学習の特性についての実験結果

例示を多く含んだ長尺コンテキスト内学習の特性は、従来のFew-shot学習（短いコンテキスト内学習）と比較するとどうなのか？その結果について詳しく見ていきます。なお、長いコンテキストを扱うモデルの特性を調べるためのテストベッドとしても、コンテキスト内学習が活用されています。

### コンテキストウィンドウ最大幅の使用は有効か

まず、コンテキストウィンドウ全体を使うことが有効かどうかが調べられました。

結果、ほとんどのデータセットで、例示の数を増やすほど性能が単調に向上することが分かりました。つまり、コンテキスト全体を使うことは有効であり、少なくとも害にはならないということです。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_68564_1-1024x443.png)

例示数を増やすと、元のLlama-2の文脈長をはるかに超えても性能が向上し続ける。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_68564_4-1024x440.png)

少数の例示ではコンテキスト内学習がファインチューニングを上回るが、多数の例示ではファインチューニングがコンテキスト内学習を上回ることがある。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_68564_3-1024x468.png)

全てのデータセットで、ICLの性能は追加の例示とともに向上し続ける。

直感的には、モデルが多数の例示を見れば見るほど、タスクについてより深く理解できるようになり、性能が向上し続けるように思えます。しかし、実験結果が示すのは、

1. 少数の例示では、コンテキスト内学習がファインチューニングよりも優れている。これは、ICLがタスクについてすぐに「理解」し、新しい例に適応できるためと考えられる。
2. 多数の例示では、ファインチューニングがコンテキスト内学習を上回ることがある。莫大な数の例示をコンテキスト内に与えるよりも、モデル自体をより少ないデータで直接tuningする方が効果的なことがある。

つまり、コンテキスト学習はFew-shot学習に優れているが、モデルを真にタスクに適応させるにはファインチューニングが必要となる、ということのようです。

※なお、本研究では例示を増やすことに焦点を当てていますが、無造作にプロンプトが長くなってしまうと精度が落ちることは別の研究で示唆されています（ [GPT-4やGeminiなどさまざまなLLMで、プロンプトの入力が長くなるにつれて推論性能に顕著な低下が見られる](https://ai-data-base.com/archives/64873) より）。

### 例の順序の影響

次に、例示の「順序」が性能に与える影響が調べられました。従来の研究では、多くのモデルが順序の影響を強く受けることが知られています。

しかし、本研究では、例示の数が増えるほど、順序の影響が大幅に減少することが明らかになりました。つまり、長いコンテキストのコンテキスト内学習では、例示の順序はあまり重要ではないということです。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_68564_5.png)

例示数が増えるほど、順序の影響は小さくなる。

### ラベルのソートの影響

さらに、同じラベルの例示をまとめる（ラベルのソート）という、順序の悪影響が出やすいケースが調べられました。例示を種類ごとにグループ化するとどうなるのか、ということです。

その結果、例示の数が少ない場合はあまり影響がありませんでしたが、数が増えるほど、ラベルのソートが性能を大きく低下させることが分かりました。異なるラベルの例を文脈化することが重要であり、その効果は比較的短い範囲でしか働かないことを示唆しています。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_68564_6-1.png)

例示数が多いほど、ソートの悪影響が大きくなる。

### 短いコンテキストでの性能

また、長いコンテキストに適応させたモデル（Llama2-32kとLlama2-80k）が、短尺の設定でどのように機能するかも調べられました。

結果、ベースモデル（Llama2）と比較して、性能は概ね同等か若干向上することが分かりました。ただし、Llama2-32kでは一部のケースで性能の低下も見られ、追加機能のためのファインチューニングが悪影響を及ぼす可能性があることが示唆されました。

### 入力の利用効率

モデルが入力を効果的に利用できているかどうかについても調べられました。実験方法としては、コンテキスト内の例をそのままコピーできるかどうかがテストされました。

どのモデルも完璧にコピーすることはできませんでしたが、例示の数が増えるほどコピー精度が向上する傾向が見られました。追加の例によってタスクがより明確になるためだと考えられています。

## なぜ長尺のコンテキストは優れた結果をもたらす？

次に、各例示が近くの少数の例示としか関連付けられないように制限を加えた実験が行われました。ただし、テストサンプル（モデルが答えを出す対象）については、全ての例示を参照できるようにしました。

この実験の目的は、以下の2つの仮説に基づいて考察することです。

1. 多数の例示を一緒に処理することで、モデルのタスク理解が徐々に洗練されていくことが、性能向上の主な理由である。
2. モデルが参照できる関連例示が増えることが、性能向上の主な理由である。

もし仮説1が正しければ、例示を小さなグループに分けて処理した場合、全体を一度に処理した場合に比べて性能が大幅に低下するはずです。なぜなら、モデルはタスクを十分に理解できないからです。

一方、仮説2が正しければ、各例示がどれだけ多くの他の例示と関連付けられているかは、それほど重要ではありません。つまり、例示を小さなグループに分けても、全体を一度に処理した場合と同等の性能が得られるはずです。

### ブロックアテンションの効果

ブロックアテンションとは、通常のアテンションとは異なる方法で、モデルが長い文脈をどのように処理するかを調べるために論文の中で導入された概念です。

通常、 [Transformer](https://ai-data-base.com/archives/26535 "Transformer") モデルでは、各トークンが文脈内の全ての他のトークンに”アテンション”を向けます。つまり、各トークンは文脈全体の情報を利用して表現を更新します。

一方、ブロックアテンションでは、文脈を固定サイズの”ブロック”に分割し、各トークンは自分のブロック内のトークンにのみアテンションを向けることができます。ただし、現在予測しようとしているトークンは、全てのブロックにアテンションを向けることができます。

例えば、100個のトークンからなる文脈を、20個のトークンずつ5つのブロックに分割するとします。この場合、最初の20個のトークンは互いにアテンションを向け合いますが、21個目以降のトークンにはアテンションを向けません。  
研究者らはブロックアテンションを使って、モデルが長い文脈をどの程度利用しているのか、また、少数の例から全体的な”タスクの理解”を獲得しているのかどうかを分析しています。

実験の結果、わずか1〜5個程度の例からなる非常に小さなブロックでは、固定された少数の例にアテンションを向ける場合よりも性能が低下しました。これは、各例の文脈化が不十分なために、エンコーディングの質が低下するためだと考えられます。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_68564_7-1024x466.png)

通常のattentionと、予測対象の例のみ全体に attending できるブロック化 attention の比較。

しかし、ブロックサイズを増やしていくと、性能は急速に向上しました。そして、50〜75個程度の例からなるブロックで、完全アテンションの95%以上の性能が得られることが分かりました。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_68564_8-1024x437.png)

ブロックサイズが比較的小さくても、全体attentionに近い性能が得られることを示す。

### ラベルでソートした場合のブロックアテンション

さらに、研究チームは、ラベルでソートした例示を用いて、ブロックアテンション の効果を調べました。この設定では、ほとんどのブロックが1〜2種類のラベルしか含まないため、各ブロック内でタスクの決定境界を学習し、それを集約することはできません。

実験の結果、ラベルでソートした場合でも、ブロックアテンションは完全アテンションと同等の性能を達成できることが分かりました。このことから、長いコンテキストのICLの性能向上は、各ブロック内でタスクの決定境界を学習することによるものではなく、より関連性の高い例からの情報検索によるものであることが示唆されました。

![](https://ai-data-base.com/wp-content/uploads/2024/05/AIDB_68564_9.png)

ソートした場合でも、ブロック化attentionは通常のattentionと同等の性能を達成できる。

## 結論

- 大量の例示を使ったIn-context learning（ICL、コンテキスト内学習）は、驚くほど高い性能を発揮する
- 例示の数が増えると、例示の選び方はあまり重要ではなくなる
- 長いコンテキスト内学習は、言語モデルの追加学習（ファインチューニング）と同等かそれ以上の性能を達成できる
- 長いコンテキスト内学習では、例示の順番の影響は小さくなる
- 長いコンテキスト内学習の性能向上は、関連性の高い例示を多く参照できるようになったことが主な理由である
- 一方で、タスクの決定境界を学習すること（つまり、与えられた例から一般的なルールを抽出すること）は、性能向上にあまり寄与していない
- 長いコンテキストICLは、言語モデルを活用する新しい方法として期待されるが、まだ課題もある

## まとめ

本記事では、長尺のコンテキスト内学習の性能と特性を調べた研究を紹介しました。研究の結果、長いコンテキスト内学習は非常に強力で、大量の例示を使う場合はランダム選択でも十分な性能が得られることが分かりました。また、その性能向上は、関連性の高い例を多数アテンション可能になることに起因していることが明らかになりました。

この研究のように長いコンテキスト内学習の特性をさらに解明することで、より高度で柔軟な言語モデルの開発が可能になるかもしれません。少ないデータでも高い性能を発揮するようになれば、アクセシビリティと実用性が大きく向上することも考えられます。

URL： [https://arxiv.org/abs/2405.00200](https://arxiv.org/abs/2405.00200)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[Apple開発のオープンソースLLM「OpenELM」](https://ai-data-base.com/archives/68614)

[マルチモーダルLLMにおける幻覚（ハルシネーション）の原因と対策　クリエイティブでの活用も推奨　AWSなどが網羅的に調査](https://ai-data-base.com/archives/68720)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)