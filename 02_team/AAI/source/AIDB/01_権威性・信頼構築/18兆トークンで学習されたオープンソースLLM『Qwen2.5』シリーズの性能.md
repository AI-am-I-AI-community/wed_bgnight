---
title: "18兆トークンで学習されたオープンソースLLM『Qwen2.5』シリーズの性能"
source: "https://ai-data-base.com/archives/81076"
author:
  - "[[AIDB Research]]"
published: 2024-12-24
created: 2025-06-13
description: "本記事では、新たに発表された大規模言語モデル（LLM）ファミリーQwen2.5シリーズを紹介します。LLMの開発競争が世界中で加速する中、モデルやデータの規模拡大だけでなく、効果的な学習手法の確立が求められてきました。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、新たに発表された大規模言語モデル（LLM）ファミリーQwen2.5シリーズを紹介します。LLMの開発競争が世界中で加速する中、モデルやデータの規模拡大だけでなく、効果的な学習手法の確立が求められてきました。研究者らは、18兆トークンにまで拡大した事前学習データと100万件以上のサンプルを用いた高度な教師あり学習、そして多段階 [強化学習](https://ai-data-base.com/archives/26125 "強化学習") を組み合わせることで、次世代のモデル開発に取り組みました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81076-1024x576.jpg)

**発表者情報**

- 研究機関：アリババQwenチーム

## 背景

LLMの開発競争が世界中で加速する中、モデルの性能向上に向けた取り組みが続けられています。言語理解や生成、推論といった基本的な能力の向上には、モデルやデータの規模拡大だけでなく、効果的な学習手法の確立が不可欠とされてきました。

従来の手法では、事前学習後のモデルに対して教師ありファインチューニングを適用し、さらに人間からのフィードバックを活用した強化学習を組み合わせることで、モデルの性能向上が図られてきました。しかし、長文の処理や構造化データの分析、指示への正確な追従といった課題が残されていました。

また、モデルの推論時における処理能力の向上も重要な課題となっています。段階的な推論や熟考のプロセスを実現することで、より複雑な問題解決への対応が模索されてきました。

こうした技術的な課題に加えて、研究開発の成果を広く活用可能にするための取り組みも進められています。オープンソースのLLMを公開することで、研究者や開発者がより容易にアクセスでき、様々な分野でのイノベーション創出につながることが期待されています。

このような背景のもと、研究者らは事前学習データを7兆トークンから18兆トークンへと大幅に拡大し、さらに100万件以上のサンプルを用いた高度な教師ありファインチューニングと多段階強化学習を組み合わせることで、次世代のLLM「Qwen2.5」の開発に取り組みました。

以下で詳しく紹介します。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81076_1-1024x347.png)

データスケーリングがQwenシリーズの発展において果たした重要な役割を示すグラフ。18兆トークンで事前学習されたQwen2.5が、特にドメイン専門知識において最も高度な能力を示している

## アーキテクチャとトークナイザー

### モデル構成

研究者らは、オープンソースで提供されるデンスモデル（全てのパラメータがアクティブに使用されるモデルのこと）とAPIサービス向けのMoEモデル（特定の入力に対して「専門家」と呼ばれるサブモデルの一部だけがアクティブになる仕組み）という2種類の [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") を開発しました。デンスモデルはパラメータ数に応じて0.5Bから72Bまでの7つのバリエーションが用意され、MoEモデルではQwen2.5-TurboとQwen2.5-Plusが提供されます。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81076_2-1024x233.png)

Qwen2.5オープンウェイトモデルの アーキテクチャ とライセンスの詳細を示す表

### デンスモデルの特徴

デンスモデルには、最新技術を詰め込んだ [Transformer](https://ai-data-base.com/archives/26535 "Transformer") ベースのデコーダー [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") が採用されています。 [Transformer](https://ai-data-base.com/archives/26535 "Transformer") は、文章やデータを処理する強力な仕組みで、デコーダー部分は特に「次に何を出力するか」を考える役割を持っています。この構造が正確で効率的な生成を可能にしています。

まず、処理をスピードアップするため、KVキャッシュという仕組みを利用しています。一度計算した結果を保存しておくことで、再計算を省き効率が向上します。さらに、Grouped Query Attentionではデータをまとめて処理する方法を採用しており、大量の情報を一括で処理する場面で威力を発揮します。

次に、モデルが柔軟に学習できるよう、SwiGLU [活性化関数](https://ai-data-base.com/archives/27011 "活性化関数") も組み込まれています。この関数は複雑なパターンを認識するための手助けをしながら、計算効率も高いのが特徴です。

また、文章やデータの「順番」を認識させるためには、Rotary Positional Embeddingsを採用しています。位置情報を数学的に回転する形で表現することで、モデルが自然な順序を理解できるようになっています。

さらに、重要な情報を的確に見つける仕組みとして、QKVバイアスを導入しています。アテンション機構が入力データの中で特に注目すべき部分をより正確に判断できるようになっています。

最後に、モデルの学習を安定させる工夫も盛り込まれています。事前 [正規化](https://ai-data-base.com/archives/26401 "正規化") 付きRMSNormによって、各層の出力が適切に調整され、計算が乱れたり不安定になることを防いでいます。

### MoEモデルの構造

MoEモデルでは、通常の処理を行う層が、複数の専門家（エキスパート）を集めた特別な「MoE層」に置き換えられています。MoE層には、いくつものエキスパートがおり、それぞれ異なる得意分野を持っています。入力データ（トークン）は、この中から適したエキスパートに振り分けられる仕組みになっています。

トークンをどのエキスパートに割り振るかを決めるルーティングメカニズムがこの仕組みを支えています。エキスパートを細かく分ける細粒度エキスパート [セグメンテーション](https://ai-data-base.com/archives/26353 "セグメンテーション") によって、それぞれの役割が明確化され、効率的にタスクを処理できるようになっています。一方で、複数のタスクでエキスパートを共有できるようにした共有エキスパートルーティングにより、無駄なく計算リソースを使うことが可能です。

### トークナイザーの改良

トークナイザーには、15万を超える語彙を持つバイトレベルの「バイトペアエンコーディング（BPE）」という手法が採用されています。単語をバイト単位に分解して処理することで、少ない語彙で幅広い表現に対応できる仕組みになっています。

制御トークンの数は従来の3個から22個へと拡張され、より細かい指示が可能になりました。さらに、ツール機能用に2つの新しいトークンが追加され、特定の操作や機能の呼び出しを簡単に行えるようになっています。

残りのトークンは他のモデル機能に割り当てられ、全モデルで統一された語彙セットが設けられています。

## 事前学習

事前学習とは、大量のテキストなどを「一般教養」として学ぶような段階で、特定の目的はなく、とにかく幅広い知識を吸収するプロセスです。

### データ品質の向上

研究者らは、データ品質の評価とフィルタリングに重点を置きました。

Qwen2-Instructモデルをフィルターとして活用し、学習サンプルの多次元分析を実施することで、高品質なデータの選別が可能になりました。数学やコーディング分野のデータが強化され、Qwen2.5-MathとQwen2.5-Coderから得られた専門的なデータセットが組み込まれています。

また、独自の報酬モデルによって合成データの品質が厳密に管理されました。

### データ分布の最適化

事前学習データの分布を最適化するため、各ドメインのバランス調整が行われました。

eコマースやソーシャルメディアなど、過剰に表現されているドメインがダウン [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") され、テクノロジーや科学研究など、高品質な情報を含むドメインがアップ [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") されました。

結果として、7兆トークンから18兆トークンへとデータ規模が拡大されています。

### ハイパーパラメータの最適化

様々なモデル [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") に対して最適なハイパーパラメータを特定するため、スケーリング則が開発されました。

モデルサイズと学習データサイズに基づき、 [バッチサイズ](https://ai-data-base.com/archives/26582 "バッチサイズ") や学習率などの重要なパラメータが決定されています。MoEモデルでは、デンスモデルと同等のパフォーマンスを達成するため、活性化パラメータと総パラメータの調整が行われました。

### 長文脈学習の実現

効率的な学習を実現するため、2段階の事前学習アプローチが採用されました。

まず4,096トークンの文脈長で学習を行い、その後32,768トークンまで拡張されています。Qwen2.5-Turboでは、最大262,144トークンまで段階的に文脈長が拡張され、YARNとDual Chunk Attentionの実装により、最大100万トークンの処理が可能になりました。

## 事後学習

事後学習は事前学習とは対照的に、具体的な仕事や課題のために「専門知識」を学ぶような段階です。

### 教師あり微調整

長い文章（8,192トークン相当）が生成できるように、豊富な練習データが開発されました。数学の問題では、Qwen2.5-Mathの「一歩ずつ考える」方式が取り入れられ、丁寧な解説付きの問題集で学習が進められました。その結果、より説得力のある解答が可能になっています。

プログラミングについては、約40種類のプログラミング言語が扱えるよう特別な訓練が実施されました。生成されたコードは自動チェックシステムによって検証され、品質が保証されています。

人の指示への従順性を高めるため、厳しいチェック体制が導入されました。質の高いデータだけが選別され、学習に使用されています。また、表やグラフなどの数値データも、簡単な質問から複雑な分析まで、幅広い対応が可能になりました。

論理的な考え方も強化するため、7万個の新しい問題が追加されました。様々な角度からの思考方法が学ばれ、英語以外の言語への対応も翻訳の仕組みを通じて拡大されています。

### オフライン強化学習

「正解はあるけど、その良し悪しを判断するのが難しい」という問題には、特別な学習方法が採用されました。実際の動作結果と望ましい答えの比較から、15万組の練習データが作成されました。人による確認と機械による確認の両方が実施され、学習の質が担保されています。

### オンライン強化学習

モデルの回答は「正しさ」「有用性」「簡潔さ」「関連性」「安全性」「公平性」などの観点から評価されています。公開データと独自の複雑な質問が組み合わされ、様々なパターンの回答が生成できるよう訓練が重ねられました。

### 長文脈の微調整

Qwen2.5-Turboの仕上げ段階では、2段階のアプローチが採用されました。最初は短い指示での練習が行われ、その後で長い指示も加えた学習が進められました。計算資源の制約から、強化学習では短い指示に焦点が当てられましたが、長い文章を扱う能力も十分な向上が確認されています。

## 評価

### ベースモデルの評価

研究者らは、自然言語理解、一般的な質問応答、コーディング、数学、科学知識、推論、多言語能力など、幅広い分野における性能評価を実施しました。

最大モデルのQwen2.5-72Bは、パラメータ数が5分の1であるにもかかわらず、Llama-3-405Bと匹敵する結果を達成しています。前身のQwen2-72Bと比較しても、一般的なタスク、数学、コーディング分野で顕著な性能向上が確認されました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81076_3-1024x681.png)

70B+サイズのベースモデル（Qwen2.5-Plus含む）の性能比較表

中規模モデルのQwen2.5-14BとQwen2.5-32Bは、既存の同規模モデルを上回る性能を示しています。MMLUやBBHといった一般的なタスクでは、より大規模なモデルをも凌駕する結果が得られました。また、Qwen2.5-Turboは、トレーニングと推論のコストを大幅に削減しながらも、Qwen2.5-14Bと同等の性能を実現しています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81076_4-1024x671.jpg)

14B-30B+サイズのベースモデルとQwen2.5-Turboの性能比較表

小規模モデルにおいても優れた性能が確認されました。7Bモデルは、埋め込み以外のパラメータ数を抑えながらも、多くのベンチマークで競合モデルを上回る結果を示しています。さらに、0.5B、1.5B、3Bの各モデルは、同規模の他モデルと比較して、数学やコーディングタスクで優位性を保っています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81076_5.png)

7B+サイズのベースモデルの性能比較表

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81076_6-1024x679.jpg)

小規模なベースモデル（0.5B-3B）の性能比較表

### 命令調整済みモデルの評価

人間の好みに沿った応答や長文処理能力に重点を置いた評価が行われました。

72Bモデルは、多くの重要ベンチマークでLlama-3.1-405B-Instructを上回る性能を実現しています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81076_7-1024x626.png)

70B+サイズの命令調整済みモデルとQwen2.5-Plusの性能比較表

14Bと32Bのモデルも、GPT4o-miniに匹敵する性能を示し、リソース効率の高さが証明されました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81076_8-1024x529.png)

14B-30B+サイズの命令調整済みモデルとQwen2.5-Turboの性能比較表

エッジデバイス向けの小規模モデルでは、3Bモデルがパラメータ数の多いPhi3.5-miniやMiniCPM3-4Bを上回る性能を達成しています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81076_9.png)

7B+サイズの命令調整済みモデルの性能比較表

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81076_10.png)

2B-4B程度の命令調整済みモデルの性能比較表

1.5Bと0.5Bモデルも、前バージョンから大幅な性能向上を実現しており、リソース制約の厳しい環境での活用が期待されます。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81076_11.png)

0.5B-1.5Bサイズの命令調整済みモデルの性能比較表

### 社内自動評価

研究者らは独自の評価データセットを開発し、知識理解、テキスト生成、コーディングなど、多角的な性能評価を実施しました。英語と中国語の両言語で評価が行われ、小規模から大規模まで、すべてのモデルサイズで前世代を上回る性能が確認されました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81076_12-1024x683.jpg)

英語での社内自動評価ベンチマークにおける性能比較表

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81076_13-1024x677.jpg)

中国語での社内自動評価ベンチマークにおける性能比較表

多言語評価では、IFEvalの多言語拡張版や、各国語版のMMMLU、拡張版MGSM8K、文化的ニュアンス評価用のBLEnDなど、包括的なベンチマークが使用されました。

結果として、命令遵守、知識活用、数学的推論において、同規模モデルと遜色ない性能が示されています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81076_14-1024x426.png)

70B+サイズの命令調整済みモデルの多言語タスクにおける性能比較表

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81076_15-1024x512.png)

7B-14Bサイズの命令調整済みモデルの多言語タスクにおける性能比較表

### 長文コンテキスト処理能力

RULER、LV-Eval、Lon [gb](https://ai-data-base.com/archives/26343 "勾配ブースティング") ench-Chatを用いた評価により、高い長文処理能力が実証されました。

DCA（長い文脈を効率的に処理するためのアテンション機構）とYARN（通常より長いテキストを処理する埋め込み手法）の組み合わせにより、72Bモデルは既存の長文処理モデルを大きく上回る性能を達成しています。Turboモデルは100万トークンの処理も可能で、スパースアテンションの導入により、計算負荷を12.5倍削減することに成功しました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81076_17-1024x492.png)

RULERベンチマークにおけるQwen2.5モデルの性能比較表。32Kトークンまでの範囲ではYARN+DCAの影響を受けないことが示されている

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81076_18-1024x460.png)

LV-EvalとLon gB ench-Chatにおけるのモデルの性能比較表。Qwen2.5モデルの長文処理能力が評価されている

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81076_19-1024x418.png)

Qwen2.5-Turboの「パスキー検索」タスクにおける性能を示す図。1Mトークンの長さにわたって100%の精度で情報を取得できることが示されている

下の図は、長いテキストを処理する際の反応速度の改善を示したグラフです。従来のフルアテンション方式（青線）と比べて、新しく開発された手法（黄線）では、テキストが長くなっても処理時間の増加を大幅に抑えられています。100万トークンという長いテキストを処理する場合、Qwen2.5-Turboで最大4.3倍、Qwen2.5-7Bで最大5.6倍の高速化を実現しています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81076_20.png)

Qwen2.5-TurboとQwen2.5-7Bにおいて、フルアテンションと開発された新手法を比較したTTFT（最初のトークンまでの時間）の比較図

### 報酬モデルの評価

強化学習プロセスの要となる報酬モデルについても、Reward Bench、RMB、PPE、独自の中国語ベンチマークを用いた総合的な評価が行われました。72B報酬モデルは、PPEと中国語評価でトップ性能を示し、他のベンチマークでも競争力のある結果を残しています。研究者らは、単一ベンチマークへの過度な最適化を避け、多様な評価指標の重要性を指摘しています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81076_16-1024x753.png)

複数の報酬モデルベンチマークにおける性能比較表

## まとめ

本記事では、Qwen2.5シリーズの大規模言語モデルに関する研究を紹介しました。事前学習データを18兆トークンに拡大し、教師あり学習と強化学習を組み合わせることで、長文生成や構造化データの分析能力が向上しています。0.5Bから72Bまでの様々な規模のモデルが公開され、MoE技術を活用したQwen2.5-TurboとQwen2.5-Plusも提供されることで、用途に応じた選択が可能になりました。

本モデルシリーズは下記のURLで公開されており、VRAMの大きくないPCでも動作可能なモデルがいくつかあります。

**参照文献情報**

- タイトル：Qwen2.5 Technical Report
- URL： [https://arxiv.org/abs/2412.15115](https://arxiv.org/abs/2412.15115)
- 著者：Qwen: An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu (additional authors not shown)
- 所属：Qwen Team（Alibaba）
- HuggingFace： [https://huggingface.co/Qwen](https://huggingface.co/Qwen)
- プロジェクトページ： [https://modelscope.cn/organization/qwen](https://modelscope.cn/organization/qwen)
- GitHub： [https://github.com/QwenLM/Qwen2.5](https://github.com/QwenLM/Qwen2.5)

## 理解度クイズ（β版）

1\. Qwen2.5シリーズの主な技術的改善点は何ですか？

Qwen2.5は事前学習データを18兆トークンに拡大し、100万件以上のサンプルによる教師あり学習を組み合わせて性能を向上させました。この手法により、長文生成や構造化データの分析能力が大幅に改善されました。

解説を見る

2\. Qwen2.5シリーズのモデル構成の特徴は何ですか？

Qwen2.5シリーズは0.5Bから72Bまでのデンスモデルと、Qwen2.5-TurboとQwen2.5-PlusのMoEモデルを提供しています。この多様なラインナップにより、用途に応じた適切なモデル選択が可能になりました。

解説を見る

3\. Qwen2.5の長文処理における革新点は何ですか？

Qwen2.5-Turboは段階的な文脈長拡張とDual Chunk Attentionを実装し、最大100万トークンの処理を実現しました。さらにスパースアテンションの導入により計算負荷を12.5倍削減しています。

解説を見る

4\. Qwen2.5の評価において重視された点は何ですか？

研究チームは知識理解、テキスト生成、コーディング、多言語評価など、包括的な評価を実施しました。特に実用的なタスクでの性能と、リソース効率の両立が重視されました。

解説を見る

5\. 小規模モデル（7B以下）の主な特徴は何ですか？

小規模モデルは数学やコーディングなど特定のタスクで競合モデルを上回る性能を示しています。リソース効率を維持しながら特定分野で優れた性能を実現する設計が特徴です。

解説を見る

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[企業実務タスクにおけるLLMエージェントの能力を評価する方法](https://ai-data-base.com/archives/81003)

[8つの質問で自分自身の答えを批評する哲学的手法を活用したLLMのプロンプティング技術](https://ai-data-base.com/archives/81166)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)