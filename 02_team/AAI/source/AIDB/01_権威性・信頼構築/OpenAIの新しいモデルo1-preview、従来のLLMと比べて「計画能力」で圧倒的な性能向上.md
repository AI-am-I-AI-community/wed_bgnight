---
title: "OpenAIの新しいモデルo1-preview、従来のLLMと比べて「計画能力」で圧倒的な性能向上"
source: "https://ai-data-base.com/archives/76177"
author:
  - "[[AIDB Research]]"
published: 2024-09-26
created: 2025-06-13
description: "本記事では、アリゾナ州立大の研究グループによるLLMの計画能力を評価した研究を紹介します。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、アリゾナ州立大の研究グループによるLLMの計画能力を評価した研究を紹介します。

研究チームは計画能力を測定するための自作のベンチマークであるPlanBenchを用いて、OpenAIのo1を含む最新モデルの性能を分析しました。様々な難易度の問題で、「精度」「効率性」「コスト」「結果の保証（正確性や信頼性）」を評価しています。

その結果、o1は大幅な性能向上を示しました。しかし、まだまだ課題も残されています。

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_76177-1024x576.jpg)

**参照論文情報**

- タイトル：LLMs Still Can’t Plan; Can LRMs? A Preliminary Evaluation of OpenAI’s o1 on PlanBench
- 著者：Karthik Valmeekam, Kaya Stechly, Subbarao Kambhampati
- 研究機関：Arizona State University

**本記事の関連研究**

- [CoT（思考の連鎖）は数学や論理で劇的に性能を向上させる一方、常識や知識のタスクでほとんど効果がない](https://ai-data-base.com/archives/75942)
- [OpenAI、大規模言語モデルの数学能力を大きく向上させることに成功](https://ai-data-base.com/archives/52505)
- [単純に生成回数を増やすとLLMの性能が大幅に向上する「推論時のスケーリング則」](https://ai-data-base.com/archives/75838)

## 背景

人工知能の分野では、目標を達成するための行動計画を立てる能力が非常に重要視されてきました。計画を立てる能力は人間の知性のコアな部分だと考えられているためです。

やがてLLMが登場し、研究者たちは、「LLMにも人間のような計画能力があるのではないか」と考えるようになりました。そして、2022年にPlanBenchというベンチマークが開発されました。LLMの計画能力を評価するためのテストです。

しかし、これまで多くの新しいモデルが登場してきたにもかかわらず、PlanBenchでの成績はあまり向上しませんでした。これは意外な結果でした。

そんな中、OpenAIが新しいモデル「o1」を発表しました。o1は従来のLLMとは異なり、推論に特化して設計・訓練されたモデルだとされています。OpenAIはこれを「大規模推論モデル（LRM）」と呼んでいます。

この新しいモデルの登場を機に、研究者たちは改めてPlanBenchを使って、最新のLLMやLRM（要するにo1）の計画能力を総合的に評価することにしました。

その結果、o1の性能は確かに従来のモデルを大きく上回っていました。しかし、まだ完璧とは言えない結果でした。

以下で詳しく紹介します。

## 最先端のLLMでも、まだ計画を立てられない

PlanBenchは、通常のLLMにとって今でも難しい課題です。最も簡単なテストセットにおいてさえ、LLMの成績があまり良くないことから、単純な情報の検索だけでは計画を立てることはできないと考えられます。

PlanBenchには様々な難易度の問題が含まれていますが、例えばブロックワールドという問題は例えばテーブル上にある複数のブロックを指定された順序で積み上げる問題です。

下の表には、600問のブロックワールド問題に対する様々なLLMの成績が示されています。このテストには、普通のブロックワールド問題と、同じ内容を難しい言葉で表現した「ミステリーブロックワールド」問題が含まれています。例えば「ブロックを積む」という動作が「object\_1をobject\_2に征服する」といった表現に置き換えられているなどの謎を含んでいます。

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_76177_1-1024x318.png)

異なるLLMファミリーにおける600のブロックワールドと謎のブロックワールドインスタンスの性能を、ゼロショットとワンショットのプロンプトを使用して比較

通常のブロックワールド問題では、LLaMA 3.1 405Bというモデルが62.6%の正答率で最も良い成績を収めました。

しかし、ミステリーブロックワールド問題になると、どのモデルも成績が大幅に下がり、5%の正答率にも達しませんでした。これは、問題の内容は同じでも、表現が変わるだけで対応できなくなることを示しています。

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_76177_2-1024x470.jpg)

o1-previewとo1-miniは驚くほど効果的だが、問題の長さが増すと性能が低下する。

LLMは通常、異なる表現間の翻訳が得意です。そのため、ミステリーブロックワールドの問題を普通のブロックワールド問題に翻訳する手がかりを与えれば、成績が大きく向上すると予想されました。しかし実際には、GPT-4でも正答率は10%にしか上がりませんでした。

興味深いことに、1つの例を示してから問題を解かせる「ワンショット学習」は、必ずしも成績を向上させませんでした。むしろ、多くのモデルでは成績が下がる傾向が見られました。

なお、これまでのベンチマークでは、LLMの処理時間や費用は考慮されていませんでした。しかし、新しい「大規模推論モデル」（LRM）が登場したことで、効率性の評価も重要になってきています。下の表では、様々なモデルの処理コストが比較されています。

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_76177_7-1024x178.png)

100インスタンスあたりのコスト（USD）を示す。LRMはLLMよりも著しく高価

## 近似的な情報検索から近似的な推論へ

### o1モデルとは

OpenAIは、新しく「大規模推論モデル」（LRM）としてo1モデルを開発しました。従来のLLMとは異なり、より高度な推論能力を持つことを目指しています。

o1の詳細な仕組みは明らかにされていませんが、研究者たちは以下のように推測しています。

1. 基本的なLLM（おそらくGPT-4の改良版）に、 [強化学習](https://ai-data-base.com/archives/26125 "強化学習") を用いた追加の学習を行っている。
2. 推論の過程で「思考の連鎖」を生成し、最適なものを選択する仕組みがある。
3. 推論に必要な計算量を問題に応じて調整できる。

### PlanBenchでのo1の性能

研究者たちはo1をPlanBenchで評価しました。その結果は以下の通りです。

1. 通常のブロックワールド問題では、97.8%の正答率（従来のLLMを大きく上回る）
2. ミステリーブロックワールド問題では、52.8%の正答率（LLMの5%未満と比べて大幅に向上）
3. さらに難しい「ランダム化ミステリーブロックワールド」問題では、37.3%の正答率

なお、従来のLLMの他に比較対象とされた「Fast Downward」とは、自動計画の分野で広く使われている古典的な計画システム（プランナー）です。2006年にMalte Helmertによって開発されました。標準的な計画問題記述言語であるPDDLで書かれた問題を解く、ヒューリスティック（近似値の発見手法）を使用するシステムです。

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_76177_3-1024x295.png)

より大きな問題（6〜20個のブロックが登場する）では、o1の性能は低下し、23.63%の正答率になりました。o1の能力はまだ完全には一般化されていないことを意味する結果です。

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_76177_5-1024x409.png)

通常のブロックワールドデータセットをより多くのステップを必要とする問題に拡張すると、o1-previewの性能が悪化する。

### 解決不可能な問題への対応

PlanBenchには、意図的に解決不可能な条件を設定した問題があります。例えば、あるブロックが同時に2つの異なるブロックの上にあることを要求するなど。

o1は、解決不可能な問題を識別する能力も示しましたが、完璧ではありませんでした。

通常のブロックワールド問題では、27%の正答率。ランダム化ミステリーブロックワールド問題では、16%の正答率でした。

また、解決可能な問題を誤って「不可能」と判断するケースも見られました。

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_76177_6.png)

### コストと効率性

なお、o1は高い性能を示す一方で、処理にかかるコストが非常に高いことが分かりました。例えば、この研究だけでo1の使用料が約1,897ドルに達しました。

![](https://ai-data-base.com/wp-content/uploads/2024/09/AIDB_76177_4-1024x448.png)

(左) o1-previewがミステリーブロックワールド問題を解く際に使用する平均推論トークン数は、最適な計画の長さと相関する (右) o1-previewが使用する平均推論トークン数は、問題を解くのに必要なステップ数に応じて予想されるほどには増加しない

### o1の特徴的な振る舞い

さらに研究者たちは、o1が間違った答えを出す際に、創造的だが無意味な理由付けをすることがあると指摘しています。「事実をねつ造する」ような振る舞いを示していると解釈できます。

## まとめ

本記事では、大規模言語・推論モデルの計画能力を評価したPlanBenchベンチマークの研究を紹介しました。

OpenAIのo1モデルは大幅な性能向上を示しましたが、複雑な問題や解決不可能な問題、コスト、正確性の保証などに課題が残ることが明らかになりました。

研究者らは、LRMの評価には精度、効率性、コスト、結果の保証など多角的な観点が必要だと結論付けています。

- 参照論文URL： [https://arxiv.org/abs/2409.13373](https://arxiv.org/abs/2409.13373)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMの「自己対話」により複雑な問題の解決能力を飛躍的に向上させる手法『Iteration of Thought』](https://ai-data-base.com/archives/76134)

[RAG-LLMシステムへのユーザークエリは4つのレベルに分類できる　最も複雑なのは「隠れた根拠からの推論が必要なクエリ」Microsoftによる研究](https://ai-data-base.com/archives/76241)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)