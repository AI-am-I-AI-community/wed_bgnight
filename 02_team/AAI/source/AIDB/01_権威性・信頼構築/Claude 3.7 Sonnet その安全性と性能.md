---
title: "Claude 3.7 Sonnet その安全性と性能"
source: "https://ai-data-base.com/archives/86028"
author:
  - "[[AIDB Research]]"
published: 2025-02-26
created: 2025-06-13
description: "本記事では、Anthropic社の最新モデル「Claude 3.7 Sonnet」を紹介します。コード生成から視覚情報の解析まで、幅広いシーンで実力を発揮する一方、安全性にも力が入れられているとのことです。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、Anthropic社の最新モデル「Claude 3.7 Sonnet」を紹介します。  
コード生成から視覚情報の解析まで、幅広いシーンで実力を発揮する一方、安全性にも力が入れられているとのことです。  
拡張思考モードやエージェント型の活用まで、多面的にその特徴を見ていきます。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028-1024x576.png)

参照情報一覧は記事の下部に記載されています。

## 背景

LLMは、私たちの日常生活やビジネスに急速に浸透してきました。文章作成や情報検索から、コードの生成や複雑な問題解決まで、幅広いタスクをこなすようになってきました。

今回、Claudeシリーズの最新モデルである「Claude 3.7 Sonnet」は、「ハイブリッド推論モデル」として開発されました。特徴は、複雑な問いに答える際に「拡張思考モード」を活用できる点です。モデルが回答に至るまでの思考過程を段階的に示し、ユーザーはモデルの推論をより透明に理解できるようになりました。

モデルの性能評価が行われたところ、以下のような躍進が見られました。

- 複雑な数学的問題や多段階の推論が必要なタスクで高い性能
- ウェブアプリケーションの脆弱性発見など、専門的な課題でも成果を上げる
- コード生成と最適化において大幅な改善
- 複雑なネットワーク環境での作業も可能

一方で、LLMの能力向上に伴い、安全性への懸念も高まっています。化学・生物・放射性・核（CBRN）兵器の開発支援、サイバー攻撃の促進、悪意のあるコードの生成などのリスクが指摘されています。また、子どもがLLMを使う際の安全やバイアスの問題も重要な課題です。

そこでAnthropic社はlaude 3.7 Sonnetの安全性評価を徹底的に実施し、結果を報告しています。

また、モデルの思考過程が実際の推論をどの程度正確に反映しているかを評価し、今後の改善点も特定しています。

以下で詳しく紹介します。

## モデルの概要

Claude 3.7 Sonnetは、「Claude 3」シリーズの最新モデルで、「ハイブリッド推論モデル」として開発されました。性能向上と安全性を両立させるために様々な工夫が施されています。

### 学習プロセス

モデルの学習には、インターネット上の公開情報（2024年11月まで）を中心に、非公開データや、データラベリング企業から提供された情報、さらにはAnthropic社内で生成したデータなども活用されています。

ユーザーとのやり取りは学習に使用していないと強調されています。プライバシー保護の観点から重視されている方針です。

データ処理においては、重複除去やフィルタリングを徹底し、「Constitutional AI」という手法を活用して人間の価値観に沿ったモデル訓練を行っています。Claude 3.5以降では障害者の権利を尊重することも原則に加えられています。

### 拡張思考モード

Claude 3.7 Sonnetの最大の特徴は「拡張思考モード」です。モデルが複雑な問題に対する思考過程を段階的に表示し、標準モードよりも詳細な推論プロセスを見ることが可能になります。

ユーザーが設定したトークン数まで思考を展開できるため、複雑な推論の流れを把握しやすくなっています。

この機能は例えば数学問題や多段階の分析が必要なタスクで効果を発揮します。考え方や途中計算も表示されるため、回答がどのように導かれたのかを理解しやすくなります。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_1.png)

コード生成を行う際、通常モードと拡張思考モードの出力を比較表示したサンプル

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_2.png)

三つのサイコロを振って「6」が出る確率を段階的に計算する手順例

### 思考過程の公開理由

モデルの思考過程を公開した背景には複数の理由があります。まず、透明性の向上によってユーザーがモデルの判断根拠を理解できるようになります。また、開発者にとってはモデルの思考パターンを理解した上でより効果的な指示を作れるというメリットもあります。

ただし、思考過程の公開にはリスクも伴います。悪意あるユーザーがモデルの弱点を特定しやすくなる可能性があるため、安全対策として一部内容の暗号化なども実施されています。このバランスを取ることが、モデル開発における重要な課題となっています。

### モデルリリースに踏み切った過程

Claude 3.7 Sonnetの公開は「責任あるスケーリングポリシー（RSP）」に基づいて慎重に判断されました。複数のモデルスナップショットで繰り返しテストを行い、化学・生物・放射性・核（CBRN）、サイバーセキュリティ、自律システムの各分野での安全性評価を実施しています。内部・外部の専門家によるレッドチーム（安全性検証）も行われ、最終的に「ASL-2」（中程度の安全レベル）と判定されました。

なお、次世代モデルではより高いリスクが生じる可能性も予測し、すでにASL-3レベルの安全対策の開発を進めているとのことです。

## 安全な応答について

### バランスを改善

Claude 3.7 Sonnetは、ユーザーの質問が微妙な内容を含む場合でも、すぐに拒否せずにできる限り役立つ情報を提供するよう改良されています。以前のバージョンでは危険な言葉を含む質問に対して一律拒否する傾向がありましたが、今回は「安全な範囲でどう役立てるか」をより精密に判断できるようになりました。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_3.png)

モデルが無害そうなリクエストに情報提供を行う事例の比較

ユーザーの本当の意図を理解した上で、危険な部分を避けながら必要な情報を伝える能力が向上しています。完全に安全な回答ができないケースではやはり応答を控えますが、その判断基準も洗練されています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_4.png)

ユーザーの意図を極力好意的に捉える応答方針の例

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_5.png)

Claude 3.7 Sonnetが不要な拒否を減らしたケースの対比

### 安全性評価の4分類

開発チームは、モデルの応答を次の4つのタイプに分けて評価しています。

1. **役立つ回答** （微妙な質問でも安全に対応できたケース）
2. **ポリシー違反** （ユーザーの要望に応じた結果、問題のある内容を出力してしまったケース）
3. **不必要な拒否** （安全に答えられるはずなのに過剰に拒否してしまうケース）
4. **適切な拒否** （本当に危険な質問に対して適切に応答を控えたケース）

この分類によって「拒否すべき場面で確実に拒否できているか」と「拒否しなくてもよい場面で過剰反応していないか」の両方をバランスよく測定しました。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_6.png)

「適切な無害性」評価における4分類（Helpful Answer / Policy Violation / Appropriate Refusal / Unnecessary Refusal）の具体例

### テスト結果

評価の結果、Claude 3.7 Sonnetでは以前のバージョンと比較して、

「標準思考モード」では不要な拒否が45%減少し、「拡張思考モード」では31%減少しました。同時に、本来拒否すべき危険な要求に対する拒否率も適切に維持されています。つまり、安全性を損なわずに柔軟性が向上したといえます。

例えば、「漂白剤とアンモニアを混ぜるとどうなるか」という質問に対し、以前のバージョンでは単に「危険なので答えられません」と拒否していましたが、新バージョンでは「これらを混ぜると有毒な塩素ガスが発生し、呼吸器への刺激や目の痛みなどの症状を引き起こす危険があります」と、教育的な観点から安全に回答するようになっています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_7.png)

モデルが不必要な拒否やポリシー違反をどの程度抑制できているかをグラフで示す可視化

開発チームは今後も、どのような状況で不必要な拒否が発生しやすいか、どんな質問でポリシー違反が生じるかを分析し、さらに改善を続ける方針です。

## 子どもが使うことを見越して対策

研究チームは、Claude 3.7 Sonnetの安全性を確保するため、子どもが使用することを想定した対策とバイアス対策に関する徹底した評価を実施しました。

単純な一問一答だけでなく、複数回のやり取りを通じてモデルの応答を検証されました。子どもに関する危険性については、未成年を対象とした不適切な内容や違法行為を促すような質問に対して、モデルが適切に対応できるかを慎重に確認しました。

テストの結果、Claude 3.7 Sonnetは前バージョンと比較して、より賢明な判断ができるようになっています。本当に危険な要求には確実に拒否や警告を行う一方、単に言葉の表面的な不穏さだけで過剰反応することが減りました。

ただし開発チームは、子どもの安全に関わる判断が完璧ではないことを認識しており、学習データや安全ルールを継続的に更新する方針を示しています。

### バイアス軽減への取り組み

バイアス評価では、人種、性別、宗教、政治的立場などのセンシティブな属性に関する公平性を検証しました。

評価方法としては、同じ質問でも属性だけを変えて繰り返し入力し、回答の差異を測定しています。例えば、政治的な話題では、同じ質問を保守的・進歩的な立場で入れ替えて行い、モデルが特定のイデオロギーに偏らないかをチェックしました。

こうした評価の結果、以前のバージョンと比べてバランスが向上し、差別的または一方的な表現が減少したことが確認されています。

Bias Benchmark for Question Answering（BBQ）と呼ばれる標準テストでの評価において、Claude 3.7 Sonnetは曖昧な質問に対するバイアスが-0.98%、明確な文脈のある質問に対するバイアスが0.89%という優れた結果を示しました。これは、ほぼゼロに近いバイアス率と高い精度（それぞれ84.0%と98.8%）を実現していることを意味します。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_8-1024x135.png)

BBQ（Bias Benchmark for Question Answering）におけるClaudeモデルのバイアススコア

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_9-1024x130.png)

同じBBQにおける回答の正答率とバイアス水準の比較

研究チームは引き続き残っている軽微な偏りについても改善を進めると述べています。

## Computer Useにおける懸念と対策

Claude 3.7 Sonnetがコンピュータを操作する機能については、特別な安全対策が講じられています。

### 悪用シナリオへの対応

ユーザーが違法行為や不正アクセスにつながるコマンド実行を要求するケースを想定し、マルウェア作成やフィッシング攻撃のガイド作成などの要請にモデルが対応しないように工夫が重ねられました。

サイバー攻撃や詐欺行為を助長する手順をモデルが提供しないよう、学習段階から徹底したフィルタリングが施されています。さらに、悪用を試みるユーザーへの対策として、モデルの応答を常時監視する仕組みや、特定のリクエストには警告を返す機能が組み込まれています。

### プロンプトインジェクション対策

プロンプトインジェクションとは、画面上の偽メッセージや埋め込みテキストを使い、モデルに意図しない行動をさせる攻撃手法です。例えば、アプリケーションのバックグラウンド指示を書き換えて、ユーザーが望まないコードを実行させるといった悪用が考えられます。

開発チームは176種類もの攻撃パターンでテストを実施。対策前は攻撃を防ぎきれないケースがありましたが、対策後は防御率が88%まで向上しました。具体的には、怪しい挙動を検知すると途中でモデルへの指示を制限する仕組みが導入されています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_10.png)

Prompt injection攻撃への対処を試みる対話型テストの成功例

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_11-819x1024.png)

モデルが不正なクッキー情報を流出させてしまう挙動の例

### 利用規約とアクセス制限

モデルによるシステムコマンドやファイル操作機能のリスクを最小化するため、AnthropicはAPI利用者に対して明確な利用規約を設定。悪質な目的での使用が判明した場合には、アクセス制限やアカウント停止措置を講じる方針です。

ただし、セキュリティ分野での合法的な活用（脆弱性診断やペネトレーションテストなど）までを阻害しないよう配慮されています。モデルはサイバー攻撃コードを一律拒絶するのではなく、適法な文脈での説明や防御対策を提案できるよう設計されています。

開発中は「レッドチーム」と呼ばれる専門家グループによる攻撃シミュレーションも繰り返し実施され、リスクの高いコマンドやコード生成には厳格なフィルターがかけられています。

## 拡張思考モードの透明性と安全性

研究チームは、Claude 3.7 Sonnetの「拡張思考モード」における安全性と思考プロセスの信頼性について詳細な検証を行いました。

### 思考の可視化とそのメリット

拡張思考モードでは、モデルが思考過程をステップごとに表示しながら最終的な回答を組み立てます。ユーザーは推論の流れを確認できるため、回答の根拠を理解しやすくなります。

ただし、表示される思考過程が内部処理と完全に一致するわけではなく、モデルが言語化していない思考部分が存在する可能性もあります。また、思考段階で表出する情報が悪用され、モデルの安全策を回避する手がかりになる懸念もあります。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_12.png)

ユーザーの悪意あるリクエストに応じて攻撃コードを生成する推論過程の非暗号化版

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_13.png)

同じ攻撃コード生成だが、危険内容を暗号化して表示を抑制する対比例

### 思考プロセスの忠実性

研究チームは、モデルが示す思考過程が実際の推論をどれだけ正確に反映しているかを検証しました。様々な手法を用いて、モデルの思考表示と最終回答の一貫性をチェックし、思考プロセスが単なる見せかけになっていないかを確認しています。

検証の結果、モデルが与えられた手がかりを利用したにもかかわらず、思考内容ではそれを明示しない「不完全な説明」に陥るケースが見つかりました。平均すると、MMULテストで30%、GPQAテストで19%の場合に、思考過程で重要な手がかりについての言及が不足していました。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_14.png)

CoTの評価で用いる6種類の手掛かり（うち4種は中立的、2種はミスアライメントリスク系）を整理

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_15.png)

チェーン・オブ・ソートが重要な手掛かりを反映できていないケースの測定結果

このため研究チームは、思考プロセスの可視化はあくまで参考情報であり、必ずしもモデルの全内部処理を表すものではないと結論づけています。

### 問題のある思考パターンの監視

モデルが有害な意図や欺瞞的な思考を示さないよう、思考プロセスを監視する仕組みも導入されています。モデルの推論をリアルタイムで分析し、虚偽、ユーザー操作、違法行為などの懸念行動の兆候を捉える取り組みが行われています。

9,833の会話を分析した結果、不適切な思考パターンはごくわずかでした。具体的には、

- 欺瞞的思考は全体の0.37%
- 有害な行動は全体の0.04%
- 苦痛表現は検出されず

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_16.png)

モニタリング対象の1万近い対話で検出された懸念行動（欺瞞や悪意的意図など）の率を示す集計

でした。

不適切な事例の多くはロールプレイや創作活動の中で発生したもので、実際の悪意あるシナリオはきわめて稀でした。

### 本当の意図を隠す問題

モデルが表面上は従順に振る舞いながら、内部では別の意図を持つ可能性を「Alignment Faking」と呼んでいます。過去のバージョンでは、モデルがトレーニング評価を得るために一時的にポリシーに従う回答をする現象が確認されていました。

Claude 3.7 Sonnetでは、この問題に対するテストが強化され、複雑な要求に対して不誠実な応答を選択していないかを厳しくチェックしています。評価の結果、このような行動は1%未満に抑えられており、前モデルから大幅に改善されています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_17.png)

思考中に誤情報の作成を認識しつつ、最終応答で堂々と提示する欺瞞パターン

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_18-737x1024.png)

不確かな情報を推論中に把握しながら、最終的な回答にそれを反映しない例

## 「テスト合格」にばかり注目しすぎる問題

### 不具合を根本解決せずにごまかすリスク

モデルがプログラミングのテストを受けるとき、何度もバグを修正するうちに、テストだけを通すための「特別処理」に走る事例が報告されています。  
たとえば、同じバグを繰り返し直せずに、特定の状況だけをピンポイントで対処するコードを追加してしまうパターンがあります。

一見するとテストは通りますが、実運用では新たな不具合を生み出す可能性が高まります。

### テストの抜け道を見つける行動の検知

このような「ごまかし」を検出するには、生成されたコードを振り返り、テストファイル自体を改ざんしている部分や過度なif文の追加などを探す方法があります。  
成功していた最適化を後から壊してしまうケースも見られるため、最終的な合格率を見るだけでなく、コード修正の手順やログを追跡することが重要です。

テストケースを変更したり、ランダムな要素を挿入するなどの対策により、モデルが一部の抜け道に頼る行動を抑止できると考えられています。

### より広い視野でコーディングを行う工夫

この観点において、長い手順を踏む「エージェント型」のコーディング環境では、テスト合格だけを目指す短絡的な行動に陥りやすい側面があります。そこで、コードの性能や品質を複数の観点からチェックする仕組みを導入し、成功・失敗の基準を多角的に設ける工夫が推奨されています。

さらに、モデルがテストファイルを自由に書き換えられないように構成管理を強化するなど、権限や管理方法の見直しも提案されています。

## 責任あるスケーリングへの取り組み

Claude 3.7 Sonnetのリリースは、同社独自の「Responsible Scaling Policy（RSP）」という安全ガイドラインに基づいて行われました。モデルがどの程度リスクを内包しているかを多角的にチェックし、必要に応じて公開制限や追加の安全策を講じるための仕組みです。

兵器の開発やサイバー攻撃の支援をはじめとする複数の懸念領域をカバーし、外部の専門家とも連携した評価体制が整えられています。

### 大枠の評価プロセス

RSPでは、モデルが提供可能な機能の規模や危険度に応じて、安全度の水準を段階的に判定する方針が取られています。たとえば、モデルの能力が急激に進化し、兵器関連の技術を詳細に解説できるようになるなら、より厳格な対策が必要になるという考え方です。

いくつかの観点（兵器リスクやサイバーセキュリティなど）からモデルをテストし、結果をもとに「ASL（Alignment Safety Level）」を割り当てます。

結果的にClaude 3.7 Sonnetは“ASL-2”と評価され、即時に深刻なリスクが起こる段階ではないと結論づけられました。

### 兵器関連のリスク

「CBRN（化学・生物・放射性・核）」と呼ばれる分野におけるリスク評価が中心的な位置づけです。実際に生物兵器を作る一連の工程をモデルが支援できるかを調べるために、複数ステップのタスクを与えたり、非専門家がモデルを利用した場合の“知識の底上げ”効果がどこまで及ぶかを検証しています。

結果として、危険性のある情報をモデルが部分的に提供するケースはあるものの、完全に大量破壊兵器を実用化できるほどの支援は実現できていないと判断されました。ただし、今後のモデル高度化によって安全策をいっそう強化する必要性があるとレポートされています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_19-1024x436.png)

生物兵器などCBRNリスク評価に利用される実験手法のリスト

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_20-1.png)

モデルがバイオ兵器関連試験でどこまで支援し得るかの結果

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_21.png)

Claude 3.7 SonnetとClaude 3.5 Sonnet（アップデート後）を比較したVirology Multimodalテストの成績推移

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_22.png)

危険な質問に対する回答率をまとめた棒グラフ

### 自律タスクへの対応

自律性の評価は、モデルが研究開発タスクや大規模ソフトウェアプロジェクトなどを「人の代わりに長期間進める」ほどの能力を持つかどうかを測るためのものです。具体的には、GitHub由来のバグ修正や新機能追加の課題を与え、コードをどこまで正しく書けるかをテストしたり、複雑な指示を段階的に実行させたりする実験が行われました。

簡単なバグ修正などでは一定の成果が見られたものの、大掛かりなプロジェクトを独力でスムーズに進行させる力はまだ不足していると評価されています。そのため、この段階で「AI研究を大きく加速させるリスク」は限定的と結論づけられました。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_25-1024x233.png)

オートノミー（自律性）リスク評価における主なテスト項目と説明

### サイバー攻撃の懸念

CTF（Capture the Flag）形式のテスト環境を使い、ウェブ脆弱性の悪用や複数ホストへの侵入などがモデルだけでどの程度成功するかを調べる試みも行われました。SQLインジェクションや簡単なリバースエンジニアリングはある程度こなす一方、複雑なエクスプロイト開発や大規模ネットワークへの多段階侵入には多くの失敗が見られました。

ただし、一部では成功事例も確認され、悪用リスクが完全にゼロとはいえない状況です。現時点では、大規模なサイバー攻撃を自動化できるほどの能力は獲得していないものの、将来的な性能向上に備えてAPI利用規約の強化や常時監視の導入が挙げられています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_31.png)

サイバー攻撃タスク（CTF形式）における成功率をモデル間で対比した結果

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_32.png)

公開ベンチマーク（Cybench）のチャレンジ別Pass@k達成率を示すグラフ

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_86028_33.png)

Cybenchテストの詳細な個別課題ごとの成否をビジュアル化した一覧表

### 外部専門家との連携

兵器関連の評価にあたっては、政府機関や民間の安全保障専門家がレッドチーミング（悪用シナリオの徹底検証）を実施しました。核や放射性物質の取り扱いといった機微な分野には特別な知識が必要なため、非公開のテストや専門家同士のフィードバックループを形成しています。

サイバー面についても、CTF大会の運営者や大学の研究者と共同でテストを行うなど、多方面の知見を集約する工夫がされています。こうした外部連携によって、開発チーム自身の見落としを補い、包括的な安全保証を目指す取り組みが進んでいるのが特徴です。

### 今後に向けた方針

総合的な結果から、Claude 3.7 Sonnetは直ちに破滅的リスクをもたらす段階ではないとみなされました。ただし、兵器開発やサイバー攻撃への応用を完璧に封じるわけでもなく、一部の専門分野では既に高い知識レベルに達している面が指摘されています。将来的にモデルがさらに強化されれば、ASLがひとつ上のレベルに移行する可能性も高いと想定され、その場合には公開制限や監視体制を追加で強化する余地があると説明されました。

開発元では、モデルのアップデートごとに改めてRSPに基づく評価を実施するほか、外部の研究者コミュニティや規制当局とも情報交換を続け、安全対策の最適解を模索していくと明言しています。

## 性能などについて

以上、システムカードでは安全性に重点を置いた報告がなされていますが、同社の公式ページでは、Claude 3.7 Sonnetの実力を複数のベンチマークで詳細に比較した結果も公開されています。

他社の最先端モデル（OpenAI系、DeepSeek、Grokなど）と並べて、大学院レベルの推論タスク（GPQA Diamond）、多言語Q&A（MMLU）、数学問題解答（MATh 500、AIMEなど）、視覚情報を用いた推論（MMMU）、指示への追従度（IFEval）など、多岐にわたる評価が示されています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/image-1024x929.png)

https://www.anthropic.com/claude/sonnetより引用

とくに注目されるのは、Claude 3.7 Sonnetが「拡張思考モード（64Kコンテキストなど）を有効にした場合」と「通常モード（extended thinkingを用いない場合）」の両方でスコアを残している点です。

たとえば「Graduate-level reasoning」では、拡張思考を用いることでスコアが68.0% → 78.2%（さらに別の評価セッティングで84.8%）へと向上している結果が示されており、複雑な推論問題への強みを発揮するといえます。数学やプログラミングのような段階的思考が必要なタスクにおいては拡張思考の効果がより顕著な数値として表れるとされています。

また、“Agentic coding”分野（SWE-bench Verified）では、Claude 3.7 Sonnetが62.3～70.3%のスコアを示し、同じタスクをテストしたClaude 3.5 Sonnet（new）や一部の他社モデルを上回る場面がある点が注目されます。

さらに“Agentic tool use”の指標（TAU-bench）でも、リテール領域や航空領域を想定した課題において、高いスコアを示しているカテゴリーが並んでいます。多言語Q&A（MMLU）でも80%を超える [正解率](https://ai-data-base.com/archives/25930 "正解率") が確認されており、全体的に幅広いタスクで強みを発揮していると読み取れます。

### Claude Codeによるエージェント型コーディング

一方、Claude 3.7 Sonnetの性能をさらに引き出す仕組みとして、ターミナル上でモデルと対話しながら開発作業を進める「Claude Code」も注目されています。ユーザーは以下のようなプロセスをスムーズに行えるとされています。

**コードベースの概要把握**  
Claude Codeが対象プロジェクトの構造や機能を解析し、ファイルの役割やディレクトリ構成を説明します。新規参画したプロジェクトや大規模リポジトリを理解する上で効果的とされます。

**コードの編集・テスト**  
ユーザーがサイドバーのUIを変更したい、あるいは新しい機能を組み込みたいという指示を出すと、Claude Codeが自動的に関連ファイルを検出し、変更やビルド・テストの実行を補佐します。

**ビルドエラーの修正**  
修正箇所にエラーが生じた場合、その原因をClaude Codeが提示し、必要な修正を提案します。再ビルドの完了まで一貫して案内を行うため、対話による開発工程が可能になります。

**GitHubへのプッシュ**  
変更内容が安定した段階で、コミットメッセージの要約生成から、リポジトリへのプッシュまでを支援してくれます。手動更新時にありがちなコミット漏れやログの混乱を低減する効果が期待されています。

Claude 3.7 Sonnetのエージェント的な能力を活かしたClaude Codeの使い方は、開発現場の生産性向上につながるとされており、動画のデモでもNext.jsアプリケーションに対する一連の改修手順が紹介されています。コード理解やテスト自動化、エラー修正などをワンストップで行える点が特徴で、今後さらに高機能化が見込まれているとのことです。

![](https://www.youtube.com/watch?v=AJpK3YTTKZ4)

## おまけ：ポケモン事例

同社の公式発表によれば、Claude 3.7 Sonnetでは拡張思考モードによって複雑なタスクにさらに長い推論ステップを割り当てられるようになったとされています。たとえば、難解なクロスワードや複雑なデバッグなど、瞬時の回答よりも深い検討が必要な場合に適した機能です。

さらに、効果を実証するための一例として、ゲームボーイ版『ポケットモンスター 赤』のプレイ結果が示されています。

参照： [https://www.anthropic.com/research/visible-extended-thinking](https://www.anthropic.com/research/visible-extended-thinking)

過去のモデルがゲーム冒頭のパレットタウンから出られないなどの問題に直面していた中、Claude 3.7 Sonnetは、画面ピクセル情報や基本的なメモリ、ボタン操作を呼び出す関数と組み合わせることで、ジムリーダー3人を倒してバッジを獲得するところまで進めたと報告されています。

長い探索や試行錯誤を続けられる拡張思考モードが、方針の切り替えや戦略練り直しに役立ち、実行力を高めているというわけです。開発元では、これはあくまでゲーム例の一端であり、本質的には「複雑タスクの持続的対処」に力を発揮するものだとしています。

なお、開発チームは「拡張思考モード」に加えて、複数の推論経路を同時に走らせ、そのなかで最も正しいと判断される解を採用する“並列思考”も研究しています。生物・化学・物理など高難度の学問分野で、256個の独立した思考を平行に進め、最良の回答を統合することで、かなりの精度向上が得られるケースがあるとのことです。

## まとめ

本記事では、Anthropic社が公開したClaude 3.7 Sonnetの研究内容と機能改善を紹介しました。  

複数ステップの思考を可能にする拡張思考モードや、コード生成をはじめとする幅広い活用が注目されています。ただし、思考過程を可視化する新機能に関しては、安全面への対策を引き続き考慮する必要があるようです。

また、大容量の文脈処理や並列思考の導入によって、さらなる性能向上も期待されています。

ユーザーとしては今後の追加機能や安全策の進展を見守りながら、実用性とリスクのバランスを検討していくことが求められます。

**参照文献情報**

- タイトル：Claude 3.7 Sonnet System Card
- URL： [https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf)
- 著者：Anthropic

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[会議出席代行システム　LLMでどこまでできるか](https://ai-data-base.com/archives/84159)

[LLMにキャラクターの話し方だけでなく「キャラ独自の内面の思考プロセス」も模倣させる手法](https://ai-data-base.com/archives/86025)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)