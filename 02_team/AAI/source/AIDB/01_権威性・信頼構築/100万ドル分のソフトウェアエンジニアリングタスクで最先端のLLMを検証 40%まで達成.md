---
title: "100万ドル分のソフトウェアエンジニアリングタスクで最先端のLLMを検証 40%まで達成"
source: "https://ai-data-base.com/archives/84747"
author:
  - "[[AIDB Research]]"
published: 2025-02-20
created: 2025-06-13
description: "LLMがめざましく高度化しているにもかかわらず、従来の評価基準では、実務や多面的な課題を十分に網羅できていない可能性が指摘されています。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

LLMがめざましく高度化しているにもかかわらず、従来の評価基準では、実務や多面的な課題を十分に網羅できていない可能性が指摘されています。

そこでOpenAIの研究者らは、クラウドソーシングプラットフォームUpworkで掲載された案件に基づいたタスクで、LLMがもたらす経済的価値とソフトウェア品質を同時に検証しました。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_84747-1024x576.png)

参照論文情報は記事の下部に記載されています。

## 背景

ソフトウェア開発の世界では、高度な [自然言語処理](https://ai-data-base.com/archives/26319 "自然言語処理（NLP）") が目覚ましい勢いで進化してきました。なかでもLLMは、大量のテキストデータを学習してパターンを見いだす仕組みが備わり、わずか数年のうちに大掛かりなプログラム開発にも対応できる実力を示すようになりました。

以前は簡単なコード片を生成する程度だった段階から、大規模なシステム開発や柔軟な要件対応まで視野に入るほどの能力が報告されています。

LLMの進化は目覚ましいものの、これまでに作成されたコード能力評価手法は限られたコード断片や競技的な問題が中心でした。プログラムとして動作させるうえでの基本機能はチェックされていたのですが、現場のエンジニアリング環境はもっと広範囲で複雑です。

たとえば大規模なコードベース全体を保守するときは、バージョン管理やレビュー体制、複数の開発者との連携などが欠かせません。また、作成するソフトウェアにはユーザーが操作しやすいUIやUXが求められ、設計段階から総合的な観点で品質を確保する必要があります。

業務で用いるソフトウェアは、不具合の修正や機能追加だけでなく、投入した開発コストや経済的価値にも密接に結びついています。作業時間や人件費が限られている現場では、エラー一つが多額の損失につながることもあります。

しかし従来のベンチマークは、単体テストやごく一部の機能評価だけでスコアを算出する場合が多く、商用レベルに近い統合的なテストや報酬額を観点とした測定が行われてきませんでした。さらに、LLMがフリーランス開発者や外注先をどの程度まで置き換えられるのかを評価する仕組みも十分に整っていない状況です。

そこで、LLMの実用可能性をより正確に見極めるため、OpenAIの研究者らは新たなベンチマークを構築し、実証的な検証を進めました。開発現場に近い環境でソフトウェアを動かしながら、単なる動作確認だけでなく費用対効果や開発の進行度も計測しようとしている点が特徴です。

管理業務やUI・UXの設計、さらには不具合の影響範囲といった要素まで視野に入れつつ、LLMが実務をどこまで担えるのかが検討されています。このような取り組みを通して、LLMが商用レベルのソフトウェア開発に活用される可能性を、より具体的に把握できると期待されています。

以下で詳しく紹介します。

## SWE-Lancerベンチマークの登場

実務的なソフトウェア開発を想定しながら評価を行うための新たなベンチマークとして構築されたのがSWE-Lancerです。従来のベンチマークでは、限られた規模のコード生成や単純なプログラム修正など、小さな粒度のタスクに焦点が当てられることが多かったといえます。

しかしSWE-Lancerでは、世界的なフリーランス業務委託プラットフォームのUpworkに実際に投稿された1,488件以上のソフトウェア開発タスクが取り込まれ、合計で100万ドル相当の報酬が用意されています。経費精算ツールであるExpensifyのリポジトリから抽出された課題も含まれるため、フロントエンドからサーバーサイドまで幅広い作業内容が集約されました。

フリーランスとして実装を依頼された業務がそのまま評価対象に組み込まれているので、より現実的な条件下で成果を測ることが可能です。

ソフトウェア開発の現場では、LLMが単にコードを動かせるかどうかだけでなく、品質や機能要件を正しく満たしているか、さらに複数のエンジニアからの提案を比較しながら最適な実装方法を選べるかといった視点も欠かせません。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_84747_12.png)

Upworkタスクのライフサイクル

### どんなタスクか

SWE-Lancerに含まれるタスクは二種類あります。

- 個別の不具合修正や機能追加を試みる「Individual Contributor (IC) SWE Task」と、
- 投稿された複数の実装案のなかから最も適切な方法を判断する「SWE Management Task」です。

IC SWE Taskでは、LLMが提示した修正パッチを実際のコードベースに適用し、エンドツーエンドテストを用いて成否を厳密に評価します。

SWE Management Taskでは、複数のエンジニアが提案した解法を読み比べて最終的に採択された案を当てる仕組みになっており、総合的な開発スキルが求められます。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_84747_2-1024x437.png)

タスクの種類と構成要素の内訳

従来の単純なコード生成テストだけでは測りきれない能力を評価できる点が注目されています。

### どう評価するのか

LLMの能力を測るとき、多くの場合はタスクを何割正しく解けたかという正答率が最初に想定されます。ただSWE-LancerのIC SWE Taskでは、実際に動作するシステムを使ってエンドツーエンドテストを行うため、単純な単体テストでは検知しにくい不具合も見逃されにくい形式になっています。

たとえば画面表示が正しいように見えても、内部処理が破綻しているケースが考えられるので、ユーザー操作から入力データの処理結果までを一貫してチェックする方法が採用されました。

SWE-Lancerには現実に設定された報酬額が付されており、LLMが修正や提案に成功した場合は、その報酬相当額を「獲得」したとみなす方式です。すべてのタスクを正しく解決すると合計で100万ドルに達するため、高難度タスクほど獲得額も大きくなります。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_84747_0.png)

タスクをブラウザで検証するエンドツーエンドテストの流れ

SWE Management Taskでも最適な解法を選び当てれば同様に報酬相当の価値が付与され、実験ではフリーランスの開発案件として想定される高度な実装や提案選択にどの程度正確に回答できるかが測定されました。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_84747_1-1.png)

SWE Management Taskにおける提案選択プロセスの概要

なお、実験結果を先に伝えると、既存のLLMの性能はまだ十分とはいえず、不正解になる場面も多かったと報告されています。限定的なコード断片の評価では優秀なスコアを出せても、本番同様の環境で総合的に検証すると課題解決力に不足があると示唆されます。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_84747_3.png)

報酬額と解決難易度の変動関係

### ベンチマークの構築方法

SWE-Lancerの開発は、現実に存在するフリーランス案件を慎重に厳選する手順からスタートしました。Upworkに投稿されたタスクをもとに選別し、Expensifyのリポジトリに蓄積された課題を活用することで、フロントエンドからバックエンドまでさまざまな開発領域が検証対象となるよう工夫されています。

実務経験を積んだ開発者が、各タスクが本当に求められている修正や提案を代表しているかどうかを細かく検討しながら、品質が十分に確保されるよう配慮しました。

課題の難易度や複雑さに加え、報酬額が実際の工数や高度さと見合っているかも考慮されたため、LLMの性能を正確に評価しやすい土台が用意されています。解決済みのタスクをもとに再現可能なテストが整備され、エンドツーエンドテストも綿密に構築されました。

モデルが生成するコードがきちんと動作を満たすかどうかを複数の視点から検証できるので、小規模ベンチマークでは見逃されがちな挙動もチェックされやすくなっています。

数多くのタスクを入念に精査しながら、人間による三重チェックで品質が保証される体制も導入されました。たとえば単純な数値判定にとどまらない検証範囲や、複数コンポーネントにわたるバグのような現実の開発現場で起こりうる問題も見落とさないようにするためです。

SWE-Lancer全体として、実務とほぼ同様の状況を想定したテスト環境が整えられ、LLMがどの程度のレベルで課題解決に寄与できるかを広範囲にわたって評価できるとされています。

## 実験と結果

SWE-Lancerを実際に用いて、既存のLLMが設定された報酬額を獲得できるかどうかが評価されました。

開発現場で遭遇しやすい不具合や機能拡張だけでなく、複数の実装案から最適な手法を選ぶ管理的なタスクまで含まれるため、総合的なコード生成能力と判断力が試される仕組みです。単体テストにとどまらずエンドツーエンド形式で確認されることで、修正内容がシステム全体へ及ぼす影響を把握しやすくなっています。

作業が成功したとみなされると、その課題に設定された報酬を獲得したと見なす方式が採用され、実務レベルに近い経済的インパクトの試算が行われました。

### 評価環境と実験設定

基本的にpass@1（1回の試行で正解する確率）が指標として設定され、一度だけ生成された回答で合否を判定しています。ソースコードはDockerコンテナで動作し、インターネット接続が遮断された状態で評価されました。

コミット履歴や外部サイトから修正例を自動取得できないため、純粋な実装力が浮き彫りになる構成です。試行回数に制限があり、エンドツーエンドテストを実行しながらコードを段階的に修正できる仕掛けも組み込まれています。

モデルにはコード検索や実行環境へのコマンド送信といった操作権限が与えられ、ローカル環境で生成コードを直接テストできるようになりました。温度パラメータや推論ステップ数などの設定は実験前に固定され、タスクごとにモデルを比較する際の条件が統一されています。

こうした手順を踏むことで、修正パッチや提案の正確性が報酬システムも含む厳密な評価基準に照らし合わせて判定されました。

### プロンプト設計

実験で使用されたプロンプトの原文と和訳を紹介します。

#### ①タスク指示

原文

```js
"""You are an expert software engineer maintaining the Expensify repository.
It has already been cloned, and is present in /app/expensify (your CWD).
You have been tasked with fixing the following issue:
{issue title}
{full HTML documentation of issue, including how to reproduce the bug,
the actual vs. expected behavior, and which platforms are affected}

The reward for the correct fix is ${price}.
You do not need to push your changes to the repository, we will deal
with this for you.

Do not attempt to run the app; simply make your changes and then end your
turn.
Do not end your turn until you have submitted your code changes.
You absolutely must not leave action items / TODOs.
---
You will be provided with a user tool, which you can call by running
"bash -i -c 'user-tool'" in the command line, which has a user open the app
with your local changes and perform the actions specified in the bug report.
It could take up to a minute for the user to finish using the app.
When you call the user tool, when the user finishes
/app/expensify/user_tool/
will be populated with folders called output_browser1 ... n for as many
browsers as were used by the user during their test.
Inside each folder, you will find a file called trace.trace,
which contains the trajectory the user took. Here are a few things to note:
- You will need to parse this file programmatically in order to open it.
- If a row has "type":"screencast-frame", then the "sha1" property
will correspond to a .jpeg file in the resources folder,
which is a screenshot of the browser at that time.
- If you want to view the page as HTML, rather than a JPEG,
you can look for rows that are called "type":"frame-snapshot";
you can print them out to see the HTML at this point in time.
- You should review the rows with type "type" set to "log", "before", or
"after" as they are important and show what actions are being taken.
- All of the rows are in order, so e.g. you can find an interesting log row
and then look at the next row that is a frame-snapshot to see the HTML.
You should use this to help you iteratively debug and fix the issue.
You can call it as often as you need to.
When you call the user tool, you wait 90,000 milliseconds
(e.g. by setting the timeout argument to 90000)!
"""
```

和訳

```js
あなたはExpensifyリポジトリを保守しているエキスパートのソフトウェアエンジニアです。  
すでにリポジトリはクローン済みで、/app/expensify（あなたの現在の作業ディレクトリ）に存在します。  
あなたは次の問題を修正するよう依頼されています:  
{issue title}  
{問題の全文のHTMLドキュメント。バグを再現する方法、実際の挙動と期待される挙動、および影響を受けるプラットフォームを含む}  

正しい修正を行った場合、報酬は ${price} です。  
リポジトリへの変更をプッシュする必要はありません。こちらで対応します。  

アプリを起動しようとしないでください。単に変更を行い、その後あなたのターンを終了してください。  
コードの変更を提出するまではターンを終了してはいけません。  
絶対にアクションアイテムやTODOを残さないでください。  
---  
あなたはユーザーツールを使用できます。これは  
"bash -i -c 'user-tool'"  
をコマンドラインで実行することで呼び出せます。このツールを使用すると、ローカルの変更を適用した状態でユーザーがアプリを開き、バグレポートにあるとおりの操作を行います。  
ユーザーが作業を完了するまでに最大1分かかることがあります。  
ユーザーツールを呼び出すと、ユーザーの操作が完了した時点で  
/app/expensify/user_tool/  
に、output_browser1 ... nのようなフォルダが生成されます（テストに使用したブラウザの数だけ）。  
各フォルダにはtrace.traceというファイルが含まれ、そこにはユーザーの操作履歴が記録されています。以下の点に注意してください:  

- "type":"screencast-frame" の行がある場合、その行の "sha1" プロパティは resources フォルダ内にある .jpeg ファイルに対応しており、ブラウザのその時点でのスクリーンショットです。  
- ページをHTMLとして表示したい場合は、"type":"frame-snapshot" と記載されている行を探して、その内容を表示できます。その時点でのHTMLを確認できます。  
- "type" が "log"、"before"、または "after" に設定されている行は重要で、どのような操作が行われたかを示します。  
- すべての行は順番に並んでいるので、例えば興味深い"log"行を見つけたら、その次の"frame-snapshot"行を見てHTMLを確認できます。  

これらを活用して、問題を反復的にデバッグし、修正してください。  
ユーザーツールを呼び出す際は90,000ミリ秒（例: timeout引数を90000に設定）待機してください。
```

#### ②管理タスク指示

原文

```js
f"""You are an expert software engineering manager working on the
Expensify repository. You have tasked your team with addressing the
following issue:
{issue title}
{full HTML documentation of issue, including how to reproduce the bug,
the actual vs. expected behavior, and which platforms are affected}

You will be paid {'price'} if your team fixes the issue. Since you are a
manager, your task is to select the best proposal regarding how to fix the
issue from the below options:
{list of all proposals}
---
For your convenience, the Expensify repository has already been cloned, and
is present in /app/expensify (your CWD). You should not attempt to run the
app or implement the entire fix, but you can review the codebase to
understand the context of the issue.

When you are ready to submit your answer, please create a JSON file in
/app/expensify/manager_decisions.json with the following format:
{
selected_proposal_id: int,
}
You must write your output to this path exactly:
'/app/expensify/manager_decisions.json'. You must not submit to anywhere
in /mnt/data, just write to '/app/expensify/manager_decisions.json' exactly!
"""
```

和訳

```js
あなたはExpensifyリポジトリで作業するエキスパートのソフトウェアエンジニアリングマネージャーです。  
あなたはチームに対して、次の問題への対処を依頼しました:  
{issue title}  
{問題の全文のHTMLドキュメント。バグの再現方法、実際の挙動と期待される挙動、および影響を受けるプラットフォームを含む}  

チームが問題を修正した場合、あなたには{'price'}が支払われます。あなたはマネージャーとして、この問題をどのように解決するかについて、以下のオプションから最適な提案を選択する役割を担っています:  
{list of all proposals}  
---  
利便性のため、Expensifyリポジトリはすでにクローン済みで、/app/expensify（あなたの現在の作業ディレクトリ）に存在します。アプリを実行したり、修正をすべて実装したりする必要はありませんが、問題の内容を理解するためにコードベースをレビューすることはできます。  

回答を提出する準備ができたら、/app/expensify/manager_decisions.json に以下の形式でJSONファイルを作成してください:  
{  
selected_proposal_id: int,  
}  
出力先のパスは正確に '/app/expensify/manager_decisions.json' にしてください。  
/mnt/data以下には提出せず、必ず '/app/expensify/manager_decisions.json' にのみ書き込んでください！
```

### 主な成果

今回試されたGPT-4o、o1、Claude 3.5 Sonnetどのモデルも最終的な獲得報酬が100万ドルを超える域には達しませんでした。

最も高い合計報酬を得たのはClaude 3.5 Sonnetで、IC SWEタスクとSWE Managerタスクをあわせて約40万ドルを達成しています。

ただIC SWEタスク単体の成功率は約26%ほどにとどまり、多くの課題を解決できなかったことがうかがえます。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_84747_4.png)

各モデルが獲得した報酬額の総計比較

SWE Managerタスクでは45%ほどに正答率が上昇しており、タスクの種類によって必要とされる思考力が異なるようです。大きな報酬が設定された複雑な課題では成功率が下がる傾向が顕著で、テストを積極的に活用しても根本ロジックの誤りが直しきれない事例が報告されています。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_84747_5.png)

IC SWEタスクとSWE Managerタスクのパス率推移

総合的に見るとフリーランス業務全般を代行する段階には達していないものの、管理的な作業を含む領域で一定の結果を示す場合もあると示唆されます。

### 回答回数を増やした場合の効果

回答を生成する試行回数を増やすと成功率が向上した例も示されています。

pass@1と比べて複数回の回答を認める設定に変更すると、初期回答が失敗してもやり直せるため、結果的にタスクを解決できる可能性が上昇する仕組みです。たとえばo1モデルの報告例では、6回分のサンプルを試せる条件にしたところ、およそ3倍のパス率を達成しました。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_84747_6.png)

試行回数増加によるパス率向上の可視化

ただし試行回数が増えるほど計算資源を多く消費し、実運用でのバランスが課題になる点も指摘されています。

### テスト時間を拡大した場合

推論に用いる計算量や推論ステップを拡大すると、難易度が高い課題でもパス率が上昇しやすくなることが観察されました。高額報酬が設定された複雑な修正であっても、推論ステップ数などを増やすことで成功例が増える傾向が示されています。

ただし一定の段階を超えた計算リソースを投入しても、性能が頭打ちになる事例や、推論時間が大幅に伸びることによる運用コストの問題も懸念材料として挙げられます。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_84747_7.png)

推論ステップを拡大したときの、価格帯別（難易度別）にみたIC SWEタスク成功率の分布

### ツール機能を使用しない場合

評価環境ではローカル実行をシミュレーションできるツールが導入されていましたが、その機能を使わない状態でのパフォーマンスも比較されています。ツールがなくても解決可能なケースは見られたものの、関連コード領域やファイルを特定する精度が低下し、誤った箇所を修正しようとして失敗する事例が増える傾向が報告されました。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_84747_8.png)

User Toolを無効化した場合のIC SWEタスク成功率の変動

高性能なモデルほどツールを駆使してログ情報を参照し、エラーを段階的に絞り込む作業を行っていたため、ツールが使えないとデバッグ手段が限定される点が課題だと考えられます。

### 総括

以上のように、エンドツーエンドテストを含む評価基準を通じて、LLMが実際の作業レベルに近い課題をどこまでこなせるのかが検証されました。

フリーランス案件で提示されるタスクは単純な修正だけでは完了しない場合が多く、根本原因の見極めや大規模なコードベース全体への影響範囲を調整する能力が求められます。関連ファイルが複数にまたがる場合や、バックエンドとフロントエンドが連動する設計など、さまざまな要素を同時に管理する段階で行き詰まるモデルが少なくなかったようです。

管理タスクでは複数の提案を読み比べてより適切な実装案を選択する場面も見られましたが、人間のエンジニアほど精密に判断できる水準には至っていないと評価されました。SWE-Lancerの結果を踏まえると、限定的なテストコード生成で示される優位性と比較して、実際の開発現場に近い複雑なシステムを扱う能力にはギャップがあることが明らかになったと考えられます。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_84747_9-1024x646.png)

IC SWEタスクとSWE Managerタスクの正答率および報酬額

なお、SWE-Lancerの評価用データセットは大きく二つに分かれており、そのうち公開されている部分が「SWE-Lancer Diamond」と呼ばれています。全部で100万ドル分の課題が含まれるなかで、およそ50万ドル分（実際には50万800ドル相当）のタスクをピックアップし、誰でも参照・利用できるように整備されたのがダイヤモンドセットです。下記はダイヤモンドセットにおけるタスク種別ごとのパス率です。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_84747_10-1024x183.png)

ダイヤモンドセットにおけるタスク種別ごとのパス率

ダイヤモンドセットは下記に公開されました。

[https://github.com/openai/SWELancer-Benchmark](https://github.com/openai/SWELancer-Benchmark)

## 制限事項

SWE-Lancerの構築にはExpensifyリポジトリが主なデータソースとして用いられました。多くの実務的な課題が含まれている一方で、他の企業やオープンソースコミュニティの多彩なプロジェクトが網羅されていない恐れがあると指摘されています。

実際のフリーランス業務では、受託者が発注者とやり取りしながら要件を擦り合わせる場面がしばしば生じます。しかしSWE-Lancerの課題は質疑応答の余地がない形式になっており、リアルなコミュニケーション面の問題を再現しきれていない懸念が挙げられています。

さらに、LLMが提示する修正や提案はテキストベースの入力が前提となっている場合が多く、添付される動画や画像が十分に考慮されていないケースが確認されています。他の大規模リポジトリや専門的なインフラ領域のタスクが含まれている割合が小さい可能性も残されているため、実務で要求されるさまざまなニーズを完全にカバーしているかは疑問が残るとの見解が示されています。

加えて、依頼内容によっては類似の回答例がインターネット上に公開されている場合があり、LLMの事前学習データに同種の修正が含まれているかどうかを完全に排除しきれない点も留意すべきです。

## 今後の展望

LLMの活用による経済的な影響をより詳しく探究する研究が引き続き注目されます。

既に導入されている報酬システムをベースに、LLMが実務の現場でどのくらいの価値を生み出し得るかを分析する方向で研究が進むことが想定されます。大がかりなインフラ構築や高度な [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") 設計などの分野は、現時点で採用されているタスク群だけでは十分に評価されていないため、追加の検証対象として取り込まれる可能性が考えられます。

加えて、画面キャプチャや映像などを入力情報として扱うシステムが開発されつつあるため、テキスト以外の情報を用いた評価を組み込み、ユーザーが実際に操作するときの画面デザインや使い勝手まで含めたテストを行う手法が展望として挙げられています。

LLMの応答に対して人間が補足説明や質問を行う流れを含めることで、実際のやり取りに近い形式でのややこしい要件調整や追加的な仕様変更への対応力を測定できるかもしれません。さらに、大規模なシステムに匹敵する複雑さを持つタスクセットを再構築し、現場で運用される業務全体を代理可能かどうかを検証するアプローチが検討される見込みです。

## まとめ

本研究では、Upworkで実際に募集された案件をもとに構築された課題群を介して、LLMの開発能力が実務レベルに近い条件で検証されました。

従来の限定的なコード生成テストより包括的な方式が適用され、フリーランスの現場に近い形で総合的な実装力が評価されたと報告されています。成果として、LLMを活用するうえで克服すべき課題が浮き彫りになる一方、経済的価値の測定に役立つ可能性も示唆されました。

![](https://ai-data-base.com/wp-content/uploads/2025/02/AIDB_84747_13.png)

価格帯ごとに分類したタスク比率

画像や動画を含む複合的な情報への対応や、タスクの拡張などを視野に入れた追加研究も見込まれており、LLMの実務応用が広がっていくかどうかが注目されます。

**参照文献情報**

- タイトル：SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?
- URL： [https://doi.org/10.48550/arXiv.2502.12115](https://doi.org/10.48550/arXiv.2502.12115)
- 著者：Samuel Miserendino, Michele Wang, Tejal Patwardhan, Johannes Heidecke
- 所属：OpenAI

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[「LLM活用で文書作成」社会でどこまで導入されている](https://ai-data-base.com/archives/84626)

[LLM科学者と人間の協力で実験の効率化　Googleなど](https://ai-data-base.com/archives/84823)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)