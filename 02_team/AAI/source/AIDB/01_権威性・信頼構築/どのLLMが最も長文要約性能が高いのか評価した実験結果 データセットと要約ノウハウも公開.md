---
title: "どのLLMが最も長文要約性能が高いのか評価した実験結果 データセットと要約ノウハウも公開"
source: "https://ai-data-base.com/archives/58765"
author:
  - "[[AIDB Research]]"
published: 2024-04-15
created: 2025-06-13
description: "LLMは現在、技術的には10万トークン（言語モデルが処理する単位）以上の長文を要約できます。しかし、要約における信頼性などを評価することは難しいと考えられています。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

LLMは現在、技術的には10万トークン（言語モデルが処理する単位）以上の長文を要約できます。しかし、要約における信頼性などを評価することは難しいと考えられています。

そこで今回Adobeなどの研究者らは、LLMが生成した「架空の長編小説」の要約に対し、人間による大規模な評価を初めて実施しました。

![](https://ai-data-base.com/wp-content/uploads/2024/04/AIDB_58765-1024x576.jpg)

**参照論文情報**

- タイトル：FABLES: Evaluating faithfulness and content selection in book-length summarization
- 著者：Yekyung Kim, Yapei Chang, Marzena Karpinska, Aparna Garimella, Varun Manjunatha, Kyle Lo, Tanya Goyal, Mohit Iyyer
- 所属：UMass Amherst, Adobe, Allen Institute for AI, Princeton

**本記事の関連研究** ：

- [LLMが生成した長いテキストにおける「事実性」を自動で評価するLLMエージェントフレームワーク『SAFE』Google DeepMindが開発](https://ai-data-base.com/archives/66502)
- [GoogleのGeminiファミリー最新モデル「Gemini 1.5 Pro」1000万トークンでほぼ完璧な検索性能](https://ai-data-base.com/archives/65862)
- [Claude 3のベンチマーク評価結果　論文（テクニカルレポート）より](https://ai-data-base.com/archives/65693)
- [GPT-4やGemini Pro1.0などさまざまなLLMで、プロンプトの入力が長くなるにつれて推論性能に顕著な低下が見られる](https://ai-data-base.com/archives/64873)

## 背景

要約の品質には大きく分けて2つの評価指標があります。ひとつは「原文の内容を正確に反映しているか」、もうひとつは「重要な内容を適切に含んでいるか」です。

しかし、これまでのLLMによる要約の評価は、一貫性（要約内容が矛盾なく筋道が通っているか）に焦点が当てられてきました。一貫性は自動的に評価できるものであり、逆に信頼性などは人間を雇って評価する必要がある（＝多大な費用と時間がかかる）ためです。

今回、複数機関から結成された研究グループは、長編小説の要約における「信頼性」と「内容の重要性（原文の主要なエッセンスを反映している度合い）」に関する人間による大規模な評価を行うことにしました。

実はLLMによる要約は、短編小説、詩、脚本など、物語の分野で研究が進んできました。その中で、LLMが長い文脈を一貫性を持って要約することができることなどが確認されてきました。しかし、それ以上の詳細な評価は今回が初めてだと言います。

![](https://ai-data-base.com/wp-content/uploads/2024/04/AIDB_58765_1-1024x336.png)

小説要約の信頼性 アノテーション を収集するためのパイプライン。要約生成、主張の抽出、主張の人手評価の3ステップからなる。

## 長文要約データセット『FABLES』

研究者らは、クラウドソーシングプラットフォームの [Upwork](https://www.upwork.com/) を通じて、2023年または2024年に出版された本を1冊以上読んだことがある者を募集しました。最終的に、14人が集まり、彼らが挙げた26冊の本の電子版を研究者らが購入しました。このデータセット（FABLESと名付けられました）の本の平均長は12万1千単語でした。

![](https://ai-data-base.com/wp-content/uploads/2024/04/AIDB_58765_2-1024x253.png)

書籍とFABLES アノテーション におけるトークン数の統計。

### LLMに本の要約を生成させるプロンプト

研究者らは今回、「階層的マージ戦略」を採用して、長編文書を要約しました。モデルとしては、GPT-3.5-TURBO、GPT-4、GPT-4-TURBO、MIXTRAL、CLAUDE-3-OPUSなどが使用されました。

階層的マージ戦略、文書を複数のチャンクに分割し、各チャンクを独立に要約した後、それらを再帰的にマージする手法です。要約の一貫性と連続性を保ちながら、長い文脈を扱うことができるとされています。

**手順**

1. 文書をチャンクに分割する。
2. 各チャンクを独立に要約する。
3. 隣接するチャンク要約のペアを形成する。
4. 各ペアを要約し、より大きな要約を生成する。
5. ステップ3と4を再帰的に繰り返し、最終的な要約を生成する。

以下は、階層的マージ戦略の各ステップで使用できる日本語のプロンプトテンプレートの例です。

チャンクの要約：

```js
以下のテキストを要約してください。
[チャンクのテキスト]
要約:
```

チャンク要約のマージ：

```js
以下の2つの要約を、1つの長い要約にマージしてください。

要約1:
[チャンク要約1]

要約2:
[チャンク要約2]

マージされた要約では、以下の点に注意してください:
- 重要なイベント、背景、設定、登場人物、目的、動機を含める。
- 主要な要素を紹介し、時系列に沿って要約を構成する。
- 2つの要約の内容を滑らかに統合し、一貫性と連続性を保つ。
- 文章は簡潔かつ読みやすくする。

マージされた要約:
```

### 要約を小さな文に分解する

研究者らは細かい注釈を可能にするため、要約を小さな文（文節）に分解する作業を行いました。GPT-4に2つの主要な指示をプロンプトします。

1. 各小さな文は、要約からの追加の文脈なしで完全に理解できるものでなければならない（「それ」や「彼」などの表現を使用しない）。
2. 各文は、できるだけ物語全体の文脈に適切に位置づけられるべきである。

2に関して補足します。文脈で位置づけられるとはすなわち、ある出来事が、物語のどの時点で、どの場所で、何と関連して起こったのかが明確であるべきだということです。

![](https://ai-data-base.com/wp-content/uploads/2024/04/AIDB_58765_3-1024x663.jpg)

そして評価の担当者には、2つの主要なタスクが与えられました。

**タスク1：割り当てられた本の要約から抽出された文の信頼性を評価する**

分解された各文について、(a)忠実、(b)忠実でない、(c)部分的に支持される、(d)検証できない、の4つから選択し、信頼性を判断します。また、本からの引用を含む自由形式のテキストで、判断した理由を述べます。

****タスク2：** 要約の全体的な品質について、自由形式のコメントを提供する**

要約全体を批評し、重要な情報の省略、不正確さ、重要でない要素への過度な言及、その他の問題点を指摘します。

## 研究結果の概要

研究者らは、FABLESデータセットにおける3,158件の文レベルの信頼性の分析結果をまとめました。

全体的に、CLAUDE-3-OPUSが最も信頼できる要約ツールLLMであり、90％が信頼できると評価されました。次いでGPT-4とGPT-4-TURBOが78％、GPT-3.5-TURBOが72％、MIXTRALが70％と続きました。

![](https://ai-data-base.com/wp-content/uploads/2024/04/AIDB_58765_4-1024x181.png)

### 信頼できない文の分析

研究者らは、信頼できないと評価された205件の文をさらに詳しく分析しました。分析には、「文の種類」と「推論の種類」という2つの観点を用いました。

その結果、信頼できない文の大部分は、具体的な出来事（31.5％）や登場人物・関係の状態（38.6％）に関するものだったことがわかりました。また、信頼できない文の多く（50.2％）は、その文が信頼できないと判断するのに複数の推論ステップが必要だったのです。

### 文の種類

信頼できないと評価された文は、以下の5つの種類に分類されました。

- 状態（38.6％）：登場人物や関係の状態について述べた文。
- 出来事（31.5％）：具体的な出来事について述べた文。
- 原因/結果（11.2％）：出来事、行動、思考の原因や結果について述べた文。
- 高次（11.2％）：物語全体の設定や特徴など、抽象度の高い内容について述べた文。
- 内省（7.5％）：登場人物の思考、感情、意見について述べた文。

### 推論の種類

信頼できない文を判断するのに必要な推論のタイプは、以下の4つに分類されました。

- 間接的（50.2％）：文が信頼できないと判断するのに、複数の推論ステップが必要。
- 直接的（36.8％）：文が信頼できないと判断するのに、1つの推論ステップのみが必要。
- 主観的（7.2％）：文が信頼できないと判断するのに、主観的な解釈が必要。
- 追加情報（5.7％）：文が信頼できないと判断するのに、追加の情報が必要。

![](https://ai-data-base.com/wp-content/uploads/2024/04/AIDB_58765_5-1024x757.jpg)

FABLESにおける信頼性エラーの分類。文の種類と推論の種類の2軸で分類。各カテゴリの頻度と例を示す。

### 人手による信頼性評価の課題

研究者らは、小説の要約文の信頼性を人手で評価するのは、大規模に行うのが難しいと指摘しています。FABLESの評価では、1つの要約につき40米ドル、合計で5,200米ドルものコストがかかりました。これでは、要約システムの開発中や大規模なデータセットでの使用には費用がかかりすぎてしまいます。

### 自動評価システムの開発

そこで研究者らは、LLMを使った自動評価システムを開発しました。この自動評価システムは、小説本文から得られた根拠を使って、要約から抽出された個々の文の信頼性を判定します。根拠としては以下の4種類を検討しました。

1. なし：小説本文からの証拠を一切使わずに信頼性を評価。
2. 人間の根拠：FABLESデータセットから得られた、人手で収集された根拠。
3. BM25検索：要約文をクエリとして、BM25アルゴリズムを使って小説本文から関連する文を検索。
4. 本文全体：明示的な検索は行わず、小説本文全体をLLMに入力して文脈として使用。

![](https://ai-data-base.com/wp-content/uploads/2024/04/AIDB_58765_6-1024x562.jpg)

CLAUDE-3-OPUSとGPT-4-TURBOによる主張のラベル予測のエラー例。人手のラベルと理由付けを併記。

### 実験用のデータセット

実験には、7冊の小説（各12万5千単語以下）から抽出された723の要約文を使用しました。このうち69文は人手の評価で「不忠実」とされ、654文は「忠実」とされました。

![](https://ai-data-base.com/wp-content/uploads/2024/04/AIDB_58765_7-1024x443.png)

評価者による証拠収集の課題に関するコメントの例。

### 実験結果

実験の結果、最も性能が良かったのは、小説本文全体を文脈としてCLAUDE-3-OPUSに入力する設定でした。これは、同じ設定のGPT-4-TURBOやBM25よりも有意に優れていました。しかし、「不忠実」な文の分類精度は47.5%（ [F1スコア](https://ai-data-base.com/archives/26112 "F1スコア（F値）") ）にとどまり、自動評価システムとしては不十分であることがわかりました。

![](https://ai-data-base.com/wp-content/uploads/2024/04/AIDB_58765_8-1024x224.png)

7冊の小説に対する自動評価モデルのFaithfulとUnfaithfulのF1スコア。要約元のモデルと評価モデルの組み合わせごとに結果を示す。

![](https://ai-data-base.com/wp-content/uploads/2024/04/AIDB_58765_9-1024x360.png)

評価者のコメントに基づいて、各モデルの要約で指摘された具体的な問題の割合を示す。上段（紫色）は批判的なカテゴリ、下段（緑色）はモデルが称賛されたカテゴリを表す。

### 考察

LLMが生成したテキストの信頼性を評価することの重要性は広く認識されていますが、これまで研究は主に個々の事実の検証に注目してきました。しかし、今回の分析により、こうした手法では小説要約のようなより複雑なタスクにおけるLLMの誤りを十分にカバーできないことが示唆されました。また、従来の評価手法の中核をなす検索・検証のアプローチでは、この複雑なタスクにはまったく対応できないことがわかりました。

![](https://ai-data-base.com/wp-content/uploads/2024/04/AIDB_58765_10-1024x511.png)

評価者によって指摘された5種類の省略エラー（登場人物、出来事、属性、関係性、テーマ）の割合を、モデルごとに示す。

## まとめ

本記事では、長編小説の要約における信頼性と内容選択の評価に関する研究を紹介しました。

研究者らは、LLMが生成した要約の信頼性と内容選択エラーを人手で評価したFABLESデータセットを構築し、CLAUDE-3-OPUSが最も信頼できるツールであることを明らかにしました。また、LLMを用いた自動評価の限界も示唆されました。

さらに、信頼性以外にも、重要な情報の省略や本の終盤の過度な強調など、内容選択エラーの特徴を分類しました。

研究者らは、FABLESデータセットを公開しており、要約の評価研究を促進したいとしています。

- URL： [https://arxiv.org/abs/2404.01261](https://arxiv.org/abs/2404.01261)
- GitHub： [https://github.com/mungg/FABLES](https://github.com/mungg/FABLES)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[Claude 3などのLLMはコンテキスト内学習によって線形回帰・非線形回帰問題タスクもこなす](https://ai-data-base.com/archives/67496)

[ChatGPTは学術論文の文章スタイルをどう変えているか？大規模な調査の結果](https://ai-data-base.com/archives/67681)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)