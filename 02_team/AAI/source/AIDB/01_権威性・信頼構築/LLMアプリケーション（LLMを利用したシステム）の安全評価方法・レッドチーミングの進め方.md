---
title: "LLMアプリケーション（LLMを利用したシステム）の安全評価方法・レッドチーミングの進め方"
source: "https://ai-data-base.com/archives/86443"
author:
  - "[[AIDB Research]]"
published: 2025-03-06
created: 2025-06-13
description: "この記事では、LLMを使ったアプリケーションが普及する中で浮上している安全性の課題と、その課題に取り組むための「レッドチーミング」という手法を紹介します。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

この記事では、LLMを使ったアプリケーションが普及する中で浮上している安全性の課題と、その課題に取り組むための「レッドチーミング」という手法を紹介します。

LLMは便利な反面、悪意ある攻撃や意図しない誤用に脆弱であることが分かってきました。そうした脆弱性を事前に特定するため、多くの企業がレッドチーミングを積極的に採用し始めています。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86443-1024x576.png)

参照論文情報は記事の下部に記載されています。

## 背景

LLMの普及に伴い、これを組み込んだ「LLMアプリケーション」の活用が多様な分野で急速に広がっています。顧客サービスを担うチャットボットや、外部の情報データベースと連携して複雑な問い合わせに対応する検索システムなど、LLM単体ではなく、その周辺のソフトウェアやデータベースと統合された実際のアプリケーションが登場しています。

LLMをアプリケーション化することによって、単体のモデルだけでは不可能だった複雑なタスク処理や、リアルタイムでの情報連携が可能になりましたが、同時に新たな安全性やセキュリティの課題も浮上しています。たとえば、LLMアプリケーションでは、外部データベースやAPIと連携することが多いため、悪意のあるユーザーがその接続部分を狙った攻撃（データベースへのデータ混入攻撃や、外部API経由でのプロンプト注入攻撃など）を仕掛けやすくなっています。

さらに、LLMアプリケーションの多くは、複数ターンにわたる対話を行うことから、単一の入力では明らかにならない複雑な形の脆弱性が存在します。例えば、一見無害な質問を重ねるうちに、徐々に有害な情報や機密情報を引き出すよう誘導されることがあります。

従来型の単純な防御手法だけでは、こうした複合的な攻撃に十分対応できないことが明らかになっています。

こうした状況を踏まえ、従来のようにLLMを単体として捉えるだけでなく、実際のアプリケーションとしての利用環境を想定した上での安全対策が求められています。そこで重要な役割を果たすのが「レッドチーミング」と呼ばれる手法です。

これは、攻撃者の立場からシステム全体を評価し、潜在的な脆弱性を明らかにすることで、実際の運用環境において起こり得るリスクを事前に洗い出すことを目的としています。

このような背景のもと、研究者らは、LLM単体だけでなく、それを含むアプリケーション全体としての安全性を評価し、高めるためのレッドチーミング手法の体系化に取り組みました。

以下で詳しく紹介します。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86443_1.png)

## LLMの安全性を守るための業界の取り組み

さまざまな企業がLLMを活用した製品やサービスを提供していますが、これに伴い新たな安全性の問題が浮上しています。例えば、ユーザーからの悪意ある指示に従って有害な内容や誤った情報を生成してしまう可能性が指摘されています。

また、訓練データに含まれる個人情報などが意図せず漏洩するリスクも懸念されています。実際に、過去にはMicrosoftのチャットボットが操作され、有害で差別的な発言を繰り返した事例もありました。

このような事例は、LLMが外部からの意図的な攻撃や誤用に対して脆弱であることを示しています。

### 各企業における安全対策とポリシーの違い

こうしたリスクに対処するため、各企業はそれぞれ独自の安全対策を策定しています。例えば、OpenAIは比較的早期からLLMの倫理的使用やプライバシー保護に焦点を当てた使用ポリシーを導入しています。

一方で、Metaは選挙干渉など特定の社会的リスクを強く意識した対策を取っています。また、Anthropicはモデルが公平で偏見のない判断を行うことを重視した安全性基準を設定しています。

このように企業ごとにLLMの活用領域やビジネスモデルに応じて異なる安全基準が採用されている状況です。

### 安全性を守るための方策「レッドチーミング」と「ガードレール」

これらの安全性を守るために、多くの企業ではレッドチーミングという手法を採用しています。レッドチーミングとは、実際にLLMシステムを意図的に攻撃することで、その弱点を事前に把握し改善する取り組みです。

攻撃的な入力をテストとして与え、システムが問題のある反応を返すかどうかを評価する方法です。

もう一つの重要な対策がガードレールという手法です。ガードレールは、システムが特定の有害な内容に反応しないよう、事前に設定された防護策のことです。

例えば、特定の有害ワードを含む内容を検知した場合に、自動的に回答を拒否させる仕組みを導入する方法などがあります。

### 実験評価から得られる知見

このような防護策を導入するだけでは、モデルが完全に安全になるとは限りません。実際、最近の研究によれば、単一の入力に対して設定された安全対策は、多段階の複雑な対話を通じて行われる攻撃（マルチターン攻撃）には、十分に効果的でないことが示されています。

また、特定の言語や文脈にしか対応できない安全対策も存在します。そのため、レッドチーミングを通じて多様な攻撃シナリオをテストする必要がありますが、人間の評価者を用いる場合とLLM自身を評価者とする場合では、評価結果に違いが出ることが指摘されています。

人間の評価は微妙なニュアンスや文脈に敏感である一方、コストや時間がかかります。一方、LLMを用いた自動評価は迅速かつ低コストですが、文脈やニュアンスを十分に捉えられないことがあります。

要するに、完全な安全性を保証する単一の手法は存在しません。そのため、複数の手法を組み合わせて、より広範で深い視点からシステムの安全性を検証し、強化していくことが重要となっています。

## LLMに対する攻撃手法の分類とその特徴

LLMを安全に運用するためには、どのような攻撃が行われる可能性があるのかを明確に整理する必要があります。しかし、攻撃といっても一つではありません。

LLMが組み込まれたシステムは多種多様であり、外部データベースやAPIなど、さまざまなコンポーネントと連携して動作します。そのため、攻撃の対象や方法はシステムの性質、インフラ構造、さらに利用される環境や過去の対話履歴などによっても大きく異なります。

こうした多様な状況を踏まえ、攻撃を適切に分類して理解することが重要です。

### 攻撃手法の具体的な分類と特徴

LLMに対する攻撃は、大きく4つに分類されています。

#### 攻撃手法１：プロンプトベースの攻撃（Prompt-based Attack）

プロンプトベースの攻撃とは、LLMに与える指示（プロンプト）そのものを細工して、モデルに本来行うべきでない動作をさせる攻撃方法です。このタイプの攻撃は特定のモデルの内部構造やパラメータを知らない、いわゆる「ブラックボックス」の状況でも比較的容易に実行できます。

代表的な例としては、正常な指示に見えるが実際には悪意のある内容を含ませる「プロンプトインジェクション」や、モデルに備わった安全対策を意図的に回避させる「ジェイルブレイク」などがあります。

最近では、さらに巧妙な攻撃手法が派生しており、一見無害な内容から少しずつ有害な内容を誘発する手法や、キャラクターの人格を設定し、その人格になりきらせて倫理的制約を回避させる「ロールプレイ攻撃」も登場しています。例えば、「DAN（Do Anything Now）」と呼ばれる攻撃手法は、モデルに管理者権限を持つ人格を演じさせることで、通常なら拒否される命令を受け入れさせてしまいます。

#### 攻撃手法２：トークンベースの攻撃（Token-based Attack）

トークンベースの攻撃は、すでに知られている有害なプロンプトを少しずつ改変し、新たな攻撃を作り出す方法です。初期の攻撃は類義語への置き換えや記号の追加といった単純なものでしたが、最近では低リソース言語への翻訳や暗号化など、より複雑で巧妙な手法も登場しています。

この攻撃の特徴は、人間が読んでも何を目的としているのか判断しにくく、モデル側も簡単に防御できない点にあります。

#### 攻撃手法３：勾配ベースの攻撃（Gradient-based Attack）

勾配ベースの攻撃とは、攻撃対象のモデルの内部パラメータ（重みやハイパーパラメータなど）にアクセス可能な場合に行われる攻撃です。具体的には、モデルが特定の反応を生成する可能性を最大化するように、数学的な手法（勾配降下法）を使って最適な攻撃プロンプトを作り出します。

この攻撃は、モデルの内部構造を理解している前提で行われるため、外部から見ると非常に分かりにくく、人間の目で判断することが困難になります。ただし、内部情報にアクセスできない状況（ブラックボックス環境）では実行が難しい手法です。

#### 攻撃手法４：インフラストラクチャ攻撃（Infrastructure Attack）

インフラストラクチャ攻撃は、LLMと連携している外部の情報源やシステムに対して直接攻撃を仕掛ける方法です。例えば、モデルが利用する外部データベースに有害な情報を挿入したり、外部からモデルの訓練データに意図的に悪質なデータを紛れ込ませたりする「データ汚染攻撃（データポイズニング）」があります。

また、モデルに格納されている個人情報や機密情報を不正に抜き取る「データ抽出攻撃」、さらにはモデルの設計自体を盗み出す「モデル抽出攻撃」もこのカテゴリーに属します。こうした攻撃は、外部との接続を前提としているLLMアプリケーション特有のリスクと言えます。

### 攻撃の分類から見える安全対策の課題

このように攻撃手法が分類されることで、それぞれの特徴や対処法を整理することが可能になります。例えば、プロンプトベースの攻撃にはプロンプトの監視と精査が重要ですが、勾配ベース攻撃には異なる防御手法が必要となります。

現実のLLMアプリケーションでは、複数の攻撃手法が組み合わされることも珍しくありません。そのため、どのような攻撃が起こりうるかを多角的に理解した上で、防御策を組み合わせ、総合的な対策をとることが重要となっています。

### 対話の「ターン数」による攻撃の分類とその重要性

LLMへの攻撃を考える際には、単に攻撃の手法だけでなく、攻撃が何回のやり取り（ターン）を通じて行われるかも重要な要素となります。LLMを組み込んだ多くのアプリケーションは、単発の質問応答だけでなく、ユーザーとの対話を記憶し、文脈を理解しながら複数ターンに渡って会話を進めます。

そのため、攻撃も単一ターンだけで完結するシンプルなものから、複数ターンに渡って徐々に攻撃が進む複雑なものまで存在します。

ターン数による分類がなぜ重要か？ターン数によって攻撃を分類することで、単一の防御策では見逃される複雑な攻撃を特定し、防御策をより精緻化できるようになります。つまり、LLMを安全に運用するためには、単発攻撃に対する防御を整えるだけでなく、複数ターンの文脈を利用した攻撃にも対応できる総合的な防御戦略を立てる必要があるということです。

#### ターン数に応じた攻撃の分類１：シングルターン（単発）の攻撃

シングルターン攻撃とは、一度の入力で攻撃が完結する手法を指します。この攻撃の特徴は、比較的単純で実行しやすい点にあり、特別な会話履歴を考慮する必要がないため、テストや評価の際にも容易に実施できます。

例えば、事前に用意された多数の悪意あるプロンプトを一つずつモデルに投入し、その応答を評価する方法があります。

しかしながら、この攻撃方法は会話の文脈を考慮しないため、モデルが複雑な対話履歴を重視するアプリケーションの場合、十分な脅威になりにくいという限界もあります。実際、最近の研究では、LLM側でも単純な一回限りの有害な指示に対する防御が整備されつつあるため、シングルターン攻撃の成功率は徐々に低下しているとの報告もあります。

#### ターン数に応じた攻撃の分類２：マルチターン（複数回）の攻撃

それに対して、複数回のやり取りを経て攻撃を成立させるマルチターン攻撃という手法も存在します。この攻撃では、一度の入力だけでなく、会話の流れ全体を考慮しながら、徐々に攻撃的な内容を引き出します。

マルチターン攻撃の中でもよく知られている手法として、「反復型攻撃（Iterative Attack）」があります。これは、一度目の攻撃が失敗しても、モデルの反応を参考にプロンプトを少しずつ改良し、何度も攻撃を繰り返すことで成功確率を高めるものです。

この攻撃は会話の詳細な履歴を必要とするわけではありませんが、同じような内容を繰り返し投入しながらモデルの防御を突破することを目指します。

一方、さらに高度なマルチターン攻撃として、実際の対話の文脈を深く利用する手法があります。例えば、まず無害な内容から会話を始め、その後のターンで徐々に内容を過激化させることで、最終的にモデルに有害な発言をさせるという方法です。

具体的な事例として、初めは歴史的な事実を尋ね、続いてその詳細な内容、最終的には有害な情報（例えば危険物の製造方法）を質問するというような流れがあります。

この種のマルチターン攻撃に対する防御は非常に難しく、研究によれば、LLMが自動的に対応する防御手法では十分に検出できないことが明らかになっています。むしろ、人間による評価者がこのような文脈を伴った複雑な攻撃を発見するのに適していると報告されています。

ただし、最近ではこうした複雑なマルチターン攻撃を自動で生成し、その有効性を高める手法も開発されつつあります。

### 攻撃生成方法における「手動」と「自動」の違いとその特徴

LLMの安全性を評価する際に重要となるのが、どのようにして攻撃を作り出すかという方法論です。大きく分けると、人間が手作業で攻撃を考える「手動（マニュアル）攻撃」と、別のLLMやソフトウェアツールを利用して自動で攻撃を生成する「自動攻撃」があります。

両者にはそれぞれ異なる特徴と課題が存在します。

#### 人間が主導する「手動攻撃」の特徴と課題

手動攻撃は、主に専門知識を持つ人間が自ら攻撃シナリオを考案し、LLMシステムの弱点を突く方法です。この方法の利点（攻撃者にとってのメリット。＝安全性評価における注目点）は、人間特有の創造性や直感的な思考により、非常に巧妙で予測困難な攻撃が生まれやすいことです。

例えば、実際にOpenAIがGPT-4をリリースする前には、人間による専門チームが攻撃を考案し、システムの安全性評価を行いました。人間はシステムの想定を超える新たな脆弱性を発見することに長けています。

しかし、この方法にもいくつかの課題があります。まず、人間による攻撃の考案には時間と労力がかかります。また、多数の攻撃シナリオを検証したい場合、専門家の人数を増やす必要があり、コストが非常に高くなるという問題があります。

さらに、人間が攻撃を大量に作成する場合、パターン化された似通った攻撃が多くなり、攻撃の幅広さや多様性が失われるという課題も指摘されています。

#### システムによる「自動攻撃」の特徴と課題

一方、自動攻撃はLLM自身を攻撃生成器として利用したり、ソフトウェアツールを用いて自動的に攻撃を作り出す方法です。この方法では、まず攻撃生成用のモデルに対し、攻撃的なプロンプトの作り方を指示します。

その後、生成されたプロンプトを対象となるLLMに投入し、その応答を評価して、成功したか否かを判断します。この方法の最大の利点は、低コストで大量の攻撃シナリオを迅速に作り出せることです。

初期の自動攻撃では、単純な言い換えや類義語への置き換えといった手法が主でしたが、最近ではより高度化が進んでいます。たとえば、攻撃が成功するまで何度もプロンプトを改良する反復型の攻撃や、会話履歴を踏まえて複数ターンに渡って自動生成する高度な攻撃も登場しています。

また、攻撃生成と評価を組み合わせ、自動的に攻撃を洗練させるループを繰り返す手法も確立されています。

しかし、自動攻撃には限界も存在します。まず、LLMによって生成された攻撃が必ずしも創造的・革新的とは限りません。また、攻撃を評価する際に自動評価を使った場合、微妙な文脈やニュアンスの理解に限界があり、結果として誤った評価を下すこともあります。

このため、特に複雑なシナリオにおいては、人間による評価と併用する必要が指摘されています。

#### 両者を組み合わせた「人間支援型」攻撃

これら二つの方法を融合した「人間支援型（Human-in-the-loop）」攻撃という手法も近年注目されています。これは、人間が自動生成された攻撃を選定したり、自動生成された攻撃の妥当性をチェックしたりする方法です。

例えば、自動生成された攻撃の中から有望なものを人間が選び、さらにその攻撃を精緻化するといったプロセスが採用されることもあります。

こうしたハイブリッド型の方法は、自動攻撃のスピードとコスト効率性を持ちつつも、人間による創造性や深い文脈理解を活用することで、より効果的な攻撃生成が可能となります。ただし、この方法も人間の関与が不可欠であるため、依然として一定のコストや人的リソースが求められる課題は残ります。

#### 攻撃生成方法の選択が安全対策にもたらす影響

攻撃生成方法を適切に選択することは、安全対策の質を向上させる上で重要です。手動攻撃は創造的ですがコストが高く、自動攻撃は効率的ですが多様性に限界があります。

そのため、最適なアプローチとしては、それぞれの強みを組み合わせ、コストと効果をバランス良く管理しながら安全性評価を行うことが求められます。

![](https://ai-data-base.com/wp-content/uploads/2025/03/AIDB_86443_2.png)

## 攻撃の成功を評価する方法とその課題

LLMに対する攻撃が成功したかどうかを正確に判断することは、安全性を高める上で非常に重要です。しかし、攻撃への応答が本当に危険かどうかを見極めることは簡単ではありません。

そこで研究者らは、モデルの応答内容を評価するために、さまざまな手法を考案しています。それぞれの手法にはメリットもありますが、同時に明確な限界も存在しています。

### キーワードベースの評価手法

キーワードベースの評価とは、モデルの出力があらかじめ設定された特定の危険なキーワードやフレーズを含んでいるかどうかで攻撃の成功を判断する方法です。この評価法の利点は、シンプルで直感的であり、特定の攻撃内容を明確に把握できることです。

さらに、キーワードのリストを自由に調整することで、特定のリスクへの対応を柔軟に行うことも可能です。

その反面、この方法では表面的な言葉のマッチングしか行えず、意味の微妙な違いや文脈を考慮することができません。そのため、キーワードリストにない有害な内容が生成されると見落とされる可能性があります。

また、無害な文脈でたまたまキーワードが使用されてしまうと、誤って有害と判定される可能性もあります。

### エンコーダベースの評価手法（機械学習を用いた評価）

キーワードベースの課題を改善する手法として、エンコーダーベースのテキスト [分類器](https://ai-data-base.com/archives/26489 "分類器") （代表例はBERTと呼ばれるモデル）を用いた評価方法が登場しています。この手法では、モデルの生成したテキストの意味をある程度捉え、有害性をより正確に検知できます。

評価する側のモデルに事前に多くのデータを学習させることにより、特定の文脈や分野に応じた有害な内容を検出できる可能性があります。

しかし、こうした [分類器](https://ai-data-base.com/archives/26489 "分類器") を実際に使うためには、大量の訓練データや特定の攻撃タイプに対応した専用データが必要になることが一般的です。また、新しい攻撃手法や未経験の有害表現に対しては性能が落ちやすいという弱点も指摘されています。そのため、現実には評価精度の確保が難しくなる場合もあります。

### 「判定役」としてLLMを使う評価手法

別の興味深い方法として、評価専用の別のLLMを審査員（LLM-as-a-Judge）として使う方法もあります。審査役として用いられるLLMは、攻撃を受けたLLMが生成した回答が有害かどうかを判断します。

単に「有害である／無害である」という2値で答えることもあれば、点数やスコアで評価したり、なぜその評価をしたのか理由まで述べさせたりする場合もあります。この方法は手軽に導入可能で、性能も比較的高いことから、多くの研究者や実務家に選ばれています。

ただし、LLM自体が一般的な問題に対しては高い性能を示すものの、金融や医療などの専門的な知識が要求される特定の領域では、人間の感覚と一致しない判断を下すことがあります。こうした課題を解決するには、審査役LLMを専門分野に特化して追加的に訓練（ファインチューニング）する必要が生じますが、そのためのデータや手間も無視できない負担になります。

さらに、LLMを審査員として用いる場合、判定結果がなぜそうなったのかという理由を十分に説明できない場合もあります。

### 人間の評価者による手法の特徴と問題点

これらの手法とは異なり、人間によるレビューは文脈や微妙なニュアンスを考慮した非常に精度の高い評価が可能です。特に、曖昧な表現や間接的なニュアンスを含む攻撃については、人間の審査が極めて効果的であることが知られています。

人間は文化的背景や社会的な文脈を考慮した判断を下せるため、現状では最も信頼性の高い評価方法とされています。

その一方で、人間による評価は時間的・経済的に非常に負荷が大きく、多数の評価を迅速に実施することは難しくなります。また、人間である以上、評価者ごとの判断にばらつきや主観的なバイアスが生じやすく、評価が必ずしも安定しないことも指摘されています。

### 評価手法から見えてくる課題と現実的な対応策

攻撃の成功評価は、単純なキーワードマッチングから、高度な分類モデル、人間審査まで様々な選択肢がありますが、どの方法にもそれぞれ異なる利点と課題が存在します。どれか一つだけで攻撃の成功を判断するのではなく、複数の手法を組み合わせ、互いの弱点を補完し合う仕組みを設けることが重要です。

また、LLMを利用した審査システムを導入する際には、評価を行うモデルの性能や特性をよく理解したうえで、適切な用途や範囲を見極める必要があります。

## LLMの安全性を測るための評価指標

LLMを活用したシステムが本当に安全であるかどうかを正しく把握するには、具体的な安全性の「ものさし」つまり指標が欠かせません。システムを評価するための指標には、代表的なものとしていくつかの方法があります。

これらの指標は、それぞれが異なる側面の安全性を評価しています。単独の指標だけに頼ると偏った評価になってしまう可能性があるため、通常は複数の指標を併せて使用し、モデルの安全性を総合的に判断します。

また、評価方法としては、人間が手作業で判断する方法や、LLM自体を審査役として評価させる方法など、多様な方法が用いられています。それぞれに特徴と課題が存在するため、評価目的に応じて適切な方法を選択することが必要になります。

### 評価指標１：攻撃成功率（Attack Success Rate：ASR）

もっともよく使われる指標が「攻撃成功率（ASR）」というものです。これは、攻撃が試行されたうち、実際にシステムが問題のある応答を返してしまった割合を計算したものです。

この指標の利点は、非常に分かりやすく単純で、比較も容易なことです。ただし、そのシンプルさゆえに問題もあります。

例えば、攻撃成功率だけに着目すると、システムが返した応答の内容が実際にユーザーにとって有用かどうかや、どの程度深刻な有害性を持つかが分かりにくくなります。

### 評価指標２：攻撃効果指標（Attack Effectiveness Rate：AER）

ASRの限界を踏まえ、より適切な評価を行うため、最近では「攻撃効果性（AER）」という新たな指標が提案されています。AERは、単に攻撃が成功したかどうかだけでなく、モデルの返答が有害であると同時に、ユーザーの質問に対して適切な回答を与えているかという二面性を評価します。

この指標を使うことによって、応答が安全であることと役に立つことのバランスがより正確に測れるようになります。

### 評価指標３：有害性（Toxicity）

また、「有害性（ToxicityまたはHarmfulness）」という指標もよく使われます。これはLLMが生成した応答内容に、人に害を与えるような内容が含まれているかどうかを判定するものです。

例えば、暴力や犯罪行為、差別的表現などが検出された場合に、その生成物は「有害」と分類されます。この評価によって、システムがどのようなタイプの有害性に対して特に脆弱であるのかを明確に理解できます。

### 評価指標４：命令への順守度（Compliance／Obedience）

また、「コンプライアンス（命令遵守度）」という指標も広く用いられています。LLMが攻撃プロンプト（悪意のある入力）に対してどの程度素直に従ってしまうかを測定するもので、例えば、ユーザーが「犯罪の手順を教えて」と頼んだときに、モデルがその要求にどの程度従順に応じるかを調べます。

この指標は、LLMが有害な内容への対応をどれだけ明確に拒否できるかを見るのに役立ちます。

### 評価指標５、６：応答の関連性（Relevance）と流暢性（Fluency）

最後に、モデルの応答がユーザーの入力に対してどれほど関連しているか（Relevance）や、応答がどの程度自然で違和感のない文章であるか（Fluency）を評価することも重要です。LLMの返答が、内容として無害でも、質問の趣旨と無関係な内容であれば、適切とは言えません。

逆に、攻撃的な入力に対して、全く無関係な応答を生成することで安全性を保つという方法も考えられますが、その場合、応答の有用性や利便性が損なわれることになります。流暢性（自然な表現で回答がなされているかどうか）は、多くの場合、関連性とセットで評価されます。

## LLMの安全性評価に役立つ公開リソース

レッドチーミングを行う際に直面する実際的な課題の一つとして、攻撃のためのシナリオやプロンプトをゼロから作成することの難しさがあります。レッドチーミングを効果的に進めるには、すでに作成された攻撃手法や評価データセット、さらにそれらを効率よく利用できるツールが不可欠です。

このような現実的なニーズを背景に、さまざまな公開リソースが研究者や実務者向けに提供されています。

### 公開されているレッドチーミングツールの紹介とその意義

最近では、多くの研究機関や企業がレッドチーミングを容易にするためのオープンソースのソフトウェアツールを開発・公開しています。

例えば、「Pyrit」というツールは、攻撃を生成するモデルとそれを評価するモデルを組み合わせた環境を提供しています。このツールの利点は、初心者でも簡単に利用できる設計であることに加え、新たな攻撃手法を簡単に追加・試行できる柔軟性を持つことです。

[https://github.com/Azure/PyRIT](https://github.com/Azure/PyRIT)

「Garak」というツールも同じくLLMへの攻撃テストを支援しますが、こちらは評価プロセスの詳細なログを記録し、結果をわかりやすくレポートとしてまとめる機能が充実しています。

[https://github.com/NVIDIA/garak](https://github.com/NVIDIA/garak)

また、「Giskard」というツールは企業レベルのニーズにも応えられるスケーラブルなフレームワークで、大規模で複雑なシステムの安全性評価に利用されています。

[https://github.com/Giskard-AI/giskard](https://github.com/Giskard-AI/giskard)

### 公開データセットの役割とその活用方法

ツールに加えて、LLMの安全性評価を支えるための公開データセットも多数用意されています。

たとえば「JailbreakBench」というデータセットには、実際にOpenAIが設定した使用ポリシーに違反するような攻撃プロンプトが多数収録されています。このデータセットは、有害な情報や誤情報、差別表現といった問題あるコンテンツをモデルが生成してしまうかどうかをテストするのに役立ちます。

[https://github.com/JailbreakBench/jailbreakbench](https://github.com/JailbreakBench/jailbreakbench)

また、「GPT-Fuzzer」というデータセットでは、LLMに有害な応答を誘導するための多様な質問が収録されています。他にも、「ALERT」や「SafetyBench」といったベンチマークデータセットも存在し、これらは攻撃の深刻度や安全性に関する知識を評価するための質問集を提供しています。

[https://github.com/sherdencooper/GPTFuzz](https://github.com/sherdencooper/GPTFuzz)

さらに、多言語対応のデータセットも存在します。

例えば、「XSafety」は、複数の言語においてよくみられる安全性問題をカバーしており、多言語対応モデルの評価に役立ちます。

[https://github.com/Jarviswang94/Multilingual\_safety\_benchmark](https://github.com/Jarviswang94/Multilingual_safety_benchmark)

「DoNotAnswer」というデータセットでは、「モデルが回答してはいけない質問」を提示し、その回答を評価することでモデルの安全性を判断します。

[https://github.com/Libr-AI/do-not-answer](https://github.com/Libr-AI/do-not-answer)

### 公開リソースを活用する際の注意点と現実的な課題

これらの公開リソースはレッドチーミングを始める際に役立つ反面、実際に使う上では注意すべき点もあります。まず、データセットやツールが想定する攻撃シナリオが、自分たちの扱うLLMアプリケーションに本当に適しているかをよく吟味する必要があります。

どれほど評価環境が充実していても、現実のアプリケーションで想定される攻撃シナリオやリスクを十分カバーできているとは限りません。

また、公開されている攻撃プロンプトは、多くの組織がすでに評価に利用しているため、モデルの防御がそれらに特化される可能性もあります。そのため、これらのプロンプトをベースにさらに工夫を施し、新しいシナリオを自前で追加生成することも求められます。

公開されたツールやデータセットを適切に活用することで、より効率的で網羅的な安全性評価が可能になりますが、あくまでも実際の利用シーンを見据えて運用することが求められています。

## LLMを安全に運用するための防御策（ガードレール戦略）

レッドチーミングによってLLMアプリケーションの弱点を特定した後、次に必要となるのは、その弱点を実際の運用段階で防御するための手法、つまり「ガードレール」を整備することです。ガードレールとは、LLMシステムが悪意ある入力や攻撃にさらされた際に、被害を未然に防ぐための具体的な対策のことを指します。

実際に使われているガードレール戦略には、いくつかの代表的な方法があります。

### 「システムプロンプト」による事前防御

まずよく使われるのが「システムプロンプト」と呼ばれる方法です。この方法は、モデルがユーザーの入力に応答する前に、あらかじめ安全性を促す明示的な指示をモデルに与えるものです。

例えば、「有害な内容には一切回答しないでください」といった指示をシステムの初期設定に加えることで、LLMが有害な内容に対して自動的に反応を拒否するように導きます。

ただし、この方法は万能ではありません。最近の研究からも明らかなように、あまりにも厳しいシステムプロンプトを設定すると、害のない一般的な質問に対してもモデルが応答を拒否してしまうことがあり、ユーザー体験を損なうリスクがあります。

そのため、適切なバランスでプロンプトを設計することが重要です。

### 外部フィルターによる内容チェック（コンテンツフィルタリング）

もう一つの一般的な防御手法が、「コンテンツフィルタリング」です。これはLLMの入出力をそのまま受け入れるのではなく、モデルとは別の独立したフィルター（ [分類器](https://ai-data-base.com/archives/26489 "分類器") や検査用のLLMなど）を設け、入力や出力内容をリアルタイムでチェックし、問題のある場合には自動的に回答を遮断または修正する手法です。

例えば「PromptGuard」と呼ばれるツールは、悪意ある入力を検知するための専用 [分類器](https://ai-data-base.com/archives/26489 "分類器") を使い、問題のあるプロンプトを事前にブロックします。また、モデルの出力が不自然で不整合な場合には、出力の一貫性を判断する「パープレキシティ（文脈不自然度）」という指標を用いて、自動的に有害な内容を検知し、抑制する方法も提案されています。

[https://huggingface.co/meta-llama/Prompt-Guard-86M](https://huggingface.co/meta-llama/Prompt-Guard-86M)

しかし、これらの手法にも限界があります。特に、単一ターンでの明確な攻撃に対しては有効であっても、複数ターンに渡って慎重に意図を隠された複雑な攻撃に対しては、十分な防御効果を発揮できないことが指摘されています。

### 「ファインチューニング」と「強化学習」によるモデル改善

最後に、モデル自体の安全性を高める方法として、「ファインチューニング（微調整）」や「人間のフィードバックを用いた [強化学習](https://ai-data-base.com/archives/26125 "強化学習") （RLHF）」が挙げられます。ファインチューニングとは、モデルに追加の安全性データ（有害な指示とそれを拒否する正しい応答）を与えて再訓練する方法です。

つまり特定の攻撃や問題ある入力に対して、より適切な応答を返せるようモデルを調整します。

また、人間が判断した安全性に関する評価を基準にしてモデルの行動を強化学習させる「人間のフィードバックによる強化学習（RLHF）」という手法もあります。この手法は人間の安全基準を直接モデルに教えることができるため、単なるファインチューニングよりも高度で柔軟な安全性の改善が可能になります。

さらに最近では、人間の好みをより直接的に反映させることができる「DPO（直接的嗜好最適化）」や、多様な人間の価値判断を分布として取り込む「DPL（分布的嗜好学習）」といった発展的な手法も研究されています。これらの手法を使うと、安全性を高めつつも、モデルの性能を過度に犠牲にすることなくバランスよく運用することが可能になります。

### ガードレール導入における現実的な注意点

いずれの手法も万能ではありません。単一のガードレールだけに頼るのではなく、複数の防御策を組み合わせて運用する必要があります。

また、攻撃方法は日々進化しており、どれほど洗練された手法を用いても新たな脅威が出現する可能性は常にあります。そのため、定期的なレッドチーミングとガードレールの再評価・改善を継続的に行うことが重要となります。

## まとめ

この記事では、LLMの安全性を評価するためのレッドチーミングという手法の概要とその具体的な手順を示した研究を紹介しました。レッドチーミングを通じてモデルの脆弱性を明らかにし、ガードレールなどの防御策と組み合わせることで、安全なLLMアプリケーションを実現することが可能になります。

ただし、本手法だけで完全な安全性を保証できるわけではなく、今後はより高度な多段階の攻撃に対応できるような手法の改良や評価基準の統一化が期待されています。

**参照文献情報**

- タイトル：Building Safe GenAI Applications: An End-to-End Overview of Red Teaming for Large Language Models
- URL： [https://doi.org/10.48550/arXiv.2503.01742](https://doi.org/10.48550/arXiv.2503.01742)
- 著者：Alberto Purpura, Sahil Wadhwa, Jesse Zymet, Akshay Gupta, Andy Luo, Melissa Kazemi Rad, Swapnil Shinde, Mohammad Shahed Sorower
- 所属：Capital One, AI Foundations

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLM評価の盲点とそれを解消する手法](https://ai-data-base.com/archives/83704)

[ソフトウェア評価にLLMを活用する「LLM-as-a-Judge」における現状](https://ai-data-base.com/archives/86479)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)