---
title: "自然言語での曖昧なリクエストが「LLMのコード生成性能に与える影響」とLLMが誤解しないよう修正するアプローチ"
source: "https://ai-data-base.com/archives/89804"
author:
  - "[[AIDB Research]]"
published: 2025-05-21
created: 2025-06-13
description: "本記事では、自然言語で書かれたあいまいなリクエストが、LLMによるコード生成を不安定にする要因となっている問題と、それに対する修正のアプローチを紹介します。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、自然言語で書かれたあいまいなリクエストが、LLMによるコード生成を不安定にする要因となっている問題と、それに対する修正のアプローチを紹介します。

単にプロンプトを工夫するのではなく、要件そのものを少し整えることで、出力が一貫しやすくなるという考え方に基づいています。モデルの出力結果を観察し、そこに現れる意味のばらつきから記述のズレを逆算していく手順が提案されています。

LLMを業務に活用する中で、仕様の書き方が結果にどのように影響するのかを見直すきっかけになるかもしれません。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89804-1024x576.png)

**本記事の関連研究**

- [LLMによるプロンプトの書き直しは本当に実用的　実際の会話データ数百万件をもとに得られた7つの知見](https://ai-data-base.com/archives/87309)
- [「自己修正機能を備えたプログラム合成」を実現するためのLLMエージェンティックワークフロー](https://ai-data-base.com/archives/83400)
- [論文本文のみをもとに実装コードを生成する　LLMベースの方法論](https://ai-data-base.com/archives/89006)

## 背景

プロンプトからコードを生成する仕組みが普及し、自然言語がそのままソフトウェア開発の入り口として使われる場面が増えています。今や、要件や仕様を自然言語で書き、それをもとにコードを出力するという流れは、実務でも珍しくなくなっています。

ただし、自然言語にはあいまいさが避けられないという特性があります。意味合いが複数ある記述があった場合に、人によって、またモデルによっても解釈が分かれてしまうことがあります。

実際に、あいまいな記述を与えた場合、モデルの精度が20%も落ちるという報告も出ています。こうした課題に対し、過去にはLLMに質問させて人間が補足説明を加えるといった手法が提案されてきましたが、これは現場で使うには煩雑です。また、モデルが本質から外れた質問を返してしまったり、説明が冗長になることで、かえって要件の見通しが悪くなることもあります。

そこで注目されているのが、「あいまいな要件そのものを自動で手直しする」という新しいアプローチです。人手を介さずに、モデルを使って自動的に自然言語の記述を見直し、より一貫したコード解釈ができるように調整する。ここで重要になるのが、コードの出力に現れるばらつきを抑えること、そして、例として書かれた入出力と矛盾しないように要件を整えることです。

以下で詳しく紹介します。

## 具体例から見る課題

自然言語で記述された要件があいまいなままだと、LLMによるコード生成にどのような影響が出るのか。研究チームはこれを、具体的な事例を通じて検証しました。

まず注目されたのは、ひとつの要件に対して複数のコードを出力させたときに、それらの動作がどれだけ食い違うかという点です。似た動きをするコードをグループにまとめて比較することで、「LLMがどのくらい一貫した解釈をしているか」を測ることができます。

### 曖昧な指示で意図が伝わらない

たとえば、「 [ノード](https://ai-data-base.com/archives/26470 "ノード") 間の重複した接続を削除する」と書かれた要件があるとします。一見すると明快な指示のようにも思えますが、「同じ接続が何度も登場した場合にすべてを削除するのか」「最初の1つだけを残すのか」といった解釈の揺れが残っています。

この要件にはテスト用の入出力例も添えられており、それを見るかぎりでは「すべてを削除する」が正しい意図と考えられます。しかし、LLM（この場合はDeepSeek-V3）は例を踏まえずに別の解釈を選び、まったく異なる動作をするコードを出力しました。20回の生成結果すべてが例と一致しないという結果も観測されています。

ただ、文章を少し言い換えて「入力に一度だけ現れる接続だけを保持する」と書き直したところ、同じモデルが安定して正しいコードを生成するようになります。つまり、意図が例で示されていても、あいまいな表現があるだけでモデルの判断は大きくずれてしまう（と同時に、表現を明確にすると判断はぶれない）ということです。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89804_1-1024x384.png)

曖昧な要件に対し、例に沿った正しい解釈を促すには要件の調整が必要であることを示す図

### あいまいさの修正には高度な判断が必要

次の例としては、「2つの単語が同じ文字を持つかどうかを判定する」という要件です。この文もまた、いくつかの読み方が可能です。たとえば、「文字の種類が一致していればよい」と考えるのか、「出現回数まで含めて完全に一致しているべき」と考えるのかで、実装が大きく変わってきます。

ここでも入出力の例が与えられましたが、それでもLLMの出力の3割近くが意図から外れました。試しに明確化のための質問を生成させてみても、核心からずれたものが多く、唯一関連のある質問にも正しく答えられないという結果でした。かえって状況を悪化させてしまったかたちです。

こうした例は、単に明確な文章を書くことが難しいだけでなく、「どう変えれば、どう動きが変わるのか」をモデル自身が理解できていないことを示しています。文の修正がモデルの出力にどう影響するかを見極めるには、それなりに深い判断が求められるというわけです。

ただし、出力された複数のコードを分析し、望ましい解釈とそうでないものを見分けたうえで、それを踏まえて要件文をわずかに書き直すことで、安定した動作を引き出すことができるとわかりました。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89804_2.png)

曖昧さの修正に従来手法を用いると不適切または冗長な明確化が生じる一方、提案手法では出力の分布を整理し対照的な推論を行うことで、正確で簡潔な修正が可能になる

### 曖昧な要件を読み解くことの限界

3つ目の例は、正四角錐の表面積を求める関数が題材です。問題は、「高さ」という語が何を指しているのかが曖昧な点です。垂直方向の高さなのか、斜面の長さなのかによって計算方法が変わってきます。

与えられた例からは「斜辺の長さ」を使う意図が読み取れるのですが、それを裏付けるには、数式を逆算したり、前提となる幾何学の知識を駆使したりと、かなり込み入った推論が必要になります。

結果、最先端の推論アプローチでも、途中までは正しく追えていたものの、数値のわずかなズレを「丸め誤差」と見なしてしまい、本質的な誤解に気づかずに終わってしまいました。結果として、すべてのコードがテストを通過しないという状態に陥っています。

一方で、コードの動きを観察したうえで、まずプログラムの修正を行い、それに合わせて要件文の記述を整えるという順序で対応したところ、モデルは安定して正しいコードを生成できるようになりました。

自然言語のわずかな表現の違いが、コード生成の成否を大きく左右することがわかります。たとえ例が示されていても、表現があいまいなままだとモデルは意図を読み違えることがあります。とはいえ、適切な手順を踏めば、動作を安定させることは十分に可能です。

あいまいさを人手を介さずに直す場合、単純に言い換えるだけでは済まないのがネックです。モデルの出力を観察し、その背後にある誤解を推定し、それに対応するように要件を整える。このような構造的なアプローチが必要になります。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89804_3-1024x689.png)

出力されたプログラムの差異をもとに対照的な推論を行うことで、例に隠れた意図を正しく引き出せる一方、プロンプト中心の手法では意図を誤って解釈してしまう

## 基本概念

あいまいな要件を自動で修正するには、まず「そもそも要件とは何か」「モデルはそれをどう解釈しているのか」という足元の整理が欠かせません。LLMが自然言語の要件に基づいてコードを生成する仕組みをあらためて見直し、その中にある“ズレ”の構造を捉えることが出発点になります。

### 要件とモデルの関係をどう捉えるか

自然言語で書かれた要件は、モデルにとって「どんなコードを出力すべきか」という指示になります。たとえば、「この関数は引数の合計を返すこと」と書かれていれば、それをもとにさまざまなコードが生成されることになります。コードの正しさを判断する基準になるのが、こうした要件です。

ここでは、主に動作に関する要件、つまり「入力に対してどんな出力を返すべきか」に注目します。プログラムの速度や見た目の書き方といった非機能面は扱わず、あくまで挙動に注目します。

モデルは要件を受け取り、それに沿うコードを出力しますが、現実には1つの要件から複数の異なるコードが生まれることがよくあります。そのとき、出てくるコードたちは「それぞれ、どれくらいその要件を満たしているのか」「同じ意味なのか、違う解釈なのか」を見ていく必要があります。

### 意味のズレはどう測れるか

たとえば、20個のコードを同じ要件から生成させたとして、それらを1つ1つ実行してみたときに、どの入力にどんな出力を返すかを比べれば「意味的に同じかどうか」がわかります。ここで「意味的に同じ」とは、「動作が同じ」ということです。たとえ書き方が違っても、出力がまったく同じであれば、同じ意味のコードとしてグループ化できます。

このようなグループのことを意味的クラスターと呼びます。出力されたコード群をこうして分類することで、モデルが「どのように要件を解釈しているか」が見えてきます。

モデルはあるコードを出す確率を持っているので、それを集計すれば「どの解釈にどれくらいの確信があるのか」も見えてきます。

### あいまいさは“出力のバラつき”として現れる

ここで注目されるのが、「モデルが出力するコードが、どれくらい意味的にバラついているか」です。たとえば、ある要件に対してほとんど同じ動作のコードばかりが出てくるのであれば、モデルはその要件を一貫して理解できていると考えられます。逆に、まったく異なる動作のコードがいくつも出てくるなら、要件の解釈にゆれがあるということになります。

このバラつき具合を数値として表したものが、「意味的エントロピー」と呼ばれる指標です。ざっくり言えば、「解釈が一つにまとまっていれば数値は低く、バラついていれば高くなる」ものです。確率的な分布の“まとまり具合”を表す概念で、情報の不確実さを測るためにも使われます。

意味的エントロピーがゼロであれば、その要件は特定の意味に明確に絞られている状態です。逆に、数値が高い場合は複数の解釈が共存しており、要件があいまいである可能性が高いと判断されます。

## 曖昧な要件を整えるには

自然言語で書かれた要件を明確に整えるには、モデルの出力結果を一度観察し、どのような意味のブレが生じているかを見極めるところから始めます。そこを起点にして、少しずつ要件の書き方を調整していきます。意味のズレをどう見つけて、どう直していくか。その流れを以下にまとめます。

### 出力のズレを見つける

最初に、あいまいな要件をもとにLLMから複数のプログラムを出力させます。たとえば20個程度をサンプルとして用意します。それぞれの動作を確認し、どんな入力に対してどんな出力を返しているのかを比べます。

この比較を通じて、似た動きをするプログラム同士をひとつのグループとして分類します。同じような意味で動いているコードをまとめるイメージです。これらは意味的クラスターとして扱います。

この段階では、意味的に何通りの解釈が現れているかを見ておきます。多くの解釈に分かれていればいるほど、要件があいまいである可能性が高くなります。

### テスト例との一致度を測る

次に、要件に含まれている入出力例と照らし合わせます。先ほどの各プログラムが、例として書かれている入力に対して正しい出力を返しているかどうかをチェックします。

一致しているかどうかを、グループごとに数え上げておくと判断しやすくなります。例に完全に一致するグループがひとつでもあれば、それが意図に近い可能性が高くなります。

逆に、どのグループも例に一致していない場合は、出力されたプログラムそのものを修正してから使う必要があります。

### 最も信頼できる出力を軸にする

複数ある意味的クラスターのうち、例と合っているものがあれば、その中で最も多く出現していたグループを選びます。このグループが、今の要件に対してモデルが最も確信を持って出力したパターンと考えられます。

このとき他のグループはすべて「望ましくない解釈」として扱います。出力としては可能性があるものの、意図とはズレている解釈になります。

### 選ばれた動作に沿って要件を書き直す

望ましい動作とそうでない動作が整理できたら、次はそれらの違いを言葉で説明してみます。たとえば、「最初に出てきた接続は残すが、2回目以降は削除する」といったように、違いが曖昧になりやすいポイントを明示するようにします。

このように、正しい解釈を強調しつつ、他の解釈を除外するような言い回しを追加します。これによって、モデルの判断が一貫しやすくなります。

### 曖昧さの原因をはっきりさせる

要件を書き換えるときには、「どの表現が誤解を招いていたのか」を明確にすることが大切です。その判断を助けるために、選んだプログラムの動作と、それ以外のプログラムの動作を対比しておきます。

たとえば、「同じ文字を持つかどうかを判定する」という要件で、出現回数を考慮するかどうかが分かれた場合には、「文字の種類が同じであればよい」や「文字の頻度も含めて一致する必要がある」などの表現を加えると誤解を防ぎやすくなります。

このように、どちらの意味にも取れる表現が含まれていたら、どちらかに限定する形で文を整えます。

### 修正後に再評価する

要件を修正したあとは、同じようにモデルからプログラムを出力させて再確認します。意味的クラスターの数が減り、出力が一貫して例と一致していれば、修正はうまくいっていると判断できます。

この再確認の手順は必要に応じて何度か繰り返しても構いません。最終的に、出力が例と安定して一致し、意味的なばらつきが少なくなっていれば、実装としても使いやすい要件になっていると考えられます。

## 評価実験

あいまいな要件を整えることで、本当にコード生成の精度が上がるのか。提案された仕組みが現実の開発現場でも役に立つのか。それを確かめるために、いくつかのモデルとタスクを使って検証が行われました。

### 検証された4つの観点

今回の評価では、以下の4つの問いに対して答えを出そうとしています。

1. 修正された要件は、シンプルな修正方法と比べて精度が向上するのか
2. モデルの推論力を強化するアプローチと比べたとき、どちらのほうが効果的か
3. 要件を修正した結果、文が過剰に長くなってしまっていないか
4. 修正した要件は、別のモデルで使っても同じように効果を発揮するのか

### モデルと評価対象の設定

検証には、3種類のLLMが使われました。ひとつは商用モデルのGPT-4o、もうひとつは数値処理やプログラミングに強みを持つDeepSeek-V3、さらにもうひとつは軽量で多言語やデバッグ用途にも対応できるQwen2.5-Coderです。

課題セットとしては、プログラミング問題集であるHumanEval+とMBPP+が使用されました。どちらもテストケースが複数用意されており、関数の説明や正解コードが含まれています。実験では、各モデルに対して3回ずつタスクを実行し、結果の平均を取っています。

### 比較された手法の構成

提案された要件修正の仕組みとあわせて、3つの既存手法が比較対象になっています。

ひとつはVanilla Repairで、要件があいまいかどうかを分類させたうえで、最もありそうな意味に誘導する方法です。もうひとつはClarifyGPT-autoで、モデルに自動的に質問を投げかけて明確化を試みる仕組みです。そしてµFixは、生成されたコードの失敗例をもとに、意味のずれを段階的に修正するアプローチです。

### 評価に使われた指標

コードの出力結果を判断するために、5つの評価軸が設定されています。

最初に出てきたコードがすべてのテストに合格するかを測るPass@1、合格したテストの平均割合を見るAvgPassRate、ひとつでも正しいコードが出せた課題の割合を示すNZP@1、20個生成されたコードのうち最も多かった解釈の [正解率](https://ai-data-base.com/archives/25930 "正解率") を表すMajority@20、そして出力されたコードがどれくらい意味的にばらけていたかを見るSemantic Entropyです。

### 提案手法による改善の効果

どのモデルでも、どの課題でも、修正された要件を使った場合のほうがコード生成の精度が高くなりました。たとえばDeepSeek-V3を使ったHumanEval+では、Pass@1のスコアが87.8%から91.99%まで向上しています。GPT-4oやQwen2.5でも同じような改善が確認されています。

平均的な合格率も上がっており、例との整合性が取れたコードがより安定して出力されるようになっています。また、意味的エントロピーも下がっており、出力のばらつきが減っていることが数値として示されました。

一方で、偶然正解に当たるようなケースはやや減少しています。ただし、複数のコードから正解を多数決で選ぶような場面では、このばらつきの少なさがむしろ有利に働くと考えられます。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89804_4-1024x377.png)

要件を修正することでコード生成の精度と一貫性が全体的に向上し、とくに出力のばらつきが大きく抑えられている

### モデルの推論力強化との比較

モデル側の賢さを高める方法として、µFixのようなアプローチもあります。しかし今回の検証では、µFixによる改善は限定的でした。既に高性能なモデルに対して、さらに推論力を加えることで得られる上積みは限られていたようです。

それに対して、要件の側を整える今回の方法は、モデルの理解力を前提にしなくても精度を押し上げることができ、µFixを上回る成果を複数のケースで示しました。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89804_5-1024x336.png)

同じ要件を対象にした比較において、提案手法による修正が他の修正手法よりも一貫して高いコード生成精度を示している

### 要件の長さに関する評価

要件を明確にしようとするあまり、文章が冗長になってしまっては本末転倒です。ClarifyGPT-autoやµFixでは、修正後の要件が元の数倍の長さになる例もありました。実際、GPT-4oを使ってMBPP+を修正した場合、要件の長さは5〜6倍に膨れ上がることもあったようです。

その点、提案手法では要件の修正内容がコンパクトに収まっており、読みやすさを維持しながら高い精度を実現しています。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89804_6-1024x401.png)

提案手法が他の方法に比べて要件の記述を簡潔に保ちつつ、修正後の情報量を効果的に増やしている

### モデルをまたいだ効果の再確認

あるモデルで修正した要件が、別のモデルでも同じように効果を発揮するのか。これも重要な検証ポイントです。

たとえばDeepSeek-V3で修正した要件をGPT-4oに使った場合でも、平均して約10%の精度向上が確認されました。モデルによっては例外的に精度が落ちる場面もありましたが、全体としては他のモデルにも改善が広がる傾向が見られています。

![](https://ai-data-base.com/wp-content/uploads/2025/05/AIDB_89804_7-1024x164.png)

あるモデルで修正した要件を別のモデルで使っても、全体的にコード生成の精度が向上することが多い

### 実験から得られたこと

- 曖昧な要件を整えると、コードの正確さが向上する
- モデルの推論能力を強化するより、要件の改善のほうが有効な場面がある
- 修正された要件は読みやすさを損なわず、過剰な情報も含まれていない
- 修正効果は他のモデルにも引き継がれやすい

要件を書くという作業が、モデルの出力を左右する重要な起点になる。今回の結果は、その基本をあらためて強く示すものになっています。

## これまでの方法では”あいまいさ”が解消しきれなかった理由

自然言語のあいまいさによってコード生成が不安定になる問題は、これまでにも多くのアプローチで取り組まれてきました。過去の代表的な手法とその限界を整理しながら、今回紹介してきた考え方がどのような点で新しいのかを整理しておきます。

### 要件の理解を補強する工夫

過去のアプローチでは、モデルの推論力に頼るスタイルが主流でした。たとえば、複数のヒントを順に与えることでモデルの理解を深めようとする方法です。曖昧な単語の意味を詳しく説明させたり、疑似コードを補助的に使ったりする工夫も提案されてきました。

これらはいずれも「与えられた要件はそのままにして、モデルに考えさせる」という方向性にあります。対して今回の考え方は、モデルに解釈させる前の段階で要件そのものを整えてしまうという、異なるアプローチを取っています。

### あいまいさに気づかせる仕組み

あいまいな要件に気づかせる工夫として、質問生成を通じて明確化を試みる仕組みも提案されてきました。モデル自身が「この点がわかりづらい」と判断し、補足を求めるという発想です。

ただし、実際にはポイントのずれた質問が生成されたり、与えられた例に基づいて正しく判断できなかったりすることも少なくありません。今回紹介しているアプローチは、自然言語の表面だけで処理しようとせず、まずコード出力のばらつきを観察してから、必要な調整を行うという流れをとります。

### プロンプトの工夫だけでは限界がある

プロンプトを調整してモデルの出力を制御する工夫も盛んに研究されています。自己反省型のプロンプトや、ステップバイステップの説明を促すテクニックもその一例です。ただし、これらは本質的に「出力の失敗にあとから気づく」形になっており、意図から外れた出力を予防する仕組みとは言いがたい面があります。

その点、今回のアプローチでは事前に意味のばらつきや例との不一致を確認し、それをもとに要件を書き換えるという手順をとることで、無駄な試行錯誤を減らす狙いがあります。

### 出力のばらつきを不確実性として捉える視点

出力されたコードが複数の意味に分かれてしまう現象は、「不確実性」として定式化する動きもあります。中でも、入力のあいまいさに起因するばらつきを「アレアトリック不確実性」と呼び、これを測る指標として「意味的エントロピー」が使われることが増えています。

今回紹介してきた方法でも、このエントロピーを手がかりにして、どの要件が修正の対象になり得るかを見極める仕組みが採用されています。

### 自然言語から仕様を読み取る研究との違い

自然言語の記述から形式的な仕様を導き出す研究も多く、バグ修正に向けて仕様推論を行うものや、形式仕様への変換を試みるものもあります。

これらはいずれも、文章の中にある意図や制約を抽出する方向で設計されていますが、今回のアプローチでは出力結果の差異を軸にして、どの表現が意図と一致するかを逆算するという構造になっており、判断の根拠がより明確になりやすいという特徴があります。

## まとめ

本記事では、曖昧な自然言語の要件によってコード生成が不安定になる問題に対し、その記述を自動で見直す仕組みを紹介しました。モデルの出力から意味のブレを捉え、それを手がかりに要件を整えるというアプローチが取られています。既存の方法の多くがモデルの側に判断を委ねていたのに対し、要件そのものの修正に着目している点が特徴的です。

評価では、精度や安定性の向上に加え、読みやすさも保たれていることが示されました。

自然言語で仕様を書く機会がある方にとって、LLMを使ううえでの設計的な視点が得られる内容だと思います。

**参照文献情報**

- タイトル：Automated Repair of Ambiguous Natural Language Requirements
- URL： [https://doi.org/10.48550/arXiv.2505.07270](https://doi.org/10.48550/arXiv.2505.07270)
- 著者：Haoxiang Jia, Robbie Morris, He Ye, Federica Sarro, Sergey Mechtaev
- 所属：Peking University, University College London

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMアプリが安全に動くという思い込み　外部から守るセキュリティ設計](https://ai-data-base.com/archives/89743)

[LLMベンチマークは現場の実用性を捉えているか？モデルを選ぶ前に確認したい評価スコアの盲点](https://ai-data-base.com/archives/89851)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)