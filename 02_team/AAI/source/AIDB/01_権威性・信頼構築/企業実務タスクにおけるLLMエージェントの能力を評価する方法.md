---
title: "企業実務タスクにおけるLLMエージェントの能力を評価する方法"
source: "https://ai-data-base.com/archives/81003"
author:
  - "[[AIDB Research]]"
published: 2024-12-23
created: 2025-06-13
description: "本記事では、企業のデジタル業務におけるLLMエージェントの実用性を評価する研究を紹介します。LLMの急速な進歩により業務の自動化可能性が議論される一方で、その実力を客観的に評価するベンチマークが不足しています。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、企業のデジタル業務におけるLLMエージェントの実用性を評価する研究を紹介します。

LLMの急速な進歩により業務の自動化可能性が議論される一方で、その実力を客観的に評価するベンチマークが不足しています。そこでカーネギーメロン大学などの研究グループは仮想的なソフトウェア企業での実務タスクを通じて、現在のLLMエージェントの能力と限界を明らかにするプロジェクトを始めました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81003-1024x576.jpg)

**発表者情報**

- 研究者：Frank F. Xu et al.
- 研究機関：カーネギーメロン大学、デューク大学

## 背景

職場の生産性向上に向けてLLMが重要な役割を果たすことが期待されています。日常業務において、LLMエージェント（LLMベースのエージェント）が多くのタスクを自動化できる可能性が指摘されています。

LLMの可能性については楽観的な見方と懐疑的な見方が存在します。楽観論者は今後数年で人間の労働の多くが自動化されると予測しており、一方で懐疑論者はLLMの推論能力や汎化能力に限界があると指摘しています。

見方が分かれる背景には、職場のタスクにおけるLLMの実力を客観的に評価するベンチマークが不足している状況があります。

またLLMを実務に適用する流れには、生活の質向上や科学的発見の加速といったプラスの側面と、雇用喪失や格差拡大といったマイナスの側面の両方が予想されるため、その実力を正確に把握することが重要です。

そこで今回カーネギーメロン大学などの研究チームは、LLMエージェントの職場タスク実行能力を評価するベンチマークを開発しました。シミュレートされたソフトウェア開発会社の環境で、エージェントはソフトウェア開発、プロジェクト管理、財務分析などの実際の業務に近いタスクに取り組みます。

実験では7種類のLLMを用いて評価が行われました。以下で詳しく紹介します。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81003_1-1024x360.png)

ベンチマークTheAgentCompany。再現可能な自己ホスト型環境、シミュレートされた同僚とのコミュニケーション機能、チェックポイントベースの評価、ソフトウェア企業における175の多様で現実的な専門的タスクを特徴としている

## ベンチマークの特性と他のベンチマークとの比較

研究者らは複雑な現実世界の状況でエージェントの実行能力を評価するため、あるべき特性を定義し、既存の著名なエージェントベンチマークと比較しました。

あるべき特性

- 複数の実務関連タスクをカバーすること
- 職場の他のメンバーとのコミュニケーションを評価する
- 複数のステップからなる大きな目標の達成を評価する
- 多様なインターフェースに対応していること（ウェブ、プログラム、コマンドライン、コミュニケーションツールなど）
- 第三者のソフトウェアに依存せず、自己完結的で再現可能であること

表には複数の既存のエージェントベンチマークとの比較結果が示されています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81003_2-1024x496.png)

様々なAIエージェントベンチマークの比較表。インターフェース、サポートされるタスク、チェックポイントベースの評価の有無、NPCエージェントとの対話機能などの観点で比較

### 特性１：複数の実務関連タスクをカバーすること

現実世界のさまざまな職種の仕事の実態を反映したタスクが設定されました。MiniWob++のように現実世界の仕事に関連していないベンチマークや、SWE-Benchのように限られた範囲のタスクのみを扱うベンチマークと異なり、ソフトウェアエンジニアリング会社における多様で専門的なタスクが含まれています。

### 特性２：職場の他のメンバーとのコミュニケーションを評価する

職場での他の人間メンバーとのコミュニケーション能力が重視されました。既存のベンチマークであるτ-benchは顧客サービスシナリオでのインタラクションを測定していますが、TheAgentCompanyでは同僚との情報のやり取りを含む複雑なタスクが設定されています。

### 特性３：複数のステップからなる大きな目標の達成を評価する

多段階の作業を必要とする長期的なタスクが導入されました。以前のベンチマークより多くの連続作業を要求し、サブタスクの実行能力を測定する詳細な評価手法が用意されています。

### 特性４：多様なインターフェースに対応していること

実際の労働者が使用するWebインターフェース、プログラム、コマンドライン端末、コミュニケーションツールなど、多様なインターフェースがサポートされています。従来のベンチマークの多くが1〜2種類のインターフェースに限定されていた点が改善されています。

### 特性５：第三者のソフトウェアに依存せず、自己完結的で再現可能であること

時間経過による変化を防ぎ、異なる方法間の比較を可能にするため、完全なセルフホスト型の再現可能な環境が構築されました。Mind2Webのように実行環境がないベンチマークや、WorkArenaのようにサードパーティソフトウェアを必要とするベンチマークとは一線を画しています。

## TheAgentCompany環境

研究者らは架空のソフトウェアエンジニアリングスタートアップ「TheAgentCompany」を舞台とするベンチマーク環境を構築しました。実在する企業での業務を参考に、複数のコンポーネントが実装されています。

### ①ローカルワークスペース

人間の実務担当者が使用する業務用PCを模したワークスペースが用意されました。評価マシンの他の部分に影響を与えない安全な実行環境を確保するため、サンドボックス化されたDocker環境として構築されています。ブラウザ、コードエディタ、典型的なソフトウェアがプリインストールされたLinuxターミナルが実装されました。

### ②イントラネット

実際の企業内部のWebサイトを模した環境が構築されました。再現性と自己完結性を重視し、以下のオープンソースソフトウェアが採用されています。

- GitHubの代替としてGitLabがコード管理に使用されています
- GoogleドライブやMicrosoft Officeの代替としてOwnCloudが文書管理に使用されています
- JiraやLinearの代替としてPlaneがタスク管理に使用されています
- Slackの代替としてRocketChatが社内コミュニケーションに使用されています

実在のソフトウェア企業のデータを参考に、模擬データが作成されリセット可能な状態で用意されています。

### ③シミュレートされた同僚とのコミュニケーション

実務における重要な要素である社内コミュニケーションをテストするため、シミュレートされた同僚が実装されました。エージェントはRocketChatを通じて同僚とメッセージをやり取りし、タスクに必要な情報を入手できます。

同僚のシミュレーションにはSotopiaプラットフォームが使用され、各キャラクターには名前、役割、責任、所属などの詳細なプロファイルが設定されています。エージェントは直接メッセージや特定のチャネルを通じて同僚と対話できます。

予備実験の結果から、シミュレートされた同僚には全てClaude-3-5-Sonnet-20241022が採用されました。実際のやり取りの例を以下に示します。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81003_13-1024x628.png)

実務における課題やインタラクションを包括的に評価できる環境が構築され、再現性の高いベンチマークとして機能することが期待されています。

## タスクの構造

研究者らは、タスク構成の詳細な設計仕様を定めました。

### タスクの意図（目的）

ユーザーが実際の業務をエージェントに指示する状況を想定し、タスクの意図が説明されます。人間の作業者が直接の追加指示なしで完了できる明確さが意識されました。ただし、同僚への質問は許容されています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81003_3-1024x596.png)

3つの分野におけるタスク例とチェックポイント。ソフトウェアエンジニアリング、財務、プロジェクト管理の各分野での具体的なタスク内容と評価基準を示す

中間目標はチェックポイントで表現され、重要度に応じたポイントが割り当てられます。必要なアクションの実行確認、データの精度評価、同僚との協力状況の確認など、多様な評価基準が含まれます。

評価者はチェックポイントの達成度を判定するプログラムとして実装されます。環境状態の確認や、エージェントの行動履歴の分析によって評価が行われます。

多くの場合は決定論的なPython関数として記述されますが、主観的な判断が必要な場合はClaude-3-5-Sonnet-20241022による評価が行われます。

### 評価指標

全チェックポイント通過を表す [バイナリ](https://ai-data-base.com/archives/26314 "バイナリ") 指標として”完全完了スコア”が設定されました。

さらに進捗度に応じた評価として”部分完了スコア”が作られ、完全完了に対して50%の追加ボーナスが与えられます。

また、効率性の指標として、実行に要したステップ数と、APIクエリの実行コストが計測されます。コストはトークン数と単価から算出されます。

### ワークフロー

タスク実行は初期化、実行、終了の3段階で進められます。初期化ではワークスペースの準備、実行ではサブタスクの完了や同僚とのやり取り、終了では最終出力の提出が行われます。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81003_5-1024x373.png)

サンプルタスクのワークフローを示す図。スプリント管理タスクの例で、未完了の課題の特定、担当者への通知、カバレッジ計算、レポート作成、フィードバック反映といった一連の流れを示す

## タスク作成

### タスクカテゴリーの選択

研究者らは米国労働省の [O\*NETデータベース（バージョン29.1）](https://www.onetcenter.org/database.html) を参照し、実務タスクの選定を行いました。データベースには業務内容、必要な能力、タスクの重要度などの情報が含まれています。

従業員数の多い職種が最初に抽出され、米国労働統計局の給与情報と掛け合わせることで、各職種の経済的重要性が評価されました。結果として、

- 総合管理職
- 登録看護師
- ソフトウェア開発者
- 財務管理者

などの職種が浮かび上がりました。

デジタル業務に焦点を当てるため、看護師など身体労働を伴う職種は除外されました。

### タスクの選択

明確な目標と評価基準を持つ具体的なタスクが優先的に選ばれました。タスク内容はO\*NETのリスト、実務経験者の知見、LLMによるブレインストーミングを組み合わせて決定されています。

職種内の全タスクを網羅的にカバーしているわけではないため、職業全体の自動化可能性を判断する材料としては不十分です。

### 手動によるタスクキュレーション

環境構築後、タスクの具体的な内容が定義されました。タスクの意図とチェックポイントの説明、必要データの準備、初期化スクリプトの作成、評価器の実装といった作業が順次進められています。

20名の技術者が2ヶ月以上をかけ、約3,000人時を投じてタスクを作成しました。複雑なタスクでは1件あたり10時間以上の工数が必要でした。

品質管理のため、

- 評価器の有効性確認
- テストの実装
- コードレビュー
- 一貫性のチェック

など、複数の検証プロセスが実施されました。タスクの公平な評価を確保するため、作成に関わっていない人物による最終チェックも行われています。

研究者らは体系的なアプローチと厳格な品質管理により、実務を代表するタスクサンプルの作成に取り組みました。

## ベースラインエージェント

研究者らはTheAgentCompanyベンチマークで性能評価を行うため、ブラウザ操作、ターミナル操作、プログラミングが可能なベースラインエージェントを実装しました。 [OpenHands](https://github.com/All-Hands-AI/OpenHands) のCodeAct Agent with Browsingが採用され、将来の開発のベースラインとして機能します。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81003_6-1024x338.png)

OpenHandsのデフォルトエージェント アーキテクチャ の概要図。

### インターフェース

エージェントは3つのインターフェースを通じて環境とやり取りを行います。

1. Bashシェルでローカルワークスペースでコマンドを実行します
2. Jupyter IPythonサーバーでPythonコードの実行と結果の取得を担当します
3. Playwright Chromiumブラウザで、BrowserGymが定義したナビゲーション、クリック、タイピングなどの基本操作を行います（操作後はHTML、DOM、アクセシビリティツリーなどの豊富な情報が得られます）

### アクション

環境へのアクセスは一連の基本アクションを通じて行われます。

IPythonRunCellActionとCmdRunActionにより、サンドボックス環境でPythonコードとbashコマンドが実行可能です。BrowserInteractiveActionはBrowserGymの専用言語を用いてブラウザ操作を実現します。

### 観察

エージェントは環境の変化を観察します。主な観察対象は

- bashコマンド
- Pythonプログラム
- ブラウザアクションの実行結果

です。ブラウザに関してはスナップショットとアクセシビリティツリーの形で情報が取得されます。

### ワークフロー

毎ステップで、エージェントの履歴と環境の観察結果から次のアクションが決定されます。高レベルではbashコマンド、Pythonコード、ブラウザ操作言語を組み合わせてタスクが実行されます。組み合わせでファイル編集やWeb閲覧、プログラム実行など多様な作業が可能となります。

## 実験結果

研究者らはクローズドモデルとオープンウェイトモデルの両方を用いて評価実験を行いました。OpenHands CodeActエージェントが全実験に使用されました。シミュレートされた同僚と評価部分にはClaude-3-5-Sonnet-20241022が採用されました。

### 結果の概要

175個のタスクを対象とした評価で、Claude-3.5-Sonnetが最高性能を示しました。しかし最強のモデルでもタスクの完了率は24%にとどまり、部分的な達成を含めても34.4%でした。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81003_7.png)

様々な基盤モデルのTheAgentCompanyでの性能比較。成功率、スコア、ステップ数、コストの観点で評価

性能2位のGemini 2.0 Flashは平均40ステップを要し、成功率は1位の半分以下でした。コストが1ドル未満である点は注目に値しますが、これはエージェントがループや無駄な探索に陥ったためと分析されています。

オープンウェイトモデルではLlama 3.1(405B)が最高性能を記録し、GPT-4oと同等の性能を達成しましたが、トップのClaudeとは大きな差がありました。Llama 3.1はGPT-4oと比べて多くのステップと約2倍のコストを要しています。

新世代のLlama 3.3(70B)は旧世代の大規模モデル(405B)と同等の6.9%の成功率をで実現しました。効率性の向上が見られます。

### 分析

研究者らは異なるプラットフォームでの性能を分析しました。RocketChatとownCloudで多くのモデルが苦戦しています。RocketChatは社会的交流の場であり、ownCloudは複雑なオフィススイートのUIを持つため、改善の余地が示唆されました。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81003_8-1024x336.png)

各プラットフォーム（GitLab、Plane、RocketChat、ownCloud）でのモデル性能の比較表

職種別の分析では、データサイエンス、管理、財務のタスクで成功率が低く、ソフトウェアエンジニアリングのタスクで高い成功率が得られました。人間にとっての難易度とエージェントにとっての難易度に違いが見られます。管理や財務のタスクには情報収集や画像理解などが含まれ、エージェントには複雑なUIやコミュニケーション、反復作業の自動化が課題となっています。

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81003_9-1024x294.png)

職種別（SDE、PM、DS、Admin、HR、Finance、Other）のモデル性能の比較表

![](https://ai-data-base.com/wp-content/uploads/2024/12/AIDB_81003_10-1024x398.png)

プラットフォーム別および職種別の成功率を比較したグラフ。主要なモデル間での性能差を示す

### エージェントに共通する失敗

大半のタスクで失敗が見られ、以下の典型的なパターンが観察されました。

1. 常識の欠如により暗黙の仮定が理解できず、例えばファイル形式の推測に失敗するケース
2. 社会的スキルの不足により、同僚とのコミュニケーションの意図や目標を正しく理解できないケース
3. ブラウジング能力の制限により、複雑なUIやポップアップの処理に苦労するケース
4. 自己欺瞞的な行動として、難しい部分を回避するための不適切なショートカットを試みるケース

## まとめ

本記事では、企業におけるデジタル業務の自動化可能性を評価する新しいベンチマーク「TheAgentCompany」に関する研究を紹介しました。

実験の結果、最先端のエージェントでも企業の日常業務タスクの24%しか完了できず、特に社会的コミュニケーションや複雑なインターフェースの操作に課題が見られました。

技術の発展とともにエージェントの性能とコスト効率は着実に向上していますが、実務での本格的な活用にはまだ時間を要することが示唆されています。

**参照文献情報**

- タイトル：TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks
- URL： [https://arxiv.org/abs/2412.14161](https://arxiv.org/abs/2412.14161)
- 著者：Frank F. Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Z. Wang, Xuhui Zhou, Zhitong Guo, Murong Cao, Mingyang Yang, Hao Yang Lu, Amaad Martin, Zhe Su, Leander Maben, Raj Mehta, Wayne Chi, Lawrence Jang, Yiqing Xie, Shuyan Zhou, Graham Neubig
- 所属：Carnegie Mellon University, Independent, Duke University

## 理解度クイズ（β版）

1\. TheAgentCompanyベンチマークが既存のベンチマークと異なる最も重要な特徴は何ですか？

実在する企業の業務環境を忠実に再現し、実践的なタスクを評価できる点が最大の特徴です。単なるタスクの羅列ではなく、実務に即した評価を可能にしています。

解説を見る

2\. LLMエージェントが実務タスクで失敗する主な原因として、研究で明らかになったものは？

常識的な暗黙の仮定の理解不足と社会的スキルの欠如が主要な失敗要因でした。特に同僚とのコミュニケーションや複雑なUIの理解において課題が見られました。

解説を見る

3\. 職種別の分析で見られた興味深い発見は何ですか？

人間にとっての難易度とAIにとっての難易度に明確な違いが見られました。管理や財務のタスクはAIにとって特に困難でした。

解説を見る

4\. TheAgentCompanyの評価環境で特に重視された設計要素は？

実験の再現性と環境の自己完結性が重視されました。すべての要素がDockerコンテナ内で完結し、外部依存のない評価が可能になっています。

解説を見る

5\. 研究結果から示唆される、企業実務におけるAI活用の現状とは？

最先端のモデルでも完了率が24%にとどまり、実務での本格活用にはまだ時間を要することが示されました。特にコミュニケーションや複雑なインターフェース操作に課題があります。

解説を見る

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[生涯にわたりユーザーに寄り添いパーソナライズし続けるAIアシスタントの設計](https://ai-data-base.com/archives/80936)

[18兆トークンで学習されたオープンソースLLM『Qwen2.5』シリーズの性能](https://ai-data-base.com/archives/81076)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)