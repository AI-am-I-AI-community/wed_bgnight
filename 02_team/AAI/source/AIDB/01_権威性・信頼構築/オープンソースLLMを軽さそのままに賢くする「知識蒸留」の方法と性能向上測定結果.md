---
title: "オープンソースLLMを軽さそのままに賢くする「知識蒸留」の方法と性能向上測定結果"
source: "https://ai-data-base.com/archives/88879"
author:
  - "[[AIDB Research]]"
published: 2025-04-30
created: 2025-06-13
description: "本記事では、オープンソースのLLMを軽量のまま性能を引き出す方法として注目されている知識蒸留手法と、その実験結果を紹介します。蒸留プロセス設計の工夫が整理されており、実務での活用を想定した内容になっています。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、オープンソースのLLMを軽量のまま性能を引き出す方法として注目されている知識蒸留手法と、その実験結果を紹介します。

蒸留プロセス設計の工夫が整理されており、実務での活用を想定した内容になっています。小型モデルでも精度を維持できるかどうか、どのような工夫が効果的かを知る手がかりになります。

LLMの導入や再設計を検討している現場にとって、一つの具体的な選択肢を考える材料になります。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_88879-1024x576.png)

**本記事の関連研究**

- [18兆トークンで学習されたオープンソースLLM『Qwen2.5』シリーズの性能](https://ai-data-base.com/archives/81076)
- [オープンソースのコード生成LLMが商用LLMに追いつく　Qwen2.5-Coderの能力値全容](https://ai-data-base.com/archives/78609)
- [DeepSeek R1が実現した教師なし強化学習による推論性能の向上](https://ai-data-base.com/archives/82540)

## 背景

LLMは、翻訳やチャットボットなどに使われる汎用的な技術として広まり、さまざまな業務に取り入れられるようになってきました。クローズドなAPIを使えばすぐに使い始められますが、運用コストやセキュリティといったハードルに加えて、「自分たちの用途にあわせてモデルをチューニングしたい」というニーズも強まりつつあります。

たとえば、社内ドキュメントに特化したモデルや、製造業向けに専門知識を取り込んだモデルなど、汎用LLMではカバーしきれないケースが出てきています。こうした背景から、オープンソースのLLMをベースに、自社仕様のモデルを育てる動きが広がっています。

その中でも注目されているのが、大きなモデルから小さなモデルへ知識を引き継ぐ「知識蒸留」という手法です。軽量化しながらも性能を保つことができるため、リソースが限られた環境での運用にも適しています。

しかし実際にどのようにモデルを構築し、どう運用すればよいかを示した実践的なガイドは多くありません。

そこで今回、知識蒸留を活用してオープンソースLLMを現場でどう使っていくかを、実例を交えてまとめられた論文を紹介します。モデルの内製や特定領域への最適化を考えている方々にとってヒントになるような内容です。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_88879_1.png)

元Qwen2.5と蒸留版のAlpacaEval 2.0（長さ制御）およびIFEvalスコア比較

## 「重いモデルを使いこなすのは大変」その解決策としての知識蒸留

オープンソースモデルの利点は先に述べた通り、セキュリティのリスクを回避したり、柔軟にチューニングできるといったことが挙げられます。そこで、まずは優秀なオープンソースモデルをベースにしようと考えることになりますが、しばしば優秀なモデルはサイズが大きいことが問題になります。

大きなモデルをそのまま業務で使おうとすると、「重すぎて動かない」「コストがかかりすぎる」といった壁に直面することがあります。そこで注目されているのが、「知識蒸留」という考え方です。

知識蒸留は、いわば“優秀な先生モデル”から“コンパクトな生徒モデル”に知識を引き継ぐ方法です。大きなモデルの賢さをうまく小さなモデルに移すことで、軽くて使いやすいモデルをつくるという発想です。

ただ、LLMのように複雑な構造を持つモデルの場合、知識を引き継ぐだけでは十分な性能が出ないことも多く、さまざまな工夫が求められます。たとえば、

- 出力の傾向をより正確に真似するための数式的な工夫や、
- 複数の教師モデルから“いいとこ取り”をして生徒を育てるアプローチなどが提案されてきました。

最近では、性能の高いモデルの多くがAPI経由でしか使えない「クローズド」な形で提供されており、こうしたモデルの出力を手がかりにして軽量モデルを育てる試みも増えています。このように教師モデルの中身にはアクセスせず、出力だけを活用する形の知識蒸留は「ブラックボックス蒸留」と呼ばれています。

「ブラックボックス蒸留」では、たとえば指示に対する応答例を生成して学習データとして使ったり、生徒モデルが学びやすいように指示の書き方を工夫したりといった手法が研究されています。ただし、こうした手法の中には、使うモデルやデータのライセンスによって制約がある場合もあるため、実運用に際しては注意が必要です。

一方で、教師モデルの構造や内部出力に直接アクセスできる場合は、「ホワイトボックス蒸留」として、より細かな知識を生徒モデルに引き継ぐことが可能になります。出力されたトークンだけでなく、その裏にある「なぜその答えに至ったか」の情報まで学ばせることができるため、より深い理解が期待されます。ただしその分、計算負荷も大きくなりがちです。

今回研究者らは、ブラックボックス蒸留とホワイトボックス蒸留の両方の強みを整理し、現実的な計算コストの中で性能を引き出すための工夫を提案しています。

## 業務で使える軽量モデルをどう作るか

オープンソースのLLMを導入するにあたって、多くの現場で課題になるのがモデルのサイズです。性能の高いモデルほどパラメータ数が多く、推論にかかる時間やメモリ消費が大きくなります。これを業務に耐える形で使えるようにするには、モデルを小さくしながら性能を保つ工夫が必要になります。

知識蒸留はそのための有力な手段のひとつです。以下では研究者の取り組みをもとに、オープンソースのLLMをベースに軽量モデルを育てていく際に活用できる蒸留の考え方と、手順の組み立て方を紹介します。

### 「ブラックボックス蒸留」の進め方

まず取り組みやすいのが、教師モデルの出力だけを活用する方法です。これは「ブラックボックス蒸留」と呼ばれ、教師モデルの内部構造にはアクセスせず、出力されたテキストだけを使って生徒モデルに学ばせます。

蒸留の成否を左右するのは、学習データの質と多様性です。単に指示と応答を集めるのではなく、どのような流れでデータを生成・選別・整形するかが重要になります。

実務でブラックボックス蒸留を進める際は、生成から選別までの流れを以下の4つの視点で設計すると効率的です。それぞれをエージェントとして切り分けると、後々の工程が整理しやすくなります。

#### 拡張エージェントで多様な指示を用意する

モデルに幅広い表現を学ばせるには、同じ意味をもつ異なる指示を多数準備することが有効です。たとえば「簡単に説明してください」「要点をまとめてください」「初心者にもわかるように説明してください」といった形で、入力のバリエーションを広げていきます。

元のタスクの種類（分類、要約、翻訳など）を崩さないように注意することで、意味のズレや幻覚を抑えることができます。

#### 書き換えエージェントで同じ内容を異なる言い回しで伝える

表面的には異なるが、意味は変わらないような書き換えを行うエージェントです。例として、「気候変動の経済的影響を要約してください」が、「気候変動が経済に与える影響を説明してください」と変換されるイメージです。

また、推論や計算が必要なタスクにおいては、思考の流れを言語化した「Chain-of-Thought（CoT）」形式の出力を使うと、生徒モデルの論理的な応答精度が向上しやすくなります。

#### 選択エージェントで使うべきデータを見極める

生成されたすべての指示応答ペアをそのまま使うのではなく、有用なものを選び取る段階が必要です。ここでは、情報の密度、学習への貢献度、同種タスクへの一般化可能性といった観点で優先順位をつけていきます。

特定のジャンルに偏りすぎないよう、タスクのバランスを取る意識も欠かせません。

#### 検証エージェントで事実性や整合性を確認する

生成されたデータのなかには、表現が自然でも中身が誤っているケースもあります。こうした不正確な情報が混入すると、モデル全体の信頼性を損ねることになります。自動または人手による検証を通じて、事実性・整合性をチェックする工程を設けておくと安心です。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_88879_2.png)

各エージェント（拡張／書き換え／選択／検証）機能フロー図

### 「ホワイトボックス蒸留」

ブラックボックス蒸留だけでも実用的なモデルは作れますが、より高度な精度を求める場面では、教師モデルの出力分布（ロジット）そのものを活用する「ホワイトボックス蒸留」も検討できます。

ただしこの方法には、計算リソースの消費やモデル間の語彙差といった課題があります。とくにモデルサイズが大きい場合、蒸留の実行すら難しくなるケースもあります。

そこで、教師モデルが出力するトークンのうち、確信度が高い上位10個ほどに絞って処理するという手法があります。そうすると必要な情報をコンパクトに伝えることができ、計算量の削減にもつながります。

また、これらの出力をあらかじめオフラインで生成しておけば、蒸留時の負荷を大きく軽減することができます。複数モデルを並行して扱う現場でも効果的です。

### 蒸留手法の組み合わせ

ブラックボックスとホワイトボックス、どちらか一方に偏るのではなく、状況に応じて段階的に組み合わせていくと、安定した精度と効率を両立しやすくなります。

たとえば、まずはブラックボックス蒸留で基礎を固め、その後にホワイトボックス蒸留を適用して細部を詰めていくような構成が、現実的かつ効果的です。

モデルの目的や使う現場の制約に応じて、構成や学習方法を柔軟に選べるようにしておくと、導入後の改善やチューニングもしやすくなります。

## そうして生まれたDistilQwen2.5シリーズ

今回研究者らは、まさに上記アプローチに沿って、DistilQwen2.5というシリーズのモデル群を作りました。ベースとなったのはオープンソースのQwen2.5シリーズで、サイズの異なる複数の生徒モデルに対して、段階的な蒸留とチューニングが実施されています。

先述したマルチエージェントによる指示・応答データの生成と選別を通じて、ブラックボックス蒸留が行われ、その後さらに、上位トークンの分布を活用したホワイトボックス蒸留によって、出力の質を高める工程が重ねられました。

なお、一部の教師モデルには、出力品質の高いクローズドなLLMが利用されており、APIを通じた応答生成を通じてデータが収集されています。ただし、これらの出力を再学習用途に使うことについては、モデル提供元の利用規約に注意が必要です。

このようにして生まれたDistilQwen2.5シリーズが、実際の自然言語タスクにおいてどのような性能を発揮したのか。次のセクションでは、各種ベンチマークを通じた評価結果を見ていきます。

知識蒸留を行った場合の性能向上例として参考になります。

## 実験評価　軽量モデルはどこまで性能を発揮できるか

どれだけ丁寧に知識蒸留を設計しても、実際にモデルが十分な性能を発揮できなければ、業務で使うには不安が残ります。そこで、今回作成されたDistilQwen2.5シリーズのモデル群がどのように評価され、どのような結果が得られたのかを紹介します。「小型モデルでもここまでできるのか」という観点から、導入の参考になります。

### 実験の設定

使用されたデータセットは、OpenHermes 2.5やCleaned Alpaca Dataset、LCCDなどの公開データに加え、独自に構築したデータも組み合わせたものです。前処理には既存の知見が活用され、その後のデータ整備にはマルチエージェント構成による拡張・書き換え・選別・検証が適用されています。

訓練対象となる生徒モデルは、Qwen2.5をベースに、0.5B、1.5B、3B、7Bの4つのサイズで構成されています。ホワイトボックス蒸留における教師モデルには、それぞれ14B、32B、72BパラメータのQwen2.5-Instructモデルが用いられました。

学習率は1×10^-5、 [エポック](https://ai-data-base.com/archives/26594 "エポック") 数は3で固定され、訓練には80 [GB](https://ai-data-base.com/archives/26343 "勾配ブースティング") メモリを搭載したA800 [GPU](https://ai-data-base.com/archives/26570 "GPU") を8枚備えたサーバーが使用されています。

※なお、この訓練環境（A800 [GPU](https://ai-data-base.com/archives/26570 "GPU") ×8台）は、個人や中小企業にとってはかなりハードルの高い構成です。「そんな金額、簡単に出せるわけない！」という声が出るかもしれません。ただ、公開された蒸留済みモデルを活用したり、小規模な再学習やプロンプト設計で代替する方法もありますので、後述の応用事例とあわせて、自分に合った段階からの活用を検討するとよいでしょう。

### 使用された評価ベンチマーク

モデルの性能は、以下のベンチマークで測定されています。

**AlpacaEval 2.0（長さ制御）**  
モデルの指示追従能力を、応答の長さを制御した状態で評価。

**MT-Bench**  
多様なタスクに対する適応力と応答の一貫性をチェック。単一ターンおよびマルチターン会話の両方に対応。

**IFEval**  
動的なユーザー対話を模したタスクでの性能を測定。柔軟性と応答の正確性が問われる。

### 蒸留による性能向上

蒸留によって得られたDistilQwen2.5モデルは、すべてのベンチマークにおいて、元のQwen2.5モデルを大きく上回る性能を示しました。ブラックボックス蒸留のあとにホワイトボックス蒸留を組み合わせたことで、さらに一段階精度が向上しています。

とくに注目すべきなのは、小さなモデルほど蒸留の恩恵を大きく受けたという点です。0.5Bサイズのモデルでは、蒸留前後の差が非常に顕著であり、小型モデルを使いたい場面での有力な選択肢となります。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_88879_3.png)

0.5B～7Bモデル間の指示追従性能比較表

### 推論速度と処理効率

教師モデルのサイズごとにロジット（出力分布）生成の速度も測定されました。

ロジットというのは、モデルが「次に出すべき単語や記号はこれかもしれない」と内部的に判断している確率のリストのようなものです。たとえば「こんにちは」と続けたい場合、「こ」が来たときに「ん」「に」「ば」などの候補に、それぞれ確信度が割り振られています。

提案手法ではこのロジット生成速度について従来手法の3〜5倍の高速化が確認されています。

さらに、ロジット出力の数を制限する工夫によって、推論品質を損なわずに処理負荷を大きく削減できていることも分かりました。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_88879_4.png)

従来実装比3～5倍高速なロジット生成レイテンシ比較

### タスクごとの能力傾向

MT-Benchによる詳細な分析では、DistilQwen2.5モデルが幅広い [NLP](https://ai-data-base.com/archives/26319 "自然言語処理（NLP）") タスクに対応できる汎用性を持ち、元のモデルと比較して全体的な応答品質が向上していることが確認されています。とくに、数学や論理推論といった難度の高いタスクにおいても、明確な改善が見られました。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_88879_5.png)

MT-Benchにおける0.5Bおよび1.5Bモデルのタスク別性能比較

### 他モデルとの比較で見えるコスト効率

他の小型モデル（Phi-3、LLaMA、Mistralなど）との比較では、DistilQwen2.5が同程度またはそれ以上の性能を示し、サイズに対する性能のコストパフォーマンスが際立っていることが分かります。1.5Bモデルが3Bモデルに匹敵する応答品質を持つなど、業務導入時のリソース制約を考える上でも非常に有益な指標となります。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_88879_6.png)

10B未満モデルのAlpacaEval 2.0ランキング比較

### モデル融合における教師モデルの選び方

ブラックボックス蒸留のあとにホワイトボックス蒸留を実施した結果、教師モデルのサイズが大きくなるほど改善は得られるものの、その効果は一定のサイズを超えると逓減する傾向が見られました。72Bモデルよりも32Bモデルで十分な効果が得られるケースもあり、実装時のコスト対効果を考えるうえで重要な示唆となります。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_88879_7.png)

ブラックボックス蒸留後の教師モデルサイズ変化による性能比較

また、追加データの量に関しても、10Kから100Kの範囲では効果が見られる一方、それを超えると改善幅はやや鈍化しています。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_88879_8.png)

モデルサイズおよびデータ量変化時のホワイトボックス蒸留性能比較

## 実務での活用　軽量モデルを賢く使う応用シーン

蒸留手法の効果を示すだけでなく、DistilQwen2.5を実際に業務に近い形で活用した事例も報告されています。

データ分析業務において、SQLの自動補完機能は大きな価値を持ちます。今回、SQL文の生成や補正にDistilQwen2.5が活用され、条件の追加やテーブル結合など、実用的なシナリオに対応できる性能が確認されました。

その結果として、モデルの精度に加えて、リアルタイム性（レイテンシ）や採用率、Pass@1といった指標でも、7Bモデルに近い性能を3Bモデルで再現できたことが報告されており、実装コストを抑えながら品質を維持できることが示されています。

![](https://ai-data-base.com/wp-content/uploads/2025/04/AIDB_88879_10.png)

7B（教師）と3B（生徒）モデルのSQL補完レイテンシ・Pass@1・採用率比較

## まとめ

本記事では、軽量なLLMを実務に適した形で活用するための蒸留手法を紹介しました。自社に合ったモデル運用を設計する際の考え方として、部分的にでも活かせる要素が含まれています。

内容としては、指示データの生成や選別を支えるエージェント構成、段階的な蒸留の進め方など、工程全体が整理されています。とくに、小型モデルへの蒸留が効果的に働くことは、限られた環境での導入を検討するうえで参考になりそうです。

**参照文献情報**

- タイトル：DistilQwen2.5: Industrial Practices of Training Distilled Open Lightweight Language Models
- URL： [https://doi.org/10.48550/arXiv.2504.15027](https://doi.org/10.48550/arXiv.2504.15027)
- モデル： [https://huggingface.co/alibaba-pai](https://huggingface.co/alibaba-pai) 内に格納
- 著者：Chengyu Wang, Junbing Yan, Yuanhao Yue, Jun Huang
- 所属：Alibaba Cloud Computing

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[科学分野におけるLLM活用の発展まとめ](https://ai-data-base.com/archives/89070)

[人間の考えに潜む認知バイアスをLLMで捉える手法](https://ai-data-base.com/archives/88923)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)