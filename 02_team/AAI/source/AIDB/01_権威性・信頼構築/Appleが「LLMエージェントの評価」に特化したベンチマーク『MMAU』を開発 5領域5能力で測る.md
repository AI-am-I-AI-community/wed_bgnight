---
title: "Appleが「LLMエージェントの評価」に特化したベンチマーク『MMAU』を開発 5領域5能力で測る"
source: "https://ai-data-base.com/archives/73656"
author:
  - "[[AIDB Research]]"
published: 2024-08-01
created: 2025-06-13
description: "本記事では、LLMの能力を総合的に評価する新ベンチマーク『MMAU』を開発したAppleの研究を紹介します。5つの領域と5つの能力を評価対象とし、3,000以上のプロンプトを含む20のタスクで構成されています。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、LLMの能力を総合的に評価する新ベンチマーク『MMAU』を開発したAppleの研究を紹介します。5つの領域と5つの能力を評価対象とし、3,000以上のプロンプトを含む20のタスクで構成されています。全てのタスクはオフラインで実施可能です。そして本ベンチマークを使用して18個のモデルを評価した結果から、各モデルの特性とLLMエージェントの現状が明らかになりました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73656-1024x576.jpg)

**参照論文情報**

- タイトル：MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains
- 著者：Guoli Yin, Haoping Bai, Shuang Ma, Feng Nan, Yanchao Sun, Zhaoyang Xu, Shen Ma, Jiarui Lu, Xiang Kong, Aonan Zhang, Dian Ang Yap, Yizhe zhang, Karsten Ahnert, Vik Kamath, Mathias Berglund, Dominic Walsh, Tobias Gindele, Juergen Wiest, Zhengfeng Lai, Xiaoming Wang, Jiulong Shan, Meng Cao, Ruoming Pang, Zirui Wang
- 所属：Apple

**本記事の関連研究**

- [LLMエージェントの評価はLLM単体の評価と大きく異なる](https://ai-data-base.com/archives/72074)
- [Among UsのようなゲームでLLMエージェントはどれほど活躍できるか](https://ai-data-base.com/archives/71915)
- [心の理論をLLMエージェントに実装することの効果](https://ai-data-base.com/archives/72954)
- [MMLUをアップデートしたベンチマーク『MMLU-Pro』Phi-3やLlama 3、Claude 3、GPT-4oなどの評価結果](https://ai-data-base.com/archives/70358)

## 背景

複雑な状況を理解し、論理的な推論を行い、適切な判断を下すLLMエージェントの活躍が期待されています。

しかし、LLMエージェントを評価する既存のベンチマークには課題があります。多くのベンチマークはアプリケーションに焦点を当てており、タスクの完了率のみを評価する傾向がありました。それだけでは、エージェントの根本的な能力を理解できるとは言えません。

何らかの問題を解く際に、例えば「理解力」「推論力」などのどの能力を伸ばすべきなのかが分からないといった状況です。

そこで今回、新しいベンチマークMMAU（Massive Multitask Agent Understanding）が開発されました。5つの重要な領域（ツールの使用、有向非巡回グラフを用いた質問応答、データサイエンスと機械学習のコーディング、コンテストレベルのプログラミング、数学）と、5つの本質的な能力（理解、推論、計画、問題解決、自己修正）を評価対象としています。

以下で詳しく紹介します。

なお下記は、MMAUにおける異なるモデルの評価結果です。左図は様々な領域での性能比較、中の図は様々な能力での性能比較、右の図はMMAU全体での性能比較を示しています。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73656_1-1024x331.jpg)

また、次の図はMMAUの概要です。MMAUは能力中心の評価（上）とドメイン中心の評価（下）を提供するように設計されていることを示す図です。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73656_2-1024x546.jpg)

下の図は数学問題におけるさまざまなエラータイプを示しています。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73656_3-1024x313.jpg)

## MMAUベンチマーク

### 評価対象の5つの能力

1. **理解力  
	**複雑な指示の解釈、ユーザーの意図の把握、統計データの解析、視覚情報の理解など、様々な側面から評価されます。例えば、長文の指示や暗黙的なユーザーの意図を正確に理解できるかどうかがテストされます。
2. **推論力と計画力  
	**問題解決のプロセスが2段階に分けられている「プランナーシフト」というタスクで評価します。第1段階では、プランナーモデルが高レベルの計画を生成し、第2段階では、ソルバーモデルがその計画に基づいて問題を解決します。各段階で、計画と推論のプロセスを個別に評価できます。
3. **問題解決能力  
	**「ソルバーシフト」というタスクで評価できます。プランナーモデルを固定し、ソルバーモデルのみを変更することで、純粋な問題解決能力の違いを測定します。
4. **自己修正能力  
	**エラーの識別、環境からの学習、過去の行動の修正などの観点から評価されます。各ドメインで特別に設計された自己修正タスクが用意されています。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73656_5-1024x344.png)

planner-shiftタスクとsolver-shiftタスクの構造

### 評価対象の5つの領域

1. **ツールの使用  
	**独自のデータセットを用いて、LLMのツール使用能力が評価されます。単一ツールの使用、並列ツールの使用、複数ツールの連続使用などのシナリオが含まれます。
2. **有向非巡回グラフ（DAG）を用いた質問応答  
	**ユーザーの要求に対して、適切なツールの選択と順序付けを行う能力が評価されます。なお、辺に方向性があるグラフを有向非巡回グラフと呼び、頂点間に順序関係や依存関係があるのが特徴です。つまりツールの間に順序や依存関係があるという状態です。
3. **データサイエンスと機械学習のコーディング  
	**Kaggleデータセットを基に、コード生成とデータ分析の能力が評価されます。テキストベースの質問応答や視覚的な質問応答も含まれます。
4. **コンテストレベルのコーディング  
	**CodeContestsデータセットを使用し、高度なプログラミング問題の解決能力が評価されます。
5. **数学  
	**DeepMind-mathデータセットを基に、数学的問題解決能力が評価されます。また、理解力を特に重視した「Comprehend+」タスクも導入されました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73656_6-1-452x1024.png)

データ分析や機械学習に関するマルチターンの質問応答例

## MMAUベンチマークによる評価

MMAUベンチマークを用いて、18種類のモデルが評価されました。評価結果は、ドメイン中心の評価と能力中心の評価の2つの観点から分析されています。

### ドメイン中心の評価

1. **商用APIモデルとオープンソースモデルの性能差  
	**全ての評価ドメインにおいて、商用APIベースのモデルがオープンソースモデルを大きく上回る性能を示しました。
2. **GPT-4ファミリーの優位性  
	**GPT-4oとGPT-4を含むGPT-4ファミリーが、一貫して他のモデルを上回る性能を発揮しました。中でも数学とコンテストレベルのコーディングにおいて、GPT-4oは顕著な優位性を示しました。
3. **その他の商用モデルの性能  
	**Claude3-OpusとGemini-1.5-proも、比較的良好な性能を示しました。
4. **オープンソースモデルの特徴  
	**ツール使用をサポートしていないオープンソースモデルが多く見られました。サポートしているモデルの中では、Hermes-2-Pro-Mistral-7Bが強力なツール使用性能を示しました。
5. **特定領域での優れた性能  
	**Mixtral-8x22B-Instruct-v0.1は、数学とDAG-QAにおいて驚くべき性能を発揮し、強力な推論と計画能力を示しました。また、Phi-3はモデルサイズを考慮すると、数学で良好な性能を示しました。
6. **課題のあるモデル  
	**Llama2ファミリーは、難解なコーディングタスクで苦戦しました。

結果は下の表で詳しく確認できます。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73656_7-1024x625.png)

ドメイン中心の評価結果。すべての値はパーセンテージで報告されている

### 能力中心の評価

各モデルの基本的な能力が詳細に分析されました。

1. **理解力の評価  
	**GPT-4oが他のモデルを大きく上回る理解力を示し、長文の文脈理解や複雑な指示の解釈、ユーザーの意図把握において優れた能力を発揮しました。GPT-4、Gemini-1.5-pro、Claude3-Opusも比較的強い理解力を示しました。
2. **推論力と計画力の評価  
	**GPT-4ファミリーが最も強力な推論力と計画力を示しました。
3. **問題解決能力の評価  
	**問題解決能力においては、モデル間で大きな差は見られませんでした。これは、”オラクル”プランが提供された場合、タスクの解決自体はそれほど困難ではない可能性を示唆しています。なお”オラクル”プランとは、問題解決のための最適または理想的な計画のことを指します。
4. **自己修正能力の評価  
	**自己修正能力においては、モデル間で顕著な差が観察されました。オープンソースモデルの中では、Mixtral-8x22Bを除いて、ほとんどのモデルは自身のエラーを修正する能力を持っていないことが明らかになりました。このことから、自己修正能力が今後のLLM研究開発において重要となる可能性が示唆されました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_73656_8-1024x447.jpg)

能力中心の評価結果。すべての値はパーセンテージで報告されている

### 計画能力の重要性

MMAUベンチマークの数学タスクにおいて、高品質な計画が全てのモデルの性能を向上させることが明らかになりました。例えば、Command Rの性能は8.21%から33.33%に、Llama-2-70Bは8.43%から32.10%に向上しました。さらに、既に強力なモデルであるMixtral-8x22Bでさえ、50%から60.02%へと性能が改善されました。

興味深いことに、モデル自身をプランナーとして使用した場合でも性能の向上が見られ、GPT-4oは53.4%から61.2%に改善しました。モデルに対して最初に高レベルの戦略を立て、その後その戦略に基づいて問題を解決するよう明示的に指示することが、性能向上の有望なアプローチとなる可能性を示唆する結果です。

### 能力の難易度差

評価結果から、各能力が異なる難易度を持つことが明らかになりました。  
問題解決能力は比較的小さな性能差を示し、多くのモデルで普遍的に達成可能な能力であることが示唆されました。  
一方、自己修正能力は大きな課題となっており、モデル間で顕著な性能差が観察されました。繰り返しになりますが、オープンソースモデルが効果的な自己修正スキルを欠いていることも判明しました。

現在のモデル設計で得やすい能力と得難い能力があることが示唆されます。

### バランスの重要性

GPT-4ファミリーなどの強力なモデルは、全ての能力においてバランスの取れた性能を示しました。これは一つの領域での改善が他の領域の性能向上にもつながる可能性を示唆しており、能力間に高い相関関係と相互依存性があることを示しています。

同様に、ある能力で低い性能を示すモデルは、他の能力でも苦戦する傾向が見られました。そのようなケースは、モデルの [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") や学習戦略に根本的な弱点がある可能性を示唆しています。

### モデルサイズと性能の関係

MistralAIファミリーとLlama-2ファミリーを比較すると、興味深い傾向が観察されます。MistralAIモデルでは、モデルサイズの増加に伴い、全てのドメインで一貫した性能向上が見られました。しかし、この傾向はLlama-2ファミリーには当てはまりませんでした。

コード関連のドメイン（DS&ML、CodeContest）では、Llama-2の全サイズバリエーションが低い性能を示しました。さらに驚くべきことに、DAG-QA（有向非巡回グラフを用いた質問応答）では、Llama-2-7Bモデルがより大きなモデルよりも良い性能を示しました。

この観察結果は先行研究の所見と一致しており、モデルサイズの増加が必ずしも性能向上につながらないこと、そして学習戦略やモデル [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") もスケーリング則に影響を与える重要な要因であることを示唆しています。

## まとめ

本記事では、LLMの能力を総合的に評価するMMAUベンチマークに関する研究を紹介しました。

MMAUは5つの領域と5つの能力を評価対象とし、20のタスクを通じてLLMの強みと限界を評価するものです。

18のモデルの評価結果から、モデル間の性能差や各モデルの特性が明らかになりました。中でも高度な能力での差が顕著でした。

- 参照論文URL： [https://arxiv.org/abs/2407.18961](https://arxiv.org/abs/2407.18961)
- データセット： [https://github.com/apple/axlearn/tree/main/docs/research/mmau](https://github.com/apple/axlearn/tree/main/docs/research/mmau)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMに専門的なドメイン知識を学ばせるのに有効な「読解タスクテキストに変換する」テクニック](https://ai-data-base.com/archives/73575)

[画像と「動画」の中にあるものを認識する『SAM 2（Segment Anything 2）』をMetaが開発](https://ai-data-base.com/archives/73710)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)