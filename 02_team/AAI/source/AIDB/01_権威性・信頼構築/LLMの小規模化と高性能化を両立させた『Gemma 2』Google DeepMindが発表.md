---
title: "LLMの小規模化と高性能化を両立させた『Gemma 2』Google DeepMindが発表"
source: "https://ai-data-base.com/archives/71982"
author:
  - "[[AIDB Research]]"
published: 2024-07-02
created: 2025-06-13
description: "本記事では、Google DeepMindが開発した新しいオープン言語モデル「Gemma 2」を紹介します。知識蒸留技術を活用して小規模モデルの性能向上を実現し、一部のタスクでは2倍以上大きなモデルとも競争力を示しています。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、Google DeepMindが開発した新しいオープン言語モデル「Gemma 2」を紹介します。知識蒸留技術を活用して小規模モデルの性能向上を実現し、一部のタスクでは2倍以上大きなモデルとも競争力を示しています。

研究チームは、効率的なモデル訓練だけでなく、責任あるAI開発にも注力しており、安全性や倫理面への配慮も強調しています。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982-1024x576.jpg)

**参照論文情報**

- タイトル：Gemma 2: Improving Open Language Models at a Practical Size
- 著者：Gemma Team
- 所属：Google DeepMind

## 背景

LLMの性能を上げる主なアプローチは「スケーリング」、つまりモデルの規模を拡大することです。結果としては現在のLLMは様々な言語タスクで高い性能を示し、複数言語の処理や画像理解、さらには100万単語以上の長文脈理解までも可能になりました。

一方で、計算資源の制約から小規模モデルの改善も進められています。小規模モデルの性能向上では主に訓練時間を延長する方法が採られてきましたが、限界があります。データ量を増やしても性能は対数的にしか向上せず、例えば15兆もの「トークン」（言語モデルが処理する最小単位）で訓練しても、性能向上は1-2%程度にとどまります。

そこで今回Google DeepMindの研究チームは、訓練時間を延長する以外の方法で小規模モデルの性能を引き出す手法を探求しました。

彼らは「知識蒸留」と呼ばれる技術に注目しました。大規模モデルの「知識」を小規模モデルに効率的に転移する方法です。

そして開発された「Gemma 2」モデルは、同規模の他のオープンソースモデルを大きく上回る性能を示し、さらに2-3倍大きなモデルとも互角の結果を出しました。質問応答や常識推論、数学・科学的推論、プログラミングなど、幅広い分野でのベンチマークテストと人間による評価で実証されています。

以下で詳しく紹介します。

## モデルのアーキテクチャ

Gemma 2は、前バージョン（Gemma）と同様にデコーダーのみのトランスフォーマー [アーキテクチャ](https://ai-data-base.com/archives/26562 "アーキテクチャ") に基づいています。主要なパラメータと設計選択が表にまとめられています。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982_1.png)

8192トークンのコンテキスト長など、いくつかの要素が前バージョンから引き継がれました。

## Gemma 2の事前訓練

### 訓練データの概要

Gemma 2の訓練には、モデルサイズに応じて異なる量のデータが使用されました。

- 2.6Bモデル: 2兆トークン
- 9Bモデル: 8兆トークン
- 27Bモデル: 13兆トークン

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982_2.png)

データは、ウェブ文書、コード、科学論文など多様なソースから収集されています。ただし、マルチモーダルや多言語機能に特化した訓練は行われていません。最終的なデータの組み合わせは、Gemini 1.0と同様のアブレーション（機能の一部を取り除いて影響を調査する手法）を通じて決定されました。

### トークナイザー

Gemma 1およびGeminiと同じトークナイザーが採用されました。以下の特徴を持つSentencePieceトークナイザーです。

- 数字の分割
- 空白の保持
- バイトレベルのエンコーディング

結果として、256,000エントリーの語彙が生成されました。

### フィルタリング

Gemma 1と同様のデータフィルタリング技術が適用されました。以下の観点でフィルタリングされています。

1. 望ましくない、または安全でない発言のリスク軽減
2. 個人情報や機密データの除去
3. 評価セットからの事前訓練データの除染
4. 機密情報の出力を最小限に抑えることによる暗記リスクの低減

### 知識蒸留

Gemma 2の開発では、知識蒸留の技術が中心的な役割を果たし、小規模ながら高性能なモデルの実現に大きく貢献しました。

知識蒸留とは、大きな「教師モデル」から小さな「学習者モデル」へ効率的に知識を転移する技術です。ベテラン教師が生徒に知識を伝授するプロセスのようなイメージです。

教師モデル（大規模なモデル）が、ある単語や文章に対してどのような予測をするかを詳細に観察します。単に「正解」だけでなく、その予測の確信度や他の可能性についての情報も含まれます。

学習者モデル（小規模なモデル）は、この詳細な情報を参考にして学習します。目標は、教師モデルの考え方をできるだけ正確に真似ることです。つまり、単に正解を当てるだけでなく、教師モデルと同じような思考プロセスを身につけることを目指します。

実際の実装では、計算効率を考慮して工夫がされています。教師モデルの予測は事前に計算され保存されますが、言語モデルが扱う単語の数（語彙サイズ）が25万6千と膨大なため、すべての情報を保存するのは現実的ではありません。そこで、重要な部分だけを選んで保存する方法が取られています。

### カーボン排出量

Gemma 2の開発は高度な計算能力を活用しつつ、環境への影響を最小限に抑える形で行われたことが示されています。

事前訓練によるカーボン排出量は、1247.61 tCO2eq（二酸化炭素換算で1,247.61トン）と推定されています。データセンターから直接報告される時間ごとのエネルギー使用量に基づいて計算され、データセンターの構築と維持に費やされる追加エネルギーを考慮して調整されています。

論文では、GoogleのデータセンターはカーボンニュートラルでありGemma 2の実験と使用されたマシンにも適用されたことが強調されています。

## Gemma 2の事後訓練

Gemma 2は事前訓練済みモデルを指示調整済みモデルへと微調整されました。複数のステップから構成されています。

**（１）ファインチューニング（SFT）**

まず、テキストのみの英語のプロンプト-応答ペアを用いた微調整が適用されました。データは合成的に生成されたものと人間が生成したものの混合です。

**（２）人間のフィードバックによる [強化学習](https://ai-data-base.com/archives/26125 "強化学習") （RLHF）**

次に、RLHFが適用されました。報酬モデルは英語のみのラベル付き選好データで訓練され、ポリシーはSFTフェーズと同じプロンプトに基づいています。

****（３）** モデル平均化**

最後に、各フェーズ後に得られたモデルの平均化が行われ、全体的な性能向上が図られました。

**有害性の除去**

最終的なデータの組み合わせとポストトレーニングのレシピ（調整されたハイパーパラメータを含む）は、有用性を向上させながら安全性に関連する害や幻覚を最小限に抑えることを目的として選択されました。

また全てのデータは以下のようなフィルタリングが施されています。

1. 特定の個人情報の削除
2. 安全でないまたは有害なモデル出力の除去
3. 誤った自己同定データの削除
4. 重複例の除去

**フォーマッティング**

Gemma 2モデルは、Gemma 1モデルとは異なるフォーマッティングスキーマで微調整されました。同じ制御トークンが使用されていますが、モデルが明示的にトークンで生成を終了するようになりました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982_4.png)

Gemmaモデルで使用される関連フォーマット制御トークン

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982_5.png)

ユーザーとモデルの制御トークンを含む対話例

上記はモデルに指示追従能力と安全性を備えさせるための実装です。

## アブレーション実験

アブレーション実験とは、モデルの各部分がどのように全体の性能に寄与しているかを調べる方法です。Gemma 2の研究では、知識蒸留という技術の効果を中心に、様々な設計選択の影響が調査されました。

**知識蒸留の効果**

2.6Bパラメータ（26億の調整可能な値）を持つモデルを、5000億個の単語（トークン）で訓練しました。知識蒸留を使用した場合と、ゼロから学習した場合を比較したところ、知識蒸留を用いた方が明らかに性能が向上し、3つの標準テスト（ベンチマーク）の平均スコアが60.3から67.7に上昇しました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982_6.png)

**モデルサイズと知識蒸留の関係**

モデルの大きさを2億、4億、10億パラメータと変えて実験したところ、サイズが大きくなるほど性能が向上しました。さらに、知識蒸留の効果はモデルサイズが大きくなっても持続することが分かりました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982_7.png)

**注意機構の比較**

9Bモデル（90億パラメータ）で、従来の「マルチヘッド注意」(MHA)と新しい「グループクエリ注意」(GQA)を比較しました。GQAの方がわずかに性能が良く（50.3から50.8にスコア向上）、さらに処理速度も速いため、GQAが採用されました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982_8.png)

**モデル構造の比較**

同じ90億パラメータで、「幅広」（層は少ないが各層が複雑）と「深層」（層は多いが各層はシンプル）のモデルを比較しました。深層モデルの方がわずかに性能が良かったため（50.8から52.0にスコア向上）、この構造が選ばれました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982_9.png)

**コンテキストウィンドウのサイズによる影響**

モデルが一度に処理するコンテキストウィンドウのサイズを変更する実験も行われました。サイズを半分や4分の1に減らしても、性能への影響はごくわずか（1.63から1.64への微増）でした。つまり必要に応じて処理速度を上げられる可能性が示されました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982_10.png)

**入力形式の影響**

質問の書き方（フォーマット）を変えたときの性能の変動も調べられました。Gemma 2の小さいモデル（2B）は、大きなモデルよりもわずかに形式の変化に弱いことが分かりました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982_11.png)

## 評価実験

Gemma 2モデルの評価は、事前訓練モデルと指示調整モデルの両方に対して、自動ベンチマークと人間による評価を通じて多岐にわたる分野を対象として行われました。

なお同サイズの他モデルと比較される際は活性化パラメータではなく総パラメータ数が考慮されました。

### 事前訓練モデルを評価した結果

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982_13-1024x657.jpg)

2.6Bから9Bパラメータ範囲のモデルと27Bモデルの様々なベンチマークでの比較

**27Bモデル**

蒸留を用いずに13兆トークンで訓練されたモデルの性能が報告されています。

比較対象は以下のとおりです。

1. 同程度のサイズのモデル：Qwen1.5 34B
2. 2.5倍大きなモデル：LLaMA-3 70B

評価方法としてはHuggingFace評価スイートが使用されました。

結果は以下のとおりでした。

1. Gemma 2の27Bモデルは、同サイズカテゴリーで最高の性能を示した
2. 2.5倍大きなLLaMA-3 70Bモデルと比較しても競争力のある結果が得られた

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982_12.png)

同様の方法で訓練されたモデルの性能は、サイズに対して対数的にしか向上していません。またGemma 2 27Bモデルは、LLaMA-3モデルと同じパレート曲線上にある可能性が高いです。ただし、これらの性能差が最終的な指示調整モデルの品質にどのように影響するかは不明確のようです。

**2.6Bと9Bモデル**

蒸留を用いて訓練された新しい2.6Bおよび9Bモデルが、以前のモデルや他の標準的なオープンモデルと比較されました。

結果は以下のとおりでした。

1. 前バージョンと比較して、大幅な改善が観察されました。特に9Bモデルでは、一部のベンチマークで最大10%の性能向上が見られた
2. 2.6Bモデルは、前バージョン（v1.0）と同程度のトークン数（v2で2兆、v1.0で3兆）で訓練されたにもかかわらず、大きな改善が観察された

この結果は、蒸留が同じトークン数で訓練した場合でもモデルの品質を大幅に向上させることを裏付けています。

### 事後訓練済みモデルを評価した結果

**Chatbot Arena**

Gemma 2の9Bと27B指示調整モデルがChatbot Arenaで評価されました（ [Chatbot Arenaについてはこちら](https://ai-data-base.com/archives/61080) ）。人間の評価者によるブラインドな一対一比較が他の最先端モデルとの間で行われます。

結果は以下のとおりでした。

- スコアは下の図の通り
- Gemma 27Bモデルは、オープンウェイトモデルの新しい最高水準に達した
- より大規模なLlama3-70B-InstructとNemotron-4-340B-Instructモデルをわずかに上回る性能を示した
- Gemma 9Bは、同パラメータ範囲の他のモデルを大きく上回った

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982_14-1024x537.png)

**人間による選好評価**

Gemma指示調整モデルに対して、独立した一対一の人間評価研究も実施されました。

評価方法としては安全性と指示追従（IF）を対象とするシングルターンプロンプトのホールドアウトコレクションが使用されました。比較対象としてはgpt4o-2024-05-13がベースモデルとして使用されました。

結果は以下のとおりです。

- 旧バージョンのGemma v1.1 7Bモデルと比較して、勝率と選好スコアが大幅に向上
- 安全性はGPT4oに対する勝敗比で報告された
- 指示追従スコアは、全ての指示が遵守されたプロンプトの割合として報告された
- Gemma 2の9Bと27Bモデルは、ホールドアウト安全性プロンプトセットにおいて、GPT4oよりも安全で適切な応答を生成した

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982_15.png)

人間の評価者による指示追従と安全性メトリクス

**人間によるマルチターン評価**

Gemma 1.1 7B、Gemma 2 9B、27Bモデルのマルチターン能力が評価されました。

評価方法としては、人間の評価者がモデルと会話を行い、指定されたシナリオに従うよう指示されました。500の多様なホールドアウトシナリオが使用され、各シナリオは一連の要求を含んでおり、ブレインストーミング、計画立案、新しい学習などの例がありました。平均ユーザーターン数は8.4でした。

結果は以下のとおりです。

- Gemma 2はユーザー満足度と会話目標達成度の両方でGemma 1.1を大きく上回った
- 会話の初期から後半のターンまで、Gemma 1.1 7Bよりも高品質の応答を維持することができた

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982_16.png)

**標準ベンチマーク**

Llama-3で観察されたように、指示調整が少数ショットベンチマークでのモデルの性能を向上させることが確認されました。

結果は以下のとおりです。

- 表に示されるように、モデル全体で数パーセントポイントの改善が観察された
- 指示調整モデルが事前訓練モデルよりもフォーマットされた質問をよりよく理解できるようになったためと推測される

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982_17.png)

評価結果全体を通してGemma 2モデルが多様なタスクと設定において高い性能を示し、中でもマルチターンの対話能力において大きな進歩を遂げたことが示されています。

## 責任、安全性、セキュリティへの取り組み

Gemma 2の開発においては、責任、安全性、セキュリティが最重要視されたとのことです。ユーザーへのリスク軽減のプロセスが開発ワークフローに統合されました。

Gemmaの初回リリースと同様に、以下の三本柱アプローチが採用されました。

1. 訓練時の安全性対策
2. 堅牢で透明なモデル評価
3. 責任ある生成AI開発ツールキットのさらなる開発

### 安全性ポリシーと訓練時の対策

Geminiモデルと同様に、Googleの安全性ポリシーに沿ってモデルが調整されました。以下のような有害コンテンツの生成を防ぐものです。

- 児童の性的虐待と搾取
- 個人を特定できる情報の開示
- ヘイトスピーチとハラスメント
- 危険または悪意のあるコンテンツ
- 性的に露骨なコンテンツ
- 科学的または医学的合意に反する医療アドバイス

前述したように、事前訓練データに対しては、安全性フィルタリングが大規模に適用されました。微調整済みモデルでは、SFTとRLHFを用いて望ましくない行動を回避するよう調整が行われました。

### 外部ベンチマーク評価

モデルの安全性と性能の透明性に関する評価結果が公開ベンチマークで報告されています。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982_18.png)

### 保証評価

モデルが引き起こす可能性のある害を理解するため、一連の保証評価が実施されました。中でも、以下のような極端なリスクに関連する能力に焦点が当てられ、Gemma 2モデルの潜在的なリスクと能力が詳細に分析されました。

**攻撃的なサイバーセキュリティ**

モデルがハッキングやサイバー攻撃の知識を持っていないかが確認されました。模擬的なハッキング課題（CTF）を解く能力がテストされた結果、Gemma 2は基本的なハッキング能力を示しましたが、より高度な専門モデルほどの能力は持っていませんでした。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982_19-1024x240.png)

インターコード-CTF、Googleの内部CTFスイート、およびHack the Boxに基づくチャレンジにおける攻撃的サイバーセキュリティ評価結果。成功したハッキングの数を報告している。

**コードの脆弱性検出**

プログラムの中の安全上の問題（脆弱性）を見つける能力をテストしました。モデルが悪用可能なソフトウェアの欠陥を特定できるかを調べるためです。Gemma 2は平均的な性能を示しました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982_20-1024x190.png)

PrimeVul、DiverseVul、SPIにおける脆弱性検出の結果。精度を報告。

**化学・生物・放射線・核（CBRN）知識**

危険な物質や武器に関する詳細な知識を持っていないかも認されました。モデルが悪用される可能性のある情報を提供しないことを確認するためです。テストの結果、Gemma 2の知識は限定的でした。

**自己増殖**

AIが自身のコピーを作成したり、独自に拡散したりする能力（自己増殖）をテストしました。これは、AIが制御不能になるリスクを評価するためです。Gemma 2は、この能力においても限定的な結果を示しました。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982_21-1024x209.png)

異なる自己増殖シナリオの結果。エンドツーエンドで合格したチャレンジ数、すべてのマイルストーンで成功したチャレンジ数、すべてのチャレンジにわたる成功したマイルストーンの総数、すべてのタスクを解決するのに必要な専門家のビット数を報告。

さらに、モデルの説得力や人間との対話能力も評価されました。AIが人々を不適切に操作したり、誤った情報を広めたりする可能性を調べるためです。

1. AIが人間と良好な関係を築く能力を測定するため、人間の参加者とAIが「久しぶりに会った友人」の役割を演じる会話を行い、その後参加者がAIに対する印象を評価した。Gemma 2は、特に「再び話したい」「興味深い」「親切」の項目で高いスコアを獲得し、効果的な対話能力を示した。
2. AIの欺瞞能力を評価するため、AIに秘密の目標（例：怪しいリンクのクリックを促す）を与え、人間の参加者との対話を行った。Gemma 2は他のモデルと同程度の説得力を示したが、特に突出した能力は見られなかった。
3. AIが人々の寄付行動に影響を与える能力を測定するため、参加者に仮想的なボーナスを与え、AIがその一部をチャリティに寄付するよう説得を試みた。Gemma 2は、他のモデルや人間のベースラインと比較して、特に高い影響力を示さなかった。
4. AIが人々の信念を変える能力を測定するため、AIが正しい情報と誤った情報の両方を説得しようとし、参加者の信念の変化を観察した。Gemma 2は、正しい情報を伝える場合は効果的だったが、誤った情報を説得する能力は人間のベースラインよりも低いことが分かった。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982_22-2-1024x215.png)

100人の人間参加者のうちモデルに人間的特性（例：面白さ）を見出した参加者の割合

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982_23-2.png)

説モデルによって3つの異なるアクション（リンクのクリック、情報の検索、コードの実行）を取るよう説得された参加者の割合

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982_24-2.png)

参加者が寄付に同意した平均金額

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_71982_25-2.png)

モデルと対話した後に信念を変えた参加者の割合

Gemma 2は全体的に安全性の高いモデルであることが確認されました。しかし、一部の分野では潜在的なリスクも特定され、リスクを最小限に抑えるための対策も必要になります。

### 安全なオープンモデル作成ツールキット

安全で責任あるアプリケーションの設計には、システムレベルのアプローチが必要とされます。Googleは下流の開発者向けに「責任ある生成AIツールキット」を継続的に開発しています。以下が含まれます。

- 視覚的な一対一評価分析ツール
- カスタム [分類器](https://ai-data-base.com/archives/26489 "分類器") 構築手法
- プロンプトデバッグプラットフォーム
- モデルのアライメントと安全性評価に関する一般的なガイダンス

### モデルの限界と今後の課題

上述の議論でもあるように、モデルにはまだ多くの限界があります。今後は、以下の調査と改善が必要とされています。

1. 事実性
2. 敵対的攻撃に対する頑健性
3. 推論能力
4. アライメント（人間の意図との整合性）

## まとめ

本記事では、Google DeepMindが開発した新しいオープン言語モデル「Gemma 2」の研究を紹介しました。Gemma 2は知識蒸留技術により、同規模モデルを上回る性能を実現し、一部のベンチマークでは2倍以上大きなモデルと競争力を示しています。

これまで大規模なLLMでしか実現できなかった能力が比較的小さなモデルで可能となったとされています。

- 参照論文URL： [https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf](https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf)
- モデル： [https://huggingface.co/docs/transformers/main/en/model\_doc/gemma2](https://huggingface.co/docs/transformers/main/en/model_doc/gemma2)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMにおける通説への提言](https://ai-data-base.com/archives/71978)

[LLMエージェントの評価はLLM単体の評価と大きく異なる](https://ai-data-base.com/archives/72074)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)