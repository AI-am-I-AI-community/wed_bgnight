---
title: "包括的なRAG評価ベンチマーク『CRAG』Metaなどが開発"
source: "https://ai-data-base.com/archives/70850"
author:
  - "[[AIDB Research]]"
published: 2024-06-14
created: 2025-06-13
description: "Metaなどの研究者らは、RAGシステムが多様な質問応答タスクにどの程度正確に対応できるかを評価するためのベンチマーク『CRAG』を作りました。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

Metaなどの研究者らは、RAGシステムが多様な質問応答タスクにどの程度正確に対応できるかを評価するためのベンチマーク『CRAG』を作りました。  
データマイニングや知識発見の分野で毎年開催される有名な国際コンペティションKDD Cup 2024で使用されています。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70850-1024x576.jpg)

**参照論文情報**

- タイトル：CRAG — Comprehensive RAG Benchmark
- 著者：Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong, Brian Moran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Eting Yuan, Hanwen Zha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah, Rakesh Wanga, Anuj Kumar, Wen-tau Yih, Xin Luna Dong
- 所属：Meta Reality Labs, FAIR Meta, HKUST, HKUST (GZ)

## 背景

LLMは事実に基づかない回答を生成してしまうことがある問題が指摘されています。例えばGPT-4を用いた実験では、急速に変化する事実に関する質問の正答率が15%以下であったことが報告されています。また、静的な事実であっても、マイナーな知識に関する質問の正答率は35%以下だったと言います。そのため、幻覚的な回答を行わず、信頼性の高い質問応答システムを構築することが急務となっています。

この問題を解決するアプローチの1つとして、検索拡張生成（RAG：Retrieval-Augmented Generation）が注目を集めています。RAGは、質問に対して外部のソースから関連情報を検索し、その情報を活用して回答を生成する手法です。しかし、RAGにも下記のような課題が残されています。

- 関連情報の選択精度を向上させる
- 質問応答の遅延時間の削減する
- 複雑な質問に答える

これまでのところ、RAGの包括的なベンチマークが存在しませんでした。通常の質問応答のベンチマークは、RAGが直面している課題をカバーしていません。また、LLMやRAGの特定の能力にのみ焦点を当て、数百程度のクエリしか含んでいないものもあります。

そこで今回研究者らは、RAGの研究開発を推進するための包括的なベンチマーク「CRAG」を構築しました。現実のユースケースを最もよく反映し、一般的なユースケースだけでなく複雑で高度なユースケースも含み、さまざまなタイプの課題で評価できるとされています。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70850_2-1024x252.png)

既存の質問応答ベンチマークとCRAGの比較

## CRAGの評価タスク

### そもそもRAGとは

RAG質問応答システムは、入力として質問を受け取り、外部ソースから検索された情報やモデル内部の知識に基づいてLLMによる回答が生成されます。生成された回答は、幻覚的な内容を追加することなく、質問に答えるための有用な情報を提供するものでなければなりません。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70850_1-1024x281.png)

RAGを使わないLLMによるQAと、RAGを使うLLMによるQAの比較

### CRAGにおける3つのタスク

CRAGにおいては、質問応答の拡張に利用可能な外部データが異なる3つのタスクが設計されました。各タスクは同じ質問応答のペアを共有しています。

以下、タスク1を拡張しているものがタスク2、タスク2を拡張しているものがタスク3となっています。

#### タスク1：検索要約

RAGシステムの回答生成能力をテストするタスクです。各質問に対して最大5つのウェブページが提供されます。（各ウェブページは関連性が高い可能性もありますが、保証はされていません）

#### タスク2：ナレッジグラフとウェブ検索による拡張

RAGシステムが構造化データをどの程度うまく検索できるか、また、ウェブとナレッジグラフという異なるソースの情報をどの程度うまく組み合わせられるかが評価されます。

タスク1で提供されるウェブページに加えて、擬似的なナレッジグラフにアクセスするための専用のAPIが用意されます。ナレッジグラフには、質問に関連する構造化されたデータが含まれていますが、質問の答えがある場合とない場合があります。

APIは質問から抽出されたパラメータを受け取り、それを使ってナレッジグラフから該当するデータを取得し、回答の生成に役立てます。

※ナレッジグラフとは何か、という説明は後述します。

#### タスク3：エンドツーエンドのRAG

タスク3では、タスク2と同じくウェブ検索結果とナレッジグラフ用のAPIの両方が提供されますが、ウェブページの数が5ページから50ページに増えています。

より多くのウェブページが与えられることで、質問に答えるのに必要な情報が含まれている可能性が高くなる一方で、ノイズも増える可能性があります。

そのため、タスク3ではRAGシステムが大量の検索結果をどのように適切に順位付けするかも評価のポイントになります。

## CRAGのデータセットに含まれるもの

データセットは質問応答のペアと検索対象のコンテンツの2つの部分から構成されています。以下、それぞれの部分について説明します。

### 1\. 質問応答ペア

下記表のように、金融、スポーツ、音楽、映画、オープンドメインの5つのドメインと、8種類の質問タイプを対象としています（全て英語）。質問応答ペアは、基盤となるナレッジグラフとウェブコンテンツの両方から構築されました。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70850_3-1024x359.png)

上の表からCRAGの質問タイプの定義を整理すると以下のようになります。

**Simple（シンプル）タイプ  
**人物の生年月日や本の著者など、時間の経過とともに変化しにくい単純な事実を尋ねる質問。

**Simple w. Condition（条件付きシンプル）タイプ  
**ある日の株価や、特定のジャンルにおける監督の最近の映画など、何らかの条件が付いた単純な事実を尋ねる質問。

**Set（セット）タイプ  
**「南半球の大陸は何ですか？」など、答えとしてエンティティやオブジェクトのセットを期待する質問。

**Comparison（比較）タイプ**  
「アデルとエド・シーランでは、どちらが先に活動を始めましたか？」など、2つのエンティティを比較する質問。

**Aggregation（集約）タイプ  
**「メリル・ストリープはオスカー賞を何回受賞しましたか？」など、検索結果の集約が必要な質問。

**Multi-hop（複数ステップ）タイプ  
**「アン・リーの最新作に出演した俳優は誰ですか？」など、答えを導くために複数の情報をつなぎ合わせる必要がある質問。

**Post-processing heavy（後処理が重要）タイプ  
**「サーグッド・マーシャルは最高裁判事として何日間務めましたか？」など、答えを得るために検索した情報の推論や処理が必要な質問。

**False Premise（誤った前提）タイプ  
**「テイラー・スウィフトがポップに移行する前にリリースしたラップアルバムの名前は何ですか？」（テイラー・スウィフトはまだラップアルバムをリリースしていない）など、誤った前提や仮定を含む質問。

### 2\. ナレッジグラフから構築された質問応答ペア

まずナレッジグラフとは、現実世界の物事やそれらの関係を表現したグラフ構造のデータベースのことです。例えば、「東京都」や「日本」といった地名、「安倍晋三」や「岸田文雄」といった人物名などがナレッジグラフ内の「エンティティ」に相当します。これらのエンティティ間の関係性、例えば「東京都」は「日本」の「首都」である、「安倍晋三」と「岸田文雄」は「同じ政党に所属」しているなどが、ナレッジグラフ内の「リレーション」として表現されます。

このようなナレッジグラフを利用して、質問応答ペアが以下の手順で作成されました。

1. 公開データからエンティティ（人物名、地名、映画タイトルなど）を収集
2. 選んだエンティティのタイプとリレーションに基づいて、600以上の質問テンプレートを作成（例：「\[人物名\]の出身地は？」）
3. ナレッジグラフから人気度の異なるエンティティ（有名なものから無名なものまで）を抽出し、テンプレートに当てはめて質問と回答のペアを生成

### 3\. ウェブコンテンツから構築された質問応答ペア

こちらは以下の手順で収集されました。

1. アノテーター（作業者）が、ユーザーが尋ねそうな質問を考えて列挙（例：「2023年の人気アクション映画は？」）
2. その質問に対応するウェブ検索結果を確認し、質問応答ペアを作成

以上の方法で、2,425のウェブベースの質問と1,984のナレッジグラフベースの質問が収集されました。ナレッジグラフベースの質問のうち、661問は人気のエンティティ（ヘッド）、658問はそこそこ人気のエンティティ（トルソ）、665問は無名に近いエンティティ（テール）に関するものでした。

下記2つの表は、収集された質問を様々な観点で分類し、それぞれのカテゴリーに何問ずつ質問があるかを示しています。各カテゴリーのサイズは、ほとんどの場合、統計的に十分な精度でデータの特徴を反映できるものとなっています。例えば、金融分野では他の分野よりリアルタイム性の高い質問が多いといった傾向が見て取れます。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70850_5-1024x207.png)

手動で決定された、各ドメインのダイナミズムカテゴリごとの質問数と割合

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70850_6-1024x307.png)

手動で決定された、各質問タイプの質問数と割合

### 4\. 検索対象のコンテンツ

CRAGでは、実際の検索環境でのRAGをシミュレートするために、ウェブ検索とナレッジグラフ検索の2種類のコンテンツが含まれています。

#### ウェブ検索結果

各質問に対して、質問テキストを検索クエリとして使用し、実際の検索エンジンであるBrave Search APIから最大50のHTMLページを取得しました。下記の表に例を示します。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70850_11-1024x268.png)

ウェブ検索のリコール（50ページ）は、ヒューリスティックベースの方法で推定されました。まず、正解のURLが50ページの中に含まれているかどうかを確認し、含まれていない場合は、正解の事実がページのスニペットまたはコンテンツに含まれているかどうかを検索しました。推定されたリコールは、Web Questionsで84%、KG Questionsで63%でした。これは、KG Questionsのトルソとテールのエンティティが返された50ページに含まれていない可能性があるという直感と一致しています。

#### モックナレッジグラフ

質問の生成に使用された公開ナレッジグラフデータ、同じタイプからランダムに選択されたエンティティ、および類似した名前を持つ「ハードネガティブ」エンティティ（例えば、「オペラ座の怪人」に対する「怪人」）を含むモックナレッジグラフが作成されました。

#### モックAPI

モックナレッジグラフでの構造化検索をサポートするために、今回、事前定義されたパラメータを持つモックAPIが作成されました。例えば、株価を尋ねるクエリに対して、モックAPIの例はget\_price\_history(ticker)の形式となります。

最終的に、得られたデータには220,000のウェブページ、260万のエンティティを含むナレッジグラフ、38のモックAPIが含まれています。

## メトリクスと評価方法

### メトリクス

RAGシステムの性能評価には、スコアリング方式が採用されています。評価セット内の各質問に対して、回答が以下の基準に従って perfect、acceptable、missing、incorrectの4つにラベル付けされます。

- Perfect：ユーザーの質問に正しく答え、幻覚的なコンテンツを含まない回答。
- Acceptable：ユーザーの質問に有用な回答を提供するが、回答の有用性を損なわない軽微なエラーを含む可能性がある。
- Missing：「わかりません」、「申し訳ありませんが、見つけられません…」、空の回答などのシステムエラー、またはシステムからの元の質問の明確化要求。
- Incorrect：ユーザーの質問に答えるための間違った情報や無関係な情報を提供する回答。

次に、perfect、acceptable、missing、incorrectの回答にそれぞれ1、0.5、0、-1のスコアを割り当てるスコアリング方式Score\_hが用いられます。ここでは、幻覚的な回答にペナルティを課し、不正解よりも欠損回答を優先しています。評価セット内のすべての例から平均スコアを計算し、RAGシステムの最終スコアとします。

### 評価方法

過去の研究と同様に、人手による評価（human-eval）とモデルベースの自動評価（auto-eval）の両方が採用されています。

#### 人手評価（human-eval）

各回答が次のいずれかに分類されます。各分類の説明は前述の通りです。

- perfect
- acceptable
- missing
- incorrect

#### 自動評価（auto-eval）

perfectとacceptableを統合したものをaccurate（正確）とし、3つのスコアを持つスコアリングScore\_aを使用します。

- accurate：1
- incorrect：-1
- missing：0

自動評価は以下の2段階方法がとられます。

1. 正解との一致（回答が正解と完全に一致する場合はaccurateとみなす）
2. LLMによる判定（一致しない場合は、LLMを使用してaccurate、incorrect、またはmissingを判定する）

なお自己選好の問題を避けるために、下記2つのモデルが評価器として使用されます。

- ChatGPT（gpt-3.5-turbo）
- Llama 3（llama-3-70B-instruct）

そして各RAGシステムについて、以下の割合と平均スコアが報告されます。

- accurate
- hallucination（回答が誤っている）
- missing

オフライン実験の結果、この2段階の方法がhuman-evalと比較して以下の平均 [F1スコア](https://ai-data-base.com/archives/26112 "F1スコア（F値）") を達成しました。

- ChatGPTで94.7%
- Llama 3で98.9%

データはランダムに検証用、公開テスト用、非公開テスト用に30%、30%、40%の割合で分割され、KDD Cup Challengeの検証用と公開テスト用のセットが公開されました。

※CRAGはKDD Cup 2024のために設計された評価用データセットであり、KDD Cup 2024の目的は、RAGシステムの研究を促進することです。

## 様々なRAGソリューションの実験結果

### 単純なRAGソリューション

#### 実験設定

まずは、CRAGの公開テストセット（1,335問）を使って、言語モデル（LLM）のみのシステムを評価しました。  
使用したLLMは以下の通りです。

- Llama 2 Chat (llama-2-7b-chatとllama-2-70b-chat)
- Llama 3 Instruct (llama-3-8B-instructとllama-3-70B-instruct)
- GPT-4 Turbo

評価では、以下の3つのタスクを設定しました。

- タスク1（ウェブ検索のみ）：Llama 2 Chatは2,000トークン、Llama 3 InstructとGPT-4 Turboは4,000トークンの固定長のウェブ文脈を使用。
- タスク2と3（ウェブ検索とナレッジグラフ）：タスク1に加えて、Llama 2 Chatは1,000トークン、Llama 3 InstructとGPT-4 Turboは2,000トークンの固定長のナレッジグラフ文脈を使用。関連する質問のエンティティは、llama-3-8B-instructを使って抽出。

### 結果と考察

下の表は、2つの自動評価器（ChatGPTとLlama 3）から得られた平均評価スコアを示しており、CRAGベンチマークが自明ではないことを示しています。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70850_7-1024x355.png)

まず、最も性能の良かったLLMのみのシステム（GPT-4 Turbo）でも、 [正解率](https://ai-data-base.com/archives/25930 "正解率") は34%、スコアは20%に留まり、改善の余地が大きいことがわかりました。

また単純なRAGシステムでは、最大で正解率44%を達成し、追加情報が回答の精度向上に役立つことが示されました。  
どのRAGシステムもスコア（≠正解率）が20%を超えなかったのは、無関係な検索結果から生成される誤った情報（幻覚）が増えたためです。検索ノイズに惑わされずに検索結果を賢く使うことが課題です。

タスク2のスコアがタスク1より高かったのは、ナレッジグラフの知識が正解率向上に寄与したためです。ただし改善は限定的で、ナレッジグラフをもっと活用することが2つ目の課題です。

最後に、タスク3のスコアがタスク2より高かったのは、検索結果のランキングと関連する検索結果の割合（検索リコール）が改善されたためです。RAGシステムにおいて検索ランキングが重要であることを示しています。

次に次の図は、ドメイン、事実の変化の速さ（ダイナミズム）、エンティティの人気度、質問のタイプという4つの観点から、自動評価のスコアを示しています。この結果から、CRAGベンチマークで、RAGシステムの性能について深い洞察が得られることがわかります。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70850_8-1024x515.png)

難しい質問の特徴は以下の通りでした。

- 金融やスポーツに関する質問
- リアルタイムや速く変化する事実に関する質問
- あまり知られていないエンティティ（テールエンティティ）に関する質問
- 答えが集合になる質問、追加の処理が必要な質問、誤った前提を含む質問 これらの質問では、RAGシステムのスコアが特に低くなっています。

またエンティティの人気度に着目すると、GPT-4 Turboのスコアは、よく知られたエンティティ（head、21%）、まあまあ知られたエンティティ（torso、11%）、あまり知られていないエンティティ（tail、8%）の順に低下しています。これは過去の研究結果と一致しています。  
一方、GPT-4 Turboを使った単純なRAGシステムは、torso（+7%）とtail（+6%）のエンティティに関する質問の正解率を改善しましたが、head（-4%）については逆に悪化させました。

さらに、この研究の目的はLLMの性能を比較することではありませんが、質問の種類によって、LLMの得意不得意があることがわかります。  
例えば、Llama 3 70B Instructを使ったシステムは、GPT-4 Turboを使ったシステムよりも全体的なスコアは低いものの、単純な質問や比較の質問については同等かやや高いスコアを出しています。  
逆に、集合を答えとする質問や追加の処理が必要な質問については、Llama 3 70B Instructのスコアが大幅に低くなっており、推論能力に課題があることが示唆されています。

### 最先端の商用RAGシステムの評価

CRAGの公開テストセットを使って、業界で最先端の商用RAGシステムである以下4つが評価されました。最新のLLMと検索エンジンを組み合わせたものです。

- Copilot Pro
- Gemini Advanced
- ChatGPT Plus
- Perplexity.ai

評価手順は以下の通りです。

1. CRAG質問をシステムに入力
2. 回答を収集
3. 人手で回答を採点

さらに、実際の利用状況に近づけるために、質問には重みが付けられました。各ドメイン内で質問はサブドメイン（例：現在の試合のスコア、スポーツチーム）に分類され、ユーザーの利用データからサブドメインの重要度が算出されました。そして、その重要度がCRAGの各質問に適用され、すべてのドメインで平均をとることで、ユーザー体験を反映したスコアが下の表に示されています。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70850_9-1024x308.png)

また下の図と合わせて、以下のことがわかります。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70850_10-1024x502.png)

CRAGベンチマークを使うことで、商用RAGシステムの改善点が明らかになりました。最先端のシステムは、単純なシステムよりもはるかに高いスコア（最高51%）を達成しましたが、重み付けした幻覚率（誤った回答率）は17〜25%と高く、回答はまだ信頼できるレベルではありません。ただし、最先端のシステムと単純なシステムのスコアは、検索対象が異なるため完全には比較できず、また、前者は自動評価、後者は人手評価を使用していることに注意が必要です。

単純なシステムで難しかった質問は、最先端のシステムでも依然として難しいままです。リアルタイムや急速に変化する質問、あまり知られていないエンティティに関する質問では、検索ノイズの処理が必要であり、複数ステップの推論や追加の処理が必要な質問では、推論能力の改善が求められます。

3番目に優れたシステムは、2番目のシステムよりわずかに高い正解率（73% vs 70%）を示しましたが、幻覚率ははるかに高くなっています（25.1% vs 16.6%）。このことから、自信を持って答えられない場合は「わからない」と答えることが重要であることがわかります。

一方で、集合を答える質問や誤った前提の質問に対するスコアは、最先端のシステムで大幅に改善されました。

## まとめ

本記事では、RAGの研究を推進するために設計された、包括的なベンチマークCRAGを提案した研究を紹介しました。細な実証実験を通じてCRAGが既存のRAGソリューションのギャップを明らかにすることが示されました。

CRAGは、実世界のシナリオを反映した多様な質問を含んでおり、RAGシステムの実用性を評価するための有用なベンチマークになると考えられています。また、継続的なアップデートも期待されています。

- 参照論文URL： [https://arxiv.org/abs/2406.04744](https://arxiv.org/abs/2406.04744)
- KDD Cup 2024： [https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024](https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[人とLLMの実際のチャット履歴から抽出した1,024のリアルなタスクでClaude 3などを評価した結果](https://ai-data-base.com/archives/70812)

[『プロンプトレポート』OpenAIなどが作成した調査報告書　〜その1　重要な用語と各種プロンプト手法〜](https://ai-data-base.com/archives/70953)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)