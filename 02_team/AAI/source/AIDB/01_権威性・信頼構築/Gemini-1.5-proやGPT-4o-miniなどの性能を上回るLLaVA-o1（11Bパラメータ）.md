---
title: "Gemini-1.5-proやGPT-4o-miniなどの性能を上回るLLaVA-o1（11Bパラメータ）"
source: "https://ai-data-base.com/archives/79215"
author:
  - "[[AIDB Research]]"
published: 2024-11-26
created: 2025-06-13
description: "本記事では、視覚と言語を組み合わせたマルチモーダルLLMの推論能力を大きく向上させた新しい研究を紹介します。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、視覚と言語を組み合わせたマルチモーダルLLMの推論能力を大きく向上させた新しい研究を紹介します。

これまでの視覚言語モデルは一般的に論理的な推論を苦手としており、また推論過程でエラーを起こしやすいという問題を抱えていました。そこで研究チームは、人間のように段階的に考えを組み立てていく新しいアプローチを開発し、その有効性を実証しました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_79215-1024x576.jpg)

**参照論文情報**

- タイトル：LLaVA-o1: Let Vision Language Models Reason Step-by-Step
- 著者：Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, Li Yuan
- 所属：Peking University, Tsinghua University, Peng Cheng Laboratory, Alibaba DAMO Academy, Lehigh University

## 背景

視覚は世界を理解し認知能力を拡張するために言語と同様に重要な要素とされています。そのため、言語と視覚を統合しながら推論するマルチモーダルモデルの開発は重要な課題とされています。

通常、視覚言語モデル（VLM）は論理的推論を必要とするタスクは得意としていません。Chain-of-Thought（ステップバイステップの思考）を導入すると性能は向上するものの、多くのVLMは依然として推論過程でエラーや幻覚出力（事実とは異なる回答）を生成するという課題を抱えています。

研究チームの分析によると、上記の問題の主な原因は、既存のVLMの推論プロセスが十分に構造化されていないことにあるようです。

推論プロセスの構造化に成功している例といえばOpenAI o1です。しかしo1の技術的詳細はブラックボックスのままです。

そこで今回研究者らは、VLMが自律的にステップバイステップの推論を行う能力を向上させるアプローチを新たに考えることにしました。そうして生まれたのがLLaVA-o1と呼ばれる方法論です。

LLaVA-o1は特定の単一モデルを呼称するものではなく、ベースモデルをトレーニングするフレームワークそのものです。なおLlama-3.2VをベースモデルとしたLLaVA-o1モデルは実際に開発されました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_79215_1.png)

LLaVA-o1が、より大規模なオープンソースモデルや一部のクローズドソースモデルを上回る性能を示すグラフ

## 提案手法

「人間のように順序立てて考える」ことで複雑な問題を解くのが本手法の肝です。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_79215_2-1024x627.jpg)

ベースモデル(Llama-3.2-11B-Vision-Instruct)とLLaVA-o1の推論プロセスの違いを示す2つの問題解決例

### 構造化思考による推論能力の向上を目指すトレーニング

LLaVA-o1における訓練目標は、長く複雑な「思考の連鎖」を扱い、体系的で深い推論を行えるVLMを開発することです。まず、推論は以下の4段階で構成されます。

（１）要約段階  
問題の解釈と、取り組むべき主要な点のまとめを行います。

（２）キャプション段階  
画像が存在する場合に、視覚要素を簡潔に説明します。（画像がない場合はスキップされます。）

（３）推論段階  
論理的な推論を行い、仮の回答を導き出します。

（４）結論段階  
推論段階での結果を統合し、最終的な回答を作成します。

このうち、結論段階の出力のみがユーザーに提示される回答となり、それ以外の3段階はLLaVA-o1の内部的な思考プロセスとして扱われます（隠れ段階）。隠れ段階の出力をどれほど生むかは、ユーザーの要望に応じて調整可能です。例えば、簡潔な回答が欲しい場合は短い結論を、詳細な説明が欲しい場合は詳しい回答を生成します。

各段階は、特別な指示や追加のプロンプトなしに、モデルが自律的に判断して開始します。「要約」「キャプション」「推論」「結論」という4つのタグが用意されており、モデルは必要に応じてこれらのタグを選択し、各段階を起動します。すべての段階は1回の推論で完了します。

#### データ準備とモデル訓練

LLaVA-o1の訓練には、詳細な推論プロセスを含むデータが必要でしたが、既存のVQA（Visual Question Answering）データセットにはそのようなデータが不足しています。そこで、複数の一般的なVQAデータセットからサンプルを収集し、99,000件の画像QAペア（各ペアには1つ以上の質問が含まれる）を含む新しいデータセット（LLaVA-o1-100k）が作成されました。

現時点では、構造化された推論を直接生成できるオープンソースのVLMが存在しないため、GPT-4oを用いて詳細な推論プロセスが生成され、LLaVA-o1-100kデータセットに追加されました。このデータセットは将来的に公開予定です。

（おそらく公開先となる）GitHubレポジトリ： [https://github.com/PKU-YuanGroup/LLaVA-CoT](https://github.com/PKU-YuanGroup/LLaVA-CoT)

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_79215_3.png)

GPT-4oを使用して各ステージ（要約、キャプション、推論、結論）のレスポンスを生成する過程を図示

データセットは、以下のカテゴリーで構成されています。

**一般VQAデータセット**

- ShareGPT4V（GPT-4Vとの対話から得られた、複数ターンの質問応答データ）
- ChartQA（グラフや図表の解釈に特化したデータ）
- A-OKVQA（可視コンテンツだけでなく、外部知識も必要とするデータ）
- DocVQA（テキスト理解を必要とする文書ベースの質問応答データ）
- PISC（社会的関係の理解を問うデータ）
- CLEVR（オブジェクトの特性、空間関係、計数タスクを扱うデータ）

**科学指向VQAデータセット**

- GeoQA+（幾何学的推論を問うデータ）
- AI2D, ScienceQA（科学知識を問うデータ）
- CLEVR-Math（視覚的文脈での算術分析に特化したデータ）

サンプル数の内訳は以下の通りです。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_79215_4.png)

各ベンチマークから選択されたサンプル数の一覧

### ”選択と評価（ビームサーチ）”による推論能力の向上

LLaVA-o1の「訓練後の目標」は、推論時におけるモデルの性能をさらに向上させることです。

そこで段階的な出力形式を活用することで、推論時のスケーリング（より良い回答を効率的に探索するプロセス）が行われました。手順は以下の通りです。

（１）最初のステージ（要約段階。このあとにキャプション段階、推論段階、結論段階がある）  
モデルにN個の異なる回答候補を生成させます。

（２）候補の選択  
生成された候補からランダムに2つを選び、モデル自身にどちらの候補が優れているかを判断させます。優れた方の候補を残し、もう一方を破棄します。

（３）繰り返し  
上記（２）のステップをN-1回繰り返し、最終的に最も優れた1つの候補を選択します。

（４）次のステージへ  
次のステージ（キャプション段階、推論段階、結論段階）についても、（１）～（３）のステップを繰り返します。すべてのステージで最適な候補が選択されるまで、このプロセスを継続します。

もしこのような推論時スケーリングを使用しない場合、たとえモデルが正しい推論ステップを踏んでいたとしても、途中で誤った方向に進んでしまい、最終的に間違った結論に至る恐れがあります。結論段階では特に推測に頼らざるを得ない状況に陥りやすく、不正確な回答を生成してしまうリスクが高まります。

一方、推論時スケーリングを使用すると、モデルは各ステージで最適な候補を選択しながら推論を進めるため、より正確な推論ステップを維持しやすくなります。その結果、最終的な回答の精度が向上し、より信頼性の高い結果を得ることができます。（といった考えのもと実装されました）

## 性能評価実験

開発したLLaVA-o1モデルの性能が評価されました。ベースモデルにLlama-3.2-11B-Vision-Instructが用いられたので、そのまま比較対象とされました。

なお、LLaVA-o1の各要素が性能にどのように貢献しているかを検証するため、以下の3つの観点で実験が行われました。

1. LLaVA-o1の学習に使用した独自のLLaVA-o1-100kデータセットは、既存のデータセットをそのまま使うよりも効果的なのか？
2. 構造化されたタグ（「要約」「キャプション」「推論」「結論」）は、モデルの性能にどのような影響を与えるのか？ タグがなくても、モデルは各段階の応答を区別できるのか？
3. LLaVA-o1は、ベースモデルと比較して、どのような分野で性能が向上しているのか？ 推論能力は本当に向上しているのか？

評価には、6つの一般的なマルチモーダルベンチマークが使用されました。役割と名称を下に列挙します。

**汎用的な視覚的質問応答能力を評価するベンチマーク**

- MMStar
- MMBench V1.1
- MMVet

**数学的・科学的推論能力を評価するベンチマーク**

- MathVista
- AI2D

**言語的な誤認識や視覚的錯覚への対処能力を評価するベンチマーク**

- HallusionBench

なおMMBenchについてはV1.1バージョンのテストセット、MathVistaはtestminiセットが使用され、その他のデータセットはそれぞれの公式テストセットが使用されました。  
評価の公平性と再現性を確保するため、すべての実験はオープンソースの評価ツールキットVLMEvalKitを用いて行われました。

### ベンチマーク結果

わずか10万件のデータセット（LLaVA-o1-100k）で学習したにも関わらず、LLaVA-o1はベースモデルと比較して顕著な性能向上を達成しました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_79215_7-1024x188.png)

異なるモデルのベンチマーク結果の比較

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_79215_8-1024x199.png)

MMStarベンチマークにおける異なるスキル領域での性能比較

#### LLAVA-o1-100kデータセットの効果

LLaVA-o1-100kデータセットで学習されたモデルと、既存のデータセットをそのまま使用して学習されたモデルが比較されたところ、以下の点が明らかになりました。

- 既存のデータセットで学習されたモデルも、ベースモデルからある程度の性能向上が見られる。
- しかし、LLaVA-o1-100kで学習されたモデルと比較すると、平均性能は大幅に低い。
- 詳細な応答が求められるMMVetベンチマークでは、既存のデータセットで学習されたモデルはベースモデルよりも性能が低下する。

よって、LLaVA-o1-100kデータセットの多段階フォーマットが、高度な推論能力を持つモデルの学習に効果的であることが示唆されています。

#### 構造化されたタグの効果

次に、構造化されたタグ（「要約」「キャプション」「推論」「結論」）の効果を検証するため、タグを削除してLLaVA-o1-100kデータセットで学習されたモデルと、タグありで学習されたLLaVA-o1が比較されました。その結果、タグを削除するとモデルの性能が著しく低下することが明らかになりました。

この結果から、構造化されたタグが推論プロセスを促進し、モデルの性能向上に大きく貢献していることが示唆されました。LLaVA-o1は、構造化された推論とタグを通じて、モデルの推論能力と全体的な性能を向上させることに成功した、初めてのモデルであると言えそうです。

#### どのような分野で性能が向上したのか

汎用的なVQA、数学的推論、科学的VQA、誤認識制御の各タスクにおいて、平均ベンチマークスコアが6.9%向上しました。

## 推論時のスケーリング効果の検証

段階的な選択と評価（ビームサーチ）手法と、従来の手法（best-of-N、センテンスレベルのビームサーチなど）の性能が比較されました。公平な実験のために、同等の計算負荷で行われました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_79215_5-1-1024x418.png)

Best-of-N search、Sentence-level Beam Search、Stage-level Beam Search（提案手法）の3つの手法の違い

実験には、前セクションと同じ6つのベンチマーク（MMStar、MMBench V1.1、MMVet、MathVista、AI2D、HallusionBench）が使用され、再現性を確保するために、すべての手法は [VLMEvalKit](https://github.com/open-compass/VLMEvalKit) を用いて評価されました。

### ベンチマーク結果

まず今回の手法は複雑な推論タスクにおいて、計算負荷を大幅に増やすことなく、高い推論精度を達成することが示されました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_79215_6-1024x211.png)

実際の問題解決例を通じて、ビームサーチの効果を示す図

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_79215_9-1024x182.png)

ベンチマーク結果

さらにMMVetベンチマークにおいて、開発された手法とベースラインとなる推論スケーリング手法の性能が比較されました。best-of-N手法ではN=10、段階的なビームサーチ手法では各段階で4つの候補応答、センテンスレベルのビームサーチでは各文で2つの候補が生成されました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_79215_10.png)

ベースライン手法との比較

比較の結果、best-of-N手法では0.6%の改善にとどまり、センテンスレベルのビームサーチでは1.9%の性能低下が確認されました。詳細な分析の結果、センテンスレベルのビームサーチの性能低下は、文レベルのアプローチでは粒度が細かすぎるため、オープンエンドな質問（選択肢式ではなく自由記述式）に効果的に対応できないことが原因であると推測されました。

一方、段階的なビームサーチ手法では2.6%の性能向上が見られ、段階ベースの探索の優位性が示されました。

### 段階的なビームサーチのスケーリング傾向

段階的なビームサーチ手法の効果をより詳細に検証するため、MMVetベンチマークを使用して異なるビームサイズが評価されました。推論時スケーリングなし（候補が1つ）の場合から始めて、各推論段階で2つ、3つ、4つの候補応答を生成し、それらの選択肢から最良の回答が選択されました。その結果、候補応答の数が増えるにつれて、モデルの性能が一貫して向上することが確認されました。

要するに段階的なビームサーチ手法がスケーラブルであることが示されました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_79215_11.png)

LLaVA-o1のスケーリング

なお今回は計算リソースの制約がありましたが、ビームサイズを増やすことで更なる性能向上が期待されます。

## 最新のVLMとの比較

LLaVA-o1の性能が、高度な推論能力が求められる6つのベンチマークにおいて、他の最新の視覚言語モデル（VLM）（オープンソース・商用両方）と比較されました。

ベンチマークとしては、MMStar、MMBench V1.1、MMVetからそれぞれ派生したカスタムベンチマーク（MMStar-R、MMBench-R、MMVet-R）が使用されました。各ベンチマークでは、単純な知覚や文字認識のみで解答可能なタスクは除外され、複雑な推論を必要とするタスクのみが含まれています。一方、MathVista、AI2D、HallusionBenchは、元から高度な推論に焦点を当てたベンチマークであるため、すべてのタスクがそのまま使用されました。

### 評価結果

評価の結果、LLaVA-o1は以下のような同規模もしくはより大規模なオープンソースモデルと比較して、一貫して優れた性能を示しました。

- InternVL2-8B
- Ovis1.5-Gemma2-9B
- MiniCPM-V2.6-8B
- Llama-3.2-90B-Vision-Instruct
- VILA-1.5-40B

さらに、GPT-4o-miniやGemini-1.5-proといった一部の商用モデルの性能をも上回りました。

なおLLaVA-o1は、推論能力に大きく依存するベンチマークにおいて特に優れた性能を示しました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_79215_12-1024x408.png)

最新のVLMモデルとの性能比較

以上の結果から、モデルの規模や計算リソースだけでなく推論の方法や構造も性能に大きな影響を与えることが示されました。

## モデルの公開について

Llama-3.2V-11BをベースモデルとしたLLaVA-o1は現在（2024年11月25日時点）、「Llama-3.2V-11B-cot」として、Hugging Faceプラットフォームで公開されています。

[https://huggingface.co/Xkev/Llama-3.2V-11B-cot](https://huggingface.co/Xkev/Llama-3.2V-11B-cot)

#### モデル概要

- **モデル名**: Xkev/Llama-3.2V-11B-cot
- **作者**: Guowei Xu（ユーザー名：Xkev）
- **ライセンス**: Apache-2.0
- **ベースモデル**: meta-llama/Llama-3.2-11B-Vision-Instruct
- **言語**: 英語

## まとめ

本記事では、視覚言語モデルに独自の構造化された推論機能を実装したLLaVA-o1の研究を紹介しました。

今回の研究の特徴は、モデルが回答を生成する際に、要約、キャプション、推論、結論という4つの段階を自律的に進めていく点にあります。研究チームは開発のために、LLaVA-o1-100kという独自のデータセットを作成し、これを用いてモデルのトレーニングを行いました。

また、推論時には段階的なビームサーチという新しい手法を導入し、各段階で複数の候補から最適な回答を選択できるようにしました。

評価実験では、わずか10万件のトレーニングデータで、より大規模なオープンソースモデルや一部の商用モデルを上回る性能を示しました。モデルの大きさやデータ量だけでなく、推論プロセスの構造化が重要であることを示唆するものといえます。

今後は、外部の検証機能の追加や [強化学習](https://ai-data-base.com/archives/26125 "強化学習") の活用など、さらなる改善の可能性が残されています。視覚言語モデルの推論能力向上に向けた、一つの重要なアプローチとして注目される研究だと言えるでしょう。

- 参照論文URL： [https://arxiv.org/abs/2411.10440](https://arxiv.org/abs/2411.10440)
- GitHub： [https://github.com/PKU-YuanGroup/LLaVA-CoT](https://github.com/PKU-YuanGroup/LLaVA-CoT)
- モデル： [https://huggingface.co/Xkev/Llama-3.2V-11B-cot](https://huggingface.co/Xkev/Llama-3.2V-11B-cot)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMによるバグ全自動修正に成功するケースと失敗するケースの違い](https://ai-data-base.com/archives/78378)

[OpenAIのo1モデルへの対抗馬　アリババが独自の推論モデル「Marco-o1」を開発　オープンソースで公開](https://ai-data-base.com/archives/79273)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)