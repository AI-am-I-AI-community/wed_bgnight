---
title: "人間のような内省メカニズムをLLMに導入することの効果 Google DeepMindなどが検証"
source: "https://ai-data-base.com/archives/72194"
author:
  - "[[AIDB Research]]"
published: 2024-07-05
created: 2025-06-13
description: "本記事では、LLMエージェントの問題解決能力を向上させる「内省」アプローチの研究を紹介します。タスクの分解と”3つの”内省メカニズムを通じて、エージェントの適応力と一貫性を向上させる手法です。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、LLMエージェントの問題解決能力を向上させる「内省」アプローチの研究を紹介します。タスクの分解と”3つの”内省メカニズムを通じて、エージェントの適応力と一貫性を向上させる手法です。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72194-1024x576.jpg)

**参照論文情報**

- タイトル：Devil’s Advocate: Anticipatory Reflection for LLM Agents
- 著者：Haoyu Wang, Tao Li, Zhiwei Deng, Dan Roth, Yang Li
- 所属：UPenn, Google DeepMind

## 背景

LLMエージェントが複雑なタスクを遂行する能力が注目されています。しかし、予期せぬ状況に直面した際の適応力や一貫性には課題があることが指摘されています。

そこで、タスク実行後の振り返りなどが提案され、過去の成功や失敗から学習し、柔軟な戦略を立てることが試されてきました。しかし、振り返りは通常1つの（仮想的な）エラーを修正するだけであり、効率性に問題があります。

また、計画の頻繁な変更はLLMエージェントに混乱をもたらすことが懸念されています。人間にとっては単なる不便さかもしれませんが、LLMエージェントにとっては方向性を失ったり、停滞したり、さらには失敗の無限ループに陥る可能性があります。

そこで今回研究者らは、一貫性と適応性のバランスを最適化する方法論を提案しています。予期せぬ事態に備えながら計画を実行するための方法です。  
タスクをサブタスクに分解し、行動と結果について内省を行うよう、LLMエージェントに促すのがポイントです。

結果としてタスク達成に必要な試行回数と計画修正回数が大幅に削減され、効率性の向上にもつながることがわかりました。

以下で詳しく見ていきます。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72194_1.png)

予測的反省と通常の反省の概念的な違いを示す図

## 方法論

### 概要

まず、LLMエージェントは与えられたタスクを小さなサブタスクに分解します。人間が複雑な問題を扱う際によく行う作業と似ています。

例えば、「オンラインで商品を購入する」というタスクは、「商品を探す」「カートに入れる」「支払い情報を入力する」などのサブタスクに分けられます。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72194_2.png)

GPT-4によって生成された5つのサブタスクを含む計画の例

LLMエージェントは、サブタスクを順番に実行していきます。タスクを順番に実行する中で、LLMは現在の状況（ウェブページの内容など）を理解し、次に何をすべきか（クリックする、テキストを入力するなど）を決定します。この決定には、今までの行動履歴も考慮されます。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72194_3.png)

各タスク内のサブタスク数に基づくWebArenaタスクの分布

さらに、LLMエージェントには3つの重要な内省を行います。

**（１）行動前の予測** ：次の行動を決める前に、「もしこの行動が間違っていたら？」と自問し、代替案も考えます。

**（２）行動後の評価** ：行動を取った後、その結果が期待通りだったかを確認します。うまくいかなかった場合は、前の状態に戻って別の方法を試みます。

**（３）全体の振り返り** ：すべてのサブタスクが終わった後、タスク全体が完了したかを判断します。完了していない場合は、計画を修正して再挑戦します。

※各内省ステップの詳細は次のセクションで紹介します。

なお、本手法の流れは下のアルゴリズムで表されています。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72194_10-1024x534.png)

実行ステップに沿って見ると以下の通りです。

1. 初期化
	- タスク、初期観察、環境を入力として受け取る
	- 時間、状態、アクション、計画、サブタスク、履歴を初期化
2. メインループ（タスクが完了するまで繰り返し）
	- a. 計画の立案/修正
	- b. アクション実行ループ（スタックが空になるまで）
		- i. スタックからアクションを取り出す
		- ii. 必要に応じてバックトラッキング
		- iii. サブタスクの実行と評価
			- 環境でアクションを実行
			- アクションの結果を評価
			- サブタスク目標との整合性をチェック
		- iv. タスク完了チェック
			- 早期終了の可能性を確認
			- サブタスク完了時は次のサブタスクへ
		- v. 次のアクションの決定
			- 新しいアクションを生成
			- 予測的反省（代替アクションを生成し、スタックに追加）
			- 元のアクションをスタックの一番上に追加
3. タスク完了
	- メインループを終了

上記プロセスを経ることで、LLMエージェントは人間のように柔軟に問題を解決できるようになります。例えば、ウェブサイトの構造が変わっても適応できたり、予期せぬエラーに遭遇しても別の方法を試みたりすることができます。

研究結果を先に伝えると、このアプローチを使用したLLMエージェントは、WebArena（ウェブ上のタスクを模擬した環境）で高い成功率を示しました。多くのタスクを4〜9ステップで完了でき、人間が行う場合に近い成績でした。

### 内省メカニズムの詳細

先述した3つの内省的アクションのメカニズムを具体的に見ていきます。  
なお、いずれも人間が複雑な問題に直面したときに行う思考プロセスを模倣しています。

**（１）行動前の予測的反省**

まずLLMエージェントに、行動を起こす前に「もしこの行動が間違っていたら？」と考えさせます。人間が重要な決定を下す前に考える、いわゆる最悪のシナリオのようなものです。

このステップはユーザーがLLMに「もし最初の計画が間違っていたら、次に何をすべきか？」と問いかけることで実行できます。  
LLMエージェントは、以下2つを行います。

1. 予定している行動が失敗する可能性を考える
2. 複数の代替案を生み出す

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72194_5-1.png)

2022年11月の注文詳細リンクをクリックするというサブタスクを解決するための一つのステップにおける画面観察。

この際、LLMは現在の状況、これまでの行動履歴、最初に考えた行動案を考慮します（そうする仕組みにします）。

例えば、ウェブサイトで特定の情報を探す場合、LLMは複数のリンクのどれをクリックすべきか考えます。最初の選択が間違っていた場合に備えて、他の選択肢も準備しておくのです。

**（２）行動後の評価とやり直し**

次に、行動を起こした後にその結果を評価させます。人間が行動の結果を振り返り、必要に応じて方針を修正するのと同じように。

以下のように指示します。

1. 各行動の後、その行動が目的達成に役立ったかどうかを判断させる
2. もし行動が期待通りの結果をもたらさなかった場合、前の状態に戻り、別の方法を試みさせる

ウェブサイトでの応用例で考えると、LLMにはウェブページのURLを記憶させておき、必要に応じて以前のページに戻させます。ページの内容が変更されている可能性も考慮し、LLMは記憶した要素と新しい要素を対応関係で記憶します。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72194_6-1024x710.jpg)

2022年9月に購入した額縁の色設定を確認するタスクにおけるエージェントの意思決定プロセス

**（３）全体計画の修正**

最後に、LLMエージェントは全体の計画が失敗した場合、ゼロから計画を立て直します。人間で例えると、プロジェクトの途中で大きな問題に直面し、アプローチを根本から見直すような流れです。

以下のステップを踏みます。

1. 全ての試みが失敗した場合、これまでの行動と記録を徹底的に分析させる
2. 分析を基に、新しい計画を立てる
3. 新しい計画ができたら、最初からやり直す

人間のように「考えて」「行動して」「振り返る」というサイクルを繰り返させるという方法論ですね。

## 実験内容

上記のように提案された内省メカニズムが、複雑なタスク解決においてLLMエージェントの一貫性と適応性をどのように向上させるか、実験が行われました。

### 実験環境ベンチマーク

実験環境として採用されたのは [WebArena](https://webarena.dev/) です。人間が注釈付けしたウェブブラウジングタスクのデータセットで、複雑な実世界のインターネット上のアクション実行能力を評価するためのものです。

WebArenaには812のタスクが含まれており、以下の3つのカテゴリーに分類されます。

1. 情報探索タスク
2. サイトナビゲーションおよびコンテンツ＆設定タスク
3. 達成不可能なタスク

そして以下5つの種類のウェブサイトが含まれています。

1. オンラインショッピングサイト
2. ソフトウェア開発サイト
3. ソーシャルフォーラムプラットフォーム
4. 地図サイト
5. Eコマース管理プラットフォーム

WebArenaは視覚的な情報としてスクリーンショットも提供していますが、本研究ではテキストのみが使用されました。

また、LLMエージェントのアクション空間（アクションの選択肢）には、以下があります。

- クリック
- タイプ
- スクロール
- 特定のURLに移動
- 前のページに戻る
- 次のページに進む
- メモを取る（情報探索質問に答えるための有用な抜粋やサマリーを記録）

### ベースライン

ベースライン手法としては、以下の3つのエージェント戦略が採用されました。提案手法とは別個のもので、比較対象です。

1. 計画と逐次的な意思決定（反省なし）　ー [ReWOO](https://arxiv.org/abs/2305.18323) に類似
2. 反省を伴う計画と逐次的な意思決定　ー [AdaPlanner](https://arxiv.org/abs/2305.16653) に類似
3. ツリー検索ベースの計画立案（反省付き）　ー [LATS](https://arxiv.org/abs/2310.04406) に類似

「計画と逐次的な意思決定（反省なし）」は”Plan + Act (w/o reflexion)”として後の実験結果で略称でまとめられています。そして「反省を伴う計画と逐次的な意思決定」は”Plan + Act”としてまとめられます。

なお、全ての手法で、タスクごとのアクション数の上限は30に設定されました。また、公平な比較のために同じプロンプトが使用されました。

### 指標

指標としては以下が採用されました。

1. 成功率（WebArenaの評価指標に従って測定）
2. 試行ごとのアクション数
3. タスクごとの計画修正回数

一部のサイトナビゲーションおよび情報探索タスクでは、exact\_matchメトリクスが使用されました。しかし厳格すぎる場合があるため、手動でのレビューが行われ、同じ内容を表示するURLの違いなどによる誤判定は修正されました。

### 使用モデル

実験には、8kトークンのコンテキストウィンドウを持つGPT-4-0613が使用されました。温度は1.0、max\_tokensは512に設定され、その他のパラメータはデフォルト値が使用されました。

## 実験結果

結果の結果、今回提案された内省駆動型アプローチがウェブ環境におけるLLMエージェントの一貫性と適応性を向上させる効果が実証されました。以下で詳しく見ます。

### 成功率

複数のエピソードにわたる各エージェント構築戦略の成功率が比較された結果は以下の通りです。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72194_7.png)

WebArenaにおける異なるエージェント構築戦略の結果

**予測的反省法（提案手法）**

- 7エピソード後に23.5%の成功率を達成
- 一貫して他の手法を上回る性能を示した

**LATS**

- 22.7%の成功率で2位を記録
- 性能曲線に一貫性の欠如が見られ、5ラウンド目で成功率が低下した

**Plan + Act法**

- 徐々に改善を示し、19.8%の成功率に到達
- ツリー検索ベースのARやLATS法と比較して、依然として大幅に低い結果となりった

LATSの性能曲線における不一致なパターンは、直接 [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") による同質的な生成アクションが原因と考えられます。対照的に、予測的反省法は、内省的なフォローアップ質問によって、より綿密な計画立案と実行が可能になっています。

上記は計画の実行と修正の両方に内省メカニズムを組み込むことの重要性を浮き彫りにしています。一貫性と効率性の向上における内省の重要な役割が強調されました。

### アクション数と計画修正数

下記の表は、異なる手法間で最初と最後の試行におけるアクション平均数が比較されています。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72194_8.png)

WebArenaでタスクを解決する異なるエージェントの軌跡の統計。最初と最後の試行におけるアクション数と計画修正回数を報告

AR（予測的反省法）の特徴として以下が挙げられます。

- 最初の試行では平均6.39アクション
- 最後の試行では平均7.07アクション
- アクション数の増加は、堅牢な学習と適応プロセスを示している

一方、Plan+Act法は以下の特徴があります。

- 最初の試行での平均アクション数は4.01
- アクション数が低いのは、完全な計画実行を行わずに早期段階で停止していることを示唆しています

AR法は、より多くのアクションを効果的に活用して優れた結果を達成しました。その結果、以下の改善が見られました。

- 計画修正回数が45%削減
- 全体的な効率性が向上

全体を通して実験結果は、提案された内省駆動型アプローチの有効性を実証しています。ウェブ環境におけるLLMエージェントの一貫性と適応性が大幅に向上しました。予測的反省メカニズムにより、より綿密な計画立案と実行が可能になり、結果として高い成功率と効率性の改善が達成されました。

## エラー分析

タスク実行時にエージェントの行動から観察されたエラーを見ていきます。

主に2つのポイントがあり、それぞれがエージェントの性能に影響を与えていることが明らかにされています。

### 過去の失敗からの不完全な学習

エージェントが新しい計画を生成する際に、過去の失敗から十分に学習できていないケースが一般的なエラーとして特定されました。

例えば下の図では、Bluetoothスピーカーの返金メッセージを作成する最終ステップにおけるエージェントの行動が示されています。

![](https://ai-data-base.com/wp-content/uploads/2024/07/AIDB_72194_9.png)

2023年2月に購入したBluetoothスピーカーの返金メッセージを作成するタスクの最終ステップにおける画面観察

エージェントは、画面上に1つのテキストボックスしかないにもかかわらず、複数の入力フィールドがあるかのように行動しました。購入日の追加や詳細な説明の提供など、一部の改善は見られましたが、入力プロセス全体の最適化には至りませんでした。

このことから以下が示唆されます。

1. エージェントは、過去の失敗から部分的には学習しているが、完全には学習できていない
2. より強力な推論能力や、より効果的な反省メカニズムが必要とされている

### 逐次的計画の限界

LLMエージェントが、単純な順序立てられたタスクには対応できるものの、より複雑な論理を必要とするタスクでは苦戦することも示されています。

例えば、以下のようなタスクは成功していません。

- 特定の条件（例：平均印刷品質について言及）を満たすレビュアーのリストアップ
- 特定の条件（例：在庫1-3個）を満たす商品のSKU提供
- 特定のユーザーによる特定のサブレディットでの全投稿へのいいね

上記タスクの共通点は、繰り返し作業や条件判断が必要であること、そして単純な順序立てられた手順では解決できないタスクであることです。

このことから、LLMエージェントには、以下のような高度な機能が必要だと示唆されます。

1. メモリ機能（情報の一時保存と再利用）
2. ループ処理（同じ操作の繰り返し）
3. 条件分岐（特定の条件に基づく判断）
4. 関数化（一連の操作をまとめて再利用可能にする）

## まとめ

本記事では、LLMエージェントの問題解決能力を向上させる内省的アプローチの研究を紹介しました。

タスクの分解と3つの内省メカニズム（予測的反省、行動後評価、計画レビュー）を通じて、エージェントの一貫性と適応性を向上させるものです。

WebArenaでの実験結果は、従来手法比3.5%の性能向上と45%の計画修正回数削減を示しました。課題も残されていますが、この研究はAIに人間らしい戦略的思考を与える重要な進展です。将来的に、ウェブ関連タスクの自動化など、実用的な応用が期待されます。

- 参照論文URL： [https://arxiv.org/abs/2405.16334](https://arxiv.org/abs/2405.16334)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[RAGシステムの最適な構築を探る](https://ai-data-base.com/archives/72121)

[LLMに量子化が与える影響とは？日本語を含む多言語でCohereが調査](https://ai-data-base.com/archives/72292)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)