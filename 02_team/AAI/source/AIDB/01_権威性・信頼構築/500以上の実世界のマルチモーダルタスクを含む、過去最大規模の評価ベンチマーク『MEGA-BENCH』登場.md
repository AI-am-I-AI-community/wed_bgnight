---
title: "500以上の実世界のマルチモーダルタスクを含む、過去最大規模の評価ベンチマーク『MEGA-BENCH』登場"
source: "https://ai-data-base.com/archives/74837"
author:
  - "[[AIDB Research]]"
published: 2024-10-21
created: 2025-06-13
description: "本記事では、マルチモーダルAIモデルの能力を包括的に評価する新しいベンチマーク「MEGA-BENCH」を紹介します。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、マルチモーダルAIモデルの能力を包括的に評価する新しいベンチマーク「MEGA-BENCH」を紹介します。

従来の評価方法では捉えきれなかったLLMの多様な能力を、500以上の実世界のタスクを通じて測定することを可能にするベンチマークです。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_74837_top2-1024x576.jpg)

**参照論文情報**

- タイトル：MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks
- 著者：Jiacheng Chen, Tianhao Liang, Sherman Siu, Zhengqing Wang, Kai Wang, Yubo Wang, Yuansheng Ni, Wang Zhu, Ziyan Jiang, Bohan Lyu, Dongfu Jiang, Xuan He, Yuan Liu, Hexiang Hu, Xiang Yue, Wenhu Chen
- 研究機関：Simon Fraser University, University of Waterloo

**本記事の関連研究**

- [複雑なプログラミングタスクに特化したベンチマーク『BigCodeBench』登場　最新モデルでも60%](https://ai-data-base.com/archives/76844)
- [マルチモーダルLLMの高難易度ベンチマーク『MMMU-Pro』で明らかになったこと](https://ai-data-base.com/archives/75326)
- [Appleが「LLMエージェントの評価」に特化したベンチマーク『MMAU』を開発　5領域5能力で測る](https://ai-data-base.com/archives/73656)
- [MMLUをアップデートしたベンチマーク『MMLU-Pro』Phi-3やLlama 3、Claude 3、GPT-4oなどの評価結果](https://ai-data-base.com/archives/70358)

## 背景

最近の言語モデルは徐々に画像や動画などのマルチモーダル情報を理解・解釈するまで発展してきました。

マルチモーダルモデルは登場当初、画像キャプション生成や視覚的質問応答などの標準的なタスクに特化していましたが、最近では適切なプロンプトを与えることで、ウェブナビゲーション、ゲーム、旅行計画作成など、より幅広い実用的なタスクに対応できるようになってきました。

しかし、網羅的な評価方法は不足しています。既存のベンチマークの多くは1つまたは少数の類似したタスクのみをカバーしており、モデルの全体的な能力を評価するには不十分です。

また、多くの既存の評価手法では多肢選択形式の質問に頼っており、モデルの生成能力を適切に評価できていません。さらに、タスクの網羅性が不十分だったり、評価コストが高すぎたりする問題もあります。

こうした背景から、より包括的な評価方法が必要と判断され、MEGA-BENCHの開発に至りました。MEGA-BENCHは500以上の実世界のタスクをカバーし、多様な出力形式に対応しながら、適度な評価コストで幅広いマルチモーダルモデルの能力を総合的に測定することを目指しているベンチマークです。

研究者らは本ベンチマークで最先端モデルの評価も実際に行いました。

以下で詳しく説明します。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_74837_1-1024x795.jpg)

MEGA-BENCHは505のマルチモーダルタスクを含み、多様なデータソース、入出力形式、必要なスキルをカバーしていることを示す図

## MEGA-BENCHの特徴

### 作り方

MEGA-BENCHは、以下のような段階を踏んで作成されました。

（１）準備段階

16人以上の専門家が協力して、まずタスクの大まかな分類が決められました。また、タスクの提出や確認を効率的に行うためのツールが開発されました。

（２）タスクの作成段階

各専門家が20個以上のタスクを作成しました。各タスクには少なくとも15個の例題が含まれ、データの出所や評価方法などの情報も記録されました。

（３）品質管理と改良段階

最新のLLMを使ってタスクの難易度が確認されました。簡単すぎるタスクや難しすぎるタスクは見直されました。最終的に505個のタスクと約8,200の例題を含むベンチマークが完成しました。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_74837_3-1024x233.jpg)

MEGA-BENCHの アノテーション プロセス。最初に「ドラフト」タクソノミーツリーを提案し、アノテーターがツリー構造を徐々に洗練させながら新しいタスクを追加していく。各タスクは多くの例を含み、タスクレベルの指示、例ごとの質問、質問ごとの正解を持つ。

### 評価方法

MEGA-BENCHは、タスクの回答形式が多様なため、それぞれに適した評価方法が用意されました。

（１） [ルールベース](https://ai-data-base.com/archives/26614 "ルールベース") の評価指標

正解が一意に決まるタスクや、ルールで正誤を判断できるタスクに使用されます。40種類以上の評価指標が実装されました。

（２）LLMベースの評価指標

正解が一意に決まらない、オープンエンドなタスクに使用されます。GPT-4oを使って回答の質が評価されました。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_74837_4-1024x587.jpg)

MEGA-BENCHの多様な出力形式の代表的な例と、それに対応するカスタマイズされた評価指標。出力はGPT-4o (OpenAI, 2024a)の実際の応答。頑健な解析を実装して、生の応答から最終的な回答を抽出している。

### 詳細な分析のための「多次元キーワード」の工夫

MEGA-BENCHでは、タスクを様々な観点から分類するために、以下のような多次元のキーワードシステムが導入されました。

1. 入力視覚タイプ（タスクで使用される画像や動画の種類）
2. 入力視覚数（タスクで使用される画像や動画の数）
3. 出力形式（回答の形式（例：選択式、数値、文章など））
4. 必要なスキル（タスクを解くために必要な能力）

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_74837_2-1024x276.jpg)

MEGA-BENCHの4つのキーワード次元とタスクレベルの統計。様々な次元での多様性により、詳細な能力分析が可能。

### 他のベンチマークとの比較

MEGA-BENCHは505の実世界のタスクと8,186の手動で注釈付けされたサンプルを含んでいます。既存の多くのベンチマークと比較して、以下の点で優れています。

- 様々な分野や場面からタスクが集められている
- 画像や動画の種類、回答の形式が多岐にわたる
- 40以上の評価指標が用意されている
- 505という多数のタスクが含まれている

また、単純な選択式の回答だけでなく、様々な形式の回答を評価できる点が特徴です。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_74837_5-1024x647.jpg)

MEGA-BENCHと既存の研究の比較。MEGA-BENCHはデータソース、入出力形式、メトリクス数、タスク数において大きな多様性を持つ。

## 実験

### 評価設定

MEGA-BENCHでは、様々な大規模マルチモーダルモデルが評価されました。評価されたモデルは以下の通りです。

独自モデル  
GPT-4o、GPT-4o mini、Claude-3.5-Sonnet、Gemini-1.5-Pro、Gemini-1.5-Flash

オープンソースモデル（大規模）  
Qwen2-VL-72B、InternVL2-Llama3-76B、LLaVA-OneVision-72B

オープンソースモデル（中規模）  
Qwen2-VL-7B、Pixtral 12B、InternVL2-8B、Phi-3.5-Vision、MiniCPM-V2.6、LLaVA-OneVision-7B、Llama-3.2-11B、Idefics3-8B-Llama3

評価は以下のように行われました。

1. 各タスクに対して、事前に定義されたプロンプトテンプレートを使用
2. 各モデルに対して、Chain-of-Thought（CoT）プロンプトありとなしの両方で実験が行われた
3. 画像や動画の入力に関しては、各モデルの制約に応じて適切に [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") された

### 主な結果と詳細分析

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_74837_6-1024x498.jpg)

MEGA-BENCHのCoreおよびOpen-endedサブセットにおける各モデルの主要結果。マクロ平均スコアを報告。

（１）フラッグシップモデルの性能

GPT-4oが全体的に最も高い性能を示し、2位のClaude-3.5と明確な差がありました。GPT-4oは多くの応用分野やスキルで優れていましたが、コーディング、数学、計画関連のタスクではClaude-3.5が上回りました。

またオープンソースモデルの中では、Qwen2-VL-72Bが特に高い性能を示しました。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_74837_7-1024x812.jpg)

フラッグシップモデルの4つの次元（入力形式、出力形式、スキル、アプリケーション）での詳細な分析。

（２）効率モデルの性能

Gemini-1.5-flash-002が全体的に最も高い性能を示しました。中でも、科学や評価関連のアプリケーションで優れた結果を出しました。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_74837_8-1024x494.jpg)

効率モデルの入力形式（左）とアプリケーション（右）に関する詳細な分析。

（３）Chain-of-Thought（CoT）の効果

独自モデルでは、CoTプロンプトが効果的に働き、詳細な推論プロセスの生成につながりました。

しかし、ほとんどのオープンソースモデルではCoTプロンプトの効果がほとんど見られませんでした。

（４）モデルごとの特徴

一部のオープンソースモデルは、そのパラメータ数に比べて低いスコアを示しました。

例えば、Llama-3.2-11Bは1ショット例を適切に活用できず、正しい出力形式を理解するのに苦労しました。

### タスクごとのサンプル数に関する分析

MEGA-BENCHの目標の一つは、推論コストを最適化することです。そのため、タスクごとのサンプル数を増やすよりも、タスク数を拡大することを優先しました。

そこで、約15個のサンプルを持つタスクでのベンチマークスコアの頑健性を理解するため、ブートストラップ分析を行いました。

分析の結果、タスクごとのサンプル数が増えるにつれて、モデルスコアの分散が急速に狭まることがわかりました。タスクあたり7個以上のサンプルがあれば、分散の減少の限界効果は小さくなりました。

この結果から、MEGA-BENCHのタスクあたり約15個のサンプル数は、適切なバランスを取れていると言えます。

![](https://ai-data-base.com/wp-content/uploads/2024/10/AIDB_74837_9-1024x496.jpg)

（左）タスクあたりのブートストラップサンプルサイズを段階的に増やした場合のモデルのベンチマークスコア分布。（右）GPT-4o (0513)の255のCoreタスクにおけるタスクごとのエラー分布。

### エラー分析

最先端の視覚言語モデル（VLM）の限界を理解するため、GPT-4o（0513）の結果について詳細なエラー分析を行いました。

分析方法としては、コアセットの255タスクのサブセットを対象に、手動でエラーの種類を特定しました。また、Chain-of-Thought（CoT）設定を使用し、推論プロセスを参考にエラーの種類を判断しました。

エラーの種類と分布は以下の通りでした。

- 推論能力の不足：48.0%
- 知覚エラー：30.3%
- 指示に従う能力の不足：6.3%
- 知識の不足：11.0%
- 回答拒否：2.0%
- [アノテーション](https://ai-data-base.com/archives/26297 "アノテーション") の不具合：2.4%

このことから、GPT-4oにとって、MEGA-BENCHでの主な失敗モードは、様々な推論能力（例：計画/コーディングタスクのための記号的推論、複雑な知覚タスクのための空間的または時間的推論など）の不足であることが分かりました。

## まとめ

本記事では、500以上の実世界タスクを網羅するMEGA-BENCHという新しいマルチモーダル評価ベンチマークを紹介しました。

このベンチマークを使った実験も行われ、GPT-4oの優位性や商用モデルとオープンソースモデルの性能差が明らかになりました。

マルチモーダルモデルの数が増え性能が上がるにつれて、ベンチマークによる評価がさらに重要になりつつあります。このような研究結果をもとに各モデルの特性をさらに理解していけるとよいですね。

- 参照論文URL： [https://arxiv.org/abs/2410.10563](https://arxiv.org/abs/2410.10563)
- プロジェクトページ： [https://tiger-ai-lab.github.io/MEGA-Bench/](https://tiger-ai-lab.github.io/MEGA-Bench/)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[「o1-preview」は自己評価メカニズムを持つ　計画立案中に自分の行動をチェックして修正](https://ai-data-base.com/archives/77179)　

[計画のステップが増えるほど、LLMは最初の目標を見失っていく傾向がある](https://ai-data-base.com/archives/77302)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)