---
title: "DeepSeek R1が実現した教師なし強化学習による推論性能の向上"
source: "https://ai-data-base.com/archives/82540"
author:
  - "[[AIDB Research]]"
published: 2025-01-21
created: 2025-06-13
description: "本記事では、LLMの推論能力を強化学習のみで向上させることに取り組んだ新しい研究を紹介します。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、LLMの推論能力を [強化学習](https://ai-data-base.com/archives/26125 "強化学習") のみで向上させることに取り組んだ新しい研究を紹介します。

大規模言語モデルの推論能力向上には従来、大量の教師あり学習データが必要とされてきましたが、DeepSeek社の研究チームは強化学習のみ、あるいは少量の初期データとの組み合わせで、高い推論性能を実現する手法を開発しました。

結果、OpenAIのo1-1217と同等の性能を持つモデルの開発に成功したようです。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82540-1024x576.jpg)

**発表者情報**

- 研究機関：DeepSeek-AI

## 背景

LLMにおいて特に注目されているのが、モデルの推論能力です。これまでOpenAIのo1シリーズが、Chain-of-Thought（思考の連鎖）推論プロセスの長さを増やすことで、数学や科学的推論などの課題で大きな成果を上げてきました。

しかし、効果的なテストタイム・スケーリング手法は依然として研究コミュニティにとって課題となっていました。テストタイム・スケーリング手法とは、モデルの推論能力をテスト時に調整し、改善するための方法です。

これまで、プロセスベースの報酬モデル（モデルが「どのように考えたか」を重視する、囲碁やチェスのようなゲームAIで広く使われているアプローチ）やモンテカルロ木探索（ランダムなシミュレーションを行って最適な選択肢を見つけるアルゴリズム）、ビーム探索（有望な候補を一定数に絞り込む探索アルゴリズム）といった技術が開発されてきました。

一方で、教師なし強化学習でモデルの推論能力を向上させることができれば、データ収集の手間を大幅に削減できる可能性がありました。強化学習とは、簡単に言うと「何かをやって、その結果が良ければご褒美をもらい、悪ければペナルティを受ける」という仕組みで学ぶ方法です。

そこで今回、研究チームは強化学習のみを用いてモデルの推論能力を向上させる手法の開発に取り組みました。さらに、少量の初期データを組み合わせることで、より強力で汎用的な推論能力を持つモデルの開発を目指しました。結果として、o1に匹敵するモデルを開発することに成功したようです。

さらに訓練の途中では研究者らは、ある現象に出合いました。モデルが **「待って、待って。待って。今、重要なことに気づいた！」** と自発的に口にする”アハ・モーメント”です。

以下でモデルの開発アプローチや評価結果全体を詳しく紹介します。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82540_1.png)

DeepSeek-R1と他モデルのベンチマーク性能を示す棒グラフ。複数のタスクにおけるパフォーマンスが比較されている。

まずは具体的なアプローチと手法について説明します。

## アプローチ

DeepSeek社の研究チームは、モデルの性能向上に2つのアプローチを採用しました。

1つは強化学習のみを用いるDeepSeek-R1-Zero、もう1つは少量の初期データと組み合わせるDeepSeek-R1です。さらに、開発したモデルの能力を小規模なモデルに蒸留する手法も検証されました。

### DeepSeek-R1-Zero（基礎モデルに対する強化学習）

強化学習は、数学や科学的推論などの課題で有効性が示されていました。しかし、従来の研究では教師あり学習データに大きく依存していました。研究チームは教師あり学習を一切使用せず、純粋な強化学習のみでモデルを進化させる手法に挑戦しました。

強化学習アルゴリズムにはGroup Relative Policy Optimization（GRPO）と呼ばれる手法が採用されました。従来の強化学習では、政策モデルと同じサイズの評価モデルが必要でしたが、GRPOではグループスコアから基準値を推定することで、評価モデルを不要としました。具体的には、各質問に対して複数の出力を [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") し、それらの報酬の平均と標準偏差を用いて各出力の相対的な優劣を判断します。

要するに、GRPOとは、「たくさんの選択肢を試して、その結果を平均的に評価することで、最適な行動を見つける方法」です。何か問題に対していくつかの答えを出してみて、それぞれがどれくらい良いか（報酬）を比較します。そして、その結果の平均やばらつき（標準偏差）を使って、どれが一番良さそうかを判断します。この方法では、評価のための特別なモデルを用意する必要がなく、効率的に学べるのが特徴です。

言い換えると、「試してみて、全体の中でどれが良いかを比べながら賢く学ぶ方法」と考えればよいでしょう。

#### 報酬のモデリング

モデルの学習には2種類の報酬が設定されました。

1つは回答の正確さを評価する報酬で、例えば数学の問題では指定された形式で最終的な答えを提供することが求められました。

もう1つは思考プロセスの形式を評価する報酬で、思考過程を特定のタグで囲むことが要求されました。

興味深いことに、 [ニューラルネットワーク](https://ai-data-base.com/archives/26117 "ニューラルネットワーク") ベースの報酬モデルは使用されませんでした。その理由は、大規模な強化学習の過程で報酬のハッキングが発生する可能性があり、また報酬モデルの再訓練に追加のリソースが必要となるためです。

#### 訓練用テンプレート

DeepSeek-R1-Zeroの訓練には、シンプルなテンプレートが使用されました。モデルは、まず推論プロセスを生成し、その後で最終的な回答を提供するように設計されています。

重要な点は、研究チームが意図的に制約を最小限に抑えたことです。反省的な思考や特定の問題解決戦略を強制せず、モデルが強化学習を通じて自然に進化できる環境が整えられました。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82540_2.png)

DeepSeek-R1-Zeroのトレーニングで使用されるテンプレート。推論プロセスと最終回答を含むフォーマットを定義。

評価結果から、DeepSeek-R1-Zeroは強化学習の進行に伴って着実な性能向上を示しました。例えば、数学オリンピックの問題であるAIME 2024でのスコアは、当初の15.6%から71.0%まで向上しました。また、多数決による予測を採用した場合、スコアは86.7%まで上昇し、OpenAI-o1-0912と同等の性能に到達しました。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82540_3.png)

DeepSeek-R1-ZeroとOpenAIモデルの推論タスクでの性能比較。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82540_4.png)

DeepSeek-R1-ZeroのAIME 2024精度のトレーニング中の推移を示す。パフォーマンスの向上を可視化。

#### 進化プロセス

モデルの思考時間は訓練の進行とともに自然に延長されていきました。外部からの調整なしに、モデルは自発的により長い思考プロセスを生成するようになり、数百から数千のトークンを使用して推論を展開するようになりました。また、過去のステップを見直して再評価したり、別のアプローチを探索したりする行動も自然に現れました。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82540_5.png)

トレーニング中のDeepSeek-R1-Zeroの平均応答長を示す。推論能力の進化を表現。

注目すべき点として、「アハモーメント」と呼ばれる現象が観察されました。訓練の途中段階で、モデルは問題に行き詰まった際に自らの思考を見直し、新しいアプローチを試みるような振る舞いを示すようになりました。研究チームにとっても、強化学習の可能性を示す重要な発見となりました。

下記に具体例が挙げられています。モデルは途中で **「待って、待って。待って。今、重要なことに気づいた！」** と自発的に口にしています。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82540_6.png)

DeepSeek-R1-Zeroが「再考」した際の「aha moment」を記述する具体例。RLの成果を強調。

上記の現象は、研究者にとっても驚きと発見の瞬間であり、強化学習が持つ可能性と美しさを象徴するものとして報告されています。

### DeepSeek-R1（初期データを活用した強化学習）

DeepSeek-R1-Zeroには課題も存在しました。読みやすさが低く、複数の言語が混在するなどの問題が見られました。研究チームはこれらの課題に対処するため、次のステップとしてDeepSeek-R1の開発に着手しました。

改善モデルDeepSeek-R1の開発では、2つの重要な問いが検討されました。

1つは「少量の高品質な初期データを用いることで、性能やモデルの収束を改善できるか」という点です。

もう1つは「明確で一貫性のある思考の連鎖（Chain of Thought）を生成しながら、同時に高い汎用性を実現できるか」という点です。

要するに、「最初に用意した少しだけの高品質なデータで、モデルの性能をもっと早く良くできるか？」「ちゃんと筋の通った考え方（思考の流れ）をしっかり作りつつ、いろんな問題に対応できるようにできるか？」といった点が考えられました。  
つまり、「少ないデータで効率よく学んで、賢く、しかも柔軟に問題を解けるようにする方法」を目指したわけです。

研究チームは4段階の訓練パイプラインを設計し、これらの課題に取り組みました。

#### 初期データ（コールドスタート）

DeepSeek-R1では、不安定な初期段階を避けるため、数千件の長い思考連鎖データが収集されました。データ収集には複数のアプローチが採用されました。

- 長い思考連鎖を例として示し、少数のサンプルから学習させる手法
- モデルに詳細な回答と検証を含む解答を直接生成させる手法
- DeepSeek-R1-Zeroの出力を読みやすい形式に整形する手法
- 人間のアノテーターによる後処理

読みやすさを重視し、各回答の最後にサマリーを含める形式が採用されました。また、読者にとって分かりにくい回答は除外されました。研究チームは結果として、DeepSeek-R1-ZeroよりもDeepSeek-R1の方が優れた性能を示すことを確認しました。

#### 推論重視の強化学習

初期データによる微調整の後、DeepSeek-R1-Zeroと同様の大規模な強化学習が適用されました。主に数学、コーディング、科学的推論など、明確な解答のある問題に焦点が当てられました。

言語の一貫性も重要な課題でした。  
強化学習の過程で複数の言語が混在する問題が観察されたため、目標言語の単語の割合に基づく報酬が導入されました。性能は若干低下したものの、人間にとってより理解しやすい出力が実現されました。最終的な報酬は、推論タスクの正確さと言語の一貫性を組み合わせて計算されました。

#### 棄却サンプリングと教師あり微調整

推論を重視した強化学習が収束段階に達した時点で、次の段階のデータ収集が開始されました。初期の段階とは異なり、推論だけでなく、文章作成やロールプレイなど、より広範な能力の向上が目指されました。

推論に関するデータは、強化学習で得られたチェックポイントから棄却 [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") によって生成されました。  
従来の [ルールベース](https://ai-data-base.com/archives/26614 "ルールベース") の評価に加え、DeepSeek-V3を用いた生成的な報酬モデルも導入され、正解と予測の比較が行われました。読みにくい回答や言語が混在する回答は除外され、各プロンプトに対して複数の回答が生成され、正しいものだけが保持されました。約60万件の推論関連データが収集されました。

推論以外のデータについては、DeepSeek-V3のパイプラインとデータセットが再利用されました。文章作成や事実に基づく質問応答、自己認識、翻訳などのタスクでは、必要に応じて思考の連鎖を含む回答が生成されました。一方で「こんにちは」といった単純な質問には、思考の連鎖なしで回答が生成されました。約20万件の非推論データが収集されました。

最終的に、約80万件のデータセットを用いて2 [エポック](https://ai-data-base.com/archives/26594 "エポック") の微調整が実施されました。

#### すべてのシナリオに対する強化学習

人間の好みにより適合させるため、第二段階の強化学習が実施されました。モデルの有用性と安全性を向上させながら、推論能力も継続的に改善することが目標でした。

推論データに関しては、DeepSeek-R1-Zeroと同様の [ルールベース](https://ai-data-base.com/archives/26614 "ルールベース") の報酬が使用されました。一般的なデータに対しては、複雑で微妙なシナリオでの人間の好みを捉えるため、報酬モデルが活用されました。DeepSeek-V3のパイプラインを基に、類似の選好ペアと訓練プロンプトの分布が採用されました。

有用性の評価では、最終的なサマリーのみに焦点が当てられ、ユーザーにとっての回答の有用性と関連性が重視されました。安全性の評価では、推論プロセスとサマリーの両方が評価対象となり、潜在的なリスク、バイアス、有害なコンテンツの特定と軽減が図られました。

報酬シグナルと多様なデータ分布の組み合わせにより、推論能力に優れながら、有用性と安全性も考慮したモデルの訓練が実現されました。

### 小規模モデルへの推論能力の転移

高性能を持つモデル（DeepSeek-R1）をより効率的な小規模モデルに実装するため、研究チームは蒸留手法を採用しました。

QwenやLlamaといったオープンソースモデルに対して、DeepSeek-R1で収集した80万件のサンプルを用いた微調整が行われたのです。

基礎モデルとして、

- Qwen2.5-Math-1.5B
- Qwen2.5-Math-7B
- Qwen2.5-14B
- Qwen2.5-32B
- Llama-3.1-8B
- Llama-3.3-70B-Instruct

が使用されました。Llama-3.3が選ばれた理由は、Llama-3.1と比較してわずかに優れた推論能力を示したためです。

現段階では教師あり学習のみが適用され、強化学習は含まれていません。強化学習を加えることで性能が大幅に向上する可能性はありますが、研究チームは蒸留技術の有効性を実証することに焦点を当てました（他の研究者が探求してくれることも期待しているようです）。

## 実験

開発されたモデルの性能を客観的に評価するため、複数のベンチマークテストが実施されました。以下では、評価方法と結果について説明します。

### 評価ベンチマーク

評価には幅広い分野のベンチマークが使用されました。

- MMLU
- MMLU-Redux
- MMLU-Pro

といった一般的な知識を問うテスト、C-EvalやCMMLUといった中国語の能力を評価するテスト、そして、

- GPQA Diamond
- SimpleQA

といった事実に基づく質問応答能力を測るテストが含まれています。

実用的なスキルの評価として、

- SWE-Bench Verifiedによるソフトウェアエンジニアリング能力の評価
- LiveCodeBenchによるコーディング能力の評価
- Codeforcesによるプログラミングコンテストでの評価

が行われました。

また、数学の能力評価には中国の数学オリンピック（CNMO 2024）やアメリカ数学コンテスト（AIME 2024）が採用されました。

自由回答形式の評価では、GPT-4-Turbo-1106を審査員として活用し、AlpacaEval 2.0とArena-Hardという2つの評価基準で出力の質が判断されました。長さのバイアスを避けるため、最終的なサマリーのみが評価対象とされました。

### 評価方法

標準的なベンチマークでは、simple-evalsフレームワークの評価プロンプトが使用されました。MMLU-Reduxでは、Zero-Evalプロンプト形式がゼロショット設定で採用されました。MMLU-ProやC-Eval、CLUE-WSCでは、元々のフューショット形式からゼロショット形式に修正が加えられました。

コーディング評価では、8つのプログラミング言語、

Python、Java、C++、C#、JavaScript、TypeScript、PHP、Bash

が対象とされ、LiveCodeBenchでは思考の連鎖形式での評価が行われました。Codeforcesの評価には、10回のDiv.2コンテストの問題と専門家が作成したテストケースが使用されました。

### ベースライン比較

DeepSeek-V3、Claude-Sonnet-3.5-1022、GPT-4o-0513、OpenAI-o1-mini、OpenAI-o1-1217といった強力なモデルとの比較が行われました。中国本土からのOpenAI-o1-1217 APIへのアクセスが困難だったため、公式レポートの性能値が参照されました。

### 生成設定

すべてのモデルで最大生成長は32,768トークンに設定されました。 [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") が必要なベンチマークでは、温度パラメータ0.6、top-p値0.95が使用され、各クエリに対して64の応答が生成されてpass@1が推定されました。

### DeepSeek-R1の評価結果

教育関連の知識ベンチマークにおいて、DeepSeek-R1はDeepSeek-V3を上回る性能を示しました。MMLU、MMLU-Pro、GPQA Diamondでの優れた成績は、主にSTEM関連の問題での正確性向上によるものでした。大規模な強化学習によって、科学技術分野での推論能力が大きく向上したと考えられます。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82540_7.png)

DeepSeek-R1と他の代表モデルの性能比較。推論や生成タスクの評価結果を含む。

また、長文の文書理解を要求するFRAMESタスクでも高い性能を示し、文書分析能力の高さが実証されました。事実に基づく質問応答のSimpleQAでも、DeepSeek-V3を上回る成績を収めています。

数学タスクでは、DeepSeek-R1はOpenAI-o1-1217と同等の性能を達成しました。同様に、LiveCodeBenchやCodeforcesといったコーディングアルゴリズムタスクでも、推論重視のモデルが優位性を示しました。ただし、ソフトウェアエンジニアリング関連のタスクでは、OpenAI-o1-1217がAiderで優れた性能を示し、DeepSeek-R1はSWE Verifiedで同等の性能にとどまりました。

### 蒸留モデルの評価結果

単純な知識蒸留だけでも、DeepSeek-R1-7B（Qwen-7Bベース）はGPT-4o-0513を全面的に上回る性能を示しました。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82540_8.png)

DeepSeek-R1の蒸留モデルと他モデルの性能比較。効率的な蒸留技術の成果を示す。

さらに、DeepSeek-R1-14BはQwQ-32B-Previewを、DeepSeek-R1-32BとDeepSeek-R1-70Bはo1-miniを多くのベンチマークで上回りました。

![](https://ai-data-base.com/wp-content/uploads/2025/01/AIDB_82540_9.png)

蒸留モデル（DeepSeek-R1-Distill-Qwen-32B）とRLモデル（DeepSeek-R1-Zero-Qwen-32B）の性能比較。蒸留モデルがRLによるトレーニングを行った小型モデルを全てのベンチマークで上回る結果を示しており、蒸留技術の有効性を強調している。

繰り返しになりますが、強化学習を追加することで更なる性能向上が期待されますが、研究チームは単純な蒸留の有効性を示すため、教師あり学習のみの結果を報告しています。実験結果から、大規模モデルで発見された推論パターンは、より小規模なモデルへ効果的に転移できることが示されました。

## 考察

さまざまな実験結果が示された中で、研究チームは重要な知見と課題を見出しました。以下では、それらについて紹介します。

### 蒸留と強化学習の比較

Qwen-32B-Baseモデルを用いた実験から興味深い発見がありました。大規模な強化学習を10,000ステップ以上実施したDeepSeek-R1-Zero-Qwen-32Bは、QwQ-32B-Previewと同程度の性能を示しました。一方、DeepSeek-R1から知識を蒸留したDeepSeek-R1-Distill-Qwen-32Bは、すべてのベンチマークでより優れた性能を発揮しました。

研究チームはこの結果から2つの重要な結論を導き出しました。

1つ目は、より強力なモデルから小規模モデルへの知識蒸留が、非常に効率的かつ効果的な手法だということです。小規模モデルに対して直接強化学習を適用する場合、莫大な計算リソースが必要にも関わらず蒸留ほどの性能は得られない可能性すらあります。

2つ目は、知的なブレークスルーを得るためには、より強力な基礎モデルと大規模な強化学習が必要かもしれないという点です。

### 失敗から得られた教訓

DeepSeek-R1の開発過程では、いくつかのチャレンジも行われ、残念ながら期待した成果は得られませんでした。しかし重要な教訓が得られています。

プロセス報酬モデル（PRM）は、推論タスクでモデルをより良いアプローチへ導くための手法として理論的には理にかなっていました。しかし、実践では3つの主な課題に直面しました。  
一般的な推論における細かいステップの明示的な定義が困難であること、中間段階の正しさの判断が難しいこと、モデルベースのPRMを導入すると報酬のハッキングが発生しやすく、報酬モデルの再訓練にも追加のリソースが必要となることです。

モンテカルロ木探索（MCTS）も試みられました。AlphaGoの成功にヒントを得たこの手法では、回答を小さな部分に分割し、体系的に解空間を探索することが目指されました。しかし、チェスと異なり、言語生成では探索空間が指数関数的に大きくなります。各 [ノード](https://ai-data-base.com/archives/26470 "ノード") の展開を制限して対処しましたが、局所的な最適解に陥りやすいという問題が生じました。また、探索プロセスを導く価値モデルの訓練自体が困難であることも判明しました。

## まとめ

本記事では、大規模言語モデルの推論能力を強化学習で向上させるDeepSeekの研究を紹介しました。

DeepSeek-R1-Zeroは強化学習のみで、DeepSeek-R1は初期データと組み合わせることで、OpenAI-o1-1217に匹敵する性能を達成しました。

今後は関数呼び出しや複雑なロールプレイといった一般的なタスクへの対応、他言語での言語混在の問題、プロンプトへの感度、ソフトウェアエンジニアリングタスクでの性能向上が課題とされています。

**参照文献情報**

- タイトル：DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning
- URL： [https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek\_R1.pdf](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf)
- GitHub： [https://github.com/deepseek-ai/DeepSeek-R1/tree/main](https://github.com/deepseek-ai/DeepSeek-R1/tree/main)
- 所属：DeepSeek-AI

作成者 [Wordpress Quiz plugin](https://ays-pro.com/wordpress/quiz-maker)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[Web3向けLLMエージェントOS登場 オープンソースの新フレームワーク](https://ai-data-base.com/archives/82344)

[企業環境での自動バグ修復に向けたGoogleの取り組み](https://ai-data-base.com/archives/82409)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)