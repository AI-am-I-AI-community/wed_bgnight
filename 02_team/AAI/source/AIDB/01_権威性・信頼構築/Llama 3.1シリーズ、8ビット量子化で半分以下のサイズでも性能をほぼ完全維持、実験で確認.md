---
title: "Llama 3.1シリーズ、8ビット量子化で半分以下のサイズでも性能をほぼ完全維持、実験で確認"
source: "https://ai-data-base.com/archives/78430"
author:
  - "[[AIDB Research]]"
published: 2024-11-14
created: 2025-06-13
description: "本記事では、LLMの推論コストを削減する「量子化」技術に関する最新の研究成果を紹介します。量子化とは、モデルの重みやアクティベーションのビット幅を削減することで、メモリと計算コストを大幅に削減する手法です。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

本記事では、LLMの推論コストを削減する「量子化」技術に関する最新の研究成果を紹介します。量子化とは、モデルの重みやアクティベーションのビット幅を削減することで、メモリと計算コストを大幅に削減する手法です。

量子化では精度低下が懸念されていますが、適切な手法を選択することで精度をほぼ維持したまま大幅なコスト削減が可能であることが今回示唆されています。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78430-1024x576.jpg)

**参照論文情報**

- タイトル：”Give Me BF16 or Give Me Death”? Accuracy-Performance Trade-Offs in LLM Quantization
- 著者：E [lda](https://ai-data-base.com/archives/26566 "線形判別分析") r Kurtic, Alexandre Marques, Shubhra Pandit, Mark Kurtz, Dan Alistarh
- 所属：Neural Magic, Institute of Science and Technology Austria

**本記事の関連研究**

- [LLMに量子化が与える影響とは？日本語を含む多言語でCohereが調査](https://ai-data-base.com/archives/72292)
- [量子化はLLMの性能にどう影響を与えるか？モデルが持つ「自信」の観点から説明](https://ai-data-base.com/archives/68518)

## 背景

LLMの実行は通常、計算コストが大きくかかってしまう問題があります。この問題を解決するため、これまで研究者たちはさまざまな方法を検討してきました。代表的なものが量子化（モデルの重みやアクティベーションのビット数を減らすこと）で、モデルを使用する際のメモリと計算コストを減らす一般的な手法として注目されています。

量子化で最も重要なのは、圧縮によって速度や使用メモリが改善される代わりにモデルの精度がどれほど落ちてしまうかのバランスをとることです。これまで量子化に関する研究は多く行われてきましたが、モデルをどの程度圧縮すればどのくらいの性能が得られるのか、実用的な指針を示した研究はあまりありませんでした。これは現在最も使用されているオープンソースLLMのひとつであるLlama-3.1モデルにおいても浮き彫りになりました。量子化によって精度が大きく下がるのではと心配されていましたが、実際にユーザーがテストしてみるとほとんど性能が落ちていないことが報告され始めたのです。

こうした背景から、今回研究者らは量子化と精度・パフォーマンスのバランスをに関する実用的な指針を示すことを目指してLlama 3.1モデルにさまざまな量子化をほどこして実験を行いました。

以下ではまず量子化とは何かといった内容をおさらいし、今回の実験内容と実験結果、そして得られた知見を紹介します。

## 量子化とは何か

元のLLMでは、パラメータの値を表現するために、高精度な「16ビット」の浮動小数点形式（BF16）が使用されています。コンピュータ内部では、すべての数値は2進数（0と1の組み合わせ）で表現されます。この2進数の桁数を「ビット」と呼び、ビット数が多いほどより多くの異なる数値を表現できます。

これを8ビットや4ビットに「量子化」することで、精度を部分的に犠牲にする代わりに、必要なメモリ容量を大きく削減できます。

例えば4ビットでは、0000から1111までの16通りの組み合わせが可能となり、つまり16個の異なる数値しか表現できません。一方、8ビットでは256通りの組み合わせが可能で、より細かな数値表現が可能になります。16ビットではさらに65,536通りの表現が可能です。

つまりLLM内部の表現をより大雑把にすることでモデルサイズを小さくし、計算を効率化できるわけです。しかし、ただ単純に数値の表現精度を落とすだけでは、モデルの性能が大きく低下してしまう可能性があります。

これまでの研究では、3ビット以下の圧縮率のきわめて高い手法も提案されていますが、実用的な計算の仕組みが整っていないため、実際の使用には適していません。

### 量子化の種類と手法について

量子化には大きく分けて「整数（INT）」と「浮動小数点（FP）」の2種類があります。整数は普通の整数値のみを扱い、浮動小数点は小数点を含む数値を扱えます。それぞれに一長一短があり、例えば整数は計算が高速ですが、表現できる数値の範囲が限られます。一方、浮動小数点は幅広い数値を表現できますが、計算にやや時間がかかります。

最近注目されている量子化手法であるGPTQは「Generative Pre-trained [Transformer](https://ai-data-base.com/archives/26535 "Transformer") Quantization」の略で、LLMの量子化のために特別に開発された手法です。この手法の特徴は、モデルの重要なパラメータをより正確に保持しながら、それ以外の部分を効率的に圧縮できる点にあります。、実際のデータを使って最適な量子化方法を学習することで、単純な四捨五入よりもずっと良い精度を実現しています。

異なる量子化手法を組み合わせることで、用途や要求される精度に応じて、最適なトレードオフを選択できるようになっています。例えば高精度が必要な場合はW8A8-FPを、より大きな圧縮率が必要な場合はW4A16-INTを選択するといった具合です。

## 今回の実験設計について

### データセットの種類

LLMの量子化が精度に与える影響を包括的に評価するために、3つの異なる評価カテゴリが採用されました。

まずはアカデミックベンチマークとして [Open LLM Leaderboard](https://huggingface.co/open-llm-leaderboard) V1とV2が使用されています。質問応答や推論などの構造化されたタスクに焦点を当てており、一貫性のある再現可能な評価を実現します。ただし、実世界の使用シナリオとは異なる面があることも考慮する必要があります。

つぎに実世界ベンチマークでは、 [Arena-Hard-Auto-v0.1](https://huggingface.co/datasets/lmarena-ai/arena-hard-auto-v0.1) 、 [HumanEval](https://huggingface.co/datasets/openai/openai_humaneval) 、 [HumanEval+](https://huggingface.co/datasets/evalplus/humanevalplus) が採用されました。チャットやコード生成など、実際の利用シーンに近い形での評価を可能にするものです。より多様な入力と出力のバリエーションを扱うことができ、人間の評価基準との高い相関も確認されています。

なお、テキスト類似度分析では、量子化前後のモデル出力の類似度が比較されます。ROUGE、BERTScore、意味的類似度などの指標を用いて、生成されたテキストの構造と意味がどの程度保持されているかが評価されます。

### モデルとフォーマット

主に3種類の量子化フォーマットが評価対象として選ばれました。

まずW8A8-FPといって、重みと活性化値を8ビットの浮動小数点形式に変換する方法です。シンプルな丸め処理で実装でき、キャリブレーションデータが不要という利点があります。

次にW8A8-INTという、重みと活性化値を8ビット整数に変換するアプローチです。GPTQとSmoothQuantの組み合わせを使用し、精度を保つためにキャリブレーションデータが必要とされます。

3つ目はW4A16-INTで、重みを4ビット整数に圧縮し、活性化値は16ビットを維持します。GPTQを使用して128個の要素をグループ化して処理し、高品質なキャリブレーションデータが精度維持に重要な役割を果たします。

なお8B、70B、405Bパラメータを持つLlama 3.1モデルが評価され、性能と精度のトレードオフが分析されました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78430_1-1024x216.png)

## 量子化と精度の関係についての実験結果

様々な量子化手法がモデルの精度にどのような影響を与えるのか、包括的な評価が行われました。以下、アカデミックベンチマークと実世界ベンチマークそれぞれを通じて得られた知見です。

### アカデミックベンチマークでの評価

学術的なベンチマークOpen LLM Leaderboardでは、Llama-3.1モデルに対して以下のように評価されました。

まず、MMLUとARC-Challengeのタスクでは、通常の確率ベースの評価ではなく、モデルがテキストとして正解を生成する形式で評価されました。また、GSM8kでは少数のサンプルを見せる標準的な方法ではなく、思考の連鎖（Chain-of-Thought）による評価が採用されました。

評価の結果、全ての量子化方式において、元のモデル（BF16形式）の精度の99%程度が維持されることが分かりました。8ビット量子化では平均して99.75%の精度が保たれ、4ビット量子化でも99.36%という高い精度が達成されています。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78430_2-1024x338.png)

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78430_4.png)

Open LLM Leaderboard V1ベンチマークにおける量子化Llama-3.1モデルの平均スコア

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78430_5.png)

Open LLM Leaderboard V2ベンチマークにおける量子化モデルの平均スコア

### 実世界ベンチマークでの評価

実際の使用場面により近い評価として、前述の通りArena-Hard-Auto-v0.1、HumanEval、HumanEval+の3つのベンチマークが使用されました。チャット対話やコード生成など、より複雑で多様な応答が必要とされるタスクです。

評価の結果、量子化されたモデルは元のモデルと非常に近い性能を示しました。8ビットモデルは99.9%の精度を維持し、4ビットモデルでも98.9%という高い精度が確認されています。実用的な場面でも量子化による性能低下が最小限に抑えられることを示しています。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78430_3-1024x371.png)

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78430_6.png)

Arena-Hard-Auto-v0.1における量子化モデルの平均スコア

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78430_7.png)

HumanEvalのpass@1スコア

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78430_8.png)

HumanEval+のpass@1スコア

### テキストの類似性分析

量子化されたモデルが生成するテキストが、元のモデルの出力とどれだけ似ているかも詳しく分析されました。大規模なモデル（70Bと405B）では、単語の選択や文章構造が良く保持され、ROUGE-1スコアで0.7、ROUGE-Lスコアで0.56という高い類似度が示されました。

さらに、BERTScoreで0.93、意味的類似度で0.96という結果は、生成されるテキストの意味内容も非常によく保持されていることを示しています。小規模なモデル（8B）では若干の違いが見られましたが、それでも意味的な一貫性は十分に保たれていました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78430_9-1024x263.png)

量子化モデルと非量子化モデルの出力のテキスト類似性メトリクス比較

全体を通して、適切に実装された量子化は、モデルの性能をほとんど損なうことなく、効率的な実行を可能にすることが確認されました。

ただし今回検証されたのはLlama 3.1モデルのみであるため、他のモデルではどのような結果になるのかはそれぞれ異なるはずです。またLlama 3.1の微調整モデル（日本語対応版など）に関しても結果が変わる可能性があります。

## 量子化と推論性能についての実験結果

LLMの推論（実行）は、大きく2つの段階に分けられます。1つ目は「プレフィル」段階で、入力された文章全体を一度に処理します。2つ目は「デコード」段階で、新しい文章を1トークン（単語や文字）ずつ生成していきます。通常、プレフィルは計算能力が、デコードはメモリアクセスが主なボトルネックとなります。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78430_10.png)

量子化モデルの性能評価に使用したユースケース一覧

### 量子化が推論性能に与える影響

重みの量子化（パラメータの圧縮）は、主にデコード段階でのメモリ読み込み時間を短縮する効果があります。一方、重みと活性化値の両方を量子化すると、行列計算をより少ないビット数で実行できるため、プレフィル段階での計算速度が向上します。

そのため、最適な量子化方式は、プレフィルとデコードの処理時間の比率によって変わってきます。また、量子化には以下のような追加的な利点もあります。

- より多くの同時実行が可能になる
- より安価な [GPU](https://ai-data-base.com/archives/26570 "GPU") での実行が可能になる
- メモリ制約のある環境でも動作可能になる

### 実践的な性能評価

研究では、3つの量子化方式（W8A8-FP、W8A8-INT、W4A16-INT）を、3種類の [GPU](https://ai-data-base.com/archives/26570 "GPU") （A6000、A100、H100）上で評価しました。評価は6つの低レイテンシーユースケースと2つのスループット重視のユースケースで行われました。

低レイテンシー（遅延）のユースケースでは、同期実行と非同期実行の両方が評価され、特に初めのトークンまでの時間が1秒未満、トークン間の遅延が0.1秒未満という厳しい制約が設けられました。

### 同期実行での性能

同期実行（一度に1つのリクエストのみを処理）では、W4A16-INT量子化（4ビット量子化）が最も高い性能向上を示しました。8Bと70Bのモデルでは、元の形式と比べて以下の改善が見られました。

- クエリあたりのコストが2-3倍削減
- レイテンシーが1.5-2倍改善

つまり量子化によって明らかに動作が軽くなっています。

405Bモデルでは、W4A16-INTを使用することで、

- クエリあたりのコストが6-7倍削減
- 必要な [GPU](https://ai-data-base.com/archives/26570 "GPU") 数を16から4に削減可能
- [GPU](https://ai-data-base.com/archives/26570 "GPU") 間通信の削減によるレイテンシーの改善

となり効果がより大きいです。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78430_11-1024x567.png)

同期推論における性能とコスト効率の比較

### 非同期実行での性能

非同期実行（複数のリクエストを同時に処理）では、状況によって最適な量子化方式が異なることが分かりました。

- 中・大規模モデルの高性能 [GPU](https://ai-data-base.com/archives/26570 "GPU") 実行ではW8A8-INT/FPが最適
- 小規模モデルの中性能 [GPU](https://ai-data-base.com/archives/26570 "GPU") 実行ではW4A16-INTが最適
- H100 [GPU](https://ai-data-base.com/archives/26570 "GPU") 上での実行ではW8A8-FPが最も効率的

要するに、モデルサイズ、 [GPU](https://ai-data-base.com/archives/26570 "GPU") の種類、実行形態によって最適な量子化方式は異なりますが、いずれの場合も量子化によって大幅なコスト削減が可能であることが示されました。8B/70Bモデルで1.5-3倍、405Bモデルで5-8倍のコスト削減が達成されています。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78430_12-1024x571.png)

非同期推論における性能とコスト効率の比較

## まとめ

本記事では、LLMにおける量子化技術の精度、性能、コストのトレードオフを包括的に調査した研究を紹介しました。研究チームは、効率的な計算サポートを持つすべての量子化フォーマット、様々な量子化アルゴリズム、実際のデプロイケース、複数の [GPU](https://ai-data-base.com/archives/26570 "GPU") タイプにわたって詳細な評価を実施しました。

結果として、適切なアルゴリズムとパラメータを選択することで、これまで考えられていたよりも高い精度を維持しながら、推論性能を大幅に向上させ、コストを削減できることが明らかになりました。

![](https://ai-data-base.com/wp-content/uploads/2024/11/AIDB_78430_13.png)

アカデミックベンチマークにおける精度維持率の傾向

ただし、最適な量子化フォーマットの選択はタスクやアルゴリズムに依存する場合があります。

- 参照論文URL： [https://arxiv.org/abs/2411.02355](https://arxiv.org/abs/2411.02355)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMにおける長文処理能力の進化を調査 Claude 3.5は情報の流れを追跡するスキルに長ける](https://ai-data-base.com/archives/78379)

[画像も文字も表も全部まとめて理解するRAGシステムの提案　Bloombergなど](https://ai-data-base.com/archives/78490)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)