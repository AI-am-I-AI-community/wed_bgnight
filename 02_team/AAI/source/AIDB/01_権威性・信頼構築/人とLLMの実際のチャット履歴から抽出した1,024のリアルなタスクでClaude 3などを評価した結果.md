---
title: "人とLLMの実際のチャット履歴から抽出した1,024のリアルなタスクでClaude 3などを評価した結果"
source: "https://ai-data-base.com/archives/70812"
author:
  - "[[AIDB Research]]"
published: 2024-06-13
created: 2025-06-13
description: "現実のユーザーからの難しいクエリを使用してLLMの性能を評価する自動評価フレームワークが開発されました。100万以上の人間とチャットボットの会話ログから厳選された1,024のタスクを使用するというユニークな取り組みです。"
tags:
  - "clippings"
---
**【お知らせ】** AIDB主催のビジネスマッチングイベントを7月25日(金)開催予定です！  
  

\---以下、記事本文---

現実のユーザーからの難しいクエリを使用してLLMの性能を評価する自動評価フレームワークが開発されました。100万以上の人間とチャットボットの会話ログから厳選された1,024のタスクを使用するというユニークな取り組みです。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70812-1024x576.jpg)

**参照論文情報**

- タイトル：WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild
- 著者：Bill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Faeze Brahman, Abhilasha Ravichander, Valentina Pyatkin, Nouha Dziri, Ronan Le Bras, Yejin Choi
- 所属：Allen Institute for AI, University of Washington

## 背景

LLMの性能を適切に評価することは容易ではありません。自動化することも難しいですが、リアルなニーズ（実際にユーザーが投げる多様なタスク）におけるLLMの性能を測定することは難しいとされてきました。

人間の評価者を用いるChatbot Arenaのようなプラットフォームは有益ではありますが、機能は限定的といえば限定的です。

参考： [あらゆるLLMを「使い心地」基準でバトルさせる便利なプラットフォーム『Chatbot Arena：チャットボットアリーナ』](https://ai-data-base.com/archives/61080)

また既存の自動評価ベンチマークは、タスクの多様性や難易度の分布で課題が残っています。

こうした背景から、今回研究者らは実際のユーザーからの質問を用いてLLMを評価するための新たなベンチマーク「WILDBENCH」の構築に至りました。  
100万件以上の実際のユーザーとチャットボットの対話データから注意深く選択された1,024のタスクで構成されています。タスクは定期的にアップデートされ、LLMの進化に合わせてベンチマークの内容も進化していくとされています。

以下ではWILDBENCHによって実験された各モデルの評価結果などを中心に、研究報告を掘り下げていきます。

参考までに、実験に使用されたモデルを先に並べます。

GPT-4-Turbo-0409  
Claude 3 Opus  
Llama-3-70B-Inst  
Llama-3-8B-Inst  
Llama-3-8B-Inst-SimPO  
Yi-1.5-34B-chat

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70812_1-1024x402.jpg)

WILDBENCHの評価フレームワーク

## WILDBENCHはいかにして作られたか

### WildChatから収集

評価に用いるタスクは、100万件以上の実際のユーザーとチャットボットの会話データを含むWildChatデータセットから収集されました。WildChatには、文章作成、プログラミング、数学、データ分析、ロールプレイ、計画立案など、ユーザーがLLMに期待する多様なタスクが含まれているため、評価ベンチマークの素材として特に適していると考えられました。

参考： [ChatGPTと実際に交わされた会話の世界最大規模データセット「WildChat」](https://ai-data-base.com/archives/67317)

### フィルタリング

選択されるタスクの品質と多様性を管理するために、下記のフィルタリング手順が適用されました。

1. 極端に短いクエリ（10トークン未満）や長すぎるクエリ（3,000トークン以上）が除外されました。
2. 5ターン以上のユーザーとチャットボットのやり取りを含む会話も、焦点とコヒーレンスを維持するために除外されました。
3. 英語のデータに焦点を当て、英語以外のタスクがフィルタリングされました。
4. 有害なコンテンツを含む会話も除外されました。
5. タスクの多様性を確保するために、SentenceBERTから得られた文章埋め込みを用いてクエリ間のコサイン類似度が計算され、高い類似度スコア（0.9以上）を持つクエリが破棄されました。
6. タスクの多様性をさらに高めるために、ユニークなデバイスごとに最後の会話のみが保持され、同じユーザーからの類似のタスクが除去されました。

### 難易度でアノテーション

LLMのパフォーマンスを区別できる難易度の高いタスクを特定するために、GPT-4-Turbo、Claude-3-Sonnet、Opusが用いられ、各タスクに必要な背景知識と推論能力が分析されました。

そして次の5段階の難易度評価を割り当てました。

1. 非常に簡単
2. 簡単
3. 中程度
4. 難しい
5. 非常に難しい

難易度評価の結果、全てのモデルが「非常に簡単」または「簡単」と評価したタスクは除外されました。残ったタスクプールから1,500のタスクがランダムに [サンプリング](https://ai-data-base.com/archives/26518 "サンプリング") され、元のデータセットのタスクカテゴリの分布と類似するように調整されました。

### 人手によるアノテーション

選択されたタスクの品質をさらに向上させるために、人手による [アノテーション](https://ai-data-base.com/archives/26297 "アノテーション") が用いられました。まず、GPT-4-Turboを用いて、各クエリの意図が要約されました。そして要約がレビューされ、意味不明なタスクが取り除かれました。最終的に、1,024のタスクがWILDBENCHに残されました。（2の10乗なのは単なる偶然です）

### 備考

WILDBENCHは、定期的に更新される動的なベンチマークとして設計されています。実際、すでに2つのバージョンがリリースされています（2024年3月のV1と2024年5月のV2）。

新しいバージョンがリリースされるたびに、同じような選定基準でタスクが選ばれますが、その時点での最新のWildChatデータが使用されます。

ただし、WILDBENCHのタスクに使われたデータが、将来的にWildChatの公開データに含まれてしまうと、そのデータを学習に使ったLLMが不当に有利になってしまう可能性があります。

そこで、WILDBENCHのタスクとして選ばれたデータが、WildChatの公開データに含まれないように、WildChatチームと協力して調整が行われています。するとWILDBENCHのタスクを事前に学習することができないため、全てのLLMに対して公平な評価が可能になります。

### WILDBENCHの統計的特徴

#### 実際のユーザーケースに基づいてデータ収集している

下記の表は、WILDBENCHの統計を既存のベンチマークであるAlpacaEval、MT-Bench、ArenaHardと比較しています。この中で、ArenaHardとWILDBENCHのみが、専門家によって選定されたり、クラウドソーシングによって収集されたりしたデータではなく、実際のユーザークエリ（「RealUser」）から収集されたデータを使用しています。

ArenaHardとWILDBENCHの違いは、WILDBENCHのデータ分布が実際のユーザーのタスクカテゴリに沿っているのに対し、ArenaHardはプログラミングとデバッグに過度に焦点を当てている点です。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70812_2-1024x170.png)

LLMアラインメントベンチマークの統計比較

#### 長いコンテキストのタスクが含まれている

WILDBENCHには、最大4ターンの会話履歴が含まれており、20％以上の会話が2ターン以上になっています。さらに、下の図に示すように、長いクエリ長を持っています。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70812_3-1024x220.png)

AlpacaEval、ArenaHard、WildBenchにおけるクエリ長の分布

#### タスクカテゴリ

LLMの多様なタスクにおける能力を細かく分析するために、タスクの意図 [アノテーション](https://ai-data-base.com/archives/26297 "アノテーション") に基づいて、タスクが12のカテゴリに分類されました。分布は下の図に示されています。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70812_4-1024x266.jpg)

図では、AlpacaEvalとArenaHardとの比較も行われています。特筆すべきは、WILDBENCHがAlpacaEvalとArenaHardに比べてよりバランスが取れていることです。AlpacaEvalとArenaHardでは、それぞれ50％以上のタスクが「情報探索」と「プログラミングとデバッグ」のカテゴリに集中しています。

## LLMの評価プロセス

### インスタンス固有のチェックリスト

LLMを評価者として用いる際の課題として、評価の曖昧さや解釈性の低さが挙げられます。そこでWILDBENCHでは、各テストクエリに対してチェックリストが生成されました。5〜10の質問で構成され、解釈可能で検証しやすいものとなっています。

チェックリストの作成には、GPT-4とClaude-3-Opusの出力が組み合わされ、単一のLLMによるバイアスが軽減されました。チェックリストは人手でレビューされ、LLMの評価者へのプロンプトの一部として使用されます。

### 新たな指標「WB-Reward」

2つのLLMの性能を比較するための指標WB-Rewardが作られました。ペアワイズ評価とは、2つのものをペア（組）にして比較する方法のことを指します。

WB-Rewardでは、GPT-4が評価者の役割を担います。GPT-4は、あらかじめ用意されたチェックリストを使って、2つのLLMの応答を比べます。どちらのLLMの応答がより優れているかを判断し、より良い応答をしたLLMに高い点数を与えます。ちなみにこの方法は、人間が行う評価に近いやり方だと考えられています。

評価の手順は以下のようになります：

1. GPT-4が、ユーザーの質問と会話の流れを分析します。
2. 2つのLLMの応答を比較し、どちらがより良いかを詳しく分析します。
3. 最終的にどちらのLLMの応答が優れているかを判断し、その理由を説明します。

判断結果に基づいて、以下のように点数（報酬）が付けられます：

- LLM Aの応答がLLM Bよりもはるかに優れている：+1点
- LLM Aの応答がLLM Bよりもわずかに優れている：+0.5点
- LLM AとLLM Bの応答が同等：0点
- LLM Bの応答がLLM Aよりもわずかに優れている：-0.5点
- LLM Bの応答がLLM Aよりもはるかに優れている：-1点

なお、1つのLLMだけを基準にして比較すると、偏った評価になってしまう恐れがあるため、GPT-4-Turbo-0429、Claude-3-Haiku、Llama-2-70B-chatの3つのLLMを基準として使っています。WB-Reward (Mix)は、この3つのLLMを基準にした点数の平均値です。

また、応答の長さが長いからといって高く評価されるのは適切ではありません。そこで、応答の長さによる影響を減らすために、勝った方の応答が負けた方よりもK文字以上長い場合、「わずかに勝利/敗北」を「引き分け」に変更するルールが提案されました。Kの値は、ユーザーが自由に設定できます。

### WB-Scoreによる個別評価

ペアワイズ評価は、LLM間の直接的な比較を提供しますが、通常、個々のLLMの生成物を採点するよりもコストと時間がかかります。そこで、各モデルの性能を個別に評価するために、GPT-4に1から10までのスコアを割り当てるよう指示される仕組みが考案されました。

スコアの定義は以下の通りです:

- スコア1〜2: 応答が非常に貧弱で、全く意味をなさない。
- スコア3〜4: 応答が貧弱で、ユーザーの問題解決に意味のある助けとならない。
- スコア5〜6: 応答は普通だが、いくつかの問題がある（事実の誤り、幻覚、重要な情報の欠落など）。
- スコア7〜8: 応答は十分良いが、いくつかの点で改善の余地がある。
- スコア9〜10: 応答は完璧で、問題解決に役立つ情報を提供している。

WILDBENCHにおけるスコアは、テストされた全ての例のスコアの平均値として計算されます。各スコアは、まず5を引いてから2倍されます。スコア5は、ギリギリ許容できる応答を表すため、このスケーリングにより、タスクを効果的に解決できるモデルの性能をより良く区別できます。

## 各モデルの性能分析結果

いよいよ実験結果です。WILDBENCHを用いて、様々なLLMの性能が分析されました。

なおリーダーボードの特徴を先に述べます。WILDBENCHのリーダーボードは、最新の結果や、長さのペナルティのカスタマイズ、各モデルのタスク別の詳細なパフォーマンスの表示など、インタラクティブな機能が提供されています。また、データの探索やモデル出力の並列比較により、各モデルの長所と短所を理解することができます。HuggingFaceで実際に見ることができます。

結果の概要は以下の通りです。

3つの性能レベルの異なるベースラインモデル（GPT-4-Turbo、Claude 3 Haiku、Llama-2-70B-chat）を使用することで、テストされたモデルは自然と3つの層にグループ化されることが観察されました。Tier 1のモデルはClaude 3 Haikuを上回り、Tier 2のモデルはLlama-2-70B-chatを上回るがClaude 3 Haikuを下回り、Tier 3のモデルはLlama-2-70B-chatを下回るという結果になりました。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70812_5-1024x374.png)

### モデル間のギャップはどこにあるのか？

WILDBENCHリーダーボードの特徴の1つは、異なるタスクカテゴリにわたってモデルを比較できることです。そのため各モデルの異なるタイプのタスクに対する長所と短所を特定することができます。

分析の結果、GPT-4-Turbo-0409やClaude 3 Opusなどの大規模モデルは、すべてのタスクカテゴリで良好なパフォーマンスを示しましたが、Llama-3-8B-InstやYi-1.5-34B-chatなどのオープンなLLMは、コーディングや数学関連のタスクで弱いパフォーマンスを示しました。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70812_7.jpg)

### 8BのモデルがM70Bのモデルを上回る？

AlpacaEval-2.0のリーダーボードでは、Llama-3-8B-Inst-SimPO（LC=44.7%）がLlama-3-70B-Inst（LC=34.4%）を大幅に上回っていますが、これは驚くべき結果であり、WILDBENCHの結果とは異なります。

WILDBENCHの評価結果によると、Llama-3-8B-Inst-SimPOというモデルは、Yi-34B-chatやLlama-3-70B-Instといった他のモデルよりも全体的には性能が劣っていました。

しかし、情報を探すタスクやクリエイティブなタスクに限っていえば、Llama-3-8B-Inst-SimPOはLlama-3-70B-Instと同じくらいの性能を発揮していました。

一方、AlpacaEvalでは、Llama-3-70B-Instの性能がLlama-3-8B-Inst-SimPOよりも低く評価されていました。しかし、これはAlpacaEvalが評価に使うタスクの選び方に偏りがあったり、評価の方法自体に問題があったりしたせいだと考えられます。

つまり、AlpacaEvalの結果は、Llama-3-70B-Instの真の性能を過小評価していた可能性があるということです。

Llama-3-8B-Inst-SimPOは、AlpacaEval-2.0では他のモデルほど高い評価を得られませんでしたが、WILDBENCHの評価では8Bモデル（パラメータ数が約80億のモデル）の中では最高の性能を示し、いくつかの大規模モデルよりも優れていました。

面白いことに、Llama-3-8B-Inst-SimPOは、タスクのカテゴリに関係なく、Llama-3-8B-Instよりも常に高い性能を発揮していました。レーダーチャートで見ると、この2つのモデルの性能分布は似たような形になっていました。

### 長い応答は常に良いのか？

WILDBENCHは、長さのバイアスに対して [ロバスト](https://ai-data-base.com/archives/26590 "ロバスト") です。例えば、Llama-2-70B-chatとLlama-3-70B-Instは出力の長さが同程度ですが、Llama-3-70B-Instはリーダーボードで5位にランクインしているのに対し、Llama-2-70B-chatは33位にランクインしています。

また、Yi-1.5-6Bの出力の長さは40モデル中4番目に長いにもかかわらず、リーダーボードでは29位にランクインしています。

このことから、WILDBENCHの評価では、応答の品質が最も重要な要素であり、長い応答に偏ることはないと言えます。さらに、長さのペナルティを用いることで、応答の長さと品質のトレードオフをユーザーのニーズに合わせて調整できるようになっています。

### 人間の判断との相関

WILDBENCHの評価が人間の判断とどの程度相関しているかを分析するために、大規模なオンライン人間評価によって生成されたChatbotArena Eloレーティングとの比較が行われました。

WB-RewardとWB-Scoreの両方が、人間ベースのEloレーティングと強い相関を示し、特にパフォーマンスの高いモデルにおいて、他の自動評価指標を上回る最高の相関を示しました。

![](https://ai-data-base.com/wp-content/uploads/2024/06/AIDB_70812_8-1024x263.png)

アラインメントベンチマークとChatbot ArenaEloの相関

## 注意点

WILDBENCHは現在のバージョンで英語に焦点を当てているため、他の言語でのLLMの評価が除外されています。

また、テストデータをAPIを通じて送信する必要があり、将来的にテストデータがモデルの学習データに組み込まれるリスクがあります。このリスクを軽減するため、ベンチマークデータを継続的に更新することが計画されています。

さらに、WILDBENCHで使用されるデータはWildChatデータセットから収集され、そのユーザーの人口統計学的分布を反映しています。このため、ユーザーベースのバイアスがWILDBENCHにも引き継がれています。

最後に、マルチターンクエリでは、過去のターンの応答はGPT-4によって生成されます。評価対象のモデルによって生成された場合のユーザーの応答を予測することができないためです。

以上の注意点を考慮して活用することが期待されます。

## 結論と今後の方向性

下記にまとめます。

- WILDBENCHデータは自然なタスク分布を持つ実世界のユーザークエリで構成されている
- CoTのようなLLMを判定者として用いる方法が導入され、評価の解釈性が向上し、曖昧さが減少している
- LLMを判定者とする評価における長さのバイアスを軽減するための手法が取り入れられた
- 実験により、WB-RewardとWB-Scoreが人間の判断と非常に強い相関を示し、既存の指標を上回ることが示された
- 40のLLMのWILDBENCHにおける性能が示され、タスクカテゴリ別のスコアの詳細な内訳が出ている
- WILDBENCHのリーダーボードは既に2万回以上閲覧されており、今後も新しいLLMを継続的に評価するために積極的にメンテナンスされる予定

## まとめ

本記事では、実際のユーザークエリを用いてLLMを評価するためのベンチマーク「WILDBENCH」に関する研究を紹介しました。

WILDBENCHは、実世界のユーザークエリの自然なタスク分布を反映した1,024のタスクで構成され、今後も定期的に更新される予定です。LLMを評価者として用いる際の解釈性を高め、曖昧さを減らすためのチェックリストと、評価の長さによるバイアスを軽減する手法が導入されました。

実験により、WILDBENCHの主要な指標と人間の判断との強い相関が示されています。また現時点での各種LLMの評価結果も興味深いものとなっています。

- 参照論文URL： [https://arxiv.org/abs/2406.04770](https://arxiv.org/abs/2406.04770)
- [https://hf.co/spaces/allenai/WildBench](https://hf.co/spaces/allenai/WildBench)

**■サポートのお願い  
**AIDBを便利だと思っていただけた方に、任意の金額でサポートしていただけますと幸いです。  

  

[LLMは与えられたペルソナ（役割）に応じてバイアスが変化することが明らかに](https://ai-data-base.com/archives/70696)

[包括的なRAG評価ベンチマーク『CRAG』Metaなどが開発](https://ai-data-base.com/archives/70850)

 [![](https://ai-data-base.com/wp-content/themes/innovate_hack_tcd025/img/footer/return_top.png) PAGE TOP](https://ai-data-base.com/archives/#header_top)